# Lab 6：注意力机制

- [x] 分析点积缩放模型可以缓解softmax函数梯度消失的原因
- [x] 实现seq2seq模型，进行输入序列逆置

---

- [实验要求](https://github.com/Charles-T-T/DeepLearning/blob/master/labs/lab6/docs/题目说明.pdf)
- [实验报告](https://github.com/Charles-T-T/DeepLearning/blob/master/labs/lab6/docs/report.pdf)
- [实验代码](https://github.com/Charles-T-T/DeepLearning/tree/master/labs/lab6/src) 