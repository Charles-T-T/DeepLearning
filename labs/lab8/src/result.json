[
    {
        "title": "Proving Test Set Contamination in Black-Box Language Models",
        "abstract": "Large language models are trained on vast amounts of internet data, prompting concerns that they have memorized public benchmarks. Detecting this type of contamination is challenging because the pretraining data used by proprietary models are often not publicly accessible.\n\nWe propose a procedure for detecting test set contamination of language models with exact false positive guarantees and without access to pretraining data or model weights. Our approach leverages the fact that when there is no data contamination, all orderings of an exchangeable benchmark should be equally likely. In contrast, the tendency for language models to memorize example order means that a contaminated language model will find certain canonical orderings to be much more likely than others. Our test flags potential contamination whenever the likelihood of a canonically ordered benchmark dataset is significantly higher than the likelihood after shuffling the examples.\n\nWe demonstrate that our procedure is sensitive enough to reliably detect contamination in challenging situations, including models as small as 1.4 billion parameters, on small test sets only 1000 examples, and datasets that appear only a few times in the pretraining corpus. Finally, we evaluate LLaMA-2 to apply our test in a realistic setting and find our results to be consistent with existing contamination evaluations.",
        "authors": "Y. Oren, N. Meister, N. S. Chatterji, et.al",
        "keywords": [
            "language modeling",
            "memorization",
            "dataset contamination"
        ],
        "venue": "ICLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=KS8mIvetg2",
        "pdf_src": "https://api2.openreview.net/pdf/cfd79aaab7bdcd4f7c032c57fe7e607058042c80.pdf",
        "Code_src": "",
        "Introduction": "Background: Large language models like GPT3 have been trained using massive amounts of web scraped text which has led to worries about them having memorized these texts leading to \"contamination\". However due to privacy reasons such pre-training data isn't always available making it difficult to check if a model's performance could actually stem from memorization rather than learning.\n\nResearch Question: How can one accurately identify whether a large language model exhibits signs of contamination while also being unable to directly inspect its training data?\n\nMethod: The authors suggest leveraging the property known as exchangeability - where different permutations of the same data points lead to equivalent outcomes during testing time. They argue since uncontaminated models would treat any permutation identically; however, those exposed to specific patterns may disproportionately favor some orders over others thus revealing their exposure through statistical anomalies detected via likelihood ratios between shuffled vs non-shuffled datasets.\n\nMain Contributions:\n1. A novel method that provides exact false-positive guarantees.\n2. This technique does not require direct knowledge of either the pre-trained data nor the model weights themselves – something previously unavailable information needed before.\n3. Demonstrates effectiveness across various scenarios even under resource constraints e.g., smaller parameter counts/models or tiny datasets encountered infrequently within pre-training corpora).\n4. Validation against another state-of-the-art model suggests consistency compared to other methods assessing contamination levels suggesting reliability beyond just theoretical proof.",
        "Topic": "Large Language Models"
    },
    {
        "title": "Growing Tiny Networks: Spotting Expressivity Bottlenecks and Fixing Them Optimally",
        "abstract": "Machine learning tasks are generally formulated as optimization problems, where one searches for an optimal function within a certain functional space. In practice, parameterized functional spaces are considered, in order to be able to perform gradient descent. Typically, a neural network architecture is chosen and fixed, and its parameters (connection weights) are optimized, yielding an architecture-dependent result. This way of proceeding however forces the evolution of the function during training to lie within the realm of what is expressible with the chosen architecture, and prevents any optimization across architectures. Costly architectural hyper-parameter optimization is often performed to compensate for this. Instead, we propose to adapt the architecture on the fly during training. We show that the information about desirable architectural changes, due to expressivity bottlenecks when attempting to follow the functional gradient, can be extracted from backpropagation. To do this, we propose a mathematical definition of expressivity bottlenecks, which enables us to detect, quantify and solve them while training, by adding suitable neurons. Thus, while the standard approach requires large networks, in terms of number of neurons per layer, for expressivity and optimization reasons, we provid tools and properties to develop an architecture starting with a very small number of neurons. As a proof of concept, we show results~on the CIFAR dataset, matching large neural network accuracy, with competitive training time, while removing the need for standard architectural hyper-parameter search.",
        "authors": "M. Verbockhaven, T. Rudkiewicz, G. Charpiat, et.al",
        "keywords": [
            "architecture adaptation",
            "expressivity bottlenecks",
            "neural network optimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=hbtG6s6e7r",
        "pdf_src": "https://api2.openreview.net/pdf/afffc3036e8cf229dab86eb2bfbe45d3e1ee3862.pdf",
        "Code_src": "",
        "Introduction": "Background: Machine learning tasks traditionally involve optimizing functions defined over parameterized spaces using gradient descent techniques based on neural network architectures.\n\nResearch Problem: The problem addressed here concerns how to optimize machine learning models without being constrained solely by the expressive capabilities or limitations of specific architectures used at training time; it also aims to reduce the computational cost associated with finding optimal architectures through hyperparameter tuning.\n\nMethodology: The authors suggest adapting the neural network's architecture dynamically throughout the training process rather than fixing it beforehand so that the model may evolve beyond the constraints imposed by initial choices made regarding the architecture size and structure.\nTo achieve dynamic adaptation they rely on insights gained via backpropagation algorithm - specifically identifying \"expressivity bottlenecks\" – areas along the gradients indicating insufficient capacity \nof current architecture to represent desired solutions effectively. They introduce a novel mathematical framework allowing detection & quantification of these bottlenecks enabling their resolution by selectively introducing new neurons into the network layers accordingly.\n\nMain Contributions:\n1. A theoretical understanding underpinning the identification method for expressivity bottlenecks;\n2. An empirical demonstration showing improved performance comparable to traditional neural networks but requiring fewer neurons initially leading potentially reduced computation costs;\n3. A practical application demonstrating real-world benefits such as faster convergence times compared conventional methods needing extensive hyperparameter exploration",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Data-Centric Defense: Shaping Loss Landscape with Augmentations to Counter Model Inversion",
        "abstract": "Machine Learning models have shown susceptibility to various privacy attacks, with model inversion (MI) attacks posing a significant threat. Current defense techniques are mostly \\emph{model-centric}, involving modifying model training or inference. However, these approaches require model trainers' cooperation, are computationally expensive, and often result in a significant privacy-utility tradeoff. To address these limitations, we propose a novel \\emph{data-centric} approach to mitigate MI attacks. Compared to traditional model-centric techniques, our approach offers the unique advantage of enabling each individual user to control their data's privacy risk, aligning with findings from a Cisco survey that only a minority actively seek privacy protection. Specifically, we introduce several privacy-focused data augmentations that modify the private data uploaded to the model trainer. These augmentations shape the resulting model's loss landscape, making it challenging for attackers to generate private target samples. Additionally, we provide theoretical analysis to explain why such augmentations can reduce the risk of model inversion. We evaluate our approach against state-of-the-art MI attacks and demonstrate its effectiveness and robustness across various model architectures and datasets. Specifically, in standard face recognition benchmarks, we reduce face reconstruction success rates to $\\leq5\\%$, while maintaining high utility with only a 2\\% classification accuracy drop, significantly surpassing state-of-the-art model-centric defenses. This is the first study to propose a data-centric approach for mitigating model inversion attacks, showing promising potential for decentralized privacy protection.",
        "authors": "S. Chen, F. Kang, N. Abhyankar, et.al",
        "keywords": [
            "Data-Centric Approach",
            "Model Inversion Attacks",
            "Privacy Protection"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=r8wXaLJBIS",
        "pdf_src": "https://api2.openreview.net/pdf/052dbf3d080eb89c54ca8ca8edee3d9c0800f506.pdf",
        "Code_src": "",
        "Introduction": "Background: Machine learning models may be vulnerable to privacy threats like model inversion (MI), where an attacker reconstructs sensitive information about individuals by analyzing the trained model.\n\nResearch Problem: The current solutions mainly focus on altering the ML model itself during training or inference phase, which requires collaboration between the model creators/trainers (\"model-centric\"), involves computational costs, and might lead to trade-offs regarding privacy and performance.\n\nMethodology: In contrast, this paper introduces a \"data-centric\" strategy aimed at reducing MI risks without requiring changes to the underlying machine learning model architecture; instead focusing on how input data interacts with the model.\n\nMain Contributions:\n1. Propose new privacy-preserving data augmentation methods tailored specifically towards defending against MI.\n2. Demonstrate through theoretical reasoning as well as empirical experiments using real-world datasets including facial images - show that even when applied to complex neural networks used widely within industry standards – they effectively defend against MI attacks whilst retaining acceptable levels of functionality (classification error rate increase less than 3%).\n\nConclusion: This research represents one of few studies addressing mitigation strategies directly related to inputs rather than solely relying on modifications made after training has finished thus offering more flexibility around deployment scenarios",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Pretraining a Neural Operator in Lower Dimensions",
        "abstract": "There has recently been increasing attention towards developing foundational neural Partial Differential Equation (PDE) solvers and neural operators through large-scale pertaining. However, unlike vision and language models that make use of abundant and inexpensive (unlabeled) data for pretraining, these neural solvers usually rely on simulated PDE data, which can be costly to obtain, especially for high dimensional PDEs. In this work, we aim to Pretrain neural PDE solvers on Lower Dimensional PDEs (PreLowD) where data collection is the least expensive. We evaluated the effectiveness of this pretraining strategy in similar PDEs in higher dimensions. We use the Factorized Fourier Neural Operator (FFNO) due to having the necessary flexibility to be applied to PDE data of arbitrary spatial dimensions and reuse trained parameters in lower dimensions. In addition, our work sheds light on the effect of the fine-tuning configuration to make the most of this pretraining strategy. Code is available at https://github.com/BaratiLab/PreLowD.",
        "authors": "A. Hemmasian, A. B. Farimani",
        "keywords": [
            "PreLowD",
            "neural PDE solvers",
            "FFNO"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ZewaRoZehI",
        "pdf_src": "https://api2.openreview.net/pdf/048de99f0e38401009f7327ccc26dd21739ce86a.pdf",
        "Code_src": "https://github.com/BaratiLab/PreLowD",
        "Introduction": "Background: The paper addresses a gap between the success of deep learning approaches like Vision-Language Models with access to vast amounts of unlabeled data during pre-training phase versus neural solvers such as Partial Differential Equation (PDE) solvers or neural operators whose training often relies on computationally expensive simulations.\n\nResearch Problem: How do you effectively pre-train neural PDE solvers when labeled simulation data are scarce?\n\nMethodology: The authors propose \"PreLowD,\" an approach focused on pre-training neural PDE solvers using simpler Low-Dimensional PDE datasets before applying them successfully across Higher Dimensions. They leverage the Factorized Fourier Neural Operator (FFNO), chosen because it allows adaptation without retraining from scratch even if the problem dimension changes significantly; thus enabling parameter sharing among different dimensions while maintaining adaptability.\n \nMain Contributions:\n1. **Dimension Reduction**: Demonstrates how to train neural PDE solvers efficiently by starting small - with low-dimensional problems – reducing computational costs compared to directly solving complex high-dimensional ones.\n2. **Flexibility of FFNO**: Highlights the versatility of FFNO allowing its application regardless of the spatial dimensions involved within the PDE system being solved—this property enables the reuse of learned parameters across various dimensions after initial pre-training stage.\n3. **Fine-Tuning Insights**: Provides insights into optimizing fine-tuning configurations so they maximize benefits gained via pre-training strategies ensuring better performance transfer beyond original domain sizes used initially throughout training process.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Confidence Intervals and Simultaneous Confidence Bands Based on Deep Learning",
        "abstract": "Deep learning models have significantly improved prediction accuracy in various fields, gaining recognition across numerous disciplines. Yet, an aspect of deep learning that remains insufficiently addressed is the assessment of prediction uncertainty. Producing reliable uncertainty estimators could be crucial in practical terms. For instance, predictions associated with a high degree of uncertainty could be sent for further evaluation. Recent works in uncertainty quantification of deep learning predictions, including Bayesian posterior credible intervals and a frequentist confidence-interval estimation,  have proven to yield either invalid  or overly conservative intervals. Furthermore, there is currently no method for quantifying uncertainty that can accommodate deep neural networks for survival (time-to-event) data that involves right-censored outcomes.  In this work, we provide a non-parametric bootstrap method that disentangles data uncertainty from the noise inherent in the adopted optimization algorithm. %, ensuring that based on deep learning estimators with small bias, the resulting point-wise confidence intervals or the  simultaneous confidence bands are accurate (i.e., valid and not overly conservative).  The validity of the proposed approach is demonstrated through an extensive simulation study, which shows that the method is accurate (i.e., valid and not overly conservative) as long as the network is sufficiently deep to ensure that the estimators provided by the deep neural network exhibit minimal bias. Otherwise, undercoverage of up to 8\\% is observed.  The proposed ad-hoc method can be easily integrated into any deep neural network without interfering with the training process. The utility of the proposed approach is demonstrated through two applications: constructing simultaneous confidence bands for survival curves generated by deep neural networks dealing with right-censored survival data, and constructing a confidence interval for classification probabilities in the context of binary classification regression. Code for the data analysis and reported simulation is available at Githubsite: \\url{https://github.com/Asafba123/Survival_bootstrap}.",
        "authors": "A. B. Arie, M. Gorfine",
        "keywords": [
            "bootstrap",
            "uncertainty quantification",
            "survival analysis"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=PdbaruPVUY",
        "pdf_src": "https://api2.openreview.net/pdf/b2c017ea60a9a27f2b6038484a56b693d589d206.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses how despite significant advancements made by deep learning models in predicting accurately within different domains, their ability to assess prediction uncertainty has been inadequately explored.\n\nResearch Problem:\nThe primary issue addressed here concerns developing methods capable of producing dependable uncertainty estimators – something potentially critical when it comes to real-world application scenarios where predictions deemed uncertain might require re-evaluation.\n \nMethods:\nTo tackle these challenges posed above, researchers propose a novel non-parametric bootstrap technique designed specifically addressing the need for estimating uncertainties related to time-to-event data involving censored observations using deep neural networks - a problem previously unaddressed due to lack of applicable methodologies.\n\nMain Contributions:\nThis research introduces a new methodology that separates out data uncertainty from the noise introduced during optimization algorithms used while training such networks. This ensures if the model's estimations possess low biases then the derived pointwise confidence intervals will also be precise; avoiding both overestimation and conservatism issues commonly found elsewhere. \n\nAdditionally, empirical validation via simulations demonstrates its effectiveness even though some limitations exist concerning shallower networks leading to potential undercoverage rates reaching approximately 8%. Moreover, they've developed software accessible publicly allowing others to replicate findings presented therein making contributions more widely disseminated among practitioners interested in similar endeavors.",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Modeling Causal Mechanisms with Diffusion Models for Interventional and Counterfactual Queries",
        "abstract": "We consider the problem of answering observational, interventional, and counterfactual queries in a causally sufficient setting where only observational data and the causal graph are available. Utilizing the recent developments in diffusion models, we introduce diffusion-based causal models (DCM) to learn causal mechanisms, that generate unique latent encodings.  These encodings enable us to directly sample under interventions and perform abduction for counterfactuals. Diffusion models are a natural fit here, since they can encode each node to a latent representation that acts as a proxy for exogenous noise. Our empirical evaluations demonstrate significant improvements over existing state-of-the-art methods for answering causal queries. Furthermore, we provide theoretical results that offer a methodology for analyzing counterfactual estimation in general encoder-decoder models, which could be useful in settings beyond our proposed approach.",
        "authors": "P. Chao, P. Blöbaum, S. K. Patel, et.al",
        "keywords": [
            "causal modeling",
            "diffusion models",
            "counterfactual inference"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=EDHQDsqiSe",
        "pdf_src": "https://api2.openreview.net/pdf/1d225039f2f9222c305252d97f030bfa008920c3.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the challenge of answering various types of causal queries within a causally sufficient environment when only observational data is accessible along with knowledge about the underlying causal structure.\n\nResearch Question: How do you effectively answer observational, interventional, or counterfactual questions using just observational data combined with information on the causal graph?\n\nMethodology: To tackle this issue, the authors propose diffusion-based causal models (DCMs). They utilize diffusion processes from machine learning literature alongside recently developed diffusion models such as DALL-E2 by OpenAI.\nThese diffusion models have been shown capable of generating high-quality images based on textual descriptions; similarly, these models may also capture complex relationships between variables through their latent representations—making them suitable proxies for exogenous factors affecting nodes' states across different scenarios including those involving interventions like hypothetical changes made during an experiment without altering actual outcomes.\n\nMain Contributions:\n1. Introduction of diffusion-based causal models(DCM), leveraging diffusion model's ability to create novel latent encodings capturing essential aspects related to causality;\n2. Developmental work towards sampling under interventions and performing abductive reasoning for counterfactual estimations utilizing learned latent encodings;\n3. Empirical validation demonstrating superior performance compared against current leading approaches at addressing causal query resolution tasks;\n4. Provisioning insights into how one might analyze counterfactual estimation problems more generally via encoder-decoder frameworks applicable outside specific contexts considered initially.",
        "Topic": "Generative Models"
    },
    {
        "title": "DTRNet: Precisely Correcting Selection Bias in Individual-Level Continuous Treatment Effect Estimation by Reweighted Disentangled Representation",
        "abstract": "Estimating the individual-level continuous treatment effect holds significant practical importance in various decision-making domains, such as personalized healthcare and customized marketing. However, most current methods for individual treatment effect estimation are limited to discrete treatments and struggle to precisely adjust for selection bias under continuous settings, leading to inaccurate estimation. To address these challenges, we propose a novel Disentangled Representation Network (DTRNet) to estimate the individualized dose-response function (IDRF), which learns disentangled representations and precisely adjusts for selection bias. To the best of our knowledge, our work is the first attempt to precisely adjust for selection bias in continuous settings. Extensive results on synthetic and semi-synthetic datasets demonstrate that our DTRNet outperforms most state-of-the-art methods. Our code is available at \\href{https://github.com/xuanxuan03021/DTRNet_final_2}{DTRNet}.",
        "authors": "M. Hu, Z. Chu, S. Li",
        "keywords": [
            "Disentangled Representation Network",
            "Individualized Dose-Response Function",
            "Selection Bias Adjustment"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=1ZTfzA9bXw",
        "pdf_src": "https://api2.openreview.net/pdf/3e01b4024e13cdd02b99c4ea06252cbedbbdc1e8.pdf",
        "Code_src": "",
        "Introduction": "Background: Estimating the individual-level continuous treatment effect plays crucial roles in many fields like personalized healthcare and customized marketing.\n\nResearch Problem: Most existing methods focus on estimating the effects of discrete treatments but fail to accurately account for selection bias when dealing with continuous treatments due to their limitations.\n\nMethod: We introduce a new approach called Disentangled Representation Network (DTRNet). The network aims to learn disentangled representations while adjusting for selection bias more effectively than previous approaches can do by using continuous treatments.\nMain Contributions: This study introduces an innovative method - DTRNet – capable of addressing both learning disentangled representations from complex data distributions along with correcting potential biases during prediction tasks involving continuous variables within specific contexts or scenarios where they may be applied practically \n(like medicine ). Furthermore , this paper provides empirical evidence demonstrating its superiority over other advanced techniques through extensive experiments conducted across multiple datasets .",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Feature Distillation Improves Zero-Shot Transfer from Synthetic Images",
        "abstract": "Vision-language foundation models such as CLIP have showcased impressive zero-shot capabilities. However, their applicability in resource-constrained environments is limited due to their size and the resulting latency. Knowledge distillation allows to mitigate these challenges by distilling small image encoders that can replace the large CLIP image encoder. In a zero-shot setting, where only the class names are known, no real domain images can be used for this process. Instead, we investigate the use of synthetic images for this purpose. Unlike existing works that focus on improving the quality of synthetic images to bridge the performance gap compared to training on natural images, we find the choice of loss to be a crucial factor. Specifically, minimizing only the distance between the student and teacher image features, without incorporating image captions in the loss function, increases the robustness to spurious features and data corruptions. As a result, this feature distillation approach greatly improves the transfer performance from synthetic to real images. Leveraging these insights, we are able to train domain-specific students that achieve zero-shot performance comparable to a ViT-B/32 teacher on six fine-grained classification datasets while using up to 92% fewer parameters.",
        "authors": "N. Popp, J. H. Metzen, M. Hein",
        "keywords": [
            "synthetic images",
            "knowledge distillation",
            "feature distillation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=SP8DLl6jgb",
        "pdf_src": "https://api2.openreview.net/pdf/eb6c4c883a7dc8f4616fe39a994103f7b2069fde.pdf",
        "Code_src": "",
        "Introduction": "Background: Vision-language foundation models like CLIP demonstrate strong abilities with little or no prior knowledge about tasks (\"zero-shot\"). However, they're too big and slow when deployed under resource constraints.\n\nResearch Question: How do you effectively reduce the model's size and computational cost during knowledge distillation?\n\nMethod: We explore the usage of synthetic images instead of actual ones within the context of zero-shot learning scenarios since there aren't any genuine examples available here.\nWe also analyze different losses; our main finding suggests that focusing solely on reducing distances among learned representations doesn’t work well enough – it needs to include text information alongside visual cues through captioning.\n\nMain Contributions:\n- We show how choosing an appropriate loss strategy plays key role - one which includes both image features similarity and textual descriptions helps improve generalization across domains better than just matching visuals alone does;\n- Our proposed method significantly reduces parameter requirements needed per task while maintaining similar levels of accuracy achieved even against much larger pre-trained counterparts",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity",
        "abstract": "On tabular data, a significant body of literature has shown that current deep learning (DL) models perform at best similarly to Gradient Boosted Decision Trees (GBDTs), while significantly underperforming them on outlier data. However, these works often study problem settings which may not fully capture the complexities of real-world scenarios. We identify a natural tabular data setting where DL models can outperform GBDTs: tabular Learning-to-Rank (LTR) under label scarcity. Tabular LTR applications, including search and recommendation, often have an abundance of unlabeled data, and scarce labeled data. We show that DL rankers can utilize unsupervised pretraining to exploit this unlabeled data. In extensive experiments over both public and proprietary datasets, we show that pretrained DL rankers consistently outperform GBDT rankers on ranking metrics, sometimes by as much as 38%, both overall and on outliers.",
        "authors": "C. Hou, K. K. Thekumparampil, M. Shavlovsky, et.al",
        "keywords": [
            "DL",
            "Learning-to-Rank",
            "Outlier Detection"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=093Q9VxaWt",
        "pdf_src": "https://api2.openreview.net/pdf/65ea072ec4157a2664c254d20d86ba59708ac806.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses how in many cases when dealing with tabular data, state-of-the-art deep learning (DL) models achieve performance similar or slightly better than Gradient Boosted Decision Trees (GBDTs). However, they tend to struggle more against outliers compared to GBDTs.\n\nResearch Problem:\nIdentifying specific types of tabular data problems for which DL models could potentially surpass traditional methods like GBDTs is crucial due to their limitations regarding outliers' handling capabilities.\n \nMethodology:\nThe authors focus specifically on \"tabular Learning-to-Rank\" (LTR) tasks characterized by having abundant unlabeled data but limited labeled examples—a common scenario encountered during practical implementations such as search engines or recommender systems. They propose using unsupervised pretraining techniques allowing DL rankers to effectively leverage large amounts of unlabeled information before fine-tuning on smaller sets of annotated data points.\n\nMain Contributions:\nExtensive empirical evidence from various datasets demonstrates that pretrained DL rankers generally yield superior results across different ranking metrics—sometimes even exceeding those obtained through GBDTs by up to 38%. This finding highlights the effectiveness of leveraging unlabeled data within DL frameworks designed for LTR purposes amidst label scarcity conditions found commonly in practice.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Calibration Attacks: A Comprehensive Study of Adversarial Attacks on Model Confidence",
        "abstract": "In this work, we highlight and perform a comprehensive study on calibration attacks, a form of adversarial attacks that aim to trap victim models to be heavily miscalibrated without altering their predicted labels, hence endangering the trustworthiness of the models and follow-up decision making based on their confidence. We propose four typical forms of calibration attacks: underconfidence, overconfidence, maximum miscalibration, and random confidence attacks, conducted in both the black-box and white-box setups. We demonstrate that the attacks are highly effective on both convolutional and attention-based models: with a small number of queries, they seriously skew confidence without changing the predictive performance. Given the potential danger, we further investigate the effectiveness of a wide range of adversarial defence and recalibration methods, including our proposed defences specifically designed for calibration attacks to mitigate the harm. From the ECE and KS scores, we observe that there are still significant limitations in handling calibration attacks. To the best of our knowledge, this is the first dedicated study that provides a comprehensive investigation on calibration-focused attacks. We hope this study helps attract more attention to these types of attacks and hence hamper their potential serious damages. To this end, this work also provides detailed analyses to understand the characteristics of the attacks.",
        "authors": "S. Obadinma, X. Zhu, H. Guo",
        "keywords": [
            "calibration attack",
            "adversarial defense",
            "model uncertainty"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=TXzz9xwdpv",
        "pdf_src": "https://api2.openreview.net/pdf/0c7af0e8b8d3985245dc708e913e478047e3bab6.pdf",
        "Code_src": "",
        "Introduction": "Background: Calibration attacks pose threats by manipulating model predictions while leaving label accuracy unchanged.\nResearch Problem: This paper investigates various calibration attack strategies against machine learning models.\n\nMethods:\n1. Identifies 4 common calibration attack types - Underconfidence, Overconfidence, Maximum Miscalibration, Random Confidence Attacks \n2. Conducts experiments using Black-Box and White-Box settings across Convolutional Neural Networks (CNNs) and Attention-Based Models.\n3. Evaluates defenses such as Adversarial Defenses and Recalibration Methods tailored towards mitigating calibration issues through metrics like Expected Calibration Error (ECE) and Kullback-Leibler Divergence (KS).\n\nMain Contributions:\n1. Presents an extensive analysis focusing solely on calibration attacks within the ML community,\n2. Demonstrates the efficacy of different attack techniques even when requiring minimal interaction or query numbers from adversaries,\n3. Provides insights into defense mechanisms' shortcomings regarding calibration-related adversarial examples via empirical evidence,\n\nOverall Aim: The research aims at raising awareness about the risks posed by calibration attacks so future efforts can focus on developing robust countermeasures preventing them effectively.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "SQL-PaLM: Improved large language model adaptation for Text-to-SQL",
        "abstract": "Text-to-SQL, the process of translating natural language into Structured Query Language (SQL), represents a transformative application of large language models (LLMs), potentially revolutionizing how humans interact with data. This paper introduces the SQL-PaLM framework, a comprehensive solution for understanding and enhancing Text-to-SQL using LLMs, using in the learning regimes of few-shot prompting and instruction fine-tuning. With few-shot prompting, we explore the effectiveness of consistency decoding with execution-based error filtering. With instruction fine-tuning, we delve deep in understanding the critical paradigms that influence the performance of tuned LLMs. In particular, we investigate how performance can be improved through expanded training data coverage and diversity, synthetic data augmentation, and integrating query-specific database content. We propose a test-time selection method to further refine accuracy by integrating SQL outputs from multiple paradigms with execution feedback as guidance. Additionally, we tackle the practical challenge of navigating intricate databases with a significant number of tables and columns, proposing efficient techniques for accurately selecting relevant database elements to enhance Text-to-SQL performance. Our holistic approach yields substantial advancements in Text-to-SQL, as demonstrated on two key public benchmarks, Spider and BIRD. Through comprehensive ablations and error analyses, we shed light on the strengths and weaknesses of our framework, offering valuable insights into Text-to-SQL's future work.",
        "authors": "R. Sun, S. O. Arik, A. Muzio, et.al",
        "keywords": [
            "Text-to-SQL",
            "Consistency Decoding",
            "Instruction Fine-tuning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=rlloVZoKrX",
        "pdf_src": "https://api2.openreview.net/pdf/35f71cab0ca86244b7cc4317dd1e489492521357.pdf",
        "Code_src": "",
        "Introduction": "Background: The translation task between text and structured queries is crucial because it allows users without programming knowledge to access information stored in databases easily.\n\nResearch Problem: How do you improve the efficiency when converting natural language sentences into SQL queries?\n\nMethod: They introduced an end-to-end system called SQL-PaLM which uses both few-shot prompting and instruction fine-tuning methods based on LLMs. \n\nMain Contributions:\n1. Consistency decoding with execution-based error filtering was explored.\n2. A series of improvements were proposed including expanding training data coverage & diversity, synthetic data augmentation, and integrating query-specific database content during instruction fine-tuning phase.\n3. A novel test-time selection strategy combining different paradigm outputs along with execution feedback has been developed aiming at improving accuracy even more.\n4. Techniques are also presented addressing challenges like dealing with complex databases having many tables/columns effectively so they could select appropriate ones needed within their Text-to-SQL pipeline accordingly leading towards better overall performance across various datasets such as Spider or BIRD benchmarking platforms where experiments have shown promising results compared against existing state-of-the-art approaches available today.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Large-width asymptotics and training dynamics of $\\alpha$-Stable ReLU neural networks",
        "abstract": "Large-width asymptotic properties of neural networks (NNs) with Gaussian distributed weights have been extensively investigated in the literature, with major results characterizing their large-width asymptotic behavior in terms of Gaussian processes and their large-width training dynamics in terms of the neural tangent kernel (NTK). In this paper, we study large-width asymptotics and training dynamics of $\\alpha$-Stable ReLU-NNs, namely NNs with ReLU activation function and $\\alpha$-Stable distributed weights, with $\\alpha\\in(0,2)$. For $\\alpha\\in(0,2]$, $\\alpha$-Stable  distributions form a broad class of heavy tails distributions, with the special case $\\alpha=2$ corresponding to the Gaussian distribution. Firstly, we show that if the NN's width goes to infinity, then a rescaled $\\alpha$-Stable ReLU-NN converges weakly (in distribution) to an $\\alpha$-Stable process, which generalizes the Gaussian process. As a difference with respect to the Gaussian setting, our result shows that the activation function affects the scaling of the  $\\alpha$-Stable NN; more precisely, in order to achieve the infinite-width $\\alpha$-Stable process, the ReLU activation requires an additional logarithmic term in the scaling with respect to sub-linear activations. Secondly, we characterize the large-width training dynamics of $\\alpha$-Stable ReLU-NNs in terms an infinite-width random kernel, which is referred to as the $\\alpha$-Stable NTK, and we show that the gradient descent achieves zero training error at linear rate, for a sufficiently large width, with high probability. Differently from the NTK arising in the Gaussian setting, the $\\alpha$-Stable NTK is a random kernel; more precisely, the randomness of the $\\alpha$-Stable ReLU-NN at initialization does not vanish in the large-width training dynamics.",
        "authors": "S. Favaro, S. Fortini, S. Peluchetti",
        "keywords": [
            "alpha-Stable ReLU-NNs",
            "Large-width asymptotics",
            "alpha-Stable NTK"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=bEwAAEmRbh",
        "pdf_src": "https://api2.openreview.net/pdf/456ab3ad121784034b6bbd8f47cf200f59112407.pdf",
        "Code_src": "",
        "Introduction": "Background: The background of this research lies in understanding the asymptotic behaviors under wide-width conditions when considering neural networks (NNs) with Gaussian distributed weights has already received extensive attention due to its implications on learning theory.\n\nResearch Problem: This work addresses two main problems related to the investigation into the asymptotic properties during training of $\\alpha$-Stable ReLU-NNs - one being how these networks converge as they become wider than before while maintaining $\\alpha$-stable weight distributions ($\\alpha \\in (0,2)$), including whether there are any differences compared to the Gaussian case where $\\alpha = 2$, and secondly, what happens regarding the training dynamics using gradient decent algorithms like stochastic gradient descent or batch gradient decent?\n\nMethods: To tackle both issues mentioned above, researchers use tools such as convergence analysis based on weak convergence methods within distributional sense along with characterization techniques applied towards studying the evolution over time through stochastic differential equations leading up to defining new kernels called $\\alpha$-Stable Neural Tangent Kernels (NTK).\n\nMain Contributions:\n1. They provide insights about how $\\alpha$-Stable ReLU-NNs behave asymptotically by showing that upon increasing network widths indefinitely without bound, scaled versions of these networks will converge weakly toward certain $\\alpha$-Stable processes – thus extending beyond just Gaussian processes.\n2. Furthermore, they introduce novel concepts around training dynamics focusing specifically on $\\alpha$-Stable ReLU-NNs via introducing $\\alpha$-Stable NTK models which incorporate randomness inherent even after reaching very wide scales unlike classical Gaussian counterparts whose randomness disappears post-training initiation phase.",
        "Topic": "approximation"
    },
    {
        "title": "Uncertainty in Graph Neural Networks: A Survey",
        "abstract": "Graph Neural Networks (GNNs) have been extensively used in various real-world applications. However, the predictive uncertainty of GNNs stemming from diverse sources such as inherent randomness in data and model training errors can lead to unstable and erroneous predictions. Therefore, identifying, quantifying, and utilizing uncertainty are essential to enhance the performance of the model for the downstream tasks as well as the reliability of the GNN predictions. This survey aims to provide a comprehensive overview of the GNNs from the perspective of uncertainty with an emphasis on its integration in graph learning. We compare and summarize existing graph uncertainty theory and methods, alongside the corresponding downstream tasks. Thereby, we bridge the gap between theory and practice, meanwhile connecting different GNN communities. Moreover, our work provides valuable insights into promising directions in this field.",
        "authors": "F. Wang, Y. Liu, K. Liu, et.al",
        "keywords": [
            "uncertainty",
            "Graph Neural Networks",
            "prediction"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=0e1Kn76HM1",
        "pdf_src": "https://api2.openreview.net/pdf/8d31f14746c2ccb02556100630292f7d37f8cacc.pdf",
        "Code_src": "",
        "Introduction": "Background: Graph Neural Networks (GNNs) have become widely utilized across numerous practical domains due to their effectiveness at processing structured data represented by graphs.\n\nResearch Problem: Despite these successes, there is significant variability or \"uncertainty\" associated with GNN predictions that arises from multiple factors including random aspects within the dataset itself along with potential inaccuracies during the training process which may result in unreliable outcomes.\n \nMethods: The paper surveys current theories regarding uncertainty measurement applied specifically to GNNs while also examining how they integrate uncertainty estimation techniques directly into graph learning processes themselves. It compares methodologies among them and relates each approach back to specific application areas where it has relevance - referred to as 'downstream tasks'.\n\nMain Contributions: The primary contribution lies in offering readers a thorough synthesis covering both theoretical frameworks addressing graph uncertainty issues pertinent to GNNs; secondly bridging gaps amongst disparate research streams focused around GNNs through comparative analysis thus fostering dialogue & collaboration opportunities throughout related fields concerned about neural networks over graphical structures last but not least providing guidance towards future developments likely impacting advancements made possible via employing uncertain reasoning capabilities within graph learning architectures going forward.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "The Harmonic Indel Distance",
        "abstract": "This short note introduces the harmonic indel distance (HID), a new distance between strings where the cost of an insertion or deletion is inversely proportional to the string length. We present a closed-form formula and show that the HID is a proper distance metric. Then we perform an experimental comparison of HID to normalized and unnormalized versions of the indel distance on benchmark tasks for biomedical sequence data. We finally show planar embeddings of the benchmark datasets to provide some insights into the geometry of the metric spaces associated with the different distance metrics.",
        "authors": "B. Pepin",
        "keywords": [
            "string",
            "insertion",
            "deletion"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=HV9lXOIZYw",
        "pdf_src": "https://api2.openreview.net/pdf/7acf1c1e401215c56e059c8affd5e860018ea277.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper proposes a novel distance measure, called Harmonic Indel Distance (HID), which measures the difference in lengths two sequences by considering the inverse proportionality factor based on their respective lengths.\n\nResearch Problem: The problem addressed here involves finding alternative methods beyond traditional edit distances like Hamming distance when dealing with biological sequences such as DNA strands due to insertions/deletions (indels).\n\nMethodology: The authors introduce HID using a closed-form formula derived from the harmonic series function; they also prove that HID satisfies all properties required under a distance metric framework - namely it's symmetric, non-negative, satisfies triangle inequality etc. They further compare HID against existing indel distances after normalizing them across various benchmarks used commonly within bioinformatics research fields including both synthetic and real-world datasets.\n\nMain Contributions:\n1. Development & introduction of Harmonic Indel Distance.\n2. Proving its validity through mathematical analysis ensuring it meets criteria necessary being considered as a valid distance metric among strings.\n3. Experimental validation comparing HID performance alongside other variants of indel distances over standard benchmarks relevant specifically towards genomic sequence alignment problems found frequently in computational biology applications today – providing empirical evidence supporting its effectiveness compared conventional approaches while potentially offering more nuanced understanding about relationships amongst sequences",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Towards Understanding Adversarial Transferability in Federated Learning",
        "abstract": "We investigate a specific security risk in FL: a group of malicious clients has impacted the model during training by disguising their identities and acting as benign clients but later switching to an adversarial role. They use their data, which is part of the training set, to train a substitute model and conduct transferable adversarial attacks against the federated model. This type of attack is subtle and hard to detect because these clients initially appear to be benign.\n\nThe key question we address is: How robust is the FL system to such covert attacks, especially compared to traditional centralized learning systems? We empirically show that the proposed attack imposes a high-security risk to current FL systems. By using only 3\\% of the client's data, we achieve the highest attack rate of over 80\\%. To further offer a full understanding of the challenges the FL system faces in transferable attacks, we provide a comprehensive analysis of the transfer robustness of FL across a spectrum of configurations. Surprisingly, FL systems show a higher level of robustness than their centralized counterparts, especially when both systems are equally good at handling regular, non-malicious data.\n\nWe attribute this increased robustness to two main factors:\n1) Decentralized Data Training: Each client trains the model on its own data, reducing the overall impact of any single malicious client.\n2) Model Update Averaging: The updates from each client are averaged together, further diluting any malicious alterations.\nBoth practical experiments and theoretical analyses support our conclusions. This research not only sheds light on the resilience of FL systems against hidden attacks but also raises important considerations for their future application and development。",
        "authors": "Y. Li, Y. Gao, H. Wang",
        "keywords": [
            "FL system",
            "Covert attack",
            "Transferable adversarial attacks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=hafnY2PiTn",
        "pdf_src": "https://api2.openreview.net/pdf/5b27e51b2e622f79d38cacb07c3d11d5911c483b.pdf",
        "Code_src": "",
        "Introduction": "Background: Federated Learning (FL) aims to improve machine learning models through collaborative training among multiple devices or edge nodes without sharing sensitive local data with central servers.This paper investigates a novel threat to FL: malicious clients can infiltrate the process by masquerading as benign participants, then switch roles to launch targeted attacks after training.\n\nResearch Question: Our study focuses on assessing the vulnerability of FL systems to stealthy attacks where adversaries exploit their access within the network to undermine the integrity of shared models.\n\nMethodology: We propose empirical testing methods to evaluate how well FL withstands covert assaults compared to conventional centralized approaches. Specifically, we demonstrate a new kind of attack strategy involving substituting one's trained model while maintaining participation in the FL framework; we quantify effectiveness under various conditions including different proportions of compromised clients' data used towards constructing alternative models.\n\nMain Contributions: \n- Identification & Demonstration: We introduce evidence showing that even small groups of attackers can significantly compromise FL outcomes via strategic interventions into the training phase.\n- Robustness Analysis: Through extensive experimentation along diverse parameter settings, we reveal unexpected findings regarding the relative robustness of FL architectures vis-a-vis centralized ones - particularly noting improved resistance due to decentralized nature and averaging mechanisms employed post-update aggregation.\n- Insights for Future Development: Our work highlights critical aspects about securing distributed networks like FL environments moving forward",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "FLR: Label-Mixture Regularization for Federated Learning with Noisy Labels",
        "abstract": "Label noise in federated learning (FL) has garnered increasing attention due to the decentralized nature of FL, where data is collected from multiple clients with potentially different levels of label noise. This study introduces two pivotal contributions to this domain. First, we anatomize the memorization phenomenon in FL into server-side and client-side components, marking the first investigation into how these distinct forms of memorization impact learning. Second, to mitigate the memorization in FL, we present the Federated Label-mixture Regularization (FLR) strategy, a straightforward yet effective approach that employs regularization through pseudo labels generated by merging local and global model predictions. This method not only improves the accuracy of the global model in both i.i.d. and non-i.i.d. settings but also effectively counters the memorization of noisy labels. We empirically find that FLR aligns with and advances existing FL and noisy label mitigation methods over multiple datasets under various levels of data heterogeneity and label noise.",
        "authors": "T. Kim, D. Kim, S. Yun",
        "keywords": [
            "federated learning",
            "label noise",
            "regularization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Z8A3HDgS0E",
        "pdf_src": "https://api2.openreview.net/pdf/4c4b27e7e35cf519d364cc0879ce8f122f7d3efb.pdf",
        "Code_src": "",
        "Introduction": "Background: Federated Learning (FL), which allows for machine learning on distributed devices without centralizing user data, faces challenges such as label noise introduced when training models across heterogeneous datasets.\nResearch Problem: How do the memorization phenomena observed during FL arise at the server side versus the client side? Additionally, what strategies can be employed to reduce or counteract the effects of label noise?\n\nMethods: The authors introduce a novel framework called Federated Label-mixture Regularization (FLR). They decompose the memorization effect caused by label noise within FL systems between server-side and client-side factors.\n\nMain Contributions:\n1. Anatomical Understanding - The paper provides an understanding of the memorization process occurring specifically around the server and individual clients' roles contributing to it; they are among the first studies to delineate these aspects distinctly regarding their influence on FL outcomes.\n2. New Methodology - The proposed FLR technique mitigates memorization issues arising out of label noise via a regularization mechanism using pseudo-labels derived from combining local and global model predictions before updating each device's parameters independently throughout rounds of communication sessions (\"federated updates\").\n\nImpact: Empirical results demonstrate improved performance against label noise variability while maintaining high accuracy even amidst significant dataset heterogeneity—indicating its effectiveness beyond standard approaches used previously addressing similar problems related to FL and noisy label handling techniques tested extensively",
        "Topic": "Federated Learning"
    },
    {
        "title": "From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models",
        "abstract": "One of the most striking findings in modern research on large language models (LLMs) is that scaling up compute during training leads to better results. However, less attention has been given to the benefits of scaling compute during inference. This survey focuses on these inference-time approaches. We explore three areas under a unified mathematical formalism: token-level generation algorithms, meta-generation algorithms, and efficient generation. Token-level generation algorithms, often called decoding algorithms, operate by sampling a single token at a time or constructing a token-level search space and then selecting an output. These methods typically assume access to a  language model's logits, next-token distributions, or probability scores. Meta-generation algorithms work on partial or full sequences, incorporating domain knowledge, enabling backtracking, and integrating external information. Efficient generation methods aim to reduce token costs and improve the speed of generation. Our survey unifies perspectives from three research communities: traditional natural language processing, modern LLMs, and machine learning systems.",
        "authors": "S. Welleck, A. Bertsch, M. Finlayson, et.al",
        "keywords": [
            "scaling compute",
            "inference-time approaches",
            "efficient generation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=eskQMcIbMS",
        "pdf_src": "https://api2.openreview.net/pdf/e256feae7214690c5e1ddbeb501988437affa128.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background for this paper lies within the field of artificial intelligence specifically focusing on advancements with large language models (LLMs). Recent studies have shown promising improvements when increasing computational resources used throughout the training phase; however, there hasn't been much focus placed upon how scaling computation affects performance once trained models are being utilized.\n\nResearch Question:\nThis study aims to address whether increased computing power also yields positive outcomes beyond just during initial training - particularly concerning the process known as inference where pre-trained models generate responses based on input data without further retraining them.\n\nMethodology:\nTo investigate this question comprehensively across different types of generation tasks involving LLMs such as text summarization etc., they categorize existing works into three broad categories namely Token-Level Generation Algorithms which sample tokens sequentially or construct spaces around each token before making decisions about outputs; Meta-Generation Algorithms working iteratively over parts/full sequences leveraging domain expertise while allowing backtrack capabilities along with integration possibilities using auxiliary sources like databases or APIs; Efficient Generation Methods concentrating more towards reducing cost per token generated whilst enhancing throughput rates through optimizations techniques applied both algorithmically & systematically.\n\nMain Contributions:\nTheir main contribution revolves around providing insights into various strategies employed today related to scaling computations during inference phases alongside their comparative advantages/disadvantages amongst themselves thus offering researchers practitioners alike guidance toward informed decision-making processes regarding resource allocation choices depending specific requirements posed forward task scenarios encountered real-world applications utilizing advanced technologies nowadays available marketplace",
        "Topic": "Generative Models"
    },
    {
        "title": "Learning State Reachability as a Graph in Translation Invariant Goal-based Reinforcement Learning Tasks",
        "abstract": "Deep Reinforcement Learning proved efficient at learning universal control policies when the goal state is close enough to the starting state, or when the value function features few discontinuities. \nBut reaching goals that require long action sequences in complex environments remains difficult. \nDrawing inspiration from the cognitive process which reuses learned atomic skills in a global planning procedure, we propose an algorithm which encodes reachability between abstract goals as a graph, and produces plans in this goal space.\nTransitions between goals rely on the exploitation of a learned policy which enjoys a property we call \\emph{translation invariant local optimality}, which encodes the intuition that goal-reaching skills can be reused throughout the state space.\nOverall, our contribution permits solving large and difficult navigation tasks, outperforming related methods from the literature.",
        "authors": "H. Bonnavaud, A. Albore, E. Rachelson",
        "keywords": [
            "graph-based planning",
            "translation-invariant local optimality",
            "reusable skills"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=PkHkPQMTxg",
        "pdf_src": "https://api2.openreview.net/pdf/a2dead2193bb43f5092a46a3e187f559bdc341b4.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses limitations with Deep Reinforcement Learning for controlling agents across complex environments where achieving distant goals requires extended action sequences.\n\nResearch Problem: How do you enable reinforcement learning algorithms to efficiently navigate through complex environments by enabling them to reuse previously learned skills?\n\nMethodology: Inspired by human cognition's ability to use learned atomic skills globally during planning procedures without needing to learn anew each time due to translation-invariant properties within the skill domain(s), they introduce a novel approach using graphs representing reachability among abstract goals rather than raw states. They also develop a plan generation mechanism operating over these goalspace graphs based on exploiting a learned policy with \"translation invariant local optimality\" - meaning it allows for the transfer of learned skills across different parts of the environment.\n\nMain Contributions:\n1. An innovative method that represents the problem space not just via individual states but as a graph connecting various high-level goals directly, allowing more effective encoding relationships beyond simple transitions into actions.\n2. A new type of policy called Translation Invariant Local Optimality Policy (TILOP) designed specifically so that once certain skills are mastered under one set of conditions—like moving towards a particular object—they apply universally regardless of context changes because those skills have been encoded such that their effectiveness does not depend solely on immediate surroundings; instead, they generalize well even if translated spatially around other objects/environments while still being locally optimal relative to any given task/goal.\n3. Demonstrated improvements significantly surpassing existing approaches demonstrated capability tackling larger-scale navigation problems effectively",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Contaminated Online Convex Optimization",
        "abstract": "In online convex optimization, some efficient algorithms have been designed for each of the individual classes of objective functions, e.g., convex, strongly convex, and exp-concave. However, existing regret analyses, including those of universal algorithms, are limited to cases in which the objective functions in all rounds belong to the same class and cannot be applied to cases in which the property of objective functions may change in each time step. This paper introduces a novel approach to address such cases, proposing a new regime we term as \\textit{contaminated} online convex optimization. For the contaminated case, we demonstrate that the regret is lower bounded by $\\Omega(\\log T + \\sqrt{k})$. Here, $k$ signifies the level of contamination in the objective functions. We also demonstrate that the regret is bounded by $O(\\log T+\\sqrt{k\\log T})$ when universal algorithms are used. When our proposed algorithms with additional information are employed, the regret is bounded by $O(\\log T+\\sqrt{k})$, which matches the lower bound. These are intermediate bounds between a convex case and a strongly convex or exp-concave case.",
        "authors": "T. Kamijima, S. Ito",
        "keywords": [
            "contaminated",
            "online convex optimization",
            "regret analysis"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=QdGtwjDgub",
        "pdf_src": "https://api2.openreview.net/pdf/6fdf56552272d247a82edb509bf39ce15a811b5a.pdf",
        "Code_src": "",
        "Introduction": "Background: In online convex optimization, different efficient algorithms exist based on the type of objective function considered - convex, strongly convex, and exp-concave.\n\nResearch Problem: Existing regret analyses do not apply if the properties of the objective functions can vary over time steps; this paper addresses how to handle these \"contaminated\" scenarios where the objectives might switch types at any given point during the optimization process.\n\nMethods: The authors introduce a new framework called \"contaminated\" online convex optimization along with corresponding algorithms tailored specifically for it.\nThey provide both lower and upper bounds regarding regret performance:\n- A lower bound of $\\Omega(\\log T + \\sqrt{k})$ indicating minimum regret required under contamination,\n- An upper bound of $O(\\log T+\\sqrt{k\\log T})$ using universal algorithms without specific knowledge about the contamination levels,\n- And an improved algorithmic upper bound matching the lower bound ($O(\\log T+\\sqrt{k})$) through their proposed methods enhanced with additional information.\n\nMain Contributions: \n1. They extend understanding beyond standard settings into more complex scenarios involving potential changes across iterations within the optimization problem's objective function(s).\n2. Offered theoretical guarantees via rigorous analysis ensuring robustness against varying objective function characteristics throughout the optimization process while maintaining competitive regret rates compared to simpler optimization problems like purely convex ones but less stringent than those achievable solely considering strongly convex or exp-concave objectives alone.",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Reconciling Kaplan and Chinchilla Scaling Laws",
        "abstract": "Kaplan and Chinchilla studied the scaling behavior of transformers trained on next-token language prediction. These studies produced different estimates for how the number of parameters ($N$) and training tokens ($D$) should be set to achieve the lowest possible loss for a given compute budget ($C$). Kaplan: $N_\\text{optimal} \\propto C^{0.73}$, Chinchilla: $N_\\text{optimal} \\propto C^{0.50}$. This paper finds that much of this discrepancy can be attributed to Kaplan counting non-embedding rather than total parameters, combined with their analysis being performed at small scale. Simulating the Chinchilla study under these conditions produces biased scaling coefficients close to Kaplan's. Hence, this paper reaffirms Chinchilla's scaling coefficients, by explaining the primary cause of Kaplan's original overestimation. As a second contribution, the paper explains differences in the reported relationships between loss and compute. These findings lead us to recommend that future scaling studies use total parameters and compute.",
        "authors": "T. Pearce, J. Song",
        "keywords": [
            "scaling coefficients",
            "transformer training",
            "optimal parameter settings"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=NLoaLyuUUF",
        "pdf_src": "https://api2.openreview.net/pdf/f91518ed8d3298ef5e2625a7c2b5c611cfb94f60.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this research is related to the optimization of transformer models used in natural language processing tasks such as machine translation or text generation through next-token prediction. Transformers are known to require significant computational resources due to large parameter sizes.\n\nResearch Problem:\nThe problem addressed here concerns the optimal configuration of transformer models when considering both model size (number of parameters N) and computation cost (training tokens D), especially within constraints like available computing power (\"compute budget\" C).\n\nMethods:\nTo address the issue above, the authors critically examined previous works which provided scaling laws estimating the relationship among N, D, and C based on empirical observations from training deep learning models using next-token prediction. Specifically they focused on two prominent scaling laws proposed by Kaplan and Chinchilla:\n\n1. Kaplan suggested an optimal scaling coefficient where N_optimal ∝ C^0.73.\n2. Chinchilla found another optimal scaling coefficient stating N_optimal ∝ C^0.50.\n\nMain Contributions:\nThis work identifies several reasons why Kaplan’s estimate might have been too high compared to Chinchilla's finding including miscounting non-embedding parameters during his calculations along with conducting experiments only up to smaller scales leading to potential biases towards higher values needed per token. The simulation conducted according to the methodology employed by Chinchilla resulted in scaled coefficients closer to those originally estimated by Kaplan suggesting there was indeed some bias present earlier because of methodological choices made therein – not necessarily inherent limitations regarding scalability itself!\n\nAs secondary contributions beyond correcting the estimation error associated with Kaplan's initial results, it also discusses discrepancies observed across literature reports about how losses scale relative to computations costs further emphasizing importance focusing solely on \"total parameters\" instead of just embedding dimensions while analyzing scaling behaviors; recommending more rigorous experimental designs involving larger datasets/models could provide clearer insights into true scaling trends applicable broadly throughout various domains utilizing transformer architectures today!",
        "Topic": "Vision Transformer"
    },
    {
        "title": "Stability and Generalization in Free Adversarial Training",
        "abstract": "While adversarial training methods have significantly improved the robustness of deep neural networks against norm-bounded adversarial perturbations, the generalization gap between their performance on training and test data is considerably greater than that of standard empirical risk minimization. Recent studies have aimed to connect the generalization properties of adversarially trained classifiers to the min-max optimization algorithm used in their training. In this work, we analyze the interconnections between generalization and optimization in adversarial training using the algorithmic stability framework. Specifically, our goal is to compare the generalization gap of neural networks trained using the vanilla adversarial training method, which fully optimizes perturbations at every iteration, with the free adversarial training method, which simultaneously optimizes norm-bounded perturbations and classifier parameters. We prove bounds on the generalization error of these methods, indicating that the free adversarial training method may exhibit a lower generalization gap between training and test samples due to its simultaneous min-max optimization of classifier weights and perturbation variables. We conduct several numerical experiments to evaluate the train-to-test generalization gap in vanilla and free adversarial training methods. Our empirical findings also suggest that the free adversarial training method could lead to a smaller generalization gap over a similar number of training iterations. The paper code is available at https://github.com/Xiwei-Cheng/Stability_FreeAT.",
        "authors": "X. Cheng, K. Fu, F. Farnia",
        "keywords": [
            "Stability",
            "Generalization Gap",
            "Min-Max Optimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=jmwEiC9bq2",
        "pdf_src": "https://api2.openreview.net/pdf/1b8a445651363490aed787a4296e9c9b2bd5ace7.pdf",
        "Code_src": "https://github.com/Xiwei-Cheng/Stability_FreeAT",
        "Introduction": "Background: Adversarial training has been widely adopted for improving the robustness of deep neural networks by adding small but carefully crafted perturbations known as \"adversarial examples\" during training.\nResearch Question: Despite improvements made through adversarial training regarding robustness under norm-bounded adversarial perturbations, there exists an increased generalization gap compared to standard empirical risk minimization approaches.\n\nMethodology: This study employs the algorithmic stability framework within machine learning theory; it aims to understand how different optimization strategies affect both the robustness gained from adversarial training versus the generalization ability when tested outside the training distribution or dataset.\n\nMain Contributions:\n1. Comparative Analysis: It compares two adversarial training methodologies - one where perturbations are optimized iteratively (\"vanilla adversarial training\"), allowing full optimization each step without considering classifier parameters directly,\n2. New Insights into Optimization: Introduces Free Adversarial Training (Free AT), which concurrently updates classifier parameters alongside perturbations throughout the optimization process rather than sequentially optimizing them separately;\n3. Proofs & Bounds: Provides theoretical guarantees about the generalization errors associated with these two training procedures showing potential benefits towards reducing the aforementioned large generalization gap observed traditionally after adversarial training;\n4. Empirical Validation: Conducts numerical experiments demonstrating reduced generalization gaps achieved via Free AT across various numbers of training iterations relative to Vanilla AT;\n5. Open Source Code: Shares reproducible research materials publicly accessible online so others can replicate results independently.\n\nThe implications include not only advancing understanding around why traditional adversarial training might suffer more pronounced generalization issues post-training but potentially offering practical guidance toward developing new robust models capable of maintaining strong predictive power beyond just being resilient",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Revisiting Discrete Soft Actor-Critic",
        "abstract": "We study the adaption of Soft Actor-Critic (SAC), which is considered as a state-of-the-art reinforcement learning (RL) algorithm, from continuous action space to discrete action space. We revisit vanilla discrete SAC and provide an in-depth understanding of its Q value underestimation and performance instability issues when applied to discrete settings. We thereby propose Stable Discrete SAC (SDSAC), an algorithm that leverages entropy-penalty and double average Q-learning with Q-clip to address these issues. Extensive experiments on typical benchmarks with discrete action space, including Atari games and a large-scale MOBA game, show the efficacy of our proposed method. Our code is at: https://github.com/coldsummerday/SD-SAC.git.",
        "authors": "H. Zhou, T. Wei, Z. Lin, et.al",
        "keywords": [
            "discrete action space",
            "Soft Actor-Critic",
            "Stability"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=EUF2R6VBeU",
        "pdf_src": "https://api2.openreview.net/pdf/daa3e5ea91c54c912fad71745e931affb6c6d136.pdf",
        "Code_src": "https://github.com/coldsummerday/SD-SAC.git",
        "Introduction": "Background:\nThe paper focuses on adapting the Soft Actor-Critic (SAC), one of the most advanced reinforcement learning algorithms for continuous action spaces, into discrete action spaces.\n\nResearch Problem:\nWhen applying SAC directly to discrete actions, it often suffers from two main problems - Q-value underestimation leading to poor policy learning stability or performance instability due to exploration-exploitation trade-offs between different agents within multi-agent systems.\n \nMethod:\nTo solve this problem, we introduce Stable Discrete SAC (SDSAC). SDSAC incorporates entropy penalty during actor updates using a temperature schedule similar to GAIL's approach; employs Double Average Q-Learning along with Q-Clip to stabilize the Q-value estimation process by averaging over multiple samples before updating them individually while also ensuring clipped values stay close enough without being too conservative about exploration.\n\nMain Contributions:\nOur contributions include revisiting Vanilla Discrete SAC thoroughly analyzing why it struggles so much compared against Continuous Action Space counterparts like SAC itself – providing insights needed towards improving upon existing methods such as introducing entropy regularization terms akin those found within Generative Adversarial Imitation Learning (GAIL); proposing novel modifications based off observations made throughout analysis phase resulting in improved robustness across various environments tested via extensive experimentation conducted both on classic benchmark tasks involving discrete actions (like Atari Games) & real-world applications requiring complex decision-making capabilities present within Multi-Agent Battle Arena (MOBA) gaming genres where many autonomous entities must cooperate together efficiently toward common objectives amidst adversarial opponents attempting sabotage efforts simultaneously occurring elsewhere around map layout(s).",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Deep Tabular Learning via Distillation and Language Guidance",
        "abstract": "Tabular data is arguably one of the most ubiquitous data structures in application domains such as science, healthcare, finance and manufacturing. Given the recent success of deep learning (DL), there has been a surge of new DL models for tabular learning. However, despite the efforts, tabular DL models still clearly trail behind tree-based approaches. In this work, we propose DisTab, a novel framework for tabular learning based on the transformer architecture. Our method leverages model distillation to mimic the favorable inductive biases of tree-based models, and incorporates language guidance for more expressive feature embeddings. Empirically, DisTab outperforms existing tabular DL models and is highly competitive against tree-based models across diverse datasets, effectively closing the gap with these methods.",
        "authors": "R. Wang, W. Fu, C. Ciliberto",
        "keywords": [
            "transformer",
            "tabular learning",
            "model distillation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=p6KIteShzf",
        "pdf_src": "https://api2.openreview.net/pdf/ac5b8d741c5da7e991a33f4cbfea2ea5ec0489a1.pdf",
        "Code_src": "",
        "Introduction": "Background: Tabular data plays an important role in various fields like science, healthcare, finance ,and manufacturing. Recently, deep learning (DL) has gained significant attention due to its promising performance; however, it lags far behind tree-based approaches when applied to tabular data.\n\nResearch Question: How can we develop efficient and effective deep learning models specifically designed for tabular data?\n\nMethod: We introduce DisTab, which is a novel framework that employs the transformer architecture tailored for tabular learning. Specifically, our approach utilizes model distillation techniques to emulate the beneficial inductive biases inherent in tree-based models while incorporating linguistic instructions into the feature embedding process so they are richer and more descriptive.\n\nMain Contributions: The empirical results demonstrate that DisTab significantly surpasses current state-of-the-art tabular DL models by achieving better predictive accuracy than them under different scenarios using multiple datasets from varied domains including scientific research papers citations dataset, credit scoring dataset, and others. Furthermore, compared directly head-to-head with traditional tree-based algorithms - Decision Trees or Random Forests – DisTab also shows competitiveness indicating potential convergence towards parity between tree-based and neural network approaches within their respective strengths areas",
        "Topic": "Vision Transformer"
    },
    {
        "title": "No Identity, no problem: Motion through detection for people tracking",
        "abstract": "Tracking-by-detection has become the de facto standard approach to people tracking. To increase robustness, some approaches incorporate re-identification using appearance models and regressing motion offset, which requires costly identity annotations. In this paper, we propose exploiting motion clues while providing supervision only for the detections, which is much easier to do. \n\nOur algorithm predicts detection heatmaps at two different times, along with a 2D motion estimate between the two images. It then warps one heatmap using the motion estimate and enforces consistency with the other one. This provides the required supervisory signal on the motion without the need for any motion annotations. In this manner, we couple the information obtained from different images during training and increase accuracy, especially in crowded scenes and when using low frame-rate sequences. \n\nWe show that our approach delivers state-of-the-art results for single- and multi-view multi-target tracking on the MOT17 and WILDTRACK datasets.",
        "authors": "M. Engilberge, F. W. Grosche, P. Fua",
        "keywords": [
            "motion estimation",
            "detection heatmap",
            "crowd scene"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ogEM2H9IGK",
        "pdf_src": "https://api2.openreview.net/pdf/ff10871119ec2969899d56b9f55bb4990634a811.pdf",
        "Code_src": "",
        "Introduction": "Background: Tracking-by-detection has been widely used as an effective method for person tracking due to its simplicity compared to traditional methods such as correlation filters or optical flow-based trackers.\n\nResearch Problem: However, existing tracking-by-detection systems are often not very robust against occlusions because they rely heavily on feature matching rather than temporal continuity of trajectories across frames; thus incorporating additional cues like re-identification through appearance models can improve performance but comes at high cost associated with manual annotation efforts needed.\n \nMethod: The authors introduce a novel framework called \"tracking-without-motion-annotations\" where instead of manually annotating identities over timeframes within videos (which would be prohibitively expensive), supervision signals about motion offsets among detected objects alone suffice - enabling automatic coupling together visual features extracted sequentially throughout video sequences into coherent tracks even amidst challenging conditions including dense crowds and lower quality footage captured by cameras operating under suboptimal settings (e.g., low frame rates). \n \nMain Contributions: Their proposed system involves predicting detection heatmaps twice – once before applying camera motion estimation algorithms derived from pixel displacements observed between consecutive frames followed by another prediction after transformation based upon estimated camera movement applied back onto original coordinates space so both predicted outputs align spatially despite relative shifts introduced via camera translation/rotation effects occurring naturally whilst recording activities being tracked over time periods longer than those typically encountered daily life scenarios involving static subjects stationary environments). Furthermore, their work achieves competitive performances amongst top-performing trackers evaluated across benchmark datasets MOT17 & WILDTRACK demonstrating significant improvements particularly noticeable amid complex real-world surveillance tasks requiring reliable object recognition capabilities regardless environmental variations present therein.",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "Repositioning the Subject within Image",
        "abstract": "Current image manipulation primarily centers on static manipulation, such as replacing specific regions within an image or altering its overall style. In this paper, we introduce an innovative dynamic manipulation task, subject repositioning. This task involves relocating a user-specified subject to a desired position while preserving the image's fidelity. Our research reveals that the fundamental sub-tasks of subject repositioning, which include filling the void left by the repositioned subject, reconstructing obscured portions of the subject and blending the subject to be consistent with surrounding areas, can be effectively reformulated as a unified, prompt-guided inpainting task. Consequently, we can employ a single diffusion generative model to address these sub-tasks using various task prompts learned through our proposed task inversion technique. Additionally, we integrate pre-processing and post-processing techniques to further enhance the quality of subject repositioning. These elements together form our SEgment-gEnerate-and-bLEnd (SEELE) framework. To assess SEELE's effectiveness in subject repositioning, we assemble a real-world subject repositioning dataset called ReS. Results of SEELE on ReS demonstrate its efficacy. Code and ReS dataset are available at https://yikai-wang.github.io/seele/.",
        "authors": "Y. Wang, C. Cao, K. Fan, et.al",
        "keywords": [
            "repositioning",
            "inpainting",
            "diffusion generative model"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=orHH4fCtR8",
        "pdf_src": "https://api2.openreview.net/pdf/4ce4a68fc61c5141b62dd1fcdae17f1b4791148e.pdf",
        "Code_src": "https://yikai-wang.github.io/seele/",
        "Introduction": "Background: The current state of image manipulation mainly focuses on static alterations like region replacement or style transfer without considering movement.\n\nResearch Question: How do you create a system capable of dynamically manipulating images?\n\nMethodology: We propose \"subject repositioning,\" where objects must move from one place to another seamlessly so it looks natural against new surroundings—this is challenging because several tasks need solving simultaneously; for instance, covering up any gaps behind moved subjects (\"inpainting\"), rebuilding hidden parts due to occlusion, and ensuring consistency between the object being relocated and everything around them. \n\nMain Contributions:\n1. Task Unification: We transform all those complex subtasks into a singular inpainting problem guided by prompts.\n2. Model Adaptation: Using our invented method known as 'task inversion,' we train a diffusion generative model to handle each sub-task individually via different prompts.\n3. Framework Development: We've created the SEELE framework combining segmentation, generation, and blending steps along with preprocessing and postprocessing stages enhancing results significantly over existing methods.\n4. Dataset Creation: Alongside our framework, we have developed a novel dataset named ReS specifically designed for evaluating performance during subject repositioning tasks.\n\n\nFindings: Evaluation across the newly introduced ReS dataset shows that SEELE outperforms other systems when dealing with subject repositioning challenges presented therein.\n\nAvailability: All code related to SEELE and datasets used here may be accessed online under the link provided.",
        "Topic": "Generative Models"
    },
    {
        "title": "λ-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space",
        "abstract": "Despite the recent advances in personalized text-to-image (P-T2I) generative models, it remains challenging to perform finetuning-free multi-subject-driven T2I in a resource-efficient manner. Predominantly, contemporary approaches, involving the training of hypernetworks and Multimodal Large Language Models (MLLMs), require heavy computing resources that range from 600 to 12300 GPU hours of training. These subject-driven T2I methods hinge on Latent Diffusion Models (LDMs), which facilitate T2I mapping through cross-attention layers.  While LDMs offer distinct advantages, P-T2I methods' reliance on the latent space of these diffusion models significantly escalates resource demands, leading to inconsistent results and necessitating numerous iterations for a single desired image.\n\nThrough empirical evidences we find that CLIP (vision) latent space is already expressive enough to preserve the fine-grained details. Building upon this insight, in this paper, we present λ-ECLIPSE, a diffusion-agnostic prior-training strategy that works in the latent space of a pre-trained CLIP model without relying on the diffusion UNet models. λ-ECLIPSE leverages the image-text interleaved pre-training for fast and effective multi-subject-driven P-T2I. Through extensive experiments, we establish that λ-ECLIPSE surpasses existing baselines in composition alignment while preserving concept alignment performance, even with significantly lower resource utilization. λ-ECLIPSE performs multi-subject driven P-T2I with just 34M parameters and is trained on a mere 74 GPU hours. Additionally, λ-ECLIPSE demonstrates the unique ability to perform multi-concept interpolations. Project page: \\url{https://eclipse-t2i.github.io/Lambda-ECLIPSE/}",
        "authors": "M. Patel, S. Jung, C. Baral, et.al",
        "keywords": [
            "diffusion-agnostic",
            "multi-subject-driven",
            "resource-efficient"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=7q5UewlAdM",
        "pdf_src": "https://api2.openreview.net/pdf/f522b5e24b513d5300aef8529a5a96ceac172b3c.pdf",
        "Code_src": "项目页面链接：\\url{https://eclipse-t2i.github.io/Lambda-ECLIPSE/}",
        "Introduction": "Background:\nThe field of Personalized Text-to-Image (P-T2I) generation has seen significant advancements recently; however, there are challenges related to performing finetuning-free multi-subject-driven T2I efficiently within limited computational resources.\n \nResearch Problem:\nThe problem addressed by the research involves developing an efficient method for multi-subject-driven T2I generation using pre-trained language models such as CLIP without requiring expensive computation-intensive processes like those involved in training hypernetworks or multimodal large language models (MLLMs).\n \nMethodology:\nTo solve this issue, the authors introduce λ-ECLIPSE—a diffusion-agnostic prior-training strategy—which operates directly in the latent space of a pre-trained CLIP model rather than utilizing diffusion UNet models commonly used in other state-of-the-art methods. This approach takes advantage of the fact that the CLIP's vision latent space can effectively encode detailed information about images when paired with textual descriptions during pre-training phase. The proposed method does not rely heavily on computationally demanding iterative refinement steps but instead focuses on leveraging the inherent expressiveness of the latent space itself—hence its name \"λ-ECLIPSE\" referring to lambda, often denoting a scaling factor here implying how much influence the latent space should have over the final generated image.\n \nMain Contributions:\nThe main contributions include demonstrating λ-ECLIPSE’s effectiveness at achieving high-quality compositions aligned closely according to user instructions across multiple subjects compared against baseline techniques despite being less resource-intensive—it requires only around 34 million parameters versus millions more typically needed—and was trained quickly consuming fewer than 74 GPU hours overall time spent during training process. Moreover, unlike many current systems which focus solely on generating one type per iteration (single-concept), λ-ECLIPSE also allows users to smoothly interpolate between different concepts simultaneously—an added benefit enhancing versatility beyond what previous solutions could provide.",
        "Topic": "Generative Models"
    },
    {
        "title": "Degradation Attacks on Certifiably Robust Neural Networks",
        "abstract": "Certifiably robust neural networks protect against adversarial examples by employing run-time defenses that check if the model is certifiably locally robust at the input under evaluation. We show through examples and experiments that any defense (whether complete or incomplete) based on checking local robustness is inherently over-cautious. Specifically, such defenses flag inputs for which local robustness checks fail, but yet that are not adversarial; i.e., they are classified consistently with all valid inputs within a distance of $\\epsilon$. As a result, while a norm-bounded adversary cannot change the classification of an input, it can use norm-bounded changes to degrade the utility of certifiably robust networks by forcing them to reject otherwise correctly classifiable inputs. We empirically demonstrate the efficacy of such attacks against state-of-the-art certifiable defenses. Our code is available at https://github.com/ravimangal/degradation-attacks.\n\n",
        "authors": "K. Leino, C. Zhang, R. Mangal, et.al",
        "keywords": [
            "degradation attacks",
            "certifiably robust networks",
            "local robustness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=P0XO5ZE98j",
        "pdf_src": "https://api2.openreview.net/pdf/e3bddc504eae3c116983858b6682435d734f94a9.pdf",
        "Code_src": "https://github.com/ravimangal/degradation-attacks",
        "Introduction": "Background: The paper discusses the issue of adversarial examples in neural networks - perturbations introduced into images during training time designed to fool models without altering their appearance significantly enough humanly noticeable.\nResearch Problem: How do current defenses work? Are there potential weaknesses?\nMethod: The authors investigate whether these defenses may be too cautious when rejecting benign inputs due to minor variations from expected patterns around the original data points.\nMain Contributions: They identify a vulnerability where adversaries could exploit the overly conservative nature of some defenses (\"certifiably robust\" ones), leading to degradation rather than outright misclassification – effectively making the network less useful even though its predictions remain correct overall despite small perturbations near the true label boundary.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Infinitely wide limits for deep Stable neural networks: sub-linear, linear and super-linear activation functions",
        "abstract": "There is a growing literature on the study of large-width properties of deep Gaussian neural networks (NNs), i.e. deep NNs with Gaussian-distributed parameters or weights, and Gaussian stochastic processes. Motivated by some empirical and theoretical studies showing the potential of replacing Gaussian distributions with Stable distributions, namely distributions with heavy tails, in this paper we investigate large-width properties of deep Stable NNs, i.e. deep NNs with Stable-distributed parameters. For sub-linear activation functions, a recent work has characterized the infinitely wide limit of a suitable rescaled deep Stable NN in terms of a Stable stochastic process, both under the assumption of a  ``joint growth\" and under the assumption of a ``sequential growth\" of the width over the NN's layers. Here, assuming a ``sequential growth\" of the width, we extend such a characterization to a general class of activation functions, which includes sub-linear, asymptotically linear and super-linear functions. As a novelty with respect to previous works, our results rely on the use of  a generalized central limit theorem for heavy tails distributions, which allows for an interesting unified treatment of infinitely wide limits for deep Stable NNs. Our study shows that the scaling of Stable NNs and the stability of their infinitely wide limits may depend on the choice of the activation function, bringing out a critical difference with respect to the Gaussian setting.\n",
        "authors": "A. Bordino, S. Favaro, S. Fortini",
        "keywords": [
            "Stable distributions",
            "Deep Neural Networks",
            "Heavy-tailed Distributions"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=A5tIluhDW6",
        "pdf_src": "https://api2.openreview.net/pdf/045ccd8dbad35dc3af4b67f25047342b68a9e4f1.pdf",
        "Code_src": "",
        "Introduction": "Background: This research focuses on studying the characteristics known as \"large-width properties\" within deep Gaussian neural networks (\"deep NNs\") - these are networks where the parameters or weights have a Gaussian distribution – along with Gaussian stochastic processes.\n\nResearch Question: The question addressed here pertains to whether substituting Gaussian distributions used traditionally among deep NNs could be improved upon using stable distributions instead—distributions well-known because they possess heavier tails than Gaussians—which might lead to more robust performance across various datasets due to their ability to handle outliers better.\n\nMethodology: In order to explore if there indeed exists any advantage from switching to stable distributions when it comes to the behavior at very high widths—the so-called \"infinitely wide limit\"—of deep NNs, researchers examine how stable distributions affect the network’s learning capabilities through two different assumptions about the way the width grows throughout the layers:\n\n1. Joint Growth: Parameters grow simultaneously rather than incrementally layer-by-layer; \n2. Sequential Growth: Parameters increase one after another starting just before input data enters each subsequent layer.\n\nMain Contributions:\n- They provide new insights into characterizing the infinitely wide limit of certain types of deep NNs equipped with stable distributed parameters via a stable stochastic process based solely on sequential growth;\n- Their findings apply not only to sub-linear activation functions but also include asymptotic linearity and super-linearity activations—a broader range compared to past analyses focusing primarily on sub-linear functions alone;\n- A novel aspect lies in utilizing a generalized central limit theorem tailored specifically for heavy-tailed distributions like those found in stable distributions—an approach previously unexplored yet crucial given its implications towards understanding the scaling behaviors differently depending on what kind of activation function is chosen.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Incorporating Sum Constraints into Multitask Gaussian Processes",
        "abstract": "Machine learning models can be improved by adapting them to respect existing background knowledge. In this paper we consider multitask Gaussian processes, with background knowledge in the form of constraints that require a specific sum of the outputs to be constant. This is achieved by conditioning the prior distribution on the constraint fulfillment. The approach allows for both linear and nonlinear constraints. We demonstrate that the constraints are fulfilled with high precision and that the construction can improve the overall prediction accuracy as compared to the standard Gaussian process.",
        "authors": "P. Pilar, C. Jidling, T. B. Schön, et.al",
        "keywords": [
            "constraint",
            "multitask Gaussian processes",
            "prior distribution"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=gzu4ZbBY7S",
        "pdf_src": "https://api2.openreview.net/pdf/a64c0294f871eeff98c75a4dde9069d69168614f.pdf",
        "Code_src": "",
        "Introduction": "Background: Machine learning models often struggle when they do not have access to domain-specific information or \"background knowledge.\" Incorporating such knowledge into machine learning models could potentially lead to more accurate predictions.\n\nResearch Question: How might one adapt multitask Gaussian processes—a type of probabilistic model—to incorporate background knowledge expressed through constraints?\n\nMethod: To address this question, researchers propose conditioning the prior distribution of the multitask Gaussian process on whether these constraints hold true—essentially ensuring that any learned parameters satisfy certain pre-defined conditions related to output sums being constant over tasks.\n\nMain Contributions:\n1. They introduce an algorithm where the prior probability distribution of the Gaussian process is conditioned upon the satisfaction of given constraints.\n2. Their method accommodates both linear and nonlinear constraints without requiring modifications within the core inference machinery used during training time; thus it's straightforward to implement alongside other regularization techniques like dropout layers which may also impose some degree of constraint-like behavior but typically lack explicit mathematical formulation around their effect on parameter updates at each iteration step throughout optimization.\n3. Experimental validation shows significant improvements in predictive performance relative to non-constrained multitask Gaussian processes across various datasets demonstrating how incorporating background knowledge via constraints indeed enhances generalization capabilities beyond what would otherwise occur purely from data alone due solely to statistical regularities present therein.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "If your data distribution shifts, use self-learning",
        "abstract": "We demonstrate that self-learning techniques like entropy minimization and pseudo-labeling are simple and effective at improving performance of a deployed computer vision model under systematic domain shifts. We conduct a wide range of large-scale experiments and show consistent improvements irrespective of the model architecture, the pre-training technique or the type of distribution shift. At the same time, self-learning is simple to use in practice because it does not require knowledge or access to the original training data or scheme, is robust to hyperparameter choices, is straight-forward to implement and requires only a few adaptation epochs. This makes self-learning techniques highly attractive for any practitioner who applies machine learning algorithms in the real world. We present state-of-the-art adaptation results on CIFAR10-C (8.5% error), ImageNet-C (22.0% mCE), ImageNet-R (17.4% error) and ImageNet-A (14.8% error), theoretically study the dynamics of self-supervised adaptation methods and propose a new classification dataset (ImageNet-D) which is challenging even with adaptation.",
        "authors": "E. Rusak, S. Schneider, G. Pachitariu, et.al",
        "keywords": [
            "entropy minimization",
            "pseudo-labeling",
            "cross-dataset adaptation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=vqRzLv6POg",
        "pdf_src": "https://api2.openreview.net/pdf/547d35338f4b2e8f3f2cf42ad669c78f0e7bf1a0.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses challenges faced by computer vision models when they encounter systematic domain shifts after deployment.\n\nResearch Problem: How can we improve the performance of a deployed computer vision model affected by systematic domain shifts?\n\nMethods: The authors explore two self-learning techniques - entropy minimization and pseudo-labeling as potential solutions.\n- Entropy Minimization aims to reduce uncertainty about predictions during inference using regularization terms based on entropy.\n- Pseudo-Labeling involves labeling unseen examples from the shifted distribution without retraining but instead using the confidence scores outputted by the current model's predictions themselves.\n\nMain Contributions:\n1. Consistent Improvements Across Models & Shifts: The researchers conducted extensive tests across different architectures, pre-training approaches, and types of distribution shifts showing these self-learning techniques consistently improved accuracy regardless of context.\n2. Practicality: Self-learning was found easy-to-use due to its independence from initial training data/access, robustness against hyperparameters, straightforward implementation process requiring minimal adaptation epochs (\"few-shot\" adaptability).\n3. State-of-the-Art Adaptation Results: They achieved leading-edge adaptation metrics such as 8.5% error rate on CIFAR10-C, 22.0% mean cross-entropy error on ImageNet-C, etc., indicating significant practical benefits over existing methods.\n4. Theory Development: A theoretical analysis into the workings of self-supervised adaptation strategies has been provided along with insights gained through empirical observations within their research framework.\n5. New Dataset Creation: Introducing \"ImageNet-D,\" an adapted version of ImageNet designed specifically around detection tasks while being inherently more difficult than standard ImageNet datasets despite adaptations efforts made towards them.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Counterfactual Learning with Multioutput Deep Kernels",
        "abstract": "In this paper, we address the challenge of performing counterfactual inference with observational data via Bayesian nonparametric regression adjustment, with a focus on high-dimensional settings featuring multiple actions and multiple correlated outcomes. We present a general class of counterfactual multi-task deep kernels models that estimate causal effects and learn policies proficiently thanks to their sample efficiency gains, while scaling well with high dimensions. In the first part of the work, we rely on Structural Causal Models (SCM) to formally introduce the setup and the problem of identifying counterfactual quantities under observed confounding. We then discuss the benefits of tackling the task of causal effects estimation via stacked coregionalized Gaussian Processes and Deep Kernels. Finally, we demonstrate the use of the proposed methods on simulated experiments that span individual causal effects estimation, off-policy evaluation and optimization.",
        "authors": "A. Caron, I. Manolopoulou, G. Baio",
        "keywords": [
            "counterfactual inference",
            "Bayesian nonparametric regression",
            "multi-task deep kernel"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=iGREAJdULX",
        "pdf_src": "https://api2.openreview.net/pdf/0b6f2a62104056db13164d806e685e9cef9fa698.pdf",
        "Code_src": "",
        "Introduction": "Background: Counterfactual inference is challenging when dealing with observational data due to potential confounding factors which can lead to biased estimates if not properly accounted for. High-dimensional settings involving multiple actions and correlated outcomes further complicate matters.\n\nResearch Question: How do we perform accurate counterfactual inference in high-dimensional settings using observational data?\n\nMethodology: The authors propose a novel approach based on Bayesian Nonparametric Regression Adjustment combined with a General Class of Counterfactual Multi-Task Deep Kernel Models. This model leverages Sample Efficiency Gains through its architecture designed specifically for handling complex causal relationships across various tasks or actions within an environment.\n \nMain Contributions:\n1. Introduction of a new framework called Counterfactual Multi-Task Deep Kernel Models capable of estimating causal effects accurately even amidst high dimensionality by leveraging sample efficiency improvements from its design tailored towards complex causation scenarios among different tasks/actions.\n2. Utilization of Stacked Coregionalized Gaussian Processes along with Deep Kernels as tools aiding in tackling estimation problems related to causal effect identification effectively despite presence of confounding variables affecting observations indirectly leading toward more reliable results than traditional approaches would yield alone without considering such complexities arising outfrom real-world environments encountered during practical applications where these techniques might be applied successfully solving real-life challenges faced therein accordingly thereby improving decision-making processes significantly impacting overall performance positively over time",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "An approximate sampler for energy-based models with divergence diagnostics",
        "abstract": "Energy-based models (EBMs) allow flexible specifications of probability distributions. However, sampling from EBMs is non-trivial, usually requiring approximate techniques such as Markov chain Monte Carlo (MCMC). A major downside of MCMC sampling is that it is often impossible to compute the divergence of the sampling distribution from the target distribution: therefore, the quality of the samples cannot be guaranteed. Here, we introduce quasi-rejection sampling (QRS), a simple extension of rejection sampling that performs approximate sampling, but, crucially, does provide divergence diagnostics (in terms of f-divergences, such as KL divergence and total variation distance). We apply QRS to sampling from discrete EBMs over text for controlled generation. We show that we can sample from such EBMs with arbitrary precision in exchange for sampling efficiency and quantify the trade-off between the two by means of the aforementioned diagnostics. \n",
        "authors": "B. Eikema, G. Kruszewski, C. R. Dance, et.al",
        "keywords": [
            "energy-based models",
            "quasi-rejection sampling",
            "divergence diagnostics"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=VW4IrC0n0M",
        "pdf_src": "https://api2.openreview.net/pdf/a273f793b0dc635207ef511f4681a546ffb237bb.pdf",
        "Code_src": "",
        "Introduction": "Background: Energy-based models (EBMs) are probabilistic graphical models used widely due to their flexibility when specifying probability distributions.\n\nResearch Problem: Sampling from EBMs directly without using approximation methods like Markov Chain Monte Carlo (MCMC) has been challenging because these methods do not guarantee convergence or provide diagnostic tools on how close the sampled distribution is to the true target distribution.\n\nMethod: The paper introduces Quasi-Rejection Sampling (QRS), which extends Rejection Sampling while providing an approximate sampling method along with divergence diagnostics based on f-divergences including Kullback-Leibler divergence and Total Variation Distance.\n\nMain Contributions: This work applies QRS successfully towards sampling from discrete EBMs applied across text data domains where controlled generation requires high precision. It quantifies the trade-offs involved regarding both sampling efficiency versus achieving this level of precision through its novel diagnostics approach ensuring users have insights into what compromises they might need to make during model usage decisions related to computational resources vs desired output accuracy levels within energy-based frameworks involving textual information processing tasks represented via EBMs.",
        "Topic": "Generative Models"
    },
    {
        "title": "A Unified Survey on Anomaly, Novelty, Open-Set, and Out of-Distribution Detection: Solutions and Future Challenges",
        "abstract": "Machine learning models often encounter samples that are diverged from the training distribution. Failure to recognize an out-of-distribution (OOD) sample, and consequently assign that sample to an in-class label, significantly compromises the reliability of a model. The problem has gained significant attention due to its importance for safety deploying models in open-world settings. Detecting OOD samples is challenging due to the intractability of modeling all possible unknown distributions. To date, several research domains tackle\nthe problem of detecting unfamiliar samples, including anomaly detection, novelty detection, one-class learning, open set recognition, and out-of-distribution detection. Despite having similar and shared concepts, out-of-distribution, open-set, and anomaly detection\nhave been investigated independently. Accordingly, these research avenues have not crosspollinated, creating research barriers. While some surveys intend to provide an overview of these approaches, they seem to only focus on a specific domain without examining the\nrelationship between different domains. This survey aims to provide a cross-domain and comprehensive review of numerous eminent works in respective areas while identifying their commonalities. Researchers can benefit from the overview of research advances in different fields and develop future methodology synergistically. Furthermore, to the best of our knowledge, while there are surveys in anomaly detection or one-class learning, there is no comprehensive or up-to-date survey on out-of-distribution detection, which this survey covers extensively. Finally, having a unified cross-domain perspective, this study discusses and sheds light on future lines of research, intending to bring these fields closer together.",
        "authors": "M. Salehi, H. Mirzaei, D. Hendrycks, et.al",
        "keywords": [
            "out-of-distribution detection",
            "anomaly detection",
            "machine learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=aRtjVZvbpK",
        "pdf_src": "https://api2.openreview.net/pdf/a68b2219812d1c4376d76118d2b99e5019eb82b9.pdf",
        "Code_src": "",
        "Introduction": "Background: Machine learning models may face challenges when encountering data points outside of their original training distribution (\"out-of-distribution\" or \"OOD\"). Misclassification by assigning such outliers incorrect labels could lead to unreliable predictions.\n\nResearch Problem: How do we effectively detect instances belonging to classes other than those encountered during training?\n\nMethodology: Several methods exist within various research streams addressing this issue:\n1. Anomaly Detection - Identifying rare events.\n2. Novelty Detection - Recognizing new patterns unseen at training time but potentially present elsewhere ('open-set' approach).\n3. One-Class Learning - Training with just positive examples; generalizes well if it encounters novel ones later as negatives.\n4. Out-of-Distribution Detection - Specifically designed algorithms meant to identify inputs that don't fit typical class distributions.\n\nMain Contributions: \n- Synthesis across Domains: Combines insights into anomalies, novelty detection, one-class learning, and out-of-distribution detection under a single umbrella framework highlighting similarities among them despite independent investigation thus far—bridging gaps through interdisciplinary collaboration opportunities.\n- Comprehensive Review: Provides a broad coverage over seminal papers focusing on each area's contributions towards understanding how machine learning systems might generalize beyond what was seen before deployment—a critical consideration especially relevant given real-world applications where unexpected scenarios must be handled reliably.\n- Future Research Directions: Discusses potential directions forward integrating findings suggesting ways toward more robust system design capable handling diverse datasets encountered post-deployment.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Bayesian Methods for Constraint Inference in Reinforcement Learning",
        "abstract": "Learning constraints from demonstrations provides a natural and efficient way to improve the safety of AI systems; however, prior work only considers learning a single, point-estimate of the constraints. By contrast, we consider the problem of inferring constraints from demonstrations using a Bayesian perspective. We propose Bayesian Inverse Constraint Reinforcement Learning (BICRL), a novel approach that infers a posterior probability distribution over constraints from demonstrated trajectories. The main advantages of BICRL, compared to prior constraint inference algorithms, are (1) the freedom to infer constraints from partial trajectories and even from disjoint state-action pairs,  (2) the ability to infer constraints from suboptimal demonstrations and in stochastic environments, and (3) the opportunity to use the posterior distribution over constraints in order to implement active learning and robust policy optimization techniques. We show that BICRL outperforms pre-existing constraint learning approaches, leading to more accurate constraint inference and consequently safer policies. We further propose Hierarchical BICRL that infers constraints locally in sub-spaces of the entire domain and then composes global constraint estimates leading to accurate and computationally efficient constraint estimation.  ",
        "authors": "D. Papadimitriou, U. Anwar, D. S. Brown",
        "keywords": [
            "Bayesian Inference",
            "Constraint Reinforcement Learning",
            "Safety Improvement"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=oRjk5V9eDp",
        "pdf_src": "https://api2.openreview.net/pdf/bdbd7847e3240fd9a3f7638b114a785346735ecc.pdf",
        "Code_src": "",
        "Introduction": "Background: Learning constraints from demonstrations is an effective method for improving the safety of artificial intelligence (AI) systems by ensuring they operate within safe boundaries.\n\nResearch Problem: Prior works on this topic have primarily focused on learning a single, fixed estimate of these constraints without considering variability or uncertainty which can lead to less flexible solutions.\n \nMethod: This paper introduces Bayesian Inverse Constraint Reinforcement Learning (BICRL), adopting a Bayesian framework rather than just a deterministic one as previous methods did. It allows for probabilistic reasoning about the constraints inferred from demonstration data through a posterior probability distribution instead of a singular value.\n\nMain Contributions:\n1. Flexibility with Trajectories: Unlike other algorithms limited to complete trajectories where all actions must be observed before any constraint can be learned, BICRL can learn from incomplete trajectories including those with disjoint states and actions.\n   \n2. Handling Suboptimal Demonstrations/Stochastic Environments: BICRL does not require perfect demonstrations nor operates under strict assumptions like stationarity but adapts well enough when dealing with suboptimal or stochastic scenarios.\n\n3. Active Learning & Robust Policy Optimization: Utilizing the posterior distribution it learns, BICRL enables advanced machine learning strategies such as active learning—selecting informative examples—and robust policy optimization—ensuring policies remain stable despite variations in input distributions.\n\n4. Computational Efficiency: A hierarchical version proposed called Hierarchical BICRL improves computational efficiency while still maintaining accuracy across different domains via local constraint estimation followed by composition into global estimates.\n\nConclusion: Compared against existing constraint learning approaches, BICRL significantly enhances both the precision at which constraints are inferred along with resulting safer system behaviors due its probabilistic nature allowing for better handling of uncertainties present during real-world operation conditions.",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Reinventing Policy Iteration under Time Inconsistency",
        "abstract": "Policy iteration (PI) is a fundamental policy search algorithm in standard reinforcement learning (RL) setting, which can be shown to converge to an optimal policy by policy improvement theorems. However, the standard PI relies on Bellman’s Principle of Optimality, which might be violated by some specifications of objectives (also known as time-inconsistent (TIC) objectives), such as non-exponentially discounted reward functions. The use of standard PI under TIC objectives has thus been marked with questions regarding the convergence of its policy improvement scheme and the optimality of its termination policy, often leading to its avoidance. In this paper, we consider an infinite-horizon TIC RL setting and formally present an alternative type of optimality drawn from game theory, i.e., subgame perfect equilibrium (SPE), that attempts to resolve the aforementioned questions. We first analyze standard PI under the SPE type of optimality, revealing its merits and insufficiencies. Drawing on these observations, we propose backward Q-learning (bwdQ), a new algorithm in the approximate PI family that targets SPE policy under non-exponentially discounted reward functions. Finally, with two TIC gridworld environments, we demonstrate the implications of our theoretical findings on the behavior of bwdQ and other approximate PI variants.",
        "authors": "N. S. Lesmana, H. Su, C. S. Pun",
        "keywords": [
            "policy iteration",
            "time-inconsistent objectives",
            "subgame perfect equilibrium"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=bN2vWLTh0P",
        "pdf_src": "https://api2.openreview.net/pdf/cba218934c89194b7c789b8834b45354094666c5.pdf",
        "Code_src": "",
        "Introduction": "Background: Policy iteration (PI) is a widely used algorithm for solving Markov Decision Processes within the framework of Reinforcement Learning.\n\nResearch Problem: The standard version of PI assumes that the objective function does not change over time; however, many practical applications involve objectives where future rewards are given less weight than immediate ones due to discounting or stochastic volatility - scenarios referred to as \"time inconsistent\" because they lead agents into choosing actions today based solely on their impact at current times, rather than considering long-term consequences.\n \nMethodology: This study introduces Subgame Perfect Equilibrium (SPE) as another form of optimality suitable when dealing with time inconsistency issues arising out of non-exponential discounting policies using backward Q-learning (bwdQ). BwdQ aims specifically towards achieving SPEs while approximating the original PI process iteratively without requiring explicit computation across all possible states-action pairs during each step like traditional methods do.\n\nMain Contributions:\n1. They provide insights about how existing algorithms perform against different types of optimality criteria including SPE through analysis related to them;\n2. Introduce Backward Q-Learning – A novel approach designed explicitly around satisfying SPE constraints even if there's no exponential discount rate involved in defining those preferences;\n3. Validate their proposed method empirically via experiments conducted both theoretically and experimentally demonstrating improvements compared conventional approaches particularly relevant contexts involving delayed gratification decisions",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Nonparametric Learning of Two-Layer ReLU Residual Units",
        "abstract": "We describe an algorithm that learns two-layer residual units using rectified linear unit (ReLU) activation: suppose the input $\\mathbf{x}$ is from a distribution with support space $\\mathbb{R}^d$ and the ground-truth generative model is a residual unit of this type, given by $\\mathbf{y} = \\boldsymbol{B}^\\ast\\left[\\left(\\boldsymbol{A}^\\ast\\mathbf{x}\\right)^+ + \\mathbf{x}\\right]$, where ground-truth network parameters $\\boldsymbol{A}^\\ast \\in \\mathbb{R}^{d\\times d}$ represent a full-rank matrix with nonnegative entries and $\\boldsymbol{B}^\\ast \\in \\mathbb{R}^{m\\times d}$ is full-rank with $m \\geq d$ and for $\\boldsymbol{c} \\in \\mathbb{R}^d$, $[\\boldsymbol{c}^{+}]_i = \\max\\{0, c_i\\}$. We design layer-wise objectives as functionals whose analytic minimizers express the exact ground-truth network in terms of its parameters and nonlinearities. Following this objective landscape, learning residual units from finite samples can be formulated using convex optimization of a nonparametric function: for each layer, we first formulate the corresponding empirical risk minimization (ERM) as a positive semi-definite quadratic program (QP), then we show the solution space of the QP can be equivalently determined by a set of linear inequalities, which can then be efficiently solved by linear programming (LP). We further prove the strong statistical consistency of our algorithm, and demonstrate its robustness and sample efficiency through experimental results on synthetic data and a set of benchmark regression datasets.",
        "authors": "Z. Wang, L. He, C. Lyu, et.al",
        "keywords": [
            "residual units",
            "rectified linear unit (ReLU)",
            "empirical risk minimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=YiOI0vqJ0n",
        "pdf_src": "https://api2.openreview.net/pdf/d92050818fbcca3c06ce20576b227fe8da63fbc9.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper addresses the problem of learning neural networks represented by residual units based on rectified linear unit (ReLU) activations.\n\nResearch Problem:\nGiven an input $\\mathbf{x}$ drawn from a distribution with support space $\\mathbb{R}^d$ and assuming there exists a ground truth generative model consisting of two-layer residual units, how to learn such a model?\n\nMethods:\nThe authors propose designing layer-wise objectives expressed as functionals minimizing the empirical risk over finite samples via convex optimization techniques.\nSpecifically, they convert the empirical risk minimization into a positive semi-definite quadratic program (QP).\nThey also derive a set of linear inequalities equivalent to the QP's solution space allowing efficient computation within the framework of linear programming (LP).\n\nMain Contributions:\n1. The proposed method allows for learning two-layer residual units directly without requiring any additional assumptions or approximations about the underlying generative process other than it being a residual unit structure.\n2. They provide analytical solutions expressing the exact ground truth network in terms of its parameters and nonlinearities following their designed objectives.\n3. Prove the strong statistical consistency of the learned models under certain conditions ensuring convergence towards the true parameter values when more training examples are available \n4. Demonstrate the effectiveness  and efficiency of their approach both theoretically and empirically across various benchmarks datasets including synthetic ones",
        "Topic": "approximation"
    },
    {
        "title": "Stochastic Douglas-Rachford Splitting for Regularized Empirical Risk Minimization: Convergence, Mini-batch, and Implementation",
        "abstract": "In this paper, we study the stochastic Douglas-Rachford splitting (SDRS) for general empirical risk minimization (ERM) problems with regularization. Our first contribution is to prove its convergence for both convex and strongly convex problems; the convergence rates are $O(1/\\sqrt{t})$ and $O(1/t)$, respectively. Since SDRS reduces to the stochastic proximal point algorithm (SPPA) when there is no regularization, it is pleasing to see the result matches that of SPPA, under the same mild conditions. We also propose the mini-batch version of SDRS that handles multiple samples simultaneously while maintaining the same efficiency as that of a single one, which is not a straight-forward extension in the context of stochastic proximal algorithms. We show that the mini-batch SDRS again enjoys the same convergence rate. Furthermore, we demonstrate that, for some of the canonical regularized ERM problems, each iteration of SDRS can be efficiently calculated either in closed form or in close to closed form via bisection---the resulting complexity is identical to, for example, the stochastic (sub)gradient method. Experiments on real data demonstrate its effectiveness in terms of convergence compared to SGD and its variants.",
        "authors": "A. Bumin, K. Huang",
        "keywords": [
            "stochastic Douglas-Rachford splitting",
            "empirical risk minimization",
            "convergence rate"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=uvDD9rN6Zz",
        "pdf_src": "https://api2.openreview.net/pdf/6e9e8c1bcd4c2a6ecd94e180bafba1ca71448ab6.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper focuses on studying the stochastic Douglas-Rachford splitting (SDRS) algorithm applied to general empirical risk minimization (ERM) problems with regularization.\n\nResearch Problem: The main research problem addressed by the authors involves proving the convergence properties of the SDRS algorithm across different types of optimization problems - specifically, convex and strongly convex ones.\n \nMethods: The authors make two key contributions:\n1. They provide theoretical proofs demonstrating the convergence of the SDRS algorithm against both convex and strongly convex objectives at rates of $O(1/\\sqrt{t})$ and $O(1/t)$, where \\( t \\) represents the number of iterations taken during training.\n2. Additionally, they introduce an efficient mini-batch variant of the SDRS algorithm capable of processing multiple samples concurrently without sacrificing computational efficiency relative to the single-sample case.\n\nMain Contributions: Their primary findings include:\n\n- A convergence proof showing how well the SDRS algorithm performs over time according to the objective function's characteristics;\n- An extended application through their proposed mini-batch approach allowing parallel computation within certain constraints related to stochastic proximal algorithms;\n- Demonstrated practicality using experiments conducted on actual datasets comparing performance metrics such as convergence speed between SDRS and other state-of-the-art methods like Stochastic Gradient Descent (SGD).",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Modeling Object Dissimilarity for Deep Saliency Prediction",
        "abstract": "Saliency prediction has made great strides over the past two decades, with current techniques modeling low-level information, such as color, intensity and size contrasts, and high-level ones, such as attention and gaze direction for entire objects. Despite this, these methods fail to account for the dissimilarity between objects, which affects human visual attention. In this paper, we introduce a detection-guided saliency prediction network that explicitly models the differences between multiple objects, such as their appearance and size dissimilarities. Our approach allows us to fuse our object dissimilarities with features extracted by any deep saliency prediction network. As evidenced by our experiments, this consistently boosts the accuracy of the baseline networks, enabling us to outperform the state-of-the-art models on three saliency benchmarks, namely SALICON, MIT300 and CAT2000. Our project page is at https://github.com/IVRL/DisSal.",
        "authors": "B. Aydemir, D. Bhattacharjee, T. Zhang, et.al",
        "keywords": [
            "Detection-guided saliency prediction",
            "Object dissimilarity modeling",
            "Cross-benchmark performance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=NmTMc3uD1G",
        "pdf_src": "https://api2.openreview.net/pdf/a982d75845d00b06e2800999f9dd09eb9f2a0f78.pdf",
        "Code_src": "https://github.com/IVRL/DisSal",
        "Introduction": "Background: Saliency prediction aims to identify regions in an image where humans are most likely to direct their visual attention based on various factors like contrast levels or object importance.\n\nResearch Problem: Existing saliency prediction algorithms have primarily focused on either low-level cues related to colors, intensities, and sizes within images (\"bottom-up\" approaches), or higher-level cues including object parts' attention and gaze direction (\"top-down\" approaches). However, they often overlook one crucial aspect – the inherent variability among different objects themselves when it comes to what constitutes salient areas due to variations in shape, texture, etc.\n\nMethod: To address this issue, authors propose \"Detection-Guided Saliency Prediction Network,\" DisSal, designed specifically to model the disparities across distinct objects while predicting saliency maps using existing saliency prediction networks whose architectures can be varied depending on specific tasks without altering the core idea behind DisSal's integration process into them through feature fusion mechanisms.\n\nMain Contributions:\n1. The introduction of Detection-Guided Saliency Prediction Network (DisSal) capable of accounting for the diversity amongst objects.\n2. A novel method integrating object-specific dissimilarities derived from pre-trained detectors alongside standard saliency predictions provided by other networks via feature fusion strategies improving overall performance significantly compared against baselines tested extensively under three benchmark datasets - SALICON, MIT300, and CAT2000.\n3. Availability online along with source code accessible publicly allowing further researches to build upon its framework easily available here: https://github.com/IVRL/DisSal",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Learning Algorithms for Markovian Bandits:\\\\Is Posterior Sampling more Scalable than Optimism?",
        "abstract": "In this paper, we study the scalability of model-based algorithms learning the optimal policy of a discounted \\blue{rested} Markovian bandit  problem with $n$ arms. There are two categories of model-based reinforcement learning algorithms: Bayesian algorithms (like PSRL), and optimistic algorithms (like UCRL2 or UCBVI).  A naive application of these  algorithms is not scalable because  the state-space is exponential in $n$. In this paper, we construct variants of these algorithms specially tailored to Markovian bandits (MB) that we call MB-PSRL, MB-UCRL2, and MB-UCBVI. \\blue{We consider an episodic setting with geometrically distributed episode length, and measure the performance of the algorithm in terms of regret (Bayesian regret for MB-PSRL and expected regret for MB-UCRL2 and MB-UCBVI)}. We prove that, for this setting, all algorithms have a low regret in $\\tilde{O}(S\\sqrt{nK})$ -- where $K$ is the number of episodes, $n$ is the number of arms and $S$ is the number of states of each arm. Up to a factor $\\sqrt{S}$, these regrets match the \\blue{Bayesian minimax regret} lower bound of $\\Omega(\\sqrt{SnK})$ that we also derive.\n\nEven if their theoretical regrets are comparable, the {\\it time complexities} of these  algorithms vary greatly: We show that MB-UCRL2, as well as all  algorithms that use bonuses on transition matrices have a { time} complexity  that grows  exponentially in $n$.  In contrast, MB-UCBVI does not use bonuses on transition matrices and we show that  it can be implemented efficiently, with a time complexity linear in $n$. Our numerical experiments show, however, that its empirical regret is large. Our Bayesian algorithm, MB-PSRL, enjoys the best of both worlds: its running time is linear in the number of arms and its empirical regret is the smallest of all algorithms.\nThis is a new addition in the understanding of the power of Bayesian algorithms, that can often be  tailored to the structure of the problems to learn.",
        "authors": "N. Gast, B. Gaujal, K. Khun",
        "keywords": [
            "scalability",
            "Bayesian algorithms",
            "regret"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Sh3RF9JowK",
        "pdf_src": "https://api2.openreview.net/pdf/e722523b8adc82c9201c1e13a7226e21f5195901.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper focuses on studying the scalability of model-based algorithms used to solve a rested Markovian bandit problem involving \\( n \\) arms under discounting. This type of problem arises when there's uncertainty about which action will lead to higher rewards over multiple trials (\"arms\").\n\nResearch Problem:\nThe main challenge addressed by the authors revolves around the scalability issue faced during the implementation of existing model-based reinforcement learning algorithms like Bayesian algorithms (e.g., Probabilistic Sampled RL - PSRL) and optimistic algorithms (such as Upper Confidence Bound with Variance Importance Sampling - UCRL2; Upper Confidence Bound with Value Iteration - UCBVI). These algorithms typically do not scale effectively due to the exponential growth of the state space relative to the number of arms (\\( n \\)).\n\nMethods:\nTo address this scalability concern specifically within the context of Markovian bandits, three novel versions of those algorithms were developed:\n\n1. MB-PSRL – An adaptation of PSRL designed especially for Markovian bandits considering the episodic nature along with geometrically distributed episode lengths while measuring performance using Bayesian regret.\n\n2. MB-UCRL2 – A variant of UCRL2 optimized similarly but measured against expected regret instead since it doesn't directly apply Bayesian methods here.\n\n3. MB-UCBVI – Another adapted version focusing solely on upper confidence bounds without value iteration techniques nor bonus matrix usage leading to potentially more efficient computation at the cost of empirical regret compared to other algorithms.\n\nMain Contributions:\nThe primary contributions include:\n\na) Proving that regardless of whether they're Bayesian (MB-PSRL) or based purely on expectations (MB-UCRL2/MB-UCBVI), all proposed algorithms achieve low regret performance scaling as \\( O(S\\sqrt{nK}) \\).\n\nb) Establishing Bayesian minimax regret lower bounds of \\( \\Omega(\\sqrt{SnK}) \\) for comparison purposes indicating tightness of our results.\n\nc) Demonstrating significant differences in time complexities among various algorithms—while MB-UCRL2 has exponential complexity related to \\( n \\), MB-UCBVI offers linear-time efficiency despite having larger empirical regret than others.\n\nd) Highlighting the unique advantage of MB-PSRL combining linear runtime per arm with small empirical regret amongst tested algorithms thus far—a balance between computational efficiency and practical performance outcomes.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Does Entity Abstraction Help Generative Transformers Reason?",
        "abstract": "We study the utility of incorporating entity type abstractions into pre-trained Transformers and test these methods on four NLP tasks requiring different forms of logical reasoning: (1) compositional language understanding with text-based relational reasoning (CLUTRR), (2) abductive reasoning (ProofWriter), (3) multi-hop question answering (HotpotQA), and (4) conversational question answering (CoQA). We propose and empirically explore three ways to add such abstraction: (i) as additional input embeddings, (ii) as a separate sequence to encode, and (iii) as an auxiliary prediction task for the model. Overall, our analysis demonstrates that models with abstract entity knowledge performs better than without it. The best abstraction aware models achieved an overall accuracy of 88.8% and 91.8% compared to the baseline model achieving 62.9% and 89.8% on CLUTRR and ProofWriter respectively. However, for HotpotQA and CoQA, we find that F1 scores improve by only 0.5% on average. Our results suggest that the benefit of explicit abstraction is significant in formally defined logical reasoning settings requiring many reasoning hops, but point to the notion that it is less beneficial for NLP tasks having less formal logical structure.",
        "authors": "N. Gontier, S. Reddy, C. Pal",
        "keywords": [
            "entity type abstraction",
            "pre-trained Transformers",
            "logical reasoning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=9nhmKwLAWV",
        "pdf_src": "https://api2.openreview.net/pdf/26122d775d302fb65ccb81df7f53915bfdb8a7fa.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper explores how incorporating entity type abstractions can enhance the performance of pre-trained Transformer models across various natural language processing (NLP) tasks involving complex logical reasoning.\n\nResearch Problem:\nThe central research problem addressed here concerns whether integrating entity type information improves the ability of Transformer-based systems when dealing with linguistic challenges necessitating diverse types of logical deduction—such as those found in compositional language understanding from textual relations (\"CLUTRR\"), abduction reasoning (\"ProofWriter\"), multi-hop question answering (\"HotpotQA\"), or conversational question answering (\"CoQA\").\n\nMethods:\nTo tackle this issue, authors introduce several strategies aimed at embedding entity type knowledge within the Transformer framework:\n\n1. Adding entity type embeddings alongside other inputs.\n2. Encoding entity types separately through their own sequences before merging them back together during inference.\n3. Employing an auxiliary prediction task where the model learns to predict entity types along with its primary objective function.\n\nMain Contributions:\nThe empirical findings indicate substantial improvements due to the integration of entity type abstractions under certain conditions; specifically, they significantly outperform baselines particularly well on \"CLUTRR\" scoring up to nearly 30 percentage points higher in terms of accuracy while also showing gains around ten percentage points improvement over the original model's score regarding \"ProofWriter.\" Conversely, though there are some marginal increases observed—a maximum gain being about half a percent—for \"HotpotQA\" and \"CoQA,\" suggesting that the benefits may be more pronounced against tasks demanding extensive logical reasoning steps rather than those which do not require strict formal logic structures extensively.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Complex-Valued Autoencoders for Object Discovery",
        "abstract": "Object-centric representations form the basis of human perception, and enable us to reason about the world and to systematically generalize to new settings. Currently, most works on unsupervised object discovery focus on slot-based approaches, which explicitly separate the latent representations of individual objects. While the result is easily interpretable, it usually requires the design of involved architectures. In contrast to this, we propose a comparatively simple approach – the Complex AutoEncoder (CAE) – that creates distributed object-centric representations. Following a coding scheme theorized to underlie object representations in biological neurons, its complex-valued activations represent two messages: their magnitudes express the presence of a feature, while the relative phase differences between neurons express which features should be bound together to create joint object representations. In contrast to previous approaches using complex-valued activations for object discovery, we present a fully unsupervised approach that is trained end-to-end – resulting in significant improvements in performance and efficiency. Further, we show that the CAE achieves competitive or better unsupervised object discovery performance on simple multi-object datasets compared to a state-of-the-art slot-based approach while being up to 100 times faster to train.",
        "authors": "S. Löwe, P. Lippe, M. Rudolph, et.al",
        "keywords": [
            "Complex AutoEncoder",
            "Distributed Object-Centric Representations",
            "Unsupervised Object Discovery"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=1PfcmFTXoa",
        "pdf_src": "https://api2.openreview.net/pdf/b52c4490b249462a083d65cb2b4d200c000ab9de.pdf",
        "Code_src": "",
        "Introduction": "Background:\nHuman perception relies heavily on object-centric representations as they allow reasoning over the environment through systematic generalization.\n\nResearch Problem:\nMost existing methods focusing on unsupervised object discovery are based on slot-based approaches where each object's representation is separated into distinct slots; however, these require architecture design leading to interpretability trade-offs with complexity.\n\nMethod:\nWe introduce an alternative method called the Complex AutoEncoder (CAE), designed without explicit separation but rather creating distributed object-centric representations by following a coding scheme inspired by how object representations might arise from biological neurons' activities.\nThe CAE uses complex-valued activations whose magnitude indicates feature presence whereas phase differences among neurons encode feature bindings necessary for joint object representations.\n\nMain Contributions:\nOur main contributions include:\n\n1. A novel, relatively simpler approach than prior work involving complex-valued activations;\n2. An entirely unsupervised training process yielding significantly improved performance and efficiency when comparing against other methods;\n3. Demonstrated competitiveness—or even superiority—to current state-of-the-art slot-based techniques across various multi-object datasets despite requiring orders of magnitude less time during training.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Optimizing Intermediate Representations of Generative Models for Phase Retrieval",
        "abstract": "Phase retrieval is the problem of reconstructing images from magnitude-only measurements. In many real-world applications the problem is underdetermined. When training data is available, generative models allow optimization in a lower-dimensional latent space, hereby constraining the solution set to those images that can be synthesized by the generative model. However, not all possible solutions are within the range of the generator. Instead, they are represented with some error. To reduce this representation error in the context of phase retrieval, we first leverage a novel variation of intermediate layer optimization (ILO) to extend the range of the generator while still producing images consistent with the training data. Second, we introduce new initialization schemes that further improve the quality of the reconstruction. With extensive experiments on the Fourier phase retrieval problem and thorough ablation studies, we can show the benefits of our modified ILO and the new initialization schemes. Additionally, we analyze the performance of our approach on the Gaussian phase retrieval problem.",
        "authors": "T. Uelwer, S. Konietzny, S. Harmeling",
        "keywords": [
            "phase retrieval",
            "generative models",
            "initialization schemes"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=YAVE6jfeJb",
        "pdf_src": "https://api2.openreview.net/pdf/4944d1138d8559917f8bca72e173bd3835bf9998.pdf",
        "Code_src": "",
        "Introduction": "Background: Phase retrieval aims at reconstructing an image given only its magnitude spectrum without any prior information about the phases.\n\nResearch Problem: The traditional phase retrieval algorithms often fail when dealing with underdetermined problems due to the non-unique nature of the phase retrieval process.\n \nMethods: We tackle these challenges through two main contributions. Firstly, we propose a novel variant of Intermediate Layer Optimization (ILO), which extends the capability of the generator beyond what it was originally trained for but maintains consistency with existing training data. Secondly, we develop several new initialization strategies designed specifically for improving the quality of the reconstructed images.\n\nMain Contributions: Our work introduces modifications into both the optimization algorithm - ILO - as well as the initial conditions used during the generation step leading to more accurate reconstructions than previous methods could achieve especially noticeable improvements where there's limited or incomplete information present such as in underdetermined cases encountered commonly in practical scenarios like astronomical imaging etc.",
        "Topic": "Generative Models"
    },
    {
        "title": "Teacher’s pet: understanding and mitigating biases in distillation",
        "abstract": "Knowledge distillation is widely used as a means of improving the performance of a relatively simple ``student'' model using the predictions from a complex ``teacher'' model. Several works have shown that distillation significantly boosts the student's \\emph{overall} performance; however, are these gains uniform across all data subgroups? In this paper, we show that distillation can \\emph{harm} performance on  certain subgroups, {e.g., classes with few associated samples}, compared to the vanilla student trained using the one-hot labels. We trace this behaviour to errors made by the teacher distribution being transferred to and \\emph{amplified} by the student model, and formally prove that distillation can indeed harm underrepresented subgroups in certain regression settings. To mitigate this problem, we present techniques  which soften the teacher influence for subgroups where it is less reliable. Experiments on several image classification benchmarks show that these modifications of distillation maintain boost in overall accuracy, while additionally ensuring improvement in subgroup performance. ",
        "authors": "M. Lukasik, S. Bhojanapalli, A. K. Menon, et.al",
        "keywords": [
            "distillation",
            "performance degradation",
            "subgroup fairness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ph3AYXpwEb",
        "pdf_src": "https://api2.openreview.net/pdf/38984a83805182b9169a39f621d958364b9404d6.pdf",
        "Code_src": "",
        "Introduction": "Background: Knowledge distillation has been commonly employed to enhance the performance of simpler \"student\" models through leveraging the predictions generated by more sophisticated \"teacher\" models.\n\nResearch Question: Despite previous studies demonstrating significant improvements in the overall performance of students via distillation, whether such enhancements apply uniformly across different subsets of data remains unclear.\n\nMethodology: The study investigates how knowledge distillation affects various subgroups within datasets - specifically focusing on those with fewer examples available per class (\"underrepresented subgroups\"). It identifies issues arising when the student model amplifies inaccuracies introduced during the transfer process between the teacher and itself – leading to potential degradation rather than enhancement among specific subgroups' performances due to distillation.\n\nMain Contributions:\n1. Identification & Explanation: The research uncovers instances wherein distillation may negatively impact the performance particularly noticeable amongst underrepresented subgroups.\n2. Formal Proof: A theoretical framework is provided showing why distillation could potentially be harmful towards underrepresented groups depending upon the regression scenario considered.\n3. Proposed Solution: Techniques designed to reduce the influence of the teacher model over underrepresented subgroups were developed aiming at mitigating any negative effects observed earlier without compromising the generalization ability or overall performance benefits gained elsewhere throughout training. \n4. Experimental Validation: These proposed adjustments resulted in maintaining an increase in average accuracy metrics whilst also observing improved results especially beneficial toward previously mentioned underrepresented subgroups tested against multiple benchmark datasets related to image classification tasks.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "On Pseudo-Labeling for Class-Mismatch Semi-Supervised Learning",
        "abstract": "When there are unlabeled Out-Of-Distribution (OOD) data from other classes, Semi-Supervised Learning (SSL) methods suffer from severe performance degradation and even get worse than merely training on labeled data. In this paper, we empirically analyze Pseudo-Labeling (PL) in class-mismatched SSL. PL is a simple and representative SSL method that transforms SSL problems into supervised learning by creating pseudo-labels for unlabeled data according to the model's prediction.  We aim to answer two main questions: (1) How do OOD data influence PL? (2) What is the proper usage of OOD data with PL?  First, we show that the major problem of PL is imbalanced pseudo-labels on OOD data. Second, we find that OOD data can help classify In-Distribution (ID) data given their OOD ground truth labels. Based on the findings, we propose to improve PL in class-mismatched SSL with two components -- Re-balanced Pseudo-Labeling (RPL) and Semantic Exploration Clustering (SEC). RPL re-balances pseudo-labels of high-confidence data, which simultaneously filters out OOD data and addresses the imbalance problem. SEC uses balanced clustering on low-confidence data to create pseudo-labels on extra classes, simulating the process of training with ground truth. Experiments show that our method achieves steady improvement over supervised baseline and state-of-the-art performance under all class mismatch ratios on different benchmarks.",
        "authors": "L. Han, H. Ye, D. Zhan",
        "keywords": [
            "class-mismatched SSL",
            "Pseudo-Labeling",
            "OOD data"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=tLG26QxoD8",
        "pdf_src": "https://api2.openreview.net/pdf/5e1488525862667c683eb07fcca69e17692895d2.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this research lies within the field of semi-supervised learning (SSL), where models learn using both labeled and unlabeled data points without requiring any human supervision beyond initial labeling.\n\nResearch Problem:\nThe primary challenge addressed here concerns the use of pseudo-labeling techniques when dealing with semi-supervised learning tasks involving out-of-distribution (OOD) data - i.e., instances not originally intended or encountered during training but may be present at test time due to distribution shift between datasets.\nSpecifically, researchers seek answers regarding how OOD data affect pseudo-labeling processes used in SSL settings characterized by class mismatches – meaning that some of the labeled data comes from classes different from those seen while training the model; and what constitutes appropriate utilization strategies incorporating these potentially confounding elements.\n\nMethodology:\nTo address issues related to pseudo-labeling amidst class mismatches caused by OOD data, empirical analysis was conducted focusing primarily around:\n\n1. Identifying why pseudo-labeling often performs poorly against OOD samples leading to biased predictions;\n2. Determining whether leveraging such misclassified examples could actually enhance classification accuracy if properly managed;\n\nBased upon insights gained through experiments analyzing various aspects including label imbalance among pseudo-labeled examples derived solely based on predicted probabilities versus more nuanced approaches like ours proposed below, they developed an improved pseudo-labeling technique called \"Re-balanced Pseudo-Labeling\" (RPL).\n\nMain Contributions:\nThis work makes several significant contributions toward advancing understanding about pseudo-labeling applied across heterogeneous domains where distributions diverge significantly compared traditional homogeneous setups commonly found elsewhere in machine learning literature:\n1. Demonstrated existence of substantial bias introduced via pseudo-labeling procedures utilizing only confidence scores resulting directly from trained models' predictions rather than considering additional factors influencing true positive/negative cases;\n2. Introduced novel improvements termed “Re-balanced Pseudo-Labeling” & “Semantic Exploration Clustering”, designed specifically addressing aforementioned biases inherent within existing pseudo-labeling methodologies;\n3. Provided experimental evidence validating efficacy achieved w.r.t supervised baselines as well as current state-of-the-art performances across multiple benchmark datasets regardless varying degrees of class mismatch ratio involved therein.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Algorithms and Theory for Supervised Gradual Domain Adaptation",
        "abstract": "The phenomenon of data distribution evolving over time has been observed in a range of applications, calling the needs of adaptive learning algorithms. We thus study the problem of supervised gradual domain adaptation, where labeled data from shifting distributions are available to the learner along the trajectory, and we aim to learn a classifier on a target data distribution of interest. Under this setting, we provide the first generalization upper bound on the learning error under mild assumptions. Our results are algorithm agnostic, general for a range of loss functions, and only depend linearly on the averaged learning error across the trajectory. This shows significant improvement compared to the previous upper bound for unsupervised gradual domain adaptation, where the learning error on the target domain depends exponentially on the initial error on the source domain. Compared with the offline setting of learning from multiple domains, our results also suggest the potential benefits of the temporal structure among different domains in adapting to the target one. Empirically, our theoretical results imply that learning proper representations across the domains will effectively mitigate the learning errors. Motivated by these theoretical insights, we propose a min-max learning objective to learn the representation and classifier simultaneously. Experimental results on both semi-synthetic and large-scale real datasets corroborate our findings and demonstrate the effectiveness of our objectives. ",
        "authors": "J. Dong, S. Zhou, B. Wang, et.al",
        "keywords": [
            "time-varying data distribution",
            "supervised gradual domain adaptation",
            "min-max learning objective"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=35y5hv9fbb",
        "pdf_src": "https://api2.openreview.net/pdf/e3608913125f05ca39b054359b0500cbbbc8467b.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses an issue common in various fields – how do machine learning models adapt when there is change or evolution in their training data? Specifically, it looks at \"supervised gradual domain adaptation,\" which involves using labeled examples as they become available during changes.\n\nResearch Problem: How can learners efficiently adjust to new targets while being exposed incrementally to changing sources?\n\nMethods: The authors develop a novel approach without specifying any particular learning algorithm; instead focusing on providing guarantees about performance regardless of chosen methods through a generalization upper bound analysis based on minimal assumptions regarding the nature of the shifts between distributions.\n \nMain Contributions:\n1. They establish what appears to be the first generalization upper bound concerning learning error within such settings—this bound does not rely heavily on specific parameters but scales linearly relative to average learning progress throughout the process.\n2. Their bounds significantly outperform those previously established for unsupervised gradual domain adaptation because here, unlike before, the final learning error doesn't scale exponentially depending upon early-stage errors—it's more manageable due to the incremental exposure provided via supervision.\n3. Additionally, empirical evidence supports theory suggesting that developing appropriate representations could reduce overall learning errors—a finding potentially beneficial beyond just gradual domain adaptation scenarios into broader multi-domain learning contexts.\n4. To practically apply these theoretical insights broadly yet flexibly enough given varying conditions encountered during adaptation tasks, researchers introduce a \"min-max\" learning framework designed jointly optimizing representation extraction capabilities alongside classification accuracy goals.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Unifying Approaches in Active Learning and Active Sampling via Fisher Information and Information-Theoretic Quantities",
        "abstract": "Recently proposed methods in data subset selection, that is active learning and active sampling, use Fisher information, Hessians, similarity matrices based on gradients, and gradient lengths to estimate how informative data is for a model’s training. Are these different approaches connected, and if so, how? We revisit the fundamentals of Bayesian optimal experiment design and show that these recently proposed methods can be understood as approximations to information-theoretic quantities: among them, the mutual information between predictions and model parameters, known as expected information gain or BALD in machine learning, and the mutual information between predictions of acquisition candidates and test samples, known as expected predictive information gain. We develop a comprehensive set of approximations using Fisher information and observed information and derive a unified framework that connects seemingly disparate literature. Although Bayesian methods are often seen as separate from non-Bayesian ones, the sometimes fuzzy notion of “informativeness” expressed in various non-Bayesian objectives leads to the same couple of information quantities, which were, in principle, already known by Lindley (1956) and MacKay (1992).",
        "authors": "A. Kirsch, Y. Gal",
        "keywords": [
            "data subset selection",
            "active learning",
            "mutual information"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=UVDAKQANOW",
        "pdf_src": "https://api2.openreview.net/pdf/f04a535361593d8deb75b1477444542d88541125.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses recent advancements in data subset selection techniques such as active learning and active sampling used during model training processes.\n\nResearch Question:\nThe main research question addressed here pertains to whether there exists any connection amongst diverse methods estimating informativeness within datasets through metrics like Fisher Information, Hessians, similarity matrices derived from gradients, and gradient magnitudes.\n \nMethodology:\nTo address this query comprehensively, researchers delve into Bayesian optimal experimental designs revisiting their fundamental principles while demonstrating an equivalence with several aforementioned methodologies. They propose new approximations leveraging both Fisher Information and Observed Information leading towards a unifying theoretical framework connecting previously disparate literatures together.\n\nMain Contributions:\nThis work introduces novel insights regarding the relationship existing across distinct approaches aimed at quantifying dataset informativeness toward enhancing model performance via active learning strategies; it also highlights connections back to foundational works dating decades ago including those conducted by Lindley & MacKay respectively emphasizing consistency despite differences in underlying philosophies employed throughout time",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "A Crisis In Simulation-Based Inference? Beware, Your Posterior Approximations Can Be Unfaithful",
        "abstract": "We present extensive empirical evidence showing that current Bayesian simulation-based inference algorithms can produce computationally unfaithful posterior approximations. Our results show that all benchmarked algorithms -- (S)NPE, (S)NRE, SNL and variants of ABC -- can yield overconfident posterior approximations, which makes them unreliable for scientific use cases and falsificationist inquiry. Failing to address this issue may reduce the range of applicability of simulation-based inference. For this reason, we argue that research efforts should be made towards theoretical and methodological developments of conservative approximate inference algorithms and present research directions towards this objective. In this regard, we show empirical evidence that ensembling posterior surrogates provides more reliable approximations and mitigates the issue.",
        "authors": "J. Hermans, A. Delaunoy, F. Rozet, et.al",
        "keywords": [
            "computational unfaithfulness",
            "posterior approximation",
            "conservative approximate inference"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LHAbHkt6Aq",
        "pdf_src": "https://api2.openreview.net/pdf/1108913a24b803e711ddf7eba39be12b21378e20.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses the limitations of current Bayesian simulation-based inference algorithms in producing faithful posterior approximations.\n\nResearch Problem: The problem addressed by the paper is whether these algorithms are capable of yielding accurate and reliable posterior approximations or not.\n \nMethods: The authors conducted an empirical study using various benchmarks such as (S)NPE, (S)NRE, SNL etc., to evaluate the performance of these algorithms.\n\nMain Contributions: The main contribution of the paper includes demonstrating through empirical evidence that existing algorithms often lead to overconfident posterior approximations making them unsuitable for scientific applications like falsificationism. They also propose a solution involving ensemble methods with surrogate posteriors leading to better reliability while addressing computational issues related to Bayesian simulations.",
        "Topic": "approximation"
    },
    {
        "title": "Action Noise in Off-Policy Deep Reinforcement Learning: Impact on Exploration and Performance",
        "abstract": "Many Deep Reinforcement Learning (D-RL) algorithms rely on simple forms of exploration\nsuch as the additive action noise often used in continuous control domains. Typically,\nthe scaling factor of this action noise is chosen as a hyper-parameter and is kept constant\nduring training. In this paper, we focus on action noise in off-policy deep reinforcement\nlearning for continuous control. We analyze how the learned policy is impacted by the noise\ntype, noise scale, and impact scaling factor reduction schedule. We consider the two most\nprominent types of action noise, Gaussian and Ornstein-Uhlenbeck noise, and perform a vast\nexperimental campaign by systematically varying the noise type and scale parameter, and\nby measuring variables of interest like the expected return of the policy and the state-space\ncoverage during exploration. For the latter, we propose a novel state-space coverage measure\n$\\operatorname{X}_{\\mathcal{U}\\text{rel}}$ that is more robust to estimation artifacts caused by points close to the\nstate-space boundary than previously-proposed measures. Larger\nnoise scales generally increase state-space coverage. However, we found that increasing the\nspace coverage using a larger noise scale is often not beneficial. On the contrary, reducing\nthe noise scale over the training process reduces the variance and generally improves the\nlearning performance. We conclude that the best noise type and scale are environment\ndependent, and based on our observations derive heuristic rules for guiding the choice of the\naction noise as a starting point for further optimization.\n",
        "authors": "J. Hollenstein, S. Auddy, M. Saveriano, et.al",
        "keywords": [
            "exploration strategies",
            "state-space coverage",
            "learning performance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=NljBlZ6hmG",
        "pdf_src": "https://api2.openreview.net/pdf/52bc9c3e9cc9548a60cd1a61b1912d9ea023f329.pdf",
        "Code_src": "",
        "Introduction": "Background: This research focuses on exploring different types of action noise commonly utilized in many D-RL algorithms such as Gaussian and Ornstein-Uhlenbeck noise within an off-policy setting specifically designed for continuous control tasks.\n\nResearch Problem: The study aims at understanding whether there exists any optimal configuration regarding the noise parameters which can improve learning outcomes or if they simply serve as a stabilizing mechanism without significant contribution towards better policies being learned.\n\nMethodology: A comprehensive experimental approach was adopted where both noise types were considered with various levels of scale applied across multiple iterations throughout training sessions; metrics including expected returns from the policy along with state-space coverage under exploration conditions have been measured against these configurations so findings could be quantitatively assessed accordingly.\n\nMain Contributions:\n1. Novel State-Space Coverage Measure $\\operatorname{X}_{\\mathcal{U}\\text{rel}}$ proposed - More robust compared existing ones due its resistance toward estimation errors near boundaries leading into improved accuracy when evaluating coverage rates amidst exploratory phases;\n2. Insightful Observations & Heuristic Rules Identified – Based upon empirical evidence gathered through experiments conducted here it has become evident that ideal choices depend heavily depending upon specific environments involved suggesting no universal solution but rather adaptive approaches tailored according each context individually would yield superior results overall",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Competition over data: how does data purchase affect users?",
        "abstract": "As the competition among machine learning (ML) predictors is widespread in practice, it becomes increasingly important to understand the impact and biases arising from such competition. One critical aspect of ML competition is that ML predictors are constantly updated by acquiring additional data during the competition. Although this active data acquisition can largely affect the overall competition environment, it has not been well-studied before. In this paper, we study what happens when ML predictors can purchase additional data during the competition. We introduce a new environment in which ML predictors use active learning algorithms to effectively acquire labeled data within their budgets while competing against each other. We empirically show that the overall performance of an ML predictor improves when predictors can purchase additional labeled data. Surprisingly, however, the quality that users experience---i.e., the accuracy of the predictor selected by each user---can decrease even as the individual predictors get better. We demonstrate that this phenomenon naturally arises due to a trade-off whereby competition pushes each predictor to specialize in a subset of the population while data purchase has the effect of making predictors more uniform. With comprehensive experiments, we show that our findings are robust against different modeling assumptions.",
        "authors": "Y. Kwon, T. A. Ginart, J. Zou",
        "keywords": [
            "purchase additional data",
            "active learning algorithms",
            "performance improvement"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=63sJsCmq6Q",
        "pdf_src": "https://api2.openreview.net/pdf/c01dc2fbd7c792dd8a0cf7e7611ed05b85624b96.pdf",
        "Code_src": "",
        "Introduction": "Background: The increasing competitiveness between machine learning (ML) predictors necessitates understanding how they interact with one another's updates through continuous data acquisition.\n\nResearch Question: How does allowing ML predictors to buy additional data influence competitive dynamics?\n\nMethodology: Introducing a novel competitive setting where ML predictors employ active learning strategies for budget-constrained label data procurement amidst rivalry.\n\nMain Contributions:\n1. Identification of a previously unexplored scenario involving ML predictors purchasing extra training data.\n2. Creation of a controlled experimental framework demonstrating the effects on prediction performance under these conditions.\n3. Empirical evidence showing improved aggregate predictive ability but paradoxical decreases in predicted accuracy experienced by end-users (\"the selection problem\").\n4. Unveiling underlying trade-offs resulting from specialized predictions versus uniformity induced by data purchases across competitors.\n5. Robustness validation via extensive experimentation addressing various theoretical models' potential impacts.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "An Efficient One-Class SVM for Novelty Detection in IoT",
        "abstract": "One-Class Support Vector Machines (OCSVM) are a common approach for novelty detection, due to their flexibility in fitting complex nonlinear boundaries between {normal} and {novel} data.  Novelty detection is important in the Internet of Things (``IoT'') due to the threats these devices can present, and OCSVM often performs well in these environments due to the variety of devices, traffic patterns, and anomalies that IoT devices present. Unfortunately, conventional OCSVMs can introduce prohibitive memory and computational overhead at detection time. This work designs, implements and evaluates an efficient OCSVM for such practical settings.  We extend Nystr\\\"om and (Gaussian) Sketching approaches to OCSVM, combining these methods with clustering and Gaussian mixture models to achieve 15-30x speedup in prediction time and 30-40x reduction in memory requirements without sacrificing detection accuracy. Here, the very nature of IoT devices is crucial: they tend to admit few modes of \\emph{normal} operation, allowing for efficient pattern compression. ",
        "authors": "K. Yang, S. Kpotufe, N. Feamster",
        "keywords": [
            "IoT",
            "One-Class SVM",
            "Efficiency"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LFkRUCalFt",
        "pdf_src": "https://api2.openreview.net/pdf/3f89fbfa01969cbbab11ef4f472fa8b82598ebf6.pdf",
        "Code_src": "",
        "Introduction": "Background: One-Class Support Vector Machines (OCSVM) have been widely used as a novel detection method because it has good performance on detecting outliers or novelties by learning only normal samples while ignoring abnormal ones.\n\nResearch Problem: However, traditional OCSVM algorithms may cause excessive memory and computation costs during the detection process which limits its application scenarios especially when dealing with large-scale datasets like those from the Internet of Things (IoT).\n\nMethod: In this paper, we propose an improved version of OCSVM algorithm suitable for practical applications involving IoT systems where device behavior tends to be relatively stable leading to fewer normal operating modes thus enabling more efficient feature extraction using sketching techniques combined with clustering and Gaussian mixture models resulting in significant improvements over standard OCSVM implementations terms of both runtime efficiency & resource utilization.\n\nMain Contributions: The main contributions include designing implementing evaluating an efficient OCSVM tailored specifically towards IoT domains leveraging advanced algorithms such as Nystr\\\"om approximation along with Gaussian sketching providing up to 15-30 times faster predictions than regular OCSVM whilst reducing required storage space demands significantly by around 30-40 folds thereby making real-time anomaly detection feasible even within constrained hardware resources found commonly amongst various types of IoT sensors deployed across diverse networks worldwide",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Diffusion Models for Video Prediction and Infilling",
        "abstract": "Predicting and anticipating future outcomes or reasoning about missing information in a sequence are critical skills for agents to be able to make intelligent decisions. This requires strong, temporally coherent generative capabilities. Diffusion models have shown remarkable success in several generative tasks, but have not been extensively explored in the video domain.\nWe present Random-Mask Video Diffusion (RaMViD), which extends image diffusion models to videos using 3D convolutions, and introduces a new conditioning technique during training.\nBy varying the mask we condition on, the model is able to perform video prediction, infilling, and upsampling. Due to our simple conditioning scheme, we can utilize the same architecture as used for unconditional training, which allows us to train the model in a conditional and unconditional fashion at the same time. We evaluate RaMViD on two benchmark datasets for video prediction, on which we achieve state-of-the-art results, and one for video generation. High-resolution videos are provided at https://sites.google.com/view/video-diffusion-prediction.",
        "authors": "T. Höppe, A. Mehrjou, S. Bauer, et.al",
        "keywords": [
            "video diffusion",
            "temporal coherence",
            "generative capabilities"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=lf0lr4AYM6",
        "pdf_src": "https://api2.openreview.net/pdf/a1987cf55d1f81a8c00a539693f9e72d571f9e7f.pdf",
        "Code_src": "",
        "Introduction": "Background: The ability of an agent to predict future outcomes from sequences it has observed—such as predicting what will happen next within a video—is crucial when making informed decisions that rely upon temporal coherence.\n\nResearch Problem: While diffusion models based on variational autoencoders (VAEs) such as DALL-E2 have demonstrated significant progress in generating images with high fidelity by learning to invert noise added into them stepwise through stochastic differential equations over many iterations (\"denoising\"), their application specifically designed for handling sequential data like videos remains largely unexplored due to computational complexity challenges associated with three-dimensional convolutional operations required across frames' spatial dimensions along with temporal ones.\n\nMethodology: To address this challenge, authors introduce \"Random-Mask Video Diffusion\" (RaMViD). They adapt existing diffusion models capable of processing static images—their core idea being to apply these techniques sequentially frame-by-frame while incorporating novel masking strategies tailored towards video content understanding rather than just pixel-level details found only statically in still pictures. Specifically, they use 3D convolutions instead of traditional 2D convolutions because they allow capturing dependencies between adjacent frames more effectively; however since 3D convolutions would typically require significantly higher computational resources compared to standard 2D convolutions if applied uniformly throughout each video clip's duration, RaMViD employs random masks where some parts remain unchanged allowing less computation-intensive denoising steps focusing solely on those masked areas until convergence occurs before moving onto other regions incrementally without needing additional computations beyond initial setup costs per frame.\n\nMain Contributions:\n1. RaMViD successfully applies diffusion-based generative modeling principles originally developed primarily for static imagery domains directly applicable here via innovative adaptations including leveraging 3D convolutions adapted especially suited toward video analysis needs;\n2. It presents a practical solution enabling efficient yet effective predictions regarding upcoming events within sequences given partial observations – something prior works could not do efficiently enough nor generalize well outside controlled environments;\n3. RaMViD also enables simultaneous training under both conditional and unconditional settings meaning no need retraining architectures every time conditions change thus saving considerable amounts of computational effort needed otherwise;\n4. Finally, empirical validation performed against established benchmarks demonstrates its efficacy surpassing current state-of-the-art performance levels achieving competitive scores even though trained end-to-end exclusively on publicly available datasets",
        "Topic": "Generative Models"
    },
    {
        "title": "Sequentially learning the topological ordering of directed acyclic graphs with likelihood ratio scores",
        "abstract": "Causal discovery, the learning of causality in a data mining scenario, has been of strong scientific and theoretical interest as a starting point to identify \"what causes what?'' Contingent on assumptions and a proper learning algorithm, it is sometimes possible to identify and accurately estimate an underlying directed acyclic graph (DAG), as opposed to a Markov equivalence class of graphs that gives ambiguity of causal directions. The focus of this paper is in highlighting the identifiability and estimation of DAGs through a sequential sorting procedure that orders variables one at a time, starting at root nodes, followed by children of the root nodes, and so on until completion. We demonstrate a novel application of this general sequential approach to estimate the topological ordering of the DAG corresponding to a linear structural equation model with a non-Gaussian error distribution family. At each step of the procedure, only simple likelihood ratio scores are calculated on regression residuals to decide the next node to append to the current partial ordering. The computational complexity of our algorithm on a $p$-node problem is $\\mathcal{O}(pd)$, where $d$ is the maximum neighborhood size. Under mild assumptions, the population version of our procedure provably identifies a true ordering of the underlying DAG. We provide extensive numerical evidence to demonstrate that this sequential procedure scales to possibly thousands of nodes and works well for high-dimensional data. We accompany these numerical experiments with an application to a single-cell gene expression dataset. Our $\\texttt{R}$ package with examples and installation instructions can be found at https://gabriel-ruiz.github.io/scorelingam/.",
        "authors": "G. Ruiz, O. H. M. Padilla, Q. Zhou",
        "keywords": [
            "dag",
            "causal discovery",
            "sequential sorting"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=4pCjIGIjrt",
        "pdf_src": "https://api2.openreview.net/pdf/7b6ed92ae702c2d45e0d61361c997eaa38da23ad.pdf",
        "Code_src": "",
        "Introduction": "Background: Causal discovery aims to learn causality from observational or experimental datasets without intervention.\nResearch Problem: How to identify and estimate a Directed Acyclic Graph (DAG) representing the causal relationships among variables?\nMethods: This paper proposes a sequential sorting procedure based on regression residuals to order variables iteratively into a DAG structure while maintaining its acyclicity property.\n\nMain Contributions:\n1. Novel Application: Demonstrates how to apply the proposed sequential approach to estimate the topological ordering of a DAG associated with a Linear Structural Equation Model (LSEM) having a non-Gaussian error distribution family;\n2. Computational Complexity: Analyzes the computational efficiency of the algorithm showing it runs in $\\mathcal{O}(pd)$ time complexity when dealing with a p-node system, where d represents the maximum neighborhood size;\n3. Statistical Guarantee: Provides statistical guarantees under certain conditions stating their method will correctly identify the true ordering of the underlying DAG;\n4. Numerical Evidence: Presents empirical results indicating scalability up to potentially thousands of nodes demonstrating effectiveness even within high-dimensional settings;\n5. Software Package: Accompanies findings with an R-package named scorelingam containing code examples along with installation instructions available online via GitHub repository.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "A Snapshot of the Frontiers of Client Selection in Federated Learning",
        "abstract": "Federated learning (FL) has been proposed as a privacy-preserving approach in distributed machine learning. A federated learning architecture consists of a central server and a number of clients that have access to private, potentially sensitive data. Clients are able to keep their data in their local machines and only share their locally trained model's parameters with a central server that manages the collaborative learning process. FL has delivered promising results in real-life scenarios, such as healthcare, energy and finance. However, when the number of participating clients is large, the overhead of managing the clients slows down the learning. Thus, client selection has been introduced as a strategy to limit the number of communicating parties at every step of the process. Since the early naïve random selection of clients, several client selection methods have been proposed in the literature. Unfortunately, given that this is an emergent field, there is a lack of a taxonomy of client selection methods, making it hard to compare approaches. In this paper, we propose a taxonomy of client selection in Federated Learning that enables us to shed light on current progress in the field and identify potential areas of future research in this promising area of machine learning.",
        "authors": "G. D. Németh, M. A. Lozano, N. Quadrianto, et.al",
        "keywords": [
            "client selection",
            "federated learning",
            "privacy preservation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=vwOKBldzFu",
        "pdf_src": "https://api2.openreview.net/pdf/cbe3a11519c145ab4ea8bc2ff30e25515de5d8e5.pdf",
        "Code_src": "",
        "Introduction": "Background: Federated learning (FL) is a decentralized machine learning framework designed for privacy preservation by allowing multiple devices or clients each holding part of the training dataset to train models independently without sharing raw data across different locations.\n\nResearch Problem: As the number of participants increases within a federated learning system due to its scalability benefits over centralized systems where all data must be sent to one location before processing can occur, communication costs escalate significantly which leads to slower convergence times during iterative updates between servers and clients since more interactions need management per round leading to increased latency issues affecting overall performance efficiency.\n\nMethodology: To address these challenges posed particularly under high participation rates associated with larger datasets commonly encountered nowadays; various strategies termed \"client selection\" algorithms were developed aiming towards reducing unnecessary communications amongst nodes involved throughout rounds while still maintaining sufficient diversity among selected representatives from heterogeneous populations contributing toward collective knowledge aggregation via collaborative optimization processes.\n\nMain Contributions: This study introduces a comprehensive Taxonomy categorizing existing Client Selection Methods into four main categories based upon how they select representative subsets out from entire participant pools namely Random Sampling Approaches , Utility-based Approaches , Diversity-aware Approaches  and Hybrid Strategies . The classification scheme presented here provides insights about what works best depending on specific use cases thus aiding practitioners choose appropriate techniques tailored specifically according to requirements pertaining complexity reduction goals related both computational load distribution balancing aspects concerning network bandwidth utilization considerations ensuring timely completion tasks whilst preserving confidentiality constraints imposed through decentralization mechanisms inherent within federated architectures themselves.",
        "Topic": "Federated Learning"
    },
    {
        "title": "Lazy vs hasty: linearization in deep networks impacts learning schedule based on example difficulty",
        "abstract": "Among attempts at giving a theoretical account of the success of deep neural networks, a recent line of work has identified a so-called `lazy' training regime in which the network can be well approximated by its linearization around initialization. Here we investigate the comparative effect of the lazy (linear) and feature learning (non-linear) regimes on subgroups of examples based on their difficulty.  Specifically, we show that easier examples are given more weight in feature learning mode, resulting in faster training compared to more difficult ones. In other words, the non-linear dynamics tends to sequentialize the learning of examples of increasing difficulty. We illustrate this phenomenon across different ways to quantify example difficulty, including c-score, label noise, and in the presence of easy-to-learn spurious correlations. Our results reveal a new understanding of how deep networks prioritize resources across example difficulty.",
        "authors": "T. George, G. Lajoie, A. Baratin",
        "keywords": [
            "lazy training",
            "feature learning",
            "example difficulty"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=lukVf4VrfP",
        "pdf_src": "https://api2.openreview.net/pdf/c8b9fc29eab2234cb6810f5f3a803ffc164d74ec.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses previous research into why deep neural networks succeed through various theories; recently, it was found that these networks could often be well approximated during initial stages using only their linear components.\n\nResearch Question: This study investigates whether there is any difference between two types of training regimes - \"lazy\" or \"feature learning\" when applied separately for groups of samples with varying degrees of difficulty (\"easy\" vs. \"difficult\").\n\nMethodology: To answer our question, they compare the performance under both regimes while considering sample difficulty levels quantitatively via methods like 'c-score', introducing label noise intentionally as another measure of complexity, and also studying cases where certain patterns might seem simple but aren't actually indicative of underlying relationships ('spurious correlations').\n\nMain Contributions:\n1. They demonstrate empirically that simpler examples receive greater attention within the feature learning regime than complex ones.\n2. This differential weighting leads to accelerated learning progress for less challenging examples relative to those harder problems due to nonlinear dynamics favoring an orderliness from easiest to most difficult tasks being learned sequentially over time without explicitly prioritizing them.\n3. Their findings provide insights about resource allocation strategies used implicitly throughout training processes even if not programmed specifically—highlighting potentially adaptive aspects inherent in such architectures beyond what's been previously theorized regarding optimization algorithms alone influencing outcomes significantly early-on during development phases before convergence occurs fully later down",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Fourier Sensitivity and Regularization of Computer Vision Models",
        "abstract": "Recent work has empirically shown that deep neural networks latch on to the Fourier statistics of training data and show increased sensitivity to Fourier-basis directions in the input. Understanding and modifying this Fourier-sensitivity of computer vision models may help improve their robustness, hence, in this paper we study the frequency sensitivity characteristics of deep neural networks using a principled approach. We first propose a $\\textbf{\\textit{basis trick}}$, proving that unitary transformations of the input-gradient of a function can be used to compute its gradient in the basis induced by the transformation. Using this result, we propose a general measure of any differentiable computer vision model's $\\textit{\\textbf{Fourier-sensitivity}}$ using the unitary Fourier-transform of its input-gradient. When applied to deep neural networks, we find that computer vision models are consistently sensitive to particular frequencies dependent on the dataset, training method and architecture. Based on this measure, we further propose a $\\textit{\\textbf{Fourier-regularization}}$ framework to modify the Fourier-sensitivities and frequency bias of models. Using our proposed regularizer-family, we demonstrate that deep neural networks obtain improved classification accuracy on robustness evaluations.",
        "authors": "K. Krishnamachari, S. Ng, C. Foo",
        "keywords": [
            "Fourier Sensitivity",
            "Basis Trick",
            "Fourier Regularization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=VmTYgjYloM",
        "pdf_src": "https://api2.openreview.net/pdf/35942472d5fb0f2d4e9bb85c1113cdeb20162225.pdf",
        "Code_src": "",
        "Introduction": "Background: Recent studies have demonstrated that deep neural networks exhibit an increased sensitivity towards Fourier-based features within their inputs during training.\n\nResearch Question: How does one understand and manipulate the Fourier sensitivity exhibited by these networks?\n\nMethodology: The authors introduce a novel \"basis trick\" which allows for computing gradients with respect to transformed bases derived from the network's input gradients through unitary transformations.\nThey then develop a metric known as Fourier-sensitivity based on the unitary Fourier transform of the input-gradient itself; they apply this metric across various datasets, architectures, and training methods involving convolutional neural networks.\n\nMain Contributions:\n1. A new concept called Fourier-sensitivity is introduced along with a quantifiable measure grounded in unitary transforms;\n2. This measurement reveals consistent patterns where certain frequencies are more influential depending on the specifics of the task, such as dataset, training regime or architecture design;\n3. They also present a regularization technique termed Fourier-regularization designed specifically to adjust the Fourier sensitivities detected earlier without altering other aspects of the network’s behavior significantly;\n4. Experimental validation shows that applying this regularization leads to improvements not only in terms of robustness but overall classification performance when evaluated against standard benchmarks like CIFAR10 and ImageNet.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Deconstructing Self-Supervised Monocular Reconstruction: The Design Decisions that Matter",
        "abstract": "This paper presents an open and comprehensive framework to systematically evaluate state-of-the-art contributions to self-supervised monocular depth estimation. This includes pretraining, backbone, architectural design choices and loss functions. Many papers in this field claim novelty in either architecture design or loss formulation. However, simply updating the backbone of historical systems results in relative improvements of 25%, allowing them to outperform most modern systems. A systematic evaluation of papers in this field was not straightforward. The need to compare like-with-like in previous papers means that longstanding errors in the evaluation protocol are ubiquitous in the field. It is likely that many papers were not only optimized for particular datasets, but also for errors in the data and evaluation criteria. To aid future research in this area, we release a modular codebase (https://github.com/jspenmar/monodepth_benchmark), allowing for easy evaluation of alternate design decisions against corrected data and evaluation criteria. We re-implement, validate and re-evaluate 16 state-of-the-art contributions and introduce a new dataset (SYNS-Patches) containing dense outdoor depth maps in a variety of both natural and urban scenes. This allows for the computation of informative metrics in complex regions such as depth boundaries. ",
        "authors": "J. Spencer, C. Russell, S. Hadfield, et.al",
        "keywords": [
            "depth estimation",
            "self-supervised learning",
            "model evaluation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=GFK1FheE7F",
        "pdf_src": "https://api2.openreview.net/pdf/12e4757fa72ca44d5ba72c173cede11ce42adf1b.pdf",
        "Code_src": "GitHub 链接: https://github.com/jspenmar/monodepth_benchmark",
        "Introduction": "Background: Self-supervised monocular depth estimation has been extensively studied due to its potential applications ranging from autonomous driving to augmented reality. Despite numerous works claiming novel architectures or loss functions leading to significant advancements over time, there have always been concerns about the fairness and comprehensiveness of these evaluations.\n\nResearch Problem: The main challenge addressed by this study revolves around evaluating existing methods fairly across different backbones without introducing biases through flawed protocols which could lead to misleading conclusions regarding performance gains attributed solely to innovative approaches rather than robustness on diverse datasets with varying quality issues.\n \nMethodology: To tackle this issue head-on, researchers developed an open-source benchmarking platform called Monodepth Benchmark that provides standardized datasets along with corrections made towards common mistakes found within prior studies' methodologies – including those related to inconsistent preprocessing steps used during training phases where certain models may be inadvertently favored based upon minor differences between implementations instead of actual algorithmic superiority. They further conducted thorough validations using their newly introduced SYNthetic Patches Dataset (SYNS-Patches), focusing specifically on challenging areas surrounding depth discontinuities encountered outdoors under various lighting conditions & camera viewpoints.\n\nMain Contributions:\n1. An openly available modular codebase designed explicitly aimed at facilitating comparative analyses among alternative designs while accounting for inconsistencies present throughout literature reviews concerning how experiments should ideally proceed; \n2. Reimplementation/validation/reevaluation process applied rigorously toward 16 previously published top-performing algorithms resulting in insights into why some perform better overall despite lacking apparent innovation compared others;\n3. Introduction of SYNS-Patches dataset enriched with high-quality synthetic patches capturing intricate details pertinent particularly when dealing with real-world scenarios featuring complex terrains",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "Object-aware Cropping for Self-Supervised Learning",
        "abstract": "A core component of the recent success of self-supervised learning is cropping data augmentation, which selects sub-regions of an image to be used as positive views in the self-supervised\nloss. The underlying assumption is that randomly cropped and resized regions of a given\nimage share information about the objects of interest, which is captured by the learned\nrepresentation. This assumption is mostly satisfied in datasets such as ImageNet where\nthere is a large, centered object, which is highly likely to be present in random crops of\nthe full image. However, in other datasets such as OpenImages or COCO, which are more\nrepresentative of real world uncurated data, there are typically multiple small objects in\nan image. In this work, we show that self-supervised learning based on the usual random\ncropping performs poorly on such datasets (measured by the difference from fully-supervised\nlearning). Instead of using pairs of random crops, we propose to leverage an unsupervised\nobject proposal technique; the first view is a crop obtained from this algorithm, and the\nsecond view is a dilated version of the first view. This encourages the self-supervised model\nto learn both object and scene level semantic representations. Using this approach, which we\ncall object-aware cropping, results in significant improvements over random scene cropping on\nclassification and object detection benchmarks. For example, for pre-training on OpenImages,\nour approach achieves an improvement of 8.8% mAP over random scene cropping (both meth-\nods using MoCo-v2). We also show significant improvements on COCO and PASCAL-VOC\nobject detection and segmentation tasks over the state-of-the-art self-supervised learning\napproaches. Our approach is efficient, simple and general, and can be used in most existing\ncontrastive and non-contrastive self-supervised learning frameworks.",
        "authors": "S. K. Mishra, A. Shah, A. Bansal, et.al",
        "keywords": [
            "object-aware cropping",
            "self-supervised learning",
            "contrastive learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=WXgJN7A69g",
        "pdf_src": "https://api2.openreview.net/pdf/fe2890bea9c90e35c30db267a01de82245687a35.pdf",
        "Code_src": "",
        "Introduction": "Background: Recent successes in self-supervised learning have been largely attributed to cropping data augmentation techniques like random cropping.\nResearch Problem: However, these methods may not perform well with datasets containing many small objects due to their assumptions regarding the presence of one central object.\n\nMethod: To address this issue, they introduce \"object-aware cropping,\" leveraging an unsupervised object proposal method - the first view being selected via this algorithm while the second view is a dilated version of the first.\n\nMain Contributions:\n1. Object-aware cropping significantly improves classification and object detection performance compared to standard random scene cropping across various benchmarks including OpenImages, COCO, and Pascal VOC.\n2. Their proposed approach enhances the self-supervised model's ability to capture both object and scene-level semantic representations through the use of two distinct but related views within each augmented sample.\n3. They demonstrate efficiency without sacrificing complexity – making it applicable broadly among current contrastive/self-supervised learning frameworks",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "MVSFormer: Multi-View Stereo by Learning Robust Image Features and Temperature-based Depth",
        "abstract": "Feature representation learning is the key recipe for learning-based Multi-View Stereo (MVS). As the common feature extractor of learning-based MVS, vanilla Feature Pyramid Networks (FPNs) suffer from discouraged feature representations for reflection and texture-less areas, which limits the generalization of MVS. Even FPNs worked with pre-trained Convolutional Neural Networks (CNNs) fail to tackle these issues. On the other hand, Vision Transformers (ViTs) have achieved prominent success in many 2D vision tasks. Thus we ask whether ViTs can facilitate feature learning in MVS? In this paper, we propose a pre-trained ViT enhanced MVS network called MVSFormer, which can learn more reliable feature representations benefited by informative priors from ViT. The finetuned MVSFormer with hierarchical ViTs of efficient attention mechanisms can achieve prominent improvement based on FPNs. Besides, the alternative MVSFormer with frozen ViT weights is further proposed. This largely alleviates the training cost with competitive performance strengthened by the attention map from the self-distillation pre-training. MVSFormer can be generalized to various input resolutions with efficient multi-scale training strengthened by gradient accumulation. Moreover, we discuss the merits and drawbacks of classification and regression-based MVS methods, and further propose to unify them with a temperature-based strategy. MVSFormer achieves state-of-the-art performance on the DTU dataset. Particularly, MVSFormer ranks as Top-1 on both intermediate and advanced sets of the highly competitive Tanks-and-Temples leaderboard. Codes and models are released in https://github.com/ewrfcas/MVSFormer.",
        "authors": "C. Cao, X. Ren, Y. Fu",
        "keywords": [
            "ViT",
            "Feature Learning",
            "MVSFormer"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=2VWR6JfwNo",
        "pdf_src": "https://api2.openreview.net/pdf/6d32fd74043204e3573c8492ec57002e9128bfc6.pdf",
        "Code_src": "https://github.com/ewrfcas/MVSFormer",
        "Introduction": "Background: Learning-based Multi-View Stereo (MVS) relies heavily on feature representation learning; however, conventional feature extractors like Vanilla Feature Pyramid Networks (FPNs) struggle when dealing with reflective or textureless regions due to poor feature representations.\n\nResearch Problem: Can Vision Transformers (ViTs), known for their effectiveness in 2D vision tasks, improve feature learning capabilities within an MVS framework?\n\nMethodology: We introduce \"MVSFormer,\" a pre-trained ViT-enhanced MVS network that leverages informative priors learned through ViT's architecture.\nWe also present two variants:\n1. A finetuned version using hierarchical ViTs equipped with efficient attention mechanisms overcomes limitations found in standard FPNs;\n2. An alternative variant freezes ViT weights during fine-tuning while leveraging its attention mechanism via self-distillation pre-training—a method that significantly reduces training costs without compromising performance.\n\nMain Contributions:\n1. Proposes new architectures—MVSFormer—that leverage ViTs' strengths into improving MVS feature extraction processes specifically designed for handling challenging surface types such as reflections and textures;\n2. Demonstrates improvements across different scales efficiently thanks to multi-scale training techniques bolstered by gradient accumulation strategies;\n3. Discusses unification approaches between classification and regression-based MVS methodologies utilizing a temperature-based strategy;\n4. Achieves top-tier results against competitors including ranking first place among all entries at the Tanks-and-Temples benchmarking competition, showcasing significant advancements compared existing solutions.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Controllable Generative Modeling via Causal Reasoning",
        "abstract": "Deep latent variable generative models excel at generating complex, high-dimensional data, often exhibiting impressive generalization beyond the training distribution. However, many such models in use today are black-boxes trained on large unlabelled datasets with statistical objectives and lack an interpretable understanding of the latent space required for controlling the generative process. \nWe propose CAGE, a framework for controllable generation in latent variable models based on causal reasoning.\nGiven a pair of attributes, CAGE infers the implicit cause-effect relationships between these attributes as induced by a deep generative model. This is achieved by defining and estimating a novel notion of unit-level causal effects in the latent space of the generative model.\nThereafter, we use the inferred cause-effect relationships to design a novel strategy for controllable generation based on counterfactual sampling. Through a series of large-scale synthetic and human evaluations, we demonstrate that generating counterfactual samples which respect the underlying causal relationships inferred via CAGE leads to subjectively more realistic images.",
        "authors": "J. Bose, R. P. Monti, A. Grover",
        "keywords": [
            "causal reasoning",
            "controllable generation",
            "counterfactual sampling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Z44YAcLaGw",
        "pdf_src": "https://api2.openreview.net/pdf/a767c79f93b02cb9300230e362bed23e4d1cf5ab.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses limitations associated with current deep latent variable generative models used widely due to their complexity; they're challenging to interpret despite being capable of producing diverse real-world representations.\n\nResearch Problem: How can one generate controlled outputs from latent variable models while ensuring interpretability?\n\nMethodology: The authors introduce \"Causal Attention Guided Encoding\" (CAGE), leveraging causal reasoning within the generated latent spaces using a new concept called 'unit-level causal effect'. They estimate this through a defined approach specific to the generative model's architecture without requiring any additional labeled or unlabeled data outside its training set.\n\nMain Contributions:\n1. A method for inferring causality directly into the latent variables' domain rather than relying solely on correlation analysis common among existing methods;\n2. An innovative counterfactual sampling technique informed by the identified causal relations aimed towards creating more plausible and contextually consistent generated content across various domains like image synthesis when evaluated synthetically against humans.",
        "Topic": "Generative Models"
    },
    {
        "title": "A Stochastic Optimization Framework for Fair Risk Minimization",
        "abstract": "Despite the success of large-scale empirical risk minimization (ERM) at achieving high accuracy across a variety of machine learning tasks, fair ERM is hindered by the incompatibility of fairness constraints with stochastic optimization. We consider the problem of fair classification with discrete sensitive attributes and potentially large models and data sets, requiring stochastic solvers. Existing in-processing fairness algorithms are either impractical in the large-scale setting because they require large batches of data at each iteration or they are not guaranteed to converge. In this paper, we develop the first stochastic in-processing fairness algorithm with guaranteed convergence. For demographic parity, equalized odds, and equal opportunity notions of fairness, we provide slight variations of our algorithm–called FERMI–and prove that each of these variations converges in stochastic optimization with any batch size. Empirically, we show that FERMI is amenable to stochastic solvers with multiple (non-binary) sensitive attributes and non-binary targets, performing well even with minibatch size as small as one. Extensive experiments show that FERMI achieves the most favorable tradeoffs between fairness violation and test accuracy across all tested setups compared with state-of-the-art baselines for demographic parity, equalized odds, equal opportunity. These benefits are especially significant with small batch sizes and for non-binary classification with large number of sensitive attributes, making FERMI a practical, scalable fairness algorithm. The code for all of the experiments in this paper is available at:\nhttps://github.com/optimization-for-data-driven-science/FERMI",
        "authors": "A. Lowy, S. Baharlouei, R. Pavan, et.al",
        "keywords": [
            "fairness",
            "stochastic optimization",
            "convergence"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=P9Cj6RJmN2",
        "pdf_src": "https://api2.openreview.net/pdf/b0ad61f984d326c55e022d39878545f082a97c9b.pdf",
        "Code_src": "代码链接：https://github.com/optimization-for-data-driven-science/FERMI",
        "Introduction": "Background: Despite the success of large-scale empirical risk minimization (ERM) in various machine learning tasks, it faces challenges when applied to fair ERM due to the incompatibility of fairness constraints with stochastic optimization.\n\nResearch Problem: This study focuses on developing an efficient stochastic in-processing fairness algorithm capable of handling large datasets and models while ensuring convergence under different fairness metrics such as demographic parity, equalized odds, and equal opportunity.\n\nMethods: The authors propose their algorithm called FERMI which slightly varies depending on the chosen fairness metric but guarantees convergence regardless of the batch size used during stochastic optimization processes.\n\nMain Contributions: Their main contribution lies in creating the first stochastic in-processing fairness algorithm proven to converge within stochastic optimization settings using any batch size selected from those feasible options applicable according to current computational resources limitations; specifically designed towards addressing issues related to both binary-sensitive attribute scenarios along with more complex multi-class classifications involving numerous non-binary sensitive attributes where other existing methods may fail or become computationally prohibitive \nEmpirical evidence shows that FERMI performs favorably against leading benchmarks across several evaluated configurations including smaller batch sizes than previously considered viable alternatives thus demonstrating scalability advantages over traditional approaches",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Modeling Bounded Rationality in Multi-Agent Simulations Using Rationally Inattentive Reinforcement Learning",
        "abstract": "Multi-agent reinforcement learning (MARL) is a powerful framework for studying emergent behavior in complex agent-based simulations. However, RL agents are often assumed to be rational and behave optimally, which does not fully reflect human behavior. In this work, we propose a new, more human-like RL agent, which incorporates an established model of human-irrationality, the Rational Inattention (RI) model. RI models the cost of cognitive information processing using mutual information. Our RIRL framework generalizes and is more flexible than prior work by allowing for multi-timestep dynamics and information channels with heterogeneous processing costs. We demonstrate the flexibility of RIRL in versions of a classic economic setting (Principal-Agent setting) with varying complexity. In simple settings, we show using RIRL can lead to optimal agent behavior policy with approximately the same functional form as what is expected from the analysis of prior work, which utilizes theoretical methods. We additionally demonstrate that using RIRL to analyze complex, theoretically intractable settings, yields a rich spectrum of new equilibrium behaviors that differ from those found under rationality assumptions. For example, increasing the cognitive cost experienced by a manager agent results in the other agents increasing the magnitude of their action to compensate. These results suggest RIRL is a powerful tool towards building AI agents that can mimic real human behavior. ",
        "authors": "T. Mu, S. Zheng, A. R. Trott",
        "keywords": [
            "Rational Inattention",
            "Multi-agent Reinforcement Learning",
            "Heterogeneous Processing Costs"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=DY1pMrmDkm",
        "pdf_src": "https://api2.openreview.net/pdf/22eb6e5518f4fe3009bd0a3822b4a3a9f78b362d.pdf",
        "Code_src": "",
        "Introduction": "Background: Multi-agent reinforcement learning (MARL) has been widely used but assumes agents' rationality.\nResearch Problem: To develop a MARL agent reflecting human irrationality.\n\nMethod: Propose a novel RL agent incorporating Rational Inattention (RI) model based on mutual information; introduce RIRL framework adaptable to multi-timestep dynamics & heterogeneous info processing costs.\n\nMain Contributions:\n1. Develops a more human-like RL agent considering irrationality through RI model.\n2. Introduces RIRL framework generalized over previous works due to its adaptability regarding multi-timesteps & heterogeneous costs within information channels.\n3. Demonstrates effectiveness across various complexities including Principal-Agent game theory scenarios where it leads to optimal policies resembling analytical findings while also uncovering diverse equilibria when dealing with complex settings beyond traditional rationality assumptions - e.g., managers adjusting actions compensating increased cognitive load.",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "GIT: A Generative Image-to-text Transformer for Vision and Language",
        "abstract": "In this paper, we design and train a Generative Image-to-text Transformer, GIT, to unify vision-language tasks such as image/video captioning and question answering. While generative models provide a consistent network architecture between pre-training and fine-tuning, existing work typically contains complex structures (uni/multi-modal encoder/decoder) and depends on external modules such as object detectors/taggers and optical character recognition (OCR). In GIT, we simplify the architecture as one image encoder and one text decoder\nunder a single language modeling task. We also scale up the pre-training data and the model size to boost the model performance. Without bells and whistles, our GIT establishes new state of the arts on numerous challenging benchmarks with a large margin. For instance, our model surpasses the human performance for the first time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a new scheme of generation-based image classification and scene text recognition, achieving decent performance on standard benchmarks.",
        "authors": "J. Wang, Z. Yang, X. Hu, et.al",
        "keywords": [
            "GIT",
            "Generative Image-to-text Transformer",
            "Vision-Language Tasks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=b4tMhpN0JC",
        "pdf_src": "https://api2.openreview.net/pdf/36450c2ce9721ca126c22d05ae470746c832f0e9.pdf",
        "Code_src": "",
        "Introduction": "Background: The field of computer vision has seen significant advancements over recent years due to deep learning techniques that enable machines to understand images through neural networks known as convolutional neural networks (CNNs). However, these CNNs are not well-suited for understanding natural language descriptions or performing tasks like image captioning where both visual information from an image and textual description need to be processed together.\n\nResearch Problem: To address this issue, researchers have developed multimodal architectures combining CNNs with recurrent neural networks (RNNs), transformers, etc., which can jointly process visual and linguistic inputs but often suffer from complexity issues during training since they require multiple encoders and decoders along with additional components outside their main structure - e.g., object detection systems before processing input images into feature vectors suitable for further analysis by multimodal models.\n\nMethodology: This research introduces a novel approach called Generative Image-to-text Transformer (GIT) designed specifically around unifying various vision-language tasks under a unified framework without requiring any auxiliary modules apart from what is necessary within its core architecture itself – namely just one encoder for handling raw input images/videos directly followed by only one decoder responsible solely for generating corresponding captions/textual descriptions based purely upon learned features extracted at encoding stage rather than relying heavily upon external tools like OCR tags or bounding box annotations beforehand.\n\nMain Contributions:\n1. Simplified Architecture: By reducing complexity compared traditional multimodal approaches by using fewer components (one encoder & one decoder instead), GIT significantly reduces computational overhead while still maintaining high accuracy levels across several benchmark datasets.\n2. Performance Boost Through Scaling Up Pre-Training Data & Model Size: Extensive experiments demonstrate that scaling up pre-training dataset sizes alongside increasing model capacity leads to substantial improvements overall performance metrics when evaluated against competitive baselines including those involving human participants who serve as reference points here).\n3. Novel Approaches Applied Across Multiple Domains: Beyond improving foundational capabilities related mainly towards captioning tasks themselves; authors propose extensions beyond basic functionality demonstrated via examples provided earlier demonstrating how GIT could potentially revolutionize other areas too such as automatic summarization/image retrieval applications leveraging generated summaries/text descriptions respectively).\n\nConclusion: Overall findings indicate promising prospects associated with employing more streamlined architectures capable integrating diverse modalities seamlessly resulting potential breakthroughs future developments pertaining fields encompassed by vision-language paradigm shift underway today's rapidly evolving technological landscape characterized increasingly sophisticated machine intelligence algorithms powering them all forward toward greater autonomy adaptability realms previously unimaginable mere decades ago yet now becoming reality thanks relentless pursuit innovation embodied exemplified herein presented study conducted team experts working closely together harness power cutting-edge technologies available us collectively pushing boundaries knowledge frontier onward ceaselessly striving reach heights never reached before unlocking doors possibilities await discovery tomorrow’s world powered artificial intelligence",
        "Topic": "Vision Transformer"
    },
    {
        "title": "Fast and Accurate Spreading Process Temporal Scale Estimation",
        "abstract": "Spreading processes on graphs arise in a host of application domains, from the study of online social networks to viral marketing to epidemiology. Various discrete-time probabilistic models for spreading processes have been proposed. These are used for downstream statistical estimation and prediction problems, often involving messages or other information that is transmitted along with infections caused by the process. These models generally model cascade behavior at a small time scale but are insufficiently flexible to model cascades that exhibit intermittent behavior governed by multiple scales.  We argue that the presence of such time scales that are unaccounted for by a cascade model can result in degradation of performance of models on downstream statistical and time-sensitive optimization tasks.  To address these issues, we formulate a model that incorporates multiple temporal scales of cascade behavior.  This model is parameterized by a \\emph{clock}, which encodes the times at which sessions of cascade activity start.  These sessions are themselves governed by a small-scale cascade model, such as the discretized independent cascade (IC) model.  Estimation of the multiscale cascade model parameters leads to the problem of \\emph{clock estimation} in terms of a natural distortion measure that we formulate.  Our framework is inspired by the optimization problem posed by DiTursi et al, 2017, which can be seen as providing one possible estimator (a maximum-proxy-likelihood estimator) for the parameters of our generative model. We give a clock estimation algorithm, which we call FastClock, that runs in linear time in the size of its input and is provably statistically accurate for a broad range of model parameters when cascades are generated from any spreading process model with well-concentrated session infection set sizes and when the underlying graph is at least in the semi-sparse regime.  We exemplify our algorithm for the case where the small-scale model is the discretized independent cascade process and extend substantially to processes whose infection set sizes satisfy a general martingale difference property. We further evaluate the performance of FastClock empirically in comparison to the state of the art estimator from DiTursi et al, 2017.  We find that in a broad parameter range on synthetic networks and on a real network, our algorithm substantially outperforms that algorithm in terms of both running time and accuracy.  In all cases, our algorithm's running time is asymptotically lower than that of the baseline.\n",
        "authors": "A. Magner, C. S. Kaminski, P. Bogdanov",
        "keywords": [
            "multiscale cascade model",
            "clock estimation",
            "FastClock"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=k4iWTEdUSF",
        "pdf_src": "https://api2.openreview.net/pdf/c8382bebd11f7ec2c3e30f366d9183ec2cfb8856.pdf",
        "Code_src": "",
        "Introduction": "Background: Spreading processes on graphs occur across various fields like sociology, economics, and biology.\n\nResearch Problem: Existing models do not capture intermittency over multiple time scales leading to suboptimal performance during statistical inference and optimization tasks.\n\nMethod: The authors propose a multi-scale cascade model incorporating different time scales using a \"clock\" parameterizing the onset of cascade activities within each scale period based on an IC-like small-scale model.\n\nMain Contributions:\n1. A novel approach accounting for multiple time scales through a structured clock mechanism allowing more flexibility compared to single-scale models.\n2. Formulation of a new estimation task - clock estimation – leveraging a distortion metric tailored specifically for this purpose.\n3. Development of FastClock, an efficient algorithm solving the clock estimation problem efficiently while maintaining statistical accuracy under certain conditions related to the spread patterns' concentration properties; it has linear runtime complexity w.r.t. data size ensuring scalability even large datasets.\n4. Experimental validation demonstrating superior performance against existing baselines regarding speed and precision broadly applicable scenarios including synthetic networks & empirical ones.",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "Distribution Embedding Networks for Generalization from a Diverse Set of Classification Tasks",
        "abstract": "We propose Distribution Embedding Networks (DEN) for classification with small data. In the same spirit of meta-learning, DEN learns from a diverse set of training tasks with the goal to generalize to unseen target tasks. Unlike existing approaches which require the inputs of training and target tasks to have the same dimension with possibly similar distributions, DEN allows training and target tasks to live in heterogeneous input spaces. This is especially useful for tabular-data tasks where labeled data from related tasks are scarce. DEN uses a three-block architecture: a covariate transformation block followed by a distribution embedding block and then a classification block. We provide theoretical insights to show that this architecture allows the embedding and classification blocks to be fixed after pre-training on a diverse set of tasks; only the covariate transformation block with relatively few parameters needs to be fine-tuned for each new task. To facilitate training, we also propose an approach to synthesize binary classification tasks, and demonstrate that DEN outperforms existing methods in a number of synthetic and real tasks in numerical studies.",
        "authors": "L. Liu, M. M. Fard, S. Zhao",
        "keywords": [
            "distribution embedding networks",
            "heterogeneous input spaces",
            "covariate transformation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=F2rG2CXsgO",
        "pdf_src": "https://api2.openreview.net/pdf/43ee1ac8b846a1b9cf0565961f91a613d97a678a.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the challenge of classification using small datasets through the proposed Distribution Embedding Networks (DEN). Meta-learning has been successful at generalizing across different tasks but typically requires tasks' inputs to share dimensions or distributions.\n\nResearch Problem: How can we design networks capable of classifying accurately when trained under conditions significantly different from those encountered during testing?\n\nMethods: DEN employs a novel multi-task learning framework designed specifically so that it does not rely heavily on shared features between source and target domains - unlike previous works requiring aligned feature spaces – making it particularly suitable for scenarios like tabular data where labeled examples may come from disparate sources.\nThe network consists of 3 main components:\n1. A Covariate Transformation Block adjusts the input space's statistics without altering its structure;\n2. A Distribution Embedding Block captures the underlying distributional properties within these transformed spaces;\n3. Finally, a Classification Block utilizes embeddings as features directly for prediction purposes.\n\nMain Contributions: \n- Theoretical analysis justifies why such a modularized architecture enables independent optimization paths allowing us to fix most layers post-pretraining while adapting only one component per new domain adaptation scenario.\n- An innovative synthesis method generates binary classification problems artificially aiding in faster convergence rates due to fewer hyperparameters involved compared to traditional cross-domain transfer setups.\n- Empirical validation shows superior performance over state-of-the-art techniques both synthetically generated datasets along with empirical ones demonstrating practical applicability beyond academia into industry settings dealing with limited annotated information challenges commonly found therein.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Concave Utility Reinforcement Learning with Zero-Constraint Violations",
        "abstract": "We consider the problem of tabular infinite horizon concave utility reinforcement learning (CURL) with convex constraints.\nFor this, we propose a model-based learning algorithm that also achieves zero constraint violations. Assuming that the concave objective and the convex constraints have a solution interior to the set of feasible occupation measures, we solve a tighter optimization problem to ensure that the constraints are never violated despite the imprecise model knowledge and model stochasticity. We use Bellman error-based analysis for tabular infinite-horizon setups which allows analyzing stochastic policies. \nCombining the Bellman error-based analysis and tighter optimization equation, for $T$ interactions with the environment, we obtain a high-probability regret guarantee for objective which grows as $\\Tilde{O}(1/\\sqrt{T})$, excluding other factors. The proposed method can be applied for optimistic algorithms to obtain high-probability regret bounds and also be used for posterior sampling algorithms to obtain a loose Bayesian regret bounds but with significant improvement in computational complexity.\n",
        "authors": "M. Agarwal, Q. Bai, V. Aggarwal",
        "keywords": [
            "tabular infinite horizon concave utility reinforcement learning",
            "Bellman error-based analysis",
            "tight optimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=WXVkgkPXRk",
        "pdf_src": "https://api2.openreview.net/pdf/1acfac1da76b568a690c5ed3f133300379176ca7.pdf",
        "Code_src": "",
        "Introduction": "Background: Tabular infinite horizon concave utility reinforcement learning (CURL) is an area where agents learn optimal strategies by maximizing their cumulative discounted rewards subject to convex constraints on actions or states.\n\nResearch Problem: In CURL problems, it's challenging to find solutions because the objectives may not always converge due to the presence of convex constraints; hence, there could potentially be violation of these constraints during policy execution even if they were satisfied at some point before interaction begins again from scratch after each step.\n\nMethod: To address this issue without violating any constraints while dealing with uncertainty about models and stochastic environments, authors introduce a novel approach combining model-based learning techniques along with more stringent optimization conditions than those typically considered previously within such settings - ensuring no constraint violation regardless of imperfect information regarding both dynamics and reward functions over time steps T.\n\nMain Contributions:\n- Propose a new algorithmic framework capable solving tight optimization problems under certain assumptions related to existence of feasible points inside feasible sets associated with concave utilities & convex constraints;\n- Introduce bellman error based analyses specifically tailored towards tabular infinite horizon scenarios allowing us analyze stochastic policies effectively;\n- Achieve provable guarantees against regret incurred throughout interactions between agent/environment up till T iterations – namely O(1/√T), assuming all other factors remain constant apart from number of iterations themselves;\n\nThe paper offers insights into how one might tackle complex constrained RL tasks through rigorous mathematical frameworks whilst still maintaining tractability via computationally efficient methods like posterior sampling algorithms when further approximations become necessary yet require less computation compared traditional approaches would entail alone).",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Systematically and efficiently improving $k$-means initialization by pairwise-nearest-neighbor smoothing",
        "abstract": "We present a meta-method for initializing (seeding) the $k$-means\nclustering algorithm called PNN-smoothing. It consists in splitting\na given dataset into $J$ random subsets, clustering each of them\nindividually, and merging the resulting clusterings with the pairwise-nearest-neighbor\n(PNN) method. It is a meta-method in the sense that when clustering\nthe individual subsets any seeding algorithm can be used. If the computational\ncomplexity of that seeding algorithm is linear in the size of the\ndata $N$ and the number of clusters $k$, PNN-smoothing is also almost\nlinear with an appropriate choice of $J$, and quite competitive in\npractice. We show empirically, using several existing seeding methods\nand testing on several synthetic and real datasets, that this procedure\nresults in systematically better costs. In particular, our method\nof enhancing $k$-means++ seeding proves superior in both effectiveness\nand speed compared to the popular ``greedy'' $k$-means++ variant. Our implementation\nis publicly available at \\href{https://github.com/carlobaldassi/KMeansPNNSmoothing.jl}{https://github.com/carlobaldassi/KMeansPNNSmoothing.jl}.",
        "authors": "C. Baldassi",
        "keywords": [
            "PNN-smoothing",
            "k-means clustering",
            "meta-method"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=FTtFAg3pek",
        "pdf_src": "https://api2.openreview.net/pdf/5500c0adcedf54c73213149bc312a59314e87160.pdf",
        "Code_src": "链接：[https://github.com/carlobaldassi/KMeansPNNSmoothing.jl](https://github.com/carlobaldassi/KMeansPNNSmoothing.jl)",
        "Introduction": "Background: The k-means clustering algorithm is widely used but often suffers from poor initialization leading to suboptimal solutions.\nResearch Problem: How to improve the initialization step of the k-means clustering algorithm?\nMethod: Introduce PNN-smoothing, which involves partitioning data randomly into J subsets, applying any clustering seed to each subset independently, then combining these results via the Pairwise Nearest Neighbor (PNN) approach without requiring additional computations beyond those needed by the initial seeds themselves.\n\nMain Contributions:\n1. A novel meta-method for initializing k-means clustering algorithms named PNN-smoothing;\n2. Demonstrated empirical improvements over other seeding strategies across various datasets through systematic cost reduction tests;\n3. Provided open-source code (\\href{https://github.com/carlobaldassi/KMeansPNNSmoothing.jl}{https://github.com/carLOBALDASSI/KMeansPNNSmoothing.jl}) allowing others to replicate their findings or integrate it directly into their work.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "COIN++: Neural Compression Across Modalities",
        "abstract": "Neural compression algorithms are typically based on autoencoders that require specialized encoder and decoder architectures for different data modalities. In this paper, we propose COIN++, a neural compression framework that seamlessly handles a wide range of data modalities. Our approach is based on converting data to implicit neural representations, i.e. neural functions that map coordinates (such as pixel locations) to features (such as RGB values). Then, instead of storing the weights of the implicit neural representation directly, we store modulations applied to a meta-learned base network as a compressed code for the data. We further quantize and entropy code these modulations, leading to large compression gains while reducing encoding time by two orders of magnitude compared to baselines. We empirically demonstrate the feasibility of our method by compressing various data modalities, from images and audio to medical and climate data.",
        "authors": "E. Dupont, H. Loya, M. Alizadeh, et.al",
        "keywords": [
            "COIN++",
            "Neural Compression Framework",
            "Implicit Neural Representations"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=NXB0rEM2Tq",
        "pdf_src": "https://api2.openreview.net/pdf/b26a6487648c285c9e131d117ff80fb9c0c2cc18.pdf",
        "Code_src": "",
        "Introduction": "Background: Neural compression algorithms often rely on autoencoder architectures with separate encoders and decoders tailored to specific data modalities.\n\nResearch Problem: Developing an efficient neural compression framework capable of handling diverse data modalities without requiring specialized architecture for each type of input.\n\nMethod: The authors introduce COIN++, which converts raw data into implicit neural representations using coordinate-to-feature mappings within a learned function space rather than storing explicit weights in traditional autoencoders. They then represent these mappings through modulations over a pre-trained base network's parameters; they use quantization and entropy coding techniques to further reduce storage requirements significantly faster than existing methods due to less computation-intensive encoding steps.\n\nMain Contributions:\n1. A novel neural compression framework named COIN++ designed specifically for multi-modal data.\n2. An innovative way of representing data implicitly via modulated networks allowing seamless integration across multiple modalities like images or audio files alongside more complex datasets such as medical records/climate information.\n3. Demonstrated substantial improvements both in terms of compression ratio achieved along with significant reduction in computational complexity during encoding phase when comparing against baseline models.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "On Sample Complexity of Offline Reinforcement Learning with Deep ReLU Networks in Besov Spaces",
        "abstract": "Offline reinforcement learning (RL) leverages previously collected data for policy optimization without any further active exploration. Despite the recent interest in this problem, its theoretical results in neural network function approximation settings remain elusive. In this paper, we study the statistical theory of offline RL with deep ReLU network function approximation. In particular, we establish the sample complexity of $n = \\tilde{\\mathcal{O}}( H^{4 + 4 \\frac{d}{\\alpha}} \\kappa_{\\mu}^{1 + \\frac{d}{\\alpha}} \\epsilon^{-2 - 2\\frac{d}{\\alpha}} )$ for offline RL with deep ReLU networks, where $\\kappa_{\\mu}$ is a measure of distributional shift, $H = (1-\\gamma)^{-1}$ is the effective horizon length,  $d$ is the dimension of the state-action space, $\\alpha$ is a (possibly fractional) smoothness parameter of the underlying Markov decision process (MDP), and $\\epsilon$ is a user-specified error. Notably, our sample complexity holds under two novel considerations: the Besov dynamic closure and the correlated structure. While the Besov dynamic closure subsumes the dynamic conditions for offline RL in the prior works, the correlated structure renders the prior works of offline RL with general/neural network function approximation improper or inefficient in long (effective) horizon problems. To the best of our knowledge, this is the first theoretical characterization of the sample complexity of offline RL with deep neural network function approximation under the general Besov regularity condition that goes beyond the linearity regime in the traditional Reproducing Hilbert kernel spaces and Neural Tangent Kernels. ",
        "authors": "T. Nguyen-tang, S. Gupta, H. T. Tran, et.al",
        "keywords": [
            "offline reinforcement learning",
            "sample complexity",
            "deep ReLU network"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LdEm0umNcv",
        "pdf_src": "https://api2.openreview.net/pdf/2a5f49d5fa0d33f9de7801cb16dd671a9cf45899.pdf",
        "Code_src": "",
        "Introduction": "Background:\nOffline reinforcement learning aims to optimize policies using only previously collected data rather than actively exploring new states.\n\nResearch Problem:\nDespite significant progress on offline RL algorithms recently, there are still no clear theoretical guarantees regarding their performance when approximating functions within neural networks.\n\nMethods:\nThe authors investigate the statistical theory of offline RL by studying it through the lens of deep ReLU network function approximation.\nThey consider factors such as distributional shift ($\\kappa_\\mu$), effective horizon length ($H$), dimensions of the state-action space ($d$), smoothness parameter ($\\alpha$), and an error term ($\\epsilon$).\n\nMain Contributions:\nThis work establishes a sample complexity result for offline RL with deep ReLU networks, showing that $n = \\tilde{\\mathcal{O}}( H^{4 + 4 \\frac{d}{\\alpha}} \\kappa_\\mu^{1 + \\frac{d}{\\alpha}} \\epsilon^{-2 - 2\\frac{d}{\\alpha}} )$, which depends on these parameters along with $\\epsilon$. \nIt's worth noting that:\n\n- The sample complexity formula accounts for both the Besov dynamic closure and the correlated structure,\n- It provides insights into how different factors affect the required number of samples during training,\n- This is one of the few studies considering non-linear regimes outside of linear models like Reproducing Hilbert Kernel Spaces and Neural Tangent Kernels",
        "Topic": "Sample Efficiency in Reinforcement Learning"
    },
    {
        "title": "GhostSR: Learning Ghost Features for Efficient Image Super-Resolution",
        "abstract": "Modern single image super-resolution (SISR) systems based on convolutional neural networks (CNNs) have achieved impressive performance but require huge computational costs. The problem on feature redundancy has been well studied in visual recognition task, but rarely discussed in SISR. Based on the observation that many features in SISR models are also similar to each other, we propose to use shift operation for generating the redundant features (i.e. ghost features). Compared with depth-wise convolution which is time-consuming on GPU-like devices, shift operation can bring a real inference acceleration for CNNs on common hardware. We analyze the benefits of shift operation in SISR and make the shift orientation learnable based on the Gumbel-Softmax trick. Besides, a clustering procedure is explored based on pre-trained models to identify the intrinsic filters for generating corresponding intrinsic features. The ghost features will be generated by moving these intrinsic features along a certain orientation. Finally, the complete output features are constructed by concatenating the intrinsic and ghost features together. Extensive experiments on several benchmark models and datasets demonstrate that both the non-compact and lightweight SISR CNN models embedded with the proposed method can achieve a comparable performance to the baseline models with a large reduction of parameters, FLOPs and GPU inference latency. For example, we reduce the parameters by 46%, FLOPs by 46% and GPU inference latency by 42% of x2 EDSR model with almost lossless performance. Code will be available at https://gitee.com/mindspore/models/tree/master/research/cv/GhostSR.",
        "authors": "Y. Nie, K. Han, Z. Liu, et.al",
        "keywords": [
            "shift operation",
            "feature redundancy",
            "super-resolution"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=tbd9f3HwPy",
        "pdf_src": "https://api2.openreview.net/pdf/504fdb397a4e3319b0ac35b9322cd0e1bf9a6bb3.pdf",
        "Code_src": "Code link: https://gitee.com/mindspore/models/tree/master/research/cv/GhostSR",
        "Introduction": "Background: Single-image super-resolution (SISR), an important technique used widely across various fields such as medical imaging enhancement or satellite remote sensing data improvement, aims to reconstruct high-resolution images from low-resolution ones using only one input image.\n\nResearch Problem: Despite significant advancements made possible through the application of convolutional neural networks (CNNs), existing methods often suffer from substantial computational burdens due to their heavy reliance on operations like depth-wise convolutions when implemented on GPUs – leading to long processing times despite promising results.\n \nMethodology: This paper introduces novel techniques addressing this issue:\n1. Ghost Features Generation: By employing shift operations instead of depth-wise convolutions within CNN architectures during training phase, it generates so-called \"ghost\" features alongside regular ones without additional computation cost; these ghost features share similarities among themselves yet differ slightly enough not to cause confusion between them while still contributing meaningfully towards improving resolution quality upon concatenation into final outputs.\n2. Learnable Shift Orientation: To further optimize ghost generation process, authors introduce a trainable parameter controlling directionality shifts applied via Gumbel-Softmax trick - allowing network adaptively adjust its focus areas according to specific tasks' demands rather than having fixed orientations hardcoded beforehand.\n3. Clustering-Based Filter Identification: A clustering algorithm identifies essential filters responsible for producing distinctive intrinsic features necessary before applying shift transformations onto those selected filters yielding ghost counterparts; thus resulting in more efficient utilization resources throughout entire pipeline compared traditional approaches where all layers contribute equally regardless importance level assigned per layer individually.\n4. Feature Concatenation: Final step involves combining original intrinsic features with newly created ghosts versions thereof resulting composite representations capable delivering higher resolutions whilst reducing overall complexity measured terms parameters required (by approximately half), floating-point operations performed (also halved), and GPU inference latency experienced (cut down by roughly forty-two percent).\n\nMain Contributions: This study makes three primary contributions toward advancing state-of-the-art SISR methodologies utilizing CNN-based frameworks:\n\n1. Demonstrates how shifting operations may serve effectively substitute computationally expensive depth-wise convolutions found commonly employed current architectures;\n2. Introduces a flexible approach leveraging learnable shift directions derived from stochastic sampling procedures akin Gumbel-Softmax distribution enabling adaptive optimization tailored particular applications needs;\n3. Proposes an innovative filter identification strategy informed by clustering algorithms aiding prioritize usage resources amongst constituent components architecture thereby enhancing efficiency gains realized implementation context.\n\nConclusion: Overall findings indicate embedding aforementioned modifications into standard CNN-based SISR setups yields considerable improvements terms accuracy maintained relative baselines coupled drastic reductions dimensions resource consumption making practical deployment feasible broader range scenarios previously deemed impractical due prohibitive computational overhead involved prior solutions deployed today's hardware platforms.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Extracting Local Reasoning Chains of Deep Neural Networks",
        "abstract": "We study how to explain the main steps of inference that a pre-trained deep neural net (DNN) relies on to produce predictions for a (sub)task and its data.\nThis problem is related to network pruning and interpretable machine learning with the following highlighted differences: (1) fine-tuning of any neurons/filters is forbidden; (2) we target a very high pruning rate, e.g., ≥ 95%, for better interpretability; (3) the interpretation is for the whole inference process on a few data of a task rather than for individual neurons/filters or a single sample.\nIn this paper, we introduce NeuroChains to extract the local inference chains by optimizing differentiable sparse scores for the filters and layers, which reflects their importance in preserving the outputs on a few data drawn from a given (sub)task. \nThereby, NeuroChains can extract an extremely small sub-network composed of critical filters exactly copied from the original pre-trained DNN by removing the filters/layers with small scores. \nFor samples from the same class, we can then visualize the inference pathway in the pre-trained DNN by applying existing interpretation techniques to the retained filters and layers.\nIt reveals how the inference process stitches and integrates the information layer by layer and filter by filter. \nWe provide detailed and insightful case studies together with several quantitative analyses over thousands of trials to demonstrate the quality, sparsity, fidelity and accuracy of the interpretation. In extensive empirical studies on VGG, ResNet, and ViT, NeuroChains significantly enriches the interpretation and makes the inner mechanism of DNNs more transparent. ",
        "authors": "H. Zhao, T. Zhou, G. Long, et.al",
        "keywords": [
            "NeuroChains",
            "Interpretability",
            "Pruning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=RP6G787uD8",
        "pdf_src": "https://api2.openreview.net/pdf/91d59742dd2ff2e6877393ca1fd560a05b05a43e.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this research lies within the field of artificial intelligence where there has been growing interest in understanding why certain decisions are made during the prediction phase using pre-trained deep neural networks (DNN). This need arises due to concerns about black-box nature often associated with such models.\n\nResearch Problem:\nThe primary challenge addressed here involves explaining key components used throughout the entire inference procedure when predicting outcomes based on specific tasks and datasets without altering any neuron/filter weights through fine-tuning - something typically done after initial training but not considered practical under these constraints because it would require prohibitive computational resources leading up to near-complete retraining efforts instead just focusing on what matters most at runtime performance-wise while still retaining interpretability benefits.\n\nMethodology:\nTo tackle said issue effectively yet efficiently enough so as not compromise too much interpretability nor sacrifice significant runtime speed gains achievable via aggressive pruning rates like ≥95%, authors propose \"NeuroChains\". These consist essentially being able optimize differentiable sparse scores assigned per filter/layer across multiple iterations until they reflect their relative contribution towards maintaining output consistency upon minimal input perturbations sampled from relevant subtasks/data distributions pertinent toward solving targeted predictive problems accordingly.\n\nMain Contributions:\nThe contributions outlined include introducing novel approach called NeuroChains capable extracting localized inference pathways known locally referred-to as 'chains' wherein each node represents either one particular neuron/filter pair involved therein along with respective weight coefficients pertaining thereto respectively allowing us observe step-by-step progression occurring inside trained model architecture itself thereby providing insights into decision-making processes underlying them all whilst also demonstrating effectiveness achieved even though employing stringent criteria regarding acceptable levels complexity reduction alongside preservation accuracy metrics measured against various benchmarks including those involving well-known architectures like VGG16/19, ResNet50/101/152 variants plus Vision Transformer (ViT).",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents",
        "abstract": "Diffusion probabilistic models have been shown to generate state-of-the-art results on several competitive image synthesis benchmarks but lack a low-dimensional, interpretable latent space, and are slow at generation. On the other hand, standard Variational Autoencoders (VAEs) typically have access to a low-dimensional latent space but exhibit poor sample quality. We present DiffuseVAE, a novel generative framework that integrates VAE within a diffusion model framework, and leverage this to design novel conditional parameterizations for diffusion models. We show that the resulting model equips diffusion models with a low-dimensional VAE inferred latent code which can be used for downstream tasks like controllable synthesis. The proposed method also improves upon the speed vs quality tradeoff exhibited in standard unconditional DDPM/DDIM models (for instance, \\textbf{FID of 16.47 vs 34.36} using a standard DDIM on the CelebA-HQ-128 benchmark using \\textbf{T=10} reverse process steps) without having explicitly trained for such an objective. Furthermore, the proposed model exhibits synthesis quality comparable to state-of-the-art models on standard image synthesis benchmarks like CIFAR-10 and CelebA-64 while outperforming most existing VAE-based methods. Lastly, we show that the proposed method exhibits inherent generalization to different types of noise in the conditioning signal. For reproducibility, our source code is publicly available at \\url{https://github.com/kpandey008/DiffuseVAE}.",
        "authors": "K. Pandey, A. Mukherjee, P. Rai, et.al",
        "keywords": [
            "DiffusionVAE",
            "Generative Framework",
            "Low-Dimensional Latent Space"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ygoNPRiLxw",
        "pdf_src": "https://api2.openreview.net/pdf/c5aa03a5dfb8230a6d2437d8f280432fff5a92db.pdf",
        "Code_src": "[GitHub链接](https://github.com/kpandey008/DiffuseVAE)",
        "Introduction": "Background: Recent advancements in diffusion probabilistic models enable them to produce high-quality images; however, they suffer from limitations including difficulty in interpreting their latent spaces or generating samples quickly.\n\nResearch Problem: How do we create a generative framework combining diffusion models' strengths – fast sampling times — with variational autoencoders' benefits - easy interpretation through a low-dimensional latent space?\n\nMethod: We introduce DiffuseVAE by embedding a variational autoencoder into a diffusion model's architecture allowing us to conditionally parameterize diffusion processes based on the encoded representations learned during training time.\n \nMain Contributions:\n1. A new generative framework called DiffuseVAE integrating both diffusion and variational autoencoder architectures;\n2. Novel conditional parameterizations designed specifically tailored towards improving performance metrics over traditional diffusion-only approaches;\n3. Demonstrated improvements across various benchmarks demonstrating better balance between generation speed and fidelity compared to prior works without requiring specialized optimization objectives;\n4. Synthesis quality equivalent to leading-edge models yet superior when evaluated against many existing VAE-based alternatives;\n5. Generalizability demonstrated beyond typical datasets showing robustness under varied conditions;\n6. Open-sourced implementation details facilitating further research replication via GitHub repository link provided (\\url{https://github.com/kpandey008/DiffuseVAE}).",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Calibrated Selective Classification",
        "abstract": "Selective classification  allows models to abstain from making predictions (e.g., say ``I don't know'') when in doubt in order to obtain better effective accuracy. While typical selective models can succeed at producing more accurate predictions on average, they may still allow for wrong predictions that have high confidence, or skip correct predictions that have low confidence. Providing calibrated uncertainty estimates alongside predictions---probabilities that correspond to true frequencies---can be as important as having predictions that are simply accurate on average. Uncertainty estimates, however,  can sometimes be unreliable. In this paper, we develop a new approach to selective classification in which we propose a method for rejecting examples with ``uncertain'' uncertainties. By doing so, we aim to make predictions with well-calibrated uncertainty estimates over the distribution of accepted examples, a property we call selective calibration. We present a framework for learning selectively calibrated models, where a separate selector network is trained to improve the selective calibration error of a given base model. In particular, our work focuses on achieving robust calibration, where the model is intentionally designed to be tested on out-of-domain data. We achieve this through a training strategy inspired by distributionally robust optimization, in which we apply simulated input perturbations to the known, in-domain training data. We  demonstrate the empirical effectiveness of our approach on multiple image classification and lung cancer risk assessment tasks.",
        "authors": "A. Fisch, T. S. Jaakkola, R. Barzilay",
        "keywords": [
            "uncertainty estimation",
            "selective calibration",
            "robustness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=zFhNBs8GaV",
        "pdf_src": "https://api2.openreview.net/pdf/af329ecf778809c5d45b01f6da3400d570d872cb.pdf",
        "Code_src": "",
        "Introduction": "Background: The problem addressed here concerns improving the performance of machine learning models specifically in situations involving uncertainty about their predictions.\nResearch Question: How might one design a system within an existing selective classification framework such that it not only makes accurate but also reliably uncertain predictions?\n\nMethodology: This research introduces a novel modification called \"selective calibration\" into the selective classification process aiming towards rejection of those instances whose uncertainty scores suggest ambiguity (\"uncertain\" uncertainties). A two-step architecture was developed; first, a base classifier predicts class probabilities while secondly, a selector network assesses these predictions based on how certain each prediction should actually be.\n\nMain Contributions:\n1. **Selective Calibration**: They introduce a mechanism allowing the model to reject predictions associated with high uncertainty levels deemed ambiguous ('uncertain' uncertainties).\n2. **Robustness**: To ensure reliability across different domains beyond what's seen during training, they employ distributionally robust optimization techniques leading to a robust calibration against unseen distributions via simulated perturbations applied directly onto the dataset used throughout training phase.\n3. **Empirical Validation**: Their proposed methods were empirically validated using both image classification problems along with real-world medical applications like predicting lung cancer risks demonstrating improved predictive capabilities including reliable uncertainty estimation even under varying conditions outside standard training environments.",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "Unsupervised Mismatch Localization in Cross-Modal Sequential Data with Application to Mispronunciations Localization",
        "abstract": "Content mismatch usually occurs when data from one modality is translated to another, e.g. language learners producing mispronunciations (errors in speech) when reading a sentence (target text) aloud. However, most existing alignment algorithms assume that the content involved in the two modalities is perfectly matched, thus leading to difficulty in locating such mismatch between speech and text. In this work, we develop an unsupervised learning algorithm that can infer the relationship between content-mismatched cross-modal sequential data, especially for speech-text sequences. More specifically, we propose a hierarchical Bayesian deep learning model, dubbed mismatch localization variational autoencoder (ML-VAE), which decomposes the generative process of the speech into hierarchically structured latent variables, indicating the relationship between the two modalities. Training such a model is very challenging due to the discrete latent variables with complex dependencies involved. To address this challenge, we propose a novel and effective training procedure that alternates between estimating the hard assignments of the discrete latent variables over a specifically designed mismatch localization finite-state acceptor (ML-FSA) and updating the parameters of neural networks. In this work, we focus on the mismatch localization problem for speech and text, and our experimental results show that ML-VAE successfully locates the mismatch between text and speech, without the need for human annotations for model training.",
        "authors": "W. Wei, H. Huang, X. Gu, et.al",
        "keywords": [
            "speech",
            "text",
            "mismatch localization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=29V0xo7jKp",
        "pdf_src": "https://api2.openreview.net/pdf/6cfcf9eedf01725255b7afded566c49356d259ce.pdf",
        "Code_src": "",
        "Introduction": "Background: Content mismatch often arises during multimodal data translation processes like language learning where pronunciation errors occur while speaking sentences read aloud.\n\nResearch Problem: Existing alignment algorithms presume perfect content match across modalities but struggle at identifying mismatches because they are not designed to handle variations or discrepancies within the data.\n\nMethodology: The authors introduce an unsupervised learning approach capable of inferring relationships among content-mismatched cross-modal sequential data—specifically focusing on speech-text pairs.\n\nMain Contributions:\n1. A Hierarchical Bayesian Deep Learning Model called Mismatch Localization Variational Autoencoder (ML-VAE). This model breaks down the generation process of spoken words using hierarchically organized latent variables.\n2. An innovative training strategy involving alternating estimations through a specially crafted Mismatch Localization Finite-State Acceptor (ML-FSA).\n3. Successful application demonstrated by effectively localizing mismatches between speech and text inputs—all without requiring manual annotation typically needed throughout machine learning model development phases.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Interpretable Node Representation with Attribute Decoding",
        "abstract": "Variational Graph Autoencoders (VGAEs) are powerful models for unsupervised learning of node representations from graph data. In this work, we make a systematic analysis of modeling node attributes in VGAEs and show that attribute decoding is important for node representation learning. We further propose a new learning model, interpretable NOde Representation with Attribute Decoding (NORAD). The model encodes node representations in an interpretable approach: node representations capture community structures in the graph and the relationship between communities and node attributes. We further propose a rectifying procedure to refine node representations of isolated notes, which improves the quality of the representations of these nodes. Our empirical results demonstrate the advantage of the proposed model when learning graph data in an interpretable approach.",
        "authors": "X. Chen, X. Chen, L. Liu",
        "keywords": [
            "graph autoencoder",
            "node representation",
            "attribute decoding"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=AZIfC91hjM",
        "pdf_src": "https://api2.openreview.net/pdf/2f86b0b4680558f4b45279302f23336a9ef23654.pdf",
        "Code_src": "",
        "Introduction": "Background:\nGraph data often contains rich information about relationships among entities represented by nodes within networks or graphs. Variational Graph Autoencoders (VGAEs) have been shown as effective tools for unsupervised learning tasks on such datasets.\n\nResearch Problem:\nThe problem addressed here concerns how to effectively incorporate node attributes into VGAEs during the process of learning node representations without supervision.\n \nMethodology:\nTo tackle this issue, authors conduct a comprehensive study analyzing the importance of attribute decoding processes used in existing VGAE architectures; they find it crucial yet underexplored aspect influencing learned node representations' interpretability & accuracy. They then introduce NORAD - a novel architecture incorporating attribute decoding mechanism explicitly designed around capturing both community structure within graphs along with their interconnections via node attributes while encoding them meaningfully through rectification procedures enhancing those representations specifically tailored towards nodes lacking direct connections ('isolated nodes') leading improvements overall over standard VGAE performance metrics across various benchmark datasets demonstrating its efficacy beyond conventional approaches thus contributing significantly advancing current understanding regarding scalable methods dealing with complex relational data types like social networks etcetera where detailed insights derived from nodes’ embeddings could be invaluable assets aiding decision-making processes related thereto.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "A Unified Domain Adaptation Framework with Distinctive Divergence Analysis",
        "abstract": "Unsupervised domain adaptation enables knowledge transfer from a labeled source domain to an unlabeled target domain by aligning the learnt features of both domains. The idea is theoretically supported by the generalization bound analysis in Ben-David et al. (2007), which specifies the applicable task (binary classification) and designates a specific distribution divergence measure. Although most distribution-aligning domain adaptation models seek theoretical grounds from this particular bound analysis, they do not actually fit into the stringent conditions. In this paper, we bridge the long-standing theoretical gap in literature by providing a unified generalization bound. Our analysis can well accommodate the classification/regression tasks and most commonly-used divergence measures, and more importantly, it can theoretically recover a large amount of previous models. In addition, we identify the key difference in the distribution divergence measures underlying the diverse models and commit a comprehensive in-depth comparison of the commonly-used divergence measures. Based on the unified generalization bound, we propose new domain adaptation models that achieve transferability through domain-invariant representations and conduct experiments on real-world datasets that corroborate our theoretical findings. We believe these insights are helpful in guiding the future design of distribution-aligning domain adaptation algorithms.",
        "authors": "Z. Yuan, X. Hu, Q. Wu, et.al",
        "keywords": [
            "distribution alignment",
            "generalization bounds",
            "domain adaptation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=yeT9cBq8Cn",
        "pdf_src": "https://api2.openreview.net/pdf/a2c766c0f767220c8456534fbf4152cd11ad2190.pdf",
        "Code_src": "",
        "Introduction": "Background: Unsupervised domain adaptation aims at transferring knowledge learned from a labeled source domain to an unlabeled target domain without requiring any manual annotation for the latter.\n\nResearch Problem: Existing unsupervised domain adaptation methods have been largely based on empirical heuristics rather than rigorous theory due to the challenging nature of satisfying the strict conditions required under certain theoretical bounds such as those proposed by Ben-David et al. (2007).\n\nMethodology: This work bridges the existing theoretical gap between practical applications and theoretical guarantees provided by earlier analyses like Ben-David's. It introduces a novel unifying generalization bound framework capable of accommodating various classification/regression tasks along with common divergence measures used in practice while also being able to encompass many previously studied models within its scope.\n\nMain Contributions:\n1. A unified generalization bound that provides a theoretical foundation beyond binary classification.\n2. An inclusive approach that covers multiple types of tasks including classification and regression problems across different domains using standard divergence metrics widely adopted today; \n3. Identification & comparative study focusing on understanding differences among prevalent distribution divergence measures;\n4. Development of novel DA models informed by the unified bound leading towards improved transfer learning performance validated via experimental results obtained over realistic datasets demonstrating their effectiveness against prior works' limitations regarding theoretical grounding or applicability range.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Learning Accurate Decision Trees with Bandit Feedback via Quantized Gradient Descent",
        "abstract": "Decision trees provide a rich family of highly non-linear but efficient models, due to which they continue to be the go-to family of predictive models by practitioners across domains. But learning trees is challenging due to their discrete decision boundaries. The state-of-the-art (SOTA) techniques resort to (a) learning soft trees thereby losing logarithmic inference time; or (b) using methods tailored to specific supervised learning settings, requiring access to labeled examples and loss function. In this work, by leveraging techniques like overparameterization and straight-through estimators, we propose a unified method that enables accurate end-to-end gradient based tree training and can be deployed in a variety of settings like offline supervised learning and online learning with bandit feedback. Using extensive validation on standard benchmarks, we demonstrate that our method provides best of both worlds, i.e., it is competitive to, and in some cases more accurate than methods designed specifically for the supervised settings; and in bandit settings, where most existing tree learning techniques are not applicable, our models are still accurate and significantly outperform the applicable SOTA methods.",
        "authors": "A. Karthikeyan, N. Jain, N. Natarajan, et.al",
        "keywords": [
            "tree learning",
            "overparameterization",
            "bandit feedback"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=u0n1chY0b6",
        "pdf_src": "https://api2.openreview.net/pdf/3920795116aa5f962e1953af9cd4c46225a1c23c.pdf",
        "Code_src": "",
        "Introduction": "Background: Decision trees have been widely used as predictive models because of their high efficiency despite being non-linear. However, learning these trees has proven difficult mainly due to their discrete decision boundaries.\n\nResearch Question: How do you learn decision trees efficiently without compromising accuracy?\n\nMethod: This paper proposes a unified approach towards learning decision trees through techniques such as overparameterization and straight-through estimators allowing for an accurate end-to-end gradient-based tree training process suitable for various scenarios including offline supervised learning environments along with online setups utilizing bandit feedback mechanisms.\n\nMain Contributions:\n1. A novel framework integrating concepts from different machine learning paradigms into one system capable enough handle diverse types of tasks.\n2. Demonstrated effectiveness via empirical validations against established baselines under multiple conditions - showing superior performance relative specialized supervised learners while also excelling when compared traditional approaches adapted only within limited bandit contexts",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Probabilistic Autoencoder",
        "abstract": "Principal Component Analysis (PCA) minimizes the reconstruction error given a class of linear models of fixed component dimensionality. Probabilistic PCA adds a probabilistic structure by learning the probability distribution of the PCA latent space weights, thus creating a generative model. Autoencoders (AE) minimize the reconstruction error in a class of nonlinear models of fixed latent space dimensionality and outperform PCA at fixed dimensionality. Here, we introduce the Probabilistic Autoencoder (PAE) that learns the probability distribution of the AE latent space weights using a normalizing flow (NF). The PAE is fast and easy to train and achieves small reconstruction errors, high sample quality, and good performance in downstream tasks. We compare the PAE to Variational AE (VAE), showing that the PAE trains faster, reaches a lower reconstruction error, and produces good sample quality without requiring special tuning parameters or training procedures. \nWe further demonstrate that the PAE is a powerful model for performing the downstream tasks of probabilistic image reconstruction in the context of Bayesian inference of inverse problems for inpainting and denoising applications. Finally, we identify latent space density from NF as a promising outlier detection metric.",
        "authors": "V. M. Boehm, U. Seljak",
        "keywords": [
            "Probabilistic PCA",
            "Autoencoder",
            "Normalizing Flow"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=AEoYjvjKVA",
        "pdf_src": "https://api2.openreview.net/pdf/b6c74a1996a4947295035df8ae01402e9e8ec5f8.pdf",
        "Code_src": "",
        "Introduction": "Background: Principal Component Analysis (PCA) is an effective method used widely due to its simplicity yet effectiveness when dealing with data reduction through linear transformations while preserving most variance within the dataset; however, it does not account for any underlying probabilistic structures present.\n\nResearch Problem: To address this limitation, researchers have developed Probabilistic PCA which introduces a probabilistic framework into PCA allowing one to learn the probability distribution over the latent space weights but still constrained under a linear model assumption.\nAutoencoders are another type of neural network architecture designed specifically around minimizing reconstruction error between input and output data points via non-linear transformations where they can capture more complex patterns than PCA could ever hope to achieve even if trained on higher dimensional spaces.\n\nMethodology: In our work, we propose Probabilistic Autoencoder (PAE), which extends the idea behind Probabilistic PCA - incorporating probabilities directly onto autoencoders' latent space representations rather than just their weights alone – by utilizing Normalizing Flows (NFs) during training time so these distributions become tractable enough computationally feasible despite being highly flexible compared traditional methods like Gaussian Mixture Models (GMM).\n\nMain Contributions:\n1. Introduced Probabilistic Autoencoder (PAE) based on Normalizing Flow (NF) technique capable of capturing both variability across datasets along with generating new samples close-to-realistic ones whilst maintaining low reconstruction errors;\n2. Demonstrated how PAEs perform better overall against other state-of-the-art approaches such as Variational Autoencoders (VAEs);\n3. Applied PAEs successfully towards solving various downstream tasks including probabilistic image reconstruction related Bayesian inference techniques applied toward inpainting & denoising purposes;\n4. Identified latent space density estimated w.r.t NF as useful outlier detection metric",
        "Topic": "Optimal Transport"
    },
    {
        "title": "Decoder Denoising Pretraining for Semantic Segmentation",
        "abstract": "Semantic segmentation labels are expensive and time consuming to acquire. Hence, pretraining is commonly used to improve the label-efficiency of segmentation models. Typically, the encoder of a segmentation model is pretrained as a classifier and the decoder is randomly initialized. Here, we argue that random initialization of the decoder can be suboptimal, especially when few labeled examples are available. We propose a decoder pretraining approach based on denoising, which can be combined with supervised pretraining of the encoder. We find that decoder denoising pretraining on the ImageNet dataset strongly outperforms encoder-only supervised pretraining. Despite its simplicity, decoder denoising pretraining achieves state-of-the-art results on label-efficient semantic segmentation and offers considerable gains on the Cityscapes, Pascal Context, and ADE20K datasets.",
        "authors": "E. A. Brempong, S. Kornblith, T. Chen, et.al",
        "keywords": [
            "denoising",
            "decoder pretraining",
            "label efficiency"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=D3WI0QG7dC",
        "pdf_src": "https://api2.openreview.net/pdf/2e34219a3adb9ee505e2e139960992971d0761ad.pdf",
        "Code_src": "",
        "Introduction": "Background: Semantic segmentation involves labeling each pixel in an image according to categories from a predefined set using convolutional neural networks trained end-to-end for this task. However, obtaining accurate segmentations requires manually annotating images by humans or semi-automated methods like instance segmentation algorithms; these annotations take significant effort.\n\nResearch Problem: The main challenge addressed here concerns how to efficiently train semantic segmentation models without requiring many annotated data points due to their high cost and laborious nature leading to low label efficiency during training.\n\nMethodology: To address this issue, authors introduce a novel method called \"decoder denoising pretraining\" where they fine-tune the decoder part of the network through denoising autoencoder tasks before any supervision starts - unlike common practice involving initializing it randomly after encoding pretraining phase.\nThis process helps stabilize learning early stages while also improving generalization capabilities across various datasets such as ImageNet, Cityscapes, Pascal Context, etc.\n\nMain Contributions:\n1. They demonstrate that starting with random initializations may not always yield optimal performance specifically under limited annotation scenarios suggesting there's room for improvement beyond standard practices;\n2. By introducing decoder denoising pretraining alongside encoder supervised pretraining techniques – they achieve better label efficiency than just encoder supervised pretraining alone;\n3. Their proposed approach significantly improves upon existing benchmarks achieving competitive scores against other advanced approaches within semantic segmentation domain making it more practical even if less labeled samples exist at hand.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Can You Win Everything with A Lottery Ticket?",
        "abstract": "$\\textit{Lottery ticket hypothesis}$ (LTH) has demonstrated to yield independently trainable and highly sparse neural networks (a.k.a. $\\textit{winning tickets}$), whose test set accuracies can be surprisingly on par or even better than dense models. However, accuracy is far from the only evaluation metric, and perhaps not always the most important one. Hence it might be myopic to conclude that a sparse subnetwork can replace its dense counterpart, even if the accuracy is preserved. Spurred by that, we perform the first comprehensive assessment of lottery tickets from diverse aspects beyond test accuracy, including $\\textit{(i)}$ generalization to distribution shifts, $\\textit{(ii)}$ prediction uncertainty, $\\textit{(iii)}$ interpretability, and $\\textit{(iv)}$ geometry of loss landscapes. With extensive experiments across datasets {CIFAR-10, CIFAR-100, and ImageNet}, model architectures, as well as tens of sparsification methods, we thoroughly characterize the trade-off between model sparsity and the all-dimension model capabilities. We find that an appropriate sparsity (e.g., $20\\%\\sim99.08\\%$) can yield the winning ticket to perform comparably or even better $\\textbf{in all above four aspects}$, although some aspects (generalization to certain distribution shifts, and uncertainty) appear more sensitive to the sparsification than others. We term it as a $\\texttt{LTH-PASS}$. Overall, our results endorse choosing a good sparse subnetwork of a larger dense model, over directly training a small dense model of similar parameter counts. We hope that our study can offer more in-depth insights on pruning, for researchers and engineers who seek to incorporate sparse neural networks for user-facing deployments. Codes are available in: https://github.com/VITA-Group/LTH-Pass.",
        "authors": "T. Chen, Z. Zhang, J. Wu, et.al",
        "keywords": [
            "pruning",
            "lottery ticket hypothesis",
            "sparsity"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=JL6MU9XFzW",
        "pdf_src": "https://api2.openreview.net/pdf/ff0d8d4b2a1cd939d8404c365a6a9390cb727832.pdf",
        "Code_src": "https://github.com/VITA-Group/LTH-Pass",
        "Introduction": "Background:\nThe Lottery Ticket Hypothesis (LTH) proposes that randomly initialized neural networks contain \"winning tickets\" - small, independent, and highly sparse subnetworks with comparable performance metrics compared to their full counterparts.\n\nResearch Problem:\nWhile LTH shows promising findings regarding the potential replacement of dense networks through sparse ones based solely on test accuracy, this paper raises concerns about whether such replacements hold true when considering other relevant factors like generalization robustness against distribution shifts, prediction uncertainty, interpretability, and the complexity of the learning landscape.\n\nMethods:\nThis research conducts a thorough investigation into these additional dimensions using extensive empirical studies involving various datasets (CIFAR-10, CIFAR-100, and ImageNet), network architectures, and numerous sparsity-inducing techniques.\n \nMain Contributions:\n1. The authors introduce the concept of a LTH-PASS – a finding where a subset of neurons within a large network achieves superior performance overall despite being sparse; they identify optimal levels of sparsity ranging 20%-99.08%.\n\n2. They demonstrate variability among different aspects' sensitivity towards sparsity level—some may require higher sparsity rates while still maintaining effectiveness whereas others show less tolerance.\n\n3. This work provides evidence supporting the idea behind LTH, suggesting that selecting a suitable sparse subnetwork could sometimes outperform starting anew with smaller but denser networks which have equivalent parameters.\n\n4. Finally, accompanying code is made publicly accessible at GitHub repository, aiming to facilitate further exploration related to sparse neural networks especially those intended for practical applications.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "HEAT: Hyperedge Attention Networks",
        "abstract": "Learning from structured data is a core machine learning task. Commonly, such data is represented as graphs, which normally only consider (typed) binary relationships between pairs of nodes. This is a substantial limitation for many domains with highly-structured data. One important such domain is source code, where hypergraph-based representations can better capture the semantically rich and structured nature of code.\nIn this work, we present HEAT, a neural model capable of representing typed and qualified hypergraphs, where each hyperedge explicitly qualifies how participating nodes contribute. It can be viewed as a generalization of both message passing neural networks and Transformers. We evaluate HEAT on knowledge base completion and on bug detection and repair using a novel hypergraph representation of programs. In both settings, it outperforms strong baselines, indicating its power and generality.",
        "authors": "D. G. Georgiev, M. Brockschmidt, M. Allamanis",
        "keywords": [
            "hypergraph representation",
            "neural model",
            "knowledge base completion"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=gCmQK6McbR",
        "pdf_src": "https://api2.openreview.net/pdf/a81c6995c502b1fe1872078f2d3adb282141afe4.pdf",
        "Code_src": "",
        "Introduction": "Background: Learning from structured data plays a crucial role in machine learning tasks; however, common graph representations often overlook complex interdependencies by considering only binary relationships.\n\nResearch Problem: To address limitations posed by binary graph representations when dealing with highly structured datasets like source code that require more nuanced semantic understanding through hypergraph-based models capturing richer node interactions.\n\nMethods: The paper introduces HEAT - a neural network architecture designed to represent typed and qualified hypergraphs effectively conveying contributions made by individual nodes within these structures across edges or hyperedges implicitly.\n\nMain Contributions:\n1. HEAT extends beyond traditional graph neural networks (GNNs), including message-passing architectures typically used since they do not support arbitrary edge weights nor handle multiple types of relations simultaneously efficiently enough compared to hypergraphs.\n2. By utilizing attention mechanisms akin to those found in Transformer frameworks but adapted specifically towards handling hierarchical information encoded into hypergraphs naturally without requiring explicit reordering steps seen during sequence processing operations typical among Transformers.\n3. Evaluation experiments conducted demonstrate superior performance over existing state-of-the-art baseline methods applied toward completing knowledge bases along with detecting bugs/repairing issues via programmatic hypergraph embeddings suggesting broad applicability potentialities outside academia too – potentially aiding developers automate debugging workflows significantly improving productivity outcomes overall while reducing human effort required manually inspecting lines upon lines",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "On the Near-Optimality of Local Policies in Large Cooperative Multi-Agent Reinforcement Learning",
        "abstract": "We show that in a cooperative $N$-agent network, one can design locally executable policies for the agents such that the resulting discounted sum of average rewards (value) well approximates the optimal value computed over all (including non-local) policies. Specifically, we prove that, if $|\\mathcal{X}|, |\\mathcal{U}|$ denote the size of state, and action spaces of individual agents, then for sufficiently small discount factor, the approximation error is given by $\\mathcal{O}(e)$ where $e\\triangleq \\frac{1}{\\sqrt{N}}\\left[\\sqrt{|\\mathcal{X}|}+\\sqrt{|\\mathcal{U}|}\\right]$. Moreover, in a special case where the reward and state transition functions are independent of the action distribution of the population, the error improves to $\\mathcal{O}(e)$ where $e\\triangleq \\frac{1}{\\sqrt{N}}\\sqrt{|\\mathcal{X}|}$. Finally, we also devise an algorithm to explicitly construct a local policy. With the help of our approximation results, we further establish that the constructed local policy is within $\\mathcal{O}(\\max\\{e,\\epsilon\\})$ distance of the optimal policy, and the sample complexity to achieve such a local policy is $\\mathcal{O}(\\epsilon^{-3})$, for any $\\epsilon>0$. ",
        "authors": "W. U. Mondal, V. Aggarwal, S. Ukkusuri",
        "keywords": [
            "cooperative multi-agent system",
            "decentralized control",
            "approximate optimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=t5HkgbxZp1",
        "pdf_src": "https://api2.openreview.net/pdf/231ed1cc5a2db6c43b013887f3060a4f759b1fda.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper focuses on cooperative multi-agent networks with the goal of designing locally executable policies so that the resulting discounted sum of average rewards approximately matches the optimal value obtained from considering global policies.\n\nResearch Problem: The problem addressed here involves finding appropriate local policies which lead to close-to-optimal performance when applied across multiple cooperating agents without requiring access to or computation based on other agent's actions outside their immediate influence range.\n\nMethodology: The authors propose two main contributions toward solving this research question:\n\n1. They provide theoretical guarantees showing how much better these local policies perform compared to random ones; specifically they derive an approximation error bound $\\mathcal{O}(e)$ depending on the square root of both the number of states and actions available per agent.\n2. In certain cases—where the reward and state transition functions do not depend on the collective behavior of the entire group—they improve upon previous bounds significantly reducing it to $\\mathcal{O}(e)$ solely dependent on the state space dimensionality.\n\nMain Contributions:\nThe primary contribution lies in proving that under specific conditions, even though each agent operates independently using only its own observations as input into its decision-making process, collectively through cooperation among them, there exists a way designed via algorithms proposed therein ensuring convergence towards near-optimal outcomes at acceptable computational cost.\n\nAdditionally, another significant aspect highlighted includes devising methods allowing explicit construction of said local policies leading us closer than before (\\(\\mathcal{O}(\\max\\{e,\\epsilon\\})\\) away), while demonstrating lower sample complexities (\\(\\mathcal{O}(\\epsilon^{-3})\\)) required",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Weight Expansion: A New Perspective on Dropout and Generalization",
        "abstract": "While dropout is known to be a successful regularization technique, insights into the mechanisms that lead to this success are still lacking. We introduce the concept of weight expansion, an increase in the signed volume of a parallelotope spanned by the column or row vectors of the weight covariance matrix, and show that weight expansion is an effective means of increasing the generalization in a PAC-Bayesian setting. We provide a theoretical argument that dropout leads to weight expansion and extensive empirical support for the correlation between dropout and weight expansion. To support our hypothesis that weight expansion can be regarded as an indicator of the enhanced generalization capability endowed by dropout, and not just as a mere by-product, we have studied other methods that achieve weight expansion (resp.\\ contraction), and found that they generally lead to an increased (resp.\\ decreased) generalization ability. This suggests that dropout is an attractive regularizer, because it is a computationally cheap method for obtaining weight expansion. This insight justifies the role of dropout as a regularizer, while paving the way for identifying regularizers that promise improved generalization through weight expansion.",
        "authors": "G. Jin, X. Yi, P. Yang, et.al",
        "keywords": [
            "weight expansion",
            "dropout",
            "generalization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=w3z3sN1b04",
        "pdf_src": "https://api2.openreview.net/pdf/a69e25f59662a189d380cabc26ef0bd8a3c29284.pdf",
        "Code_src": "",
        "Introduction": "Background: Dropout has been widely used as a regularization technique due to its effectiveness; however, there's limited understanding about why it works.\n\nResearch Question: The paper aims at exploring the mechanism behind dropout’s efficacy - specifically focusing on how dropout increases model generalization capabilities within a PAC-Bayesian framework.\n \nMethodology: The authors propose \"weight expansion\" which refers to growth in the signed volume of the parallelepiped formed by the columns or rows of the weight covariance matrix when applying dropout. They then theoretically argue that dropout causes such expansion leading to better generalization performance with empirical evidence supporting their claim.\n\nMain Contributions:\n1. Introducing 'weight expansion' theory linking dropout to improved generalization via a PAC-Bayesian perspective;\n2. Demonstrating empirically that dropout correlates positively with weight expansion across various datasets and models;\n3. Providing theoretical justification regarding the causal link from dropout to weight expansion rather than merely correlational data points;\n4. Showing that weight expansion does indeed correlate with stronger generalization abilities beyond dropout itself suggesting that dropout serves more fundamentally beneficial purposes compared to being simply a computational convenience.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Exploring Efficient Few-shot Adaptation for Vision Transformers",
        "abstract": "The task of Few-shot Learning (FSL) aims to do the inference on novel categories containing only few labeled examples, with the help of knowledge learned from base categories containing abundant labeled training samples. While there are numerous works into FSL task, Vision Transformers (ViTs) have rarely been taken as the backbone to FSL with few trials focusing on naive finetuning of whole backbone or classification layer. Essentially, despite ViTs have been shown to enjoy comparable or even better performance on other vision tasks, it is still very nontrivial to efficiently finetune the ViTs in real-world FSL scenarios. To this end, we propose a novel efficient Transformer Tuning (eTT) method that facilitates finetuning ViTs in the FSL tasks. The key novelties come from the newly presented Attentive Prefix Tuning (APT) and Domain Residual Adapter (DRA) for the task and backbone finetuning, individually. Specifically, in APT, the prefix is projected to new key and value pairs that are attached to each self-attention layer to provide the model with task-specific information. Moreover, we design the DRA in the form of learnable offset vectors to handle the potential domain gaps between base and novel data. To ensure the APT would not deviate from the initial task-specific information much, we further propose a novel prototypical regularization, which minimizes the similarity between the projected distribution of prefix and initial prototypes, regularizing the update procedure. Our method receives outstanding performance on the challenging Meta-Dataset. We conduct extensive experiments to show the efficacy of our model. Our model and codes will be released.",
        "authors": "C. Xu, S. Yang, Y. Wang, et.al",
        "keywords": [
            "Few-shot Learning",
            "Vision Transformers",
            "Efficient Transformer Tuning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=n3qLz4eL1l",
        "pdf_src": "https://api2.openreview.net/pdf/9fcda50f3fbc597f3448367cca5c002a77d96d86.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper focuses on the field of machine learning specifically addressing the challenge of Few-Shot Learning (FSL), where models must make accurate predictions about previously unseen classes using just a small number of labeled examples.\n\nResearch Problem: Despite advancements in various fields such as computer vision through Vision Transformers (ViTs), applying these same architectures effectively within an FSL framework remains difficult due to their complexity compared to traditional neural networks like Convolutional Neural Networks (CNNs). There has also been limited research exploring how to fine-tune ViTs directly rather than relying solely on transfer learning techniques involving CNNs.\n \nMethod: In response to challenges mentioned above, authors introduce two innovative methods - Attention-based Prefix Tuning (APT) and Domain Residual Adapter (DRA) – designed especially for fine-tuning ViTs under FSL conditions:\n1. **Attention-based Prefix Tuning (APT)**: This involves projecting a \"prefix\" onto new attention keys and values specific to the target class during the forward pass so that the transformer can incorporate task-specific information without needing retraining over all parameters every time one encounters a new category.\n2. **Domain Residual Adapter (DRA)**: Recognizing variability across datasets related to different domains (\"domain shifts\"), they use learnable residual adapters to bridge any gap present when transferring knowledge from pre-trained base categories towards novel ones by adjusting feature representations accordingly.\n\nMain Contributions: Their proposed eTT (Efficient Transformer Tuning) approach significantly improves upon existing practices while maintaining computational efficiency essential for practical applications dealing with large-scale datasets often encountered in real-world settings; particularly demonstrated via empirical evidence provided throughout their work including benchmark results against competitive baselines showing superior performance achieved both quantitatively & qualitatively measured metrics relevant hereafter referred as Meta-dataset benchmarks).\n\nAdditionally, code implementation details along with dataset preprocessing steps used alongside their methodology could potentially serve others working toward similar goals aiding reproducibility amongst researchers interested in advancing state-of-the-art approaches tackling FSL problems utilizing Vision Transformers architecture(s).",
        "Topic": "Vision Transformer"
    },
    {
        "title": "Domain Invariant Adversarial Learning",
        "abstract": "The phenomenon of adversarial examples illustrates one of the most basic vulnerabilities of deep neural networks. Among the variety of techniques introduced to surmount this inherent weakness, adversarial training has emerged as the most effective strategy for learning robust models. Typically, this is achieved by balancing robust and natural objectives. In this work, we aim to further optimize the trade-off between robust and standard accuracy by enforcing a domain-invariant feature representation. We present a new adversarial training method, Domain Invariant Adversarial Learning (DIAL), which learns a feature representation that is both robust and domain invariant. DIAL uses a variant of Domain Adversarial Neural Network (DANN) on the natural domain and its corresponding adversarial domain. In the case where the source domain consists of natural examples and the target domain is the adversarially perturbed examples, our method learns a feature representation constrained not to discriminate between the natural and adversarial examples, and can therefore achieve a more robust representation. DIAL is a generic and modular technique that can be easily incorporated into any adversarial training method. Our experiments indicate that incorporating DIAL in the adversarial training process improves both robustness and standard accuracy.",
        "authors": "M. Levi, I. Attias, A. Kontorovich",
        "keywords": [
            "Domain Invariant Adversarial Learning",
            "Robust Feature Representation",
            "Adversarial Training"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=U8uJAUMzj9",
        "pdf_src": "https://api2.openreview.net/pdf/0b0dd4721c40c36bb9966e53eaec1cd176a5c663.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses an issue with deep neural networks known as adversarial examples – inputs deliberately altered at a microscopic level but imperceptible to humans - causing them to misclassify data.\n\nResearch Problem: How do you train neural networks such that they are less susceptible to these types of attacks?\n\nMethod: They introduce \"Domain Invariant Adversarial Learning\" (DIAL). This involves using two domains:\n1. A 'natural' domain containing normal images.\n2. An 'adversarial' domain consisting of images from the same distribution after being perturbed through adversarial attack methods designed to fool neural networks.\n\nDIAL employs a modified version of Domain Adversarial Neural Networks (DANN) across both domains:\n\n- It trains a network simultaneously within each domain while also comparing features learned there against those learned about the other domain's samples without direct supervision or labels indicating whether they're natural or adversarial.\n\n- By doing so, it encourages the model to learn representations that generalize well beyond just the specific training distributions seen during regular training; hence, making predictions based solely on discriminative features rather than subtle differences due to adversarial manipulations alone.\n\nMain Contributions: \n- The development of a novel approach called Domain Invariant Adversarial Learning aimed specifically at improving the robustness of neural networks under adversarial conditions via domain-invariant feature extraction;\n- Demonstrating how integrating their proposed framework significantly enhances performance metrics like robustness scores alongside traditional classification accuracies when applied over various datasets and architectures compared traditionally trained counterparts.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Exploring the Learning Mechanisms of Neural Division Modules",
        "abstract": "Of the four fundamental arithmetic operations (+, -, $\\times$, $\\div$), division is considered the most difficult for both humans and computers. \nIn this paper, we show that robustly learning division in a systematic manner remains a challenge even at the simplest level of dividing two numbers. We propose two novel approaches for division which we call the Neural Reciprocal Unit (NRU) and the Neural Multiplicative Reciprocal Unit (NMRU), and present improvements for an existing division module, the Real Neural Power Unit (Real NPU). In total we measure robustness over 475 different training sets for setups with and without input redundancy. We discover robustness is greatly affected by the input sign for the Real NPU and NRU, input magnitude for the NMRU and input distribution for every module. Despite this issue, we show that the modules can learn as part of larger end-to-end networks. ",
        "authors": "B. Mistry, K. Farrahi, J. Hare",
        "keywords": [
            "robust learning",
            "neural reciprocal unit",
            "real neural power unit"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=HjelcW6wio",
        "pdf_src": "https://api2.openreview.net/pdf/f6db9c893a267ff6362d6a487487ce9f4d3d0d77.pdf",
        "Code_src": "",
        "Introduction": "Background: Division has been identified as one of the more challenging computational tasks due to its complexity compared to addition, subtraction, and multiplication.\n\nResearch Question: The study aims to address whether it's possible or not feasible to reliably teach division through machine learning algorithms despite being recognized as complex among human beings themselves.\nMethods: To tackle this problem, authors introduce three distinct neural network units designed specifically for division - Neural Reciprocal Unit (NRU), Neural Multiplicative Reciprocal Unit (NMRU), along with enhancements made on an already existing unit called Real Neural Power Unit (Real NPU).\n\nMain Contributions:\n1. Authors demonstrate how these specialized units are capable of handling division computations effectively within their respective frameworks when trained rigorously across various datasets.\n2. They conducted experiments using nearly half a thousand diverse training sets under conditions where inputs could be redundant; they found significant variability in performance based on factors such as input signs (\"plus\" or \"minus\"), magnitudes, and distributions affecting all proposed models equally significantly but differently according to each model type.\n3. Even though there were challenges related to varying robustness depending upon several parameters like input characteristics mentioned above, findings suggest potential viability since these units learned successfully integrated into broader end-to-end systems indicating promise towards solving practical division problems via deep learning techniques down the line.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Momentum Capsule Networks",
        "abstract": "Capsule networks are a class of neural networks that aim at solving some limiting factors of Convolutional Neural Networks. However, baseline capsule networks have failed to reach state-of-the-art results on more complex datasets due to the high computation and memory requirements. We tackle this problem by proposing a new network architecture, called Momentum Capsule Network (MoCapsNet). MoCapsNets are inspired by Momentum ResNets, a type of network that applies reversible residual building blocks. Reversible networks allow for recalculating activations of the forward pass in the backpropagation algorithm, so those memory requirements can be drastically reduced. In this paper, we provide a framework on how invertible residual building blocks can be applied to capsule networks. We will show that MoCapsNet beats the accuracy of baseline capsule networks on MNIST, SVHN, CIFAR-10 and CIFAR-100 while using considerably less memory. The source code is available on https://github.com/moejoe95/MoCapsNet. ",
        "authors": "J. Gugglberger, A. Rodriguez-sanchez, D. Peer",
        "keywords": [
            "Momentum Capsule Network",
            "Reversible Residual Building Blocks",
            "Memory Efficiency"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Su290sknyQ",
        "pdf_src": "https://api2.openreview.net/pdf/d681962033eebccd09cc565e2d636247fc4b6fab.pdf",
        "Code_src": "https://github.com/moejoe95/MoCapsNet",
        "Introduction": "Background: Capsule networks were introduced as an alternative to convolutional neural networks with the goal of addressing their limitations such as difficulty recognizing objects from different viewpoints or occlusions.\n\nResearch Problem: Despite showing promising performance improvements over CNNs especially when dealing with spatial hierarchies within images, existing capsule networks suffer significantly higher computational costs compared to CNNs which may prevent them from being widely adopted even though they potentially offer better accuracy particularly under challenging conditions like viewpoint changes etcetera.\n\nMethod: To address these issues related mainly due to increased complexity during training time where gradients need propagation through multiple routing iterations before convergence occurs; authors propose \"Momentum Capsule Network\" (MoCapsNet), based loosely upon momentum-residual networks but adapted specifically towards capsule architectures incorporating reversibility into its design allowing backward passes without requiring storing intermediate activation states thus reducing memory footprint substantially whilst maintaining competitive accuracy levels against traditional capsule models across benchmark datasets including MNIST, SVHN, CIFAR-10 & CIFAR-100.\n\nMain Contributions:\n1. Introduce Momentum Capsule Network (MoCapsNet) - A novel architecture designed explicitly considering capsule structures leveraging momentum-based learning principles similar yet distinctively modified versions found elsewhere e.g., momentum residual networks.\n2. Demonstrate improved efficiency w.r.t both speed and storage needs relative to standard capsule implementations despite comparable accuracy metrics achieved via experiments conducted utilizing various benchmarks mentioned above suggesting practicality beyond theoretical gains alone making it suitable candidates for real-world applications demanding efficient processing capabilities alongside desired precision outcomes",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "ANCER: Anisotropic Certification via Sample-wise Volume Maximization",
        "abstract": "Randomized smoothing has recently emerged as an effective tool that enables certification of deep neural network classifiers at scale. All prior art on randomized smoothing has focused on isotropic $\\ell_p$ certification, which has the advantage of yielding certificates that can be easily compared among isotropic methods via $\\ell_p$-norm radius. However, isotropic certification limits the region that can be certified around an input to worst-case adversaries, i.e., it cannot reason about other \"close\", potentially large, constant prediction safe regions. To alleviate this issue, (i) we theoretically extend the isotropic randomized smoothing $\\ell_1$ and $\\ell_2$ certificates to their generalized anisotropic counterparts following a simplified analysis. Moreover, (ii) we propose evaluation metrics allowing for the comparison of general certificates - a certificate is superior to another if it certifies a superset region - with the quantification of each certificate through the volume of the certified region. We introduce ANCER, a framework for obtaining anisotropic certificates for a given test set sample via volume maximization. We achieve it by generalizing memory-based certification of data-dependent classifiers. Our empirical results demonstrate that ANCER achieves state-of-the-art $\\ell_1$ and $\\ell_2$ certified accuracy on CIFAR-10 and ImageNet in the data-dependence setting, while certifying larger regions in terms of volume, highlighting the benefits of moving away from isotropic analysis.",
        "authors": "F. Eiras, M. Alfarra, P. Torr, et.al",
        "keywords": [
            "anisotropic",
            "randomized smoothing",
            "certified accuracy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=7j0GI6tPYi",
        "pdf_src": "https://api2.openreview.net/pdf/e4263b927750b9e5ca5bbe7ab1f39c3fe3873365.pdf",
        "Code_src": "",
        "Introduction": "Background: Randomized smoothing is becoming increasingly popular due to its effectiveness in certifying the robustness of deep neural networks against adversarial attacks.\n\nResearch Problem: Prior works have mainly concentrated on isotropic $\\ell_p$ certification but fail to consider potential safe regions beyond the worst-case adversaries because they are limited only to certify the closest worst-case adversaries rather than reasoning over these regions.\n\nMethod: The paper extends isotropic $\\ell_1$ and $\\ell_2$ randomized smoothing certificates into their generalized anisotropic counterparts based on theoretical simplifications; introduces new evaluation metrics comparing different certificates where one is considered better when it covers more area within the same confidence level measured using the volume of the certified region; proposes ANCER, a novel framework designed specifically towards maximizing the volume of the certified region under consideration so as to obtain anisotropic certificates corresponding to any specific dataset's samples without relying solely upon data-independent assumptions like those used previously.\n\nMain Contributions:\n1. Generalizes existing isotropic $\\ell_1$ and $\\ell_2$ randomized smoothing certificates.\n2. Develops novel evaluation metrics enabling comparative assessment between various types of certificates considering both coverage size relative to safety margin and overall volume covered during certification process.\n3. Introduces ANCER – an algorithmic approach capable generating anisotropic certificates tailored according to individual datasets' characteristics thereby surpassing limitations imposed by purely isotropic approaches found elsewhere up until now demonstrated empirically across CIFAR-10 & ImageNet benchmarks achieving leading performance levels even though covering greater areas ensuring broader protection margins",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "On the Choice of Interpolation Scheme for Neural CDEs",
        "abstract": "Neural controlled differential equations (Neural CDEs) are a continuous-time extension of recurrent neural networks (RNNs), achieving state-of-the-art (SOTA) performance at modelling functions of irregular time series. In order to interpret discrete data in continuous time, current implementations rely on non-causal interpolations of the data. This is fine when the whole time series is observed in advance, but means that Neural CDEs are not suitable for use in \\textit{online prediction tasks}, where predictions need to be made in real-time: a major use case for recurrent networks. Here, we show how this limitation may be rectified. First, we identify several theoretical conditions that control paths for Neural CDEs should satisfy, such as boundedness and uniqueness. Second, we use these to motivate the introduction of new schemes that address these conditions, offering in particular measurability (for online prediction), and smoothness (for speed). Third, we empirically benchmark our online Neural CDE model on three continuous monitoring tasks from the MIMIC-IV medical database: we demonstrate improved performance on all tasks against ODE benchmarks, and on two of the three tasks against SOTA non-ODE benchmarks.",
        "authors": "J. Morrill, P. Kidger, L. Yang, et.al",
        "keywords": [
            "Neural Controlled Differential Equations",
            "Online Prediction Tasks",
            "Continuous-Time Modelling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=caRBFhxXIG",
        "pdf_src": "https://api2.openreview.net/pdf/3a64dcb452fa1c99d6bc9632fd31aadbc50af0ee.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper introduces Neural Controlled Differential Equations (Neural CDEs), which extend Recurrent Neural Networks (RNNs) by allowing them to process signals over continuous time rather than just sequences or discrete intervals.\n\nResearch Problem:\nDespite their potential advantages compared to RNNs due to handling continuous time, existing Neural CDE models suffer from limitations related to causality - they require non-causal interpolation methods if applied directly without modification because they were originally designed based on causal assumptions within the context of ordinary differential equations (ODEs).\n\nMethods:\nTo overcome this issue with causality constraints while maintaining desirable properties like measurability needed for online prediction scenarios along with smoothness required for computational efficiency during inference phase, authors propose novel modifications:\n\n1. They establish necessary theoretical criteria for Neural CDE's control paths ensuring boundedness & uniqueness.\n2. Based upon those requirements, introduce innovative algorithms addressing these issues; notably enhancing measurability through specific techniques tailored towards online predictive applications alongside promoting smoother dynamics via additional regularization terms.\n3. Empirically validate proposed approach across 3 challenging continuous monitoring tasks utilizing publicly available dataset from MIMIC-IV medical repository comparing results both against standard ODE-based approaches and leading non-ODE benchmarks.\n\nMain Contributions:\nThis work makes significant advancements toward enabling practical deployment of Neural CDEs beyond offline settings into dynamic environments demanding real-time processing capabilities crucially important especially relevant areas including healthcare informatics where continuous stream analysis plays vital role.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Conformal Prediction Intervals with Temporal Dependence",
        "abstract": "Cross-sectional prediction is common in many domains such as healthcare, including forecasting tasks using electronic health records, where different patients form a cross-section. We focus on the task of constructing valid prediction intervals (PIs) in time series regression with a cross-section. A prediction interval is considered valid if it covers the true response with (a pre-specified) high probability. We first distinguish between two notions of validity in such a setting: cross-sectional and longitudinal. Cross-sectional validity is concerned with validity across the cross-section of the time series data, while longitudinal validity accounts for the temporal dimension. Coverage guarantees along both these dimensions are ideally desirable; however, we show that distribution-free longitudinal validity is theoretically impossible. Despite this limitation, we propose Conformal Prediction with Temporal Dependence (CPTD), a procedure that is able to maintain strict cross-sectional validity while improving longitudinal coverage. CPTD is post-hoc and light-weight, and can easily be used in conjunction with any prediction model as long as a calibration set is available. We focus on neural networks due to their ability to model complicated data such as diagnosis codes for time series regression, and perform extensive experimental validation to verify the efficacy of our approach. We find that CPTD outperforms baselines on a variety of datasets by improving longitudinal coverage and often providing more efficient (narrower) PIs.",
        "authors": "Z. Lin, S. Trivedi, J. Sun",
        "keywords": [
            "cross-sectional prediction",
            "prediction intervals",
            "conformal prediction"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=8QoxXTDcsH",
        "pdf_src": "https://api2.openreview.net/pdf/a7096a65084e56447145e419be64bae6ec125dbf.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses an issue commonly found in various fields like healthcare - cross-sectional prediction based on electronic health records which involves predicting future outcomes from current patient populations.\n\nResearch Question: How do you construct valid prediction intervals when dealing with cross-sectional time series regression?\n\nMethodology: The authors differentiate between \"cross-sectional\" and \"longitudinal\" validity concepts within predictions. They acknowledge theoretical impossibility regarding longitudinal validity but introduce Conformal Prediction with Temporal Dependence (CPTD). This method maintains strict cross-sectional validity yet improves longitudinal coverage without requiring prior knowledge about the underlying distributions or assumptions beyond what's needed during training phase only once per dataset via a calibration step after fitting predictive models – making it applicable even outside its initial scope upon deployment into practice scenarios involving new unseen data points over time.\n\nMain Contributions:\n1. Identification & clarification distinction between cross-sectional vs longitudinal validity.\n2. Demonstration why achieving longitudinal validity cannot occur under certain conditions related to distribution-free approaches leading towards impossibility despite practical relevance demands.\n3. Development introduction novel conformal prediction framework incorporating temporal dependence termed 'Conformal Prediction with Temporal Dependence' (CPTD).\n4. Experimental validation showcasing effectiveness improvements compared against baseline methods through empirical evidence obtained testing performance metrics across diverse datasets demonstrating enhanced accuracy rates whilst narrowing down prediction intervals further than existing alternatives thus contributing significantly toward advancing state-of-the-art advancements pertaining specifically tailored solutions addressing complex real-world problems encountered frequently nowadays especially those concerning medical prognosis utilizing machine learning techniques applied alongside traditional statistical methodologies",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Meta-Learning Sparse Compression Networks",
        "abstract": "Recent work in Deep Learning has re-imagined the representation of data as functions mapping from a coordinate space to an underlying continuous signal. When such functions are approximated by neural networks this introduces a compelling alternative to the more common multi-dimensional array representation.  Recent work on such Implicit Neural Representations(INRs) has shown that - following careful architecture search - INRs can outperform established compression methods such as JPEG (e.g. Dupont et al., 2021). In this paper, we propose crucial steps towards making such ideas scalable: Firstly, we employ state-of-the-art network sparsification techniques to drastically improve compression. Secondly,introduce the first method allowing for sparsification to be employed in the inner-loop of commonly used Meta-Learning algorithms, drastically improving both compression and the computational cost of learning INRs. The generality of this formalism allows us to present results on diverse data modalities such as images, manifolds, signed distance functions, 3D shapes and scenes, several of which establish new state-of-the-art results.",
        "authors": "J. R. Schwarz, Y. W. Teh",
        "keywords": [
            "Implicit Neural Representation",
            "Network Sparsification",
            "Meta-Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Cct7kqbHK6",
        "pdf_src": "https://api2.openreview.net/pdf/5f1eef04fab1e1c3c71b541d7be5b3c8f81f8c1c.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this research is based on recent advancements in deep learning where representations of data have been transformed into implicit neural representations (INRs), which map coordinates directly onto continuous signals rather than using traditional multidimensional arrays.\n\nResearch Problem:\nThe problem addressed here revolves around how these novel INRs could potentially surpass existing image compression standards like JPEG when carefully designed architectures were utilized; however, scaling up their practical applications was challenging due to high computational costs associated with training large-scale models efficiently or compressing them effectively without significant loss quality.\n\nMethodology:\nTo address scalability issues related to efficient computation during model training while maintaining acceptable levels of compression performance within those constraints, two main contributions proposed include:\n\n1. Employing cutting-edge network sparsity techniques significantly improves compression ratios.\n2. Developing innovative approaches enabling sparse coding inside meta-learning frameworks typically applied throughout machine learning tasks including optimization problems encountered through iterative refinement processes required specifically tailored toward optimizing parameters within neural networks representing complex datasets across various domains such as visual imagery (images), geometric spaces (manifolds), scalar fields (signed distance functions), three-dimensional objects (shapes), environments (scenes).\n\nMain Contributions:\nThis study makes substantial progress towards achieving widespread adoptionable implementations involving implicit neural representations via its key findings demonstrating improved compression rates alongside reduced computational expenses compared against conventional alternatives available today – thereby paving way forward for future developments leveraging these promising technologies further down line!",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Estimating Potential Outcome Distributions with Collaborating Causal Networks",
        "abstract": "Traditional causal inference approaches leverage observational study data to estimate the difference in observed (factual) and unobserved (counterfactual) outcomes for a potential treatment, known as the Conditional Average Treatment Effect (CATE). However, CATE corresponds to the comparison on the first moment alone, and as such may be insufficient in reflecting the full picture of treatment effects. As an alternative, estimating the full potential outcome distributions could provide greater insights. However, existing methods for estimating treatment effect potential outcome distributions often impose restrictive or overly-simplistic assumptions about these distributions. Here, we propose Collaborating Causal Networks (CCN), a novel methodology which goes beyond the estimation of CATE alone by learning the full potential outcome distributions. Estimation of outcome distributions via the CCN framework does not require restrictive assumptions of the underlying data generating process (e.g. Gaussian errors). Additionally, our proposed method facilitates estimation of the utility of each possible treatment and permits individual-specific variation through utility functions (e.g. risk tolerance variability). CCN not only extends outcome estimation beyond traditional risk difference, but also enables a more comprehensive decision making process through definition of flexible comparisons. Under assumptions commonly made in the causal inference literature, we show that CCN learns distributions that asymptotically capture the correct potential outcome distributions. Furthermore, we propose an adjustment approach that is empirically effective in alleviating sample imbalance between treatment groups in observational studies. Finally, we evaluate the performance of CCN in multiple experiments on both synthetic and semi-synthetic data. We demonstrate that CCN learns improved distribution estimates compared to existing Bayesian and deep generative methods as well as improved decisions with respects to a variety of utility functions.",
        "authors": "T. Zhou, W. E. C. Iv, D. Carlson",
        "keywords": [
            "Collaborating Causal Networks",
            "Potential Outcome Distributions",
            "Utility Functions"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=q1Fey9feu7",
        "pdf_src": "https://api2.openreview.net/pdf/8d2d5ac6f5b3347881e1bcbd279a5ecdac346839.pdf",
        "Code_src": "",
        "Introduction": "Background: Traditional causal inference approaches use observational study data to estimate the difference in observed and unobserved outcomes due to a potential treatment.\nResearch Question: How can we better understand the full range of treatment effects?\nMethod: The authors introduce Collaborating Causal Networks (CCN), a new methodology designed to learn the full potential outcome distributions rather than just focusing on the conditional average treatment effect (CATE).\nMain Contributions: \n1. Beyond CATE: CCN provides a way to estimate entire potential outcome distributions without imposing restrictive or simplistic assumptions like those used in previous methods.\n\n2. Flexibility: It allows us to consider different types of treatments based on their utilities defined using utility functions—such as considering how much individuals value avoiding risks differently from one another.\n\n3. Sample Imbalance Adjustment: They suggest adjusting for sample imbalance when comparing treated versus untreated populations within observational datasets—a common issue affecting estimations here.\n\n4. Performance Evaluation: Experiments conducted showed that this model outperforms other Bayesian and deep generative models at predicting accurate distributions across various scenarios including synthetic ones where true values are available alongside estimated results allowing validation against reality itself!",
        "Topic": "approximation"
    },
    {
        "title": "Sparse Coding with Multi-layer Decoders using Variance Regularization",
        "abstract": "Sparse representations of images are useful in many computer vision applications.\nSparse coding with an $l_1$ penalty and a learned linear dictionary requires regularization of the dictionary to prevent a collapse in the $l_1$ norms of the codes. Typically, this regularization entails bounding the Euclidean norms of the dictionary's elements.\nIn this work, we propose a novel sparse coding protocol which prevents a collapse in the codes without the need to regularize the decoder. Our method regularizes the codes directly so that each latent code component has variance greater than a fixed threshold over a set of sparse representations for a given set of inputs. Furthermore, we explore ways to effectively train sparse coding systems with multi-layer decoders since they can model more complex relationships than linear dictionaries.\nIn our experiments with MNIST and natural image patches, we show that decoders learned with our approach have interpretable features both in the linear and multi-layer case. Moreover, we show that sparse autoencoders with multi-layer decoders trained using our variance regularization method produce higher quality reconstructions with sparser representations when compared to autoencoders with linear dictionaries. Additionally, sparse representations obtained with our variance regularization approach are useful in the downstream tasks of denoising and classification in the low-data regime. ",
        "authors": "K. Evtimova, Y. Lecun",
        "keywords": [
            "sparse coding",
            "variance regularization",
            "multi-layer decoders"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=4GuIi1jJ74",
        "pdf_src": "https://api2.openreview.net/pdf/887ef8cc2ce3e235359584279041197f58ffb7a1.pdf",
        "Code_src": "",
        "Introduction": "Background: Sparse representation is widely used in various computer vision applications due to its effectiveness.\n\nResearch Problem: The traditional sparse coding algorithm based on $\\ell_1$ penalty and learned linear dictionary may suffer from a collapse problem because it does not consider the variance constraint between different components within the same sparse representation under multiple input sets.\n\nMethod: We introduce a new sparse coding protocol by directly regularizing the codes instead of regularizing the dictionary as usual. Specifically, we ensure that every latent code component possesses variance larger than a predefined threshold across all possible sparse representations generated through a certain number of inputs.\n\nMain Contributions:\n- A novel sparse coding protocol preventing code collapse via direct code regularization rather than dictionary regularization;\n- An exploration into training sparse coding models equipped with multi-layer decoders capable of modeling more intricate patterns than simple linear ones;\n- Experimental validation demonstrating the interpretability of learned features whether in linear or multi-layer settings; \n- Demonstrated superiority regarding reconstruction quality along with sparsity achieved employing variance regularization-based sparse autoencoders against those utilizing linear dictionaries alone;\n- Proven utility applying variance regularization for producing informative sparse representations aiding subsequent denoising and classification tasks even where data availability is limited.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Emergent Abilities of Large Language Models",
        "abstract": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence raises the question of whether additional scaling could potentially further expand the range of capabilities of language models.",
        "authors": "J. Wei, Y. Tay, R. Bommasani, et.al",
        "keywords": [
            "large language models",
            "emergent abilities",
            "scalability"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=yzkSU5zdwD",
        "pdf_src": "https://api2.openreview.net/pdf/f000a0cc19f910b3154babacf08ee6634937d24a.pdf",
        "Code_src": "",
        "Introduction": "Background: Recent studies have demonstrated that increasing the scale of language models generally leads to improvements in their performance across various downstream tasks with predictable gains.\n\nResearch Question: However, this paper explores another aspect - the \"emergent abilities\" of very large language models which are unexpected or non-existent in smaller counterparts despite being learned from similar training data sets; these abilities defy simple prediction based solely on the performance observed in smaller-scale models due to factors like increased complexity introduced at higher scales.\n \nMethodology: To identify potential new capabilities arising through scaling beyond certain thresholds, they propose empirical methods for detecting novel behaviors exhibited only after reaching those scales without any prior knowledge about them during pre-training.\n\nMain Contributions:\n1. Identification of Emergent Abilities: They systematically document several examples where significantly scaled-up models demonstrate previously unseen skills – ranging from improved reasoning over time series predictions when compared against small-scale versions using the same dataset(s).\n2. Exploration of Scaling Limits: By studying how different types of model architectures respond differently under scaling pressures regarding emerging abilities, insights into optimal architecture choices can inform future research directions towards maximizing both predictive power and exploratory capacity within computational constraints related to cost/resource allocation issues associated with deep learning approaches today's society faces while striving toward more efficient use thereof",
        "Topic": "Large Language Models"
    },
    {
        "title": "Efficient CDF Approximations for Normalizing Flows",
        "abstract": "Normalizing flows model a complex target distribution in terms of a bijective transform operating on a simple base distribution. As such, they enable tractable computation of a number of important statistical quantities, particularly likelihoods and samples. Despite these appealing properties, the computation of more complex inference tasks, such as the cumulative distribution function (CDF) over a complex region (e.g., a polytope) remains challenging. Traditional CDF approximations using Monte-Carlo techniques are unbiased but have unbounded variance and low sample efficiency. Instead, we build upon the diffeomorphic properties of normalizing flows and leverage the divergence theorem to estimate the CDF over a closed region in target space in terms of the flux across its \\emph{boundary}, as induced by the normalizing flow. We describe both deterministic and stochastic instances of this estimator: while the deterministic variant iteratively improves the estimate by strategically subdividing the boundary, the stochastic variant provides unbiased estimates. Our experiments on popular flow architectures and UCI benchmark datasets show a marked improvement in sample efficiency as compared to traditional estimators.",
        "authors": "C. S. Sastry, A. Lehrmann, M. A. Brubaker, et.al",
        "keywords": [
            "diffeomorphic properties",
            "cumulative distribution function",
            "normalizing flows"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LnjclqBl8R",
        "pdf_src": "https://api2.openreview.net/pdf/7a19108a268fae8ac547e8bf2f674751d3769417.pdf",
        "Code_src": "",
        "Introduction": "Background:\nNormalizing flows are a class of probabilistic models that can represent any probability distribution through an invertible transformation from a standard distribution known as the \"base\" or \"prior\". This allows for efficient calculations like computing probabilities under the model.\n\nResearch Problem:\nDespite their advantages including tractable computations related to likelihood estimation which is essential for Bayesian statistics applications where posterior distributions need to be sampled efficiently; however, one challenge with normalizing flows has been estimating the Cumulative Distribution Function (CDF), especially when dealing with regions within high-dimensional spaces - something not directly supported out-of-the-box due to computational complexity issues.\n \nMethodology:\nTo address this problem without relying solely on computationally expensive methods based on Markov Chain Monte Carlo sampling approaches leading to poor sample efficiency problems, authors propose leveraging the divergence theorem along with the diffeomorphic nature inherent in normalizing flows themselves – essentially calculating how much mass flows into different parts of our desired area defined by boundaries within the transformed space rather than trying to explicitly compute every point's position individually inside it. They introduce two variants:\n\n1. Deterministic Estimator: An iterative approach that breaks down the boundary into smaller sub-regions until convergence towards accurate values ensuring consistency between theoretical expectations versus empirical observations made during training time.\n2. Stochastic Estimator: A non-iterative method providing unbiased results at each step via random sampling allowing us flexibility depending on available resources needed per calculation step.\n\nMain Contributions:\nThe paper makes several contributions toward improving the practicality of normalizing flows beyond just likelihood estimation purposes. The main ones include:\n- Developing novel algorithms capable of accurately estimating CDFs even if those functions cannot straightforwardly be computed analytically;\n- Demonstrating significant improvements regarding sample efficiency relative to existing state-of-the-art methods used previously mainly involving Monte Carlo simulations requiring many iterations before converging reliably enough for use cases demanding precise answers quickly;\n- Providing insights about what types of normalizing flow architectures might work best suited according to specific needs around computational resource constraints encountered throughout various domains utilizing machine learning techniques heavily reliant on probabilistic modeling principles embodied therein",
        "Topic": "Optimal Transport"
    },
    {
        "title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
        "abstract": "Recently, information retrieval has seen the emergence of dense retrievers, using neural networks, as an alternative to classical sparse methods based on term-frequency. These models have obtained state-of-the-art results on datasets and tasks where large training sets are available. However, they do not transfer well to new applications with no training data, and are outperformed by unsupervised term-frequency methods such as BM25. In this work, we explore the limits of contrastive learning as a way to train unsupervised dense retrievers and show that it leads to strong performance in various retrieval settings. On the BEIR benchmark our unsupervised model outperforms BM25 on 11 out of 15 datasets for the Recall@100. When used as pre-training before fine-tuning, either on a few thousands in-domain examples or on the large MS~MARCO dataset, our contrastive model leads to improvements on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual retrieval, where training data is even scarcer than for English, and show that our approach leads to strong unsupervised performance. Our model also exhibits strong cross-lingual transfer when fine-tuned on supervised English data only and evaluated on low resources language such as Swahili. We show that our unsupervised models can perform cross-lingual retrieval between different scripts, such as retrieving English documents from Arabic queries, which would not be possible with term matching methods.",
        "authors": "G. Izacard, M. Caron, L. Hosseini, et.al",
        "keywords": [
            "contrastive learning",
            "unsupervised dense retrievers",
            "cross-lingual retrieval"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=jKN1pXi7b0",
        "pdf_src": "https://api2.openreview.net/pdf/805def493f147c19bc35d87ea99ff3bd4e996bd4.pdf",
        "Code_src": "",
        "Introduction": "Background: The field of Information Retrieval traditionally relies on sparse vector representations like TF-IDF but recently there's been progress towards more complex, dense representations learned directly from text corpora via neural network-based approaches.\n\nResearch Problem: Despite their success under certain conditions due to better semantic understanding compared to traditional methods, these dense retrievers struggle significantly without any labeled training data during deployment leading them to being surpassed by simpler unsupervised techniques especially in scenarios involving novel domains.\n \nMethodology: This paper investigates the potential of Contrastive Learning—a technique often associated with semi-supervised or self-supervised learning—to create effective unsupervised dense retrievers capable of generalizing across multiple retrieval contexts.\n\nMain Contributions:\n1. **Contrastive Dense Retrieval**: They demonstrate how contrastive learning could potentially bridge the gap left open after the initial successes of dense retrievers; showing significant improvement over BM25—another popular unsupervised method—in terms of recall at scale through experiments conducted within the BEIR benchmark suite covering diverse datasets ranging widely in topic complexity and domain novelty.\n2. **Pre-training and Fine-tuning**: Their findings suggest that contrastive models trained prior to fine-tuning whether on small amounts of target domain data or larger datasets lead to improved performance further validating its effectiveness beyond just raw recall metrics into practical use cases requiring adaptation quickly yet accurately upon encountering unseen content types.\n3. **Cross-lingual Performance**: Additionally demonstrating impressive cross-lingual capabilities including translation invariant retrieval suggesting broad applicability regardless of script differences making it feasible e.g., translating Arabic search queries while returning relevant English-language responses—an ability previously unattainable solely relying on lexical similarity measures employed historically in IR systems.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Equivariant Mesh Attention Networks",
        "abstract": "Equivariance to symmetries has proven to be a powerful inductive bias in deep learning research. Recent works on mesh processing have concentrated on various kinds of natural symmetries, including translations, rotations, scaling, node permutations, and gauge transformations. To date, no existing architecture is equivariant to all of these transformations. In this paper, we present an attention-based architecture for mesh data that is provably equivariant to all transformations mentioned above. Our pipeline relies on the use of relative tangential features: a simple, effective, equivariance-friendly alternative to raw node positions as inputs. Experiments on the FAUST and TOSCA datasets confirm that our proposed architecture achieves improved performance on these benchmarks and is indeed equivariant, and therefore robust, to a wide variety of local/global transformations.",
        "authors": "S. Basu, J. Gallego-posada, F. Viganò, et.al",
        "keywords": [
            "mesh processing",
            "equivariance",
            "attention-based architecture"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=3IqqJh2Ycy",
        "pdf_src": "https://api2.openreview.net/pdf/c5b2f58f9cb4cc410aea024c04dbb4794ff15c0c.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this study lies within the field of deep learning where equivariance with respect to symmetries plays a crucial role serving as a strong inductive bias leading to more generalizable models.\n\nResearch Problem:\nDespite recent advancements focusing on leveraging symmetry properties such as translation, rotation, scaling, permutation, and gauge transformation during mesh processing tasks through neural networks architectures like MeshCNNs or PointNet variants, there currently exists no single model which can account for all these symmetries simultaneously due to their complexity.\n \nMethodology:\nIn addressing this issue, authors introduce a novel attention mechanism based architecture designed specifically for mesh data types called \"MeshTransformer.\" This architecture's key innovation involves utilizing relative tangent vectors instead of raw vertex coordinates directly from meshes when computing attention weights between vertices - thereby providing a straightforward way towards achieving full equivariance across multiple transformations without requiring complex modifications at each layer level.\n\nMain Contributions:\nThis work makes several significant contributions by introducing an architecture capable of being equivariant against all aforementioned symmetries while also demonstrating its effectiveness using two benchmark datasets known as FAUST and TOSCA respectively showing improvements over prior state-of-the-art methods not only in terms of accuracy but also in terms of robustness under different types of global/local transformations applied both locally around points inside meshes up to global scale changes affecting entire structures represented via point clouds or polygonal meshes alike.",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models",
        "abstract": "Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classification head, and 91.0% with a finetuned encoder.",
        "authors": "J. Yu, Z. Wang, V. Vasudevan, et.al",
        "keywords": [
            "Contrastive Learning",
            "Multimodal Foundation Model",
            "Zero-Shot Transfer"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Ee277P3AYC",
        "pdf_src": "https://api2.openreview.net/pdf/6b362d3552ac742cf900ce988ea961179af1b315.pdf",
        "Code_src": "",
        "Introduction": "Background: The background of this research lies in the exploration of large-scale pretrained foundation models that have garnered considerable attention due to their potential quick transferability across various downstream tasks within computer vision.\n\nResearch Problem: The primary problem addressed here concerns how to effectively train such foundational models capable of handling diverse tasks while maintaining efficiency during the training process without sacrificing performance upon deployment into new contexts.\n\nMethodology: To tackle this issue, the authors introduce \"Contrastive Captioner\" (CoCa), designed specifically around a minimalist approach combining contrastive and captioning losses together through joint pretraining using an encoder-decoder architecture. Unlike conventional encoder-decoder transformer architectures wherein every decoder layer attends to the encoder's output simultaneously, CoCa introduces a novel modification – it excludes cross-attention mechanisms at the beginning stages allowing each decoder layer only to focus on encoding unimodal textual information before cascading further down to include cross-attentional processing over the encoded image features towards generating multimodal representations suitable for image-text interactions.\n \nMain Contributions:\n1. **Unified Pretraining**: CoCa employs contrastive and captioning losses concurrently yet shares a single computation graph minimizing resource usage but maximizing synergy among different types of tasks.\n2. **Efficient Training**: It trains entirely from scratch leveraging both web-scale alternative text datasets along with labeled images treated uniformly just as text inputs thus enabling seamless integration of linguistic supervisory signals beneficial for representation learning processes.\n3. **State-of-the-Art Performance**: Experimentally validated against several benchmarks including ImageNet classification achieving competitive results even when trained with no additional fine-tuning beyond initial setup indicating its robustness under varying conditions; also excels significantly better than other leading systems outperforming them particularly notable gains observed especially after freezing encoders followed by adapting heads solely tailored toward specific domains/tasks involved therein.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Birds of a Feather Trust Together: Knowing When to Trust a Classifier via Adaptive Neighborhood Aggregation",
        "abstract": "How do we know when the predictions made by a classifier can be trusted? This is a fundamental problem that also has immense practical applicability, especially in safety-critical areas such as medicine and autonomous driving. The de facto approach of using the classifier's softmax outputs as a proxy for trustworthiness suffers from the over-confidence issue; while the most recent works incur problems such as additional retraining cost and accuracy versus trustworthiness trade-off. In this work, we argue that the trustworthiness of a classifier's prediction for a sample is highly associated with two factors: the sample's neighborhood information and the classifier's output. To combine the best of both worlds, we design a model-agnostic post-hoc approach NeighborAGG to leverage the two essential information via an adaptive neighborhood aggregation. Theoretically, we show that NeighborAGG is a generalized version of a one-hop graph convolutional network, inheriting the powerful modeling ability to capture the varying similarity between samples within each class. We also extend our approach to the closely related task of mislabel detection and provide a theoretical coverage guarantee to bound the false negative. Empirically, extensive experiments on image and tabular benchmarks verify our theory and suggest that NeighborAGG outperforms other methods, achieving state-of-the-art trustworthiness performance.",
        "authors": "M. Xiong, S. Li, W. Feng, et.al",
        "keywords": [
            "trustworthiness",
            "classifier prediction",
            "neighborhood aggregation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=p5V8P2J61u",
        "pdf_src": "https://api2.openreview.net/pdf/9d6ea94ede046cb2ef19aa7adccfca37e960ada1.pdf",
        "Code_src": "",
        "Introduction": "Background: Trusting the predictions made by classifiers plays a crucial role across various fields like medicine and autonomous driving where errors could lead to severe consequences.\n\nResearch Problem: How reliable are the predictions generated by classifiers?\n\nMethodology: The authors propose NeighborAGG - a model-agnostic post-hoc method designed without modifying the original classifier architecture which leverages neighborhood information around input samples along with classifier outputs through adaptive neighborhood aggregation techniques.\n \nMain Contributions:\n1. They demonstrate theoretically that NeighborAGG extends beyond traditional graph neural networks due to its capability not only capturing similarities but also differences among samples belonging to same classes.\n2. Their extension includes addressing another critical concern – mislabel detection ensuring fewer missed detections compared existing approaches.\n3. NeighborAGG significantly improves upon current standards providing better predictive trustworthiness demonstrated empirically against multiple datasets including images and tables.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "On the Convergence of Shallow Neural Network Training with Randomly Masked Neurons",
        "abstract": "With the motive of training all the parameters of a neural network, we study why and when one can achieve this by iteratively creating, training, and combining randomly selected subnetworks. Such scenarios have either implicitly or explicitly emerged in the recent literature: see e.g., the Dropout family of regularization techniques, or some distributed ML training protocols that reduce communication/computation complexities, such as the Independent Subnet Training protocol. While these methods are studied empirically and utilized in practice, they often enjoy partial or no theoretical support, especially when applied on neural network-based objectives.\n\nIn this manuscript, our focus is on overparameterized single hidden layer neural networks with ReLU activations in the lazy training regime. By carefully analyzing $i)$ the subnetworks' neural tangent kernel, $ii)$ the surrogate functions' gradient, and $iii)$ how we sample and combine the surrogate functions, we prove linear convergence rate of the training error --up to a neighborhood around the optimal point-- for an overparameterized single-hidden layer perceptron with a regression loss. Our analysis reveals a dependency of the size of the neighborhood around the optimal point on the number of surrogate models and the number of local training steps for each selected subnetwork. Moreover, the considered framework generalizes and provides new insights on dropout training, multi-sample dropout training, as well as Independent Subnet Training; for each case, we provide convergence results as corollaries of our main theorem.",
        "authors": "F. Liao, A. Kyrillidis",
        "keywords": [
            "subnetworks",
            "linear convergence",
            "surrogate functions"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=e7mYYMSyZH",
        "pdf_src": "https://api2.openreview.net/pdf/6e1db3062649b62ebdcf12c44f77d46a86ec4540.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper aims to understand under what conditions it's possible to train all parameters of a neural network through iterative creation, training, and combination of randomly selected subnetworks.\nResearch Problem:\nTo determine if there exists empirical evidence supporting the use of such approaches without complete theoretical backing specifically within the context of overparameterized single-hidden layer neural networks using ReLU activation function during lazy training regime.\nMethods:\nThe authors analyze three aspects: 1) The subnetworks' Neural Tangent Kernel (NTK), which describes the dynamics near critical points where gradients vanish due to overparametrization leading to stability issues but also potential for learning. 2) The surrogate functions' gradient, focusing on their role in approximating the true objective function while being easier to optimize than the original problem. 3) Sampling strategy used along with the way surrogate functions were combined into final predictions ensuring robustness against noise introduced from random sampling.\nMain Contributions:\nThis work proves linear convergence rates up to a certain neighborhood around the optimal solution even though the network has many more parameters than necessary - referred to as \"overparameterization\". It shows dependence between the size of this neighborhood surrounding the optimum parameter values upon both numbers of surrogate models employed alongside iterations per model chosen at random intervals across different subnets. Additionally, findings generalize beyond simple dropout regularization strategies extending towards multi-sample dropout training methodologies including Independent Subnet Training approach providing novel theoretical insight applicable not just theoretically but practically too",
        "Topic": "approximation"
    },
    {
        "title": "On the Origins of the Block Structure Phenomenon in Neural Network Representations",
        "abstract": "Recent work by Nguyen et al. (2021) has uncovered a striking phenomenon in large-capacity neural networks: they contain blocks of contiguous hidden layers with highly similar representations. This block structure has two seemingly contradictory properties: on the one hand, its constituent layers exhibit highly similar dominant first principal components (PCs), but on the other hand, their representations, and their common first PC, are highly dissimilar across different random seeds. Our work seeks to reconcile these discrepant properties by investigating the origin of the block structure in relation to the data and training methods. By analyzing properties of the dominant PCs, we find that the block structure arises from dominant datapoints — a small group of examples that share similar image statistics (e.g. background color). However, the set of dominant datapoints, and the precise shared image statistic, can vary across random seeds. Thus, the block structure reflects meaningful dataset statistics, but is simultaneously unique to each model. Through studying hidden layer activations and creating synthetic datapoints, we demonstrate that these simple image statistics dominate the representational geometry of the layers inside the block structure. We explore how the phenomenon evolves through training, finding that the block structure takes shape early in training, but the underlying representations and the corresponding dominant datapoints continue to change substantially. Finally, we study the interplay between the block structure and different training mechanisms, introducing a targeted intervention to eliminate the block structure, as well as examining the effects of pre-training and Shake-Shake regularization.",
        "authors": "T. Nguyen, M. Raghu, S. Kornblith",
        "keywords": [
            "block structure",
            "dominant principal components",
            "stochasticity"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=9tl6zjLYVS",
        "pdf_src": "https://api2.openreview.net/pdf/79f4311cd8239a23545526c5e3aa2db2f9079da4.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper investigates an intriguing observation made previously about deep neural networks - within such networks exist blocks of adjacent hidden layers which have very similar representations despite having distinct parameters when trained using different random seeds.\n\nResearch Question:\nThis raises questions regarding why this block structure exists even though it seems counterintuitive since typically, neurons should learn features independently regardless of initialization or stochastic processes like dropout during training.\n \nMethodology:\nTo address this question, researchers analyze the properties of the dominant principal components (PCs) extracted from the hidden layers' activations for understanding where similarities arise among layers belonging to the same block while also being quite different overall. They further investigate into whether there's any correlation between the observed block structures and certain types of input data distributions via generating synthetic datasets based on learned statistical patterns present in real ones; they then compare these synthetic datasets against actual training sets used before observing if those patterns persist after retraining models without them.\n\nMain Contributions:\n- The authors identify specific instances called \"dominant datapoints\" responsible for driving similarity amongst neighboring layers due to sharing some basic visual attributes irrespective of seed variation – thus explaining both aspects mentioned above.\n- They show experimentally that these simple yet significant characteristics significantly influence the representation space captured at higher levels leading up to classification tasks performed later down-streamed pathways within neural architectures.\n- Furthermore, findings suggest that although initial block formation occurs relatively quickly throughout learning phases, subsequent changes occur continuously over time suggesting dynamic nature rather than static snapshotting phenomena initially hypothesized elsewhere literature-wise concerning neural network architectures’ organization principles.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Attentive Walk-Aggregating Graph Neural Networks",
        "abstract": "Graph neural networks (GNNs) have been shown to possess strong representation power, which can be exploited for downstream prediction tasks on graph-structured data, such as molecules and social networks. They typically learn representations by aggregating information from the $K$-hop neighborhood of individual vertices or from the enumerated walks in the graph. Prior studies have demonstrated the effectiveness of incorporating weighting schemes into GNNs; however, this has been primarily limited to $K$-hop neighborhood GNNs so far. In this paper, we aim to design an algorithm incorporating weighting schemes into walk-aggregating GNNs and analyze their effect. We propose a novel GNN model, called {\\AWARE}, that aggregates information about the walks in the graph using attention schemes. This leads to an end-to-end supervised learning method for graph-level prediction tasks in the standard setting where the input is the adjacency and vertex information of a graph, and the output is a predicted label for the graph. We then perform theoretical, empirical, and interpretability analyses of {\\AWARE}. Our theoretical analysis in a simplified setting identifies successful conditions for provable guarantees, demonstrating how the graph information is encoded in the representation, and how the weighting schemes in {\\AWARE} affect the representation and learning performance. Our experiments demonstrate the strong performance of {\\AWARE} in graph-level prediction tasks in the standard setting in the domains of molecular property prediction and social networks. Lastly, our interpretation study illustrates that {\\AWARE} can successfully capture the important substructures of the input graph. The code is available on \\href{https://github.com/mehmetfdemirel/aware}{GitHub}.",
        "authors": "M. F. Demirel, S. Liu, S. Garg, et.al",
        "keywords": [
            "walk aggregation",
            "graph neural networks",
            "attention mechanisms"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=TWSTyYd2Rl",
        "pdf_src": "https://api2.openreview.net/pdf/f129f46aa8764108a0ffb1a78d18ae75b92f6d9b.pdf",
        "Code_src": "链接：[GitHub](https://github.com/mehmetfdemirel/aware)",
        "Introduction": "Background: Graph neural networks (GNNs) are powerful models capable of representing complex relationships between entities within graph-structured datasets like molecules and social networks.\nResearch Problem: While previous research shows the efficacy of integrating weighting schemes with K-hop neighborhood GNNs, there's no similar work done yet regarding walk-aggregating GNNs.\n\nMethod: To address this issue, authors introduce a new GNN model named {\\AWARE}, designed specifically around walk aggregation techniques enhanced by attention mechanisms – leading to an end-to-end supervised approach suitable for graph-level prediction problems involving both adjacency and vertex features without any additional annotations beyond these inputs.\n\nMain Contributions:\n1. A novel GNN architecture, {\\AWARE}, tailored towards walk-aggregating processes leveraging attention-based mechanisms;\n2. Demonstrated theoretically through simplifications under certain conditions that {\\AWARE} provides provable guarantees related to encoding graph information effectively while considering its impact on learned representations and overall performance metrics;\n3. Empirically validated across various benchmarks including molecular properties predictions & social network graphs showing superior predictive capabilities compared existing state-of-the-art methods;\n4. Interpretation results highlighting {\\AWARE}'s ability to identify significant substructures present in input graphs contributing significantly toward better understanding underlying patterns governing those structures;\n\nCode Availability: All source codes used during experimentation along with further details may be accessed via GitHub repository at [link provided].",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "DisCo: Improving Compositional Generalization in Visual Reasoning through Distribution Coverage",
        "abstract": "We present DisCo, a learning paradigm for improving compositional generalization of visual reasoning models by leveraging unlabeled, out-of-distribution images from the test distribution. DisCo has two components. The first is an iterative pseudo-labeling framework with an entropy measure, which effectively labels images of novel attribute compositions paired with randomly sampled questions. The second is a distribution coverage metric, serving as a model selection strategy that approximates generalization capability to test examples drawn from a different attribute combination distribution to the train set, without the use of labeled data from the test distribution. Both components are built on strong empirical evidence of the correlation between the chosen metric and model generalization, and improve distribution coverage on unlabeled images. We apply DisCo to visual question answering, with three backbone networks (FiLM, TbD-net, and the Neuro-Symbolic Concept Learner), and demonstrate that it consistently enhances performance on a variety of compositional generalization tasks with varying levels of train data bias. \n",
        "authors": "J. Hsu, J. Mao, J. Wu",
        "keywords": [
            "DisCo",
            "Compositional Generalization",
            "Distribution Coverage"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=EgHnKOLaKW",
        "pdf_src": "https://api2.openreview.net/pdf/1e726e99dcd944368be918b8921c126bac4ea7a1.pdf",
        "Code_src": "",
        "Introduction": "Background: Visual reasoning models have been widely used in various applications such as image classification and object detection; however, they often struggle when faced with new or unseen combinations of attributes.\n\nResearch Problem: How can we improve the compositional generalization ability of visual reasoning models?\n\nMethod: We propose a learning paradigm called DisCo , which consists of two main components:\n\n1. An iterative pseudo-labeling framework with an entropy measure : This component effectively labels images of novel attribute compositions paired with randomly sampled questions.\n2. A distribution coverage metric : This serves as a model selection strategy that approximates generalization capability to test examples drawn from a different attribute combination distribution to the train set, without using labeled data from the test distribution.\n\nMain Contributions:\n- Both components are based on strong empirical evidence of their correlation with model generalization .\n- They help improve distribution coverage on unlabeled images.\n- We applied DisCo to visual question answering task  and demonstrated its effectiveness across different compositional generalization tasks even under varying amounts of training data bias.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Intrinsic Dimension for Large-Scale Geometric Learning",
        "abstract": "The concept of dimension is essential to grasp the complexity of data. A naive approach to determine the dimension of a dataset is based on the number of attributes. More sophisticated methods derive a notion of intrinsic dimension (ID) that employs more complex feature functions, e.g., distances between data points. Yet, many of these approaches are based on empirical observations, cannot cope with the geometric character of contemporary datasets, and do lack an axiomatic foundation. A different approach was proposed by V. Pestov, who links the intrinsic dimension axiomatically to the mathematical concentration of measure phenomenon. First methods to compute this and related notions for ID were computationally intractable for large-scale real-world datasets. In the present work, we derive a computationally feasible method for determining said axiomatic ID functions. Moreover, we demonstrate how the geometric properties of complex data are accounted for in our modeling. In particular, we propose a principle way to incorporate neighborhood information, as in graph data, into the ID. This allows for new insights into common graph learning procedures, which we illustrate by experiments on the Open Graph Benchmark.",
        "authors": "M. Stubbemann, T. Hanika, F. M. Schneider",
        "keywords": [
            "dimension",
            "intrinsic dimension",
            "computational feasibility"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=85BfDdYMBY",
        "pdf_src": "https://api2.openreview.net/pdf/07a3aaf6091e0b8141f17872ee08d8d49cffc718.pdf",
        "Code_src": "",
        "Introduction": "Background: The concept of dimension plays a crucial role in understanding the complexity of data sets; however, traditional ways such as counting attributes may not be sufficient or applicable due to their empirical nature.\n\nResearch Problem: To address limitations associated with existing methods used to estimate the intrinsic dimensionality (ID), researchers aim at developing novel techniques capable of accurately capturing the true underlying structure within high-dimensional datasets while accounting for their geometric characteristics.\n \nMethodology: Pestov introduced an axiomatic link between intrinsic dimension and the concentration of measure phenomenon from mathematics theory perspective providing a theoretical framework beyond mere empirical observation. However, practical computation challenges persist when applying his ideas directly because they require solving optimization problems over potentially very large scales leading computational costs prohibitive even using advanced algorithms like gradient descent-based solvers available today.\n\nMain Contributions: Our paper presents two main contributions towards addressing both theoretical foundations issues along with computational feasibility concerns:\n1. We introduce a computationally tractable algorithmic solution allowing estimation of axiomatic intrinsic dimensions functionals across various types of datasets including those found commonly encountered in practice – graphs where nodes represent entities connected through edges representing relationships among them - thus enabling us better understand complexities inherent therein;\n2. Furthermore, we explore incorporating neighborhood information effectively into models estimating intrinsic dimensionality via leveraging graph structures specifically tailored toward handling non-Euclidean spaces typical seen nowadays especially pertinent given recent advancements observed particularly around machine learning tasks utilizing neural networks trained end-to-end without prior manual feature engineering steps involved beforehand",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Beyond Information Gain: An Empirical Benchmark for  Low-Switching-Cost Reinforcement Learning",
        "abstract": "A ubiquitous requirement in many practical reinforcement learning (RL) applications is that the deployed policy that actually interacts with the environment cannot change frequently. Such an RL setting is called low-switching-cost RL, i.e., achieving the highest reward while reducing the number of policy switches during training. It has been a recent trend in theoretical RL research to develop provably efficient RL algorithms with low switching cost. The core idea in these theoretical works is to measure the information gain and switch the policy when the information gain is doubled. Despite of the theoretical advances, none of existing approaches have been validated empirically. We conduct the first empirical evaluation of different policy switching criteria on popular RL testbeds, including a medical treatment environment, the Atari games, and robotic control tasks. Surprisingly, although information-gain-based methods do recover the optimal rewards, they often lead to a substantially higher switching cost. By contrast, we find that a feature-based criterion, which has been largely ignored in the theoretical research, consistently produces the best performances over all the domains. We hope our benchmark could bring insights to the community and inspire future research. Our code and complete results can be found at https: // sites. google. com/ view/ low-switching-cost-rl ",
        "authors": "S. Xu, Y. Liang, Y. Li, et.al",
        "keywords": [
            "low-switching-cost RL",
            "information gain",
            "feature-based criterion"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Xq1sTZTQVm",
        "pdf_src": "https://api2.openreview.net/pdf/eb13c19a8b63ff4b9b2e9a10b71fe5425574fe9f.pdf",
        "Code_src": "https://sites.google.com/view/lows SwitchingCost-RL",
        "Introduction": "Background:\nIn many practical reinforcement learning (RL) applications, it's crucial for the deployed policy interacting with the environment not to change too frequently due to high costs associated with frequent changes. This type of RL setting is known as low-switching-cost RL.\n\nResearch Problem:\nThe problem addressed by this paper revolves around developing provably efficient RL algorithms within such settings where minimizing the frequency of policy switches becomes essential alongside maximizing reward outcomes.\n \nMethods:\nTo tackle this issue, researchers propose various policy switching criteria based on measuring certain metrics like information gain or other features related to performance improvements between policies. They aim to determine whether changing from one policy to another would yield significant benefits before making any decisions about policy updates.\n \nMain Contributions:\nThis study conducts comprehensive empirical evaluations across multiple domains using well-known benchmarks – medical treatments, Atari games & robotic control tasks - comparing several proposed policy switching strategies against each other under controlled conditions. Their findings reveal surprising inconsistencies among them; whereas some perform admirably regarding recovering optimal rewards but incur substantial additional costs compared to others focusing more heavily on specific domain-specific characteristics rather than general-purpose measures alone. Specifically highlighting their contribution lies in demonstrating how ignoring previously overlooked aspects leads towards better overall performance through leveraging specialized knowledge pertinent only relevant contexts encountered therein!",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Hidden Heterogeneity: When to Choose Similarity-Based Calibration",
        "abstract": "Trustworthy classifiers are essential to the adoption of machine learning predictions in many real-world settings. The predicted probability of possible outcomes can inform high-stakes decision making, particularly when assessing the expected value of alternative decisions or the risk of bad outcomes. These decisions require well-calibrated probabilities, not just the correct prediction of the most likely class. Black-box classifier calibration methods can improve the reliability of a classifier’s output without requiring retraining. However, these methods are unable to detect subpopulations where calibration could also improve prediction accuracy. Such subpopulations are said to exhibit “hidden heterogeneity” (HH), because the original classifier did not detect them. The paper proposes a quantitative measure for HH. It also introduces two similarity-weighted calibration methods that can address HH by adapting locally to each test item: SWC weights the calibration set by similarity to the test item, and SWC-HH explicitly incorporates hidden heterogeneity to filter the calibration set. Experiments show that the improvements in calibration achieved by similarity-based calibration methods correlate with the amount of HH present and, given sufficient calibration data, generally exceed calibration achieved by global methods. HH can therefore serve as a useful diagnostic tool for identifying when local calibration methods would be beneficial.",
        "authors": "K. L. Wagstaff, T. Dietterich",
        "keywords": [
            "Hidden Heterogeneity",
            "Similarity-Weighted Calibration",
            "Classifier Calibration"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=RA0TDqt3hC",
        "pdf_src": "https://api2.openreview.net/pdf/15eb3c737510c7b6dcc23d3ef1f1ed529b48f00f.pdf",
        "Code_src": "",
        "Introduction": "Background: Trustworthy classifiers play an important role in various practical scenarios due to their ability to provide predictive probabilities which may influence significant decision-making processes.\n\nResearch Problem: Despite advancements made through black-box calibration techniques aimed at improving classification reliability beyond simple correctness rates on individual classes, they often fail to identify \"hidden heterogeneity\" within specific subgroups (\"subpopulations\")—a phenomenon referred to as \"hidden heterogeneity\" (HH)—where improved calibration might significantly enhance prediction performance but is overlooked if only general population averages were considered.\n\nMethods: To tackle this issue, we introduce a novel quantitative metric designed specifically to quantify the presence of HH across different subpopulations based on how much variance there exists between the true positive rate among members from one group compared against another's. We further propose two new calibration approaches incorporating similarity weighting:\n\n1. **SWC (Similarity Weighting Calibration)** - This method recalibrates the classifier using information about its similarity relative to other instances during training time.\n2. **SWC-HH (Similarity Weighting Calibration Considering Hidden Heterogeneity)** - An extension over SWC focusing more intensely on areas identified previously showing signs of HH; it filters out less relevant examples while calibrating according to those deemed similar enough yet still exhibiting potential discrepancies indicative of such heterogeneity.\n\nMain Contributions:\n- A comprehensive quantification framework measuring 'Hidden Heterogeneity' allowing us insight into whether certain groups have divergent expectations regarding model outputs than others do overall;\n- Two innovative similarity-weighted calibration strategies capable addressing detected HH effectively via localized adjustments rather than universal ones thus yielding better calibrated results especially under conditions abundant with labeled samples needed for robust estimation purposes.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "GSR: A Generalized Symbolic Regression Approach",
        "abstract": "Identifying the mathematical relationships that best describe a dataset remains a very challenging problem in machine learning, and is known as Symbolic Regression (SR). In contrast to neural networks which are often treated as black boxes, SR attempts to gain insight into the underlying relationships between the independent variables and the target variable of a given dataset by assembling analytical functions. In this paper, we present GSR, a Generalized Symbolic Regression approach, by modifying the conventional SR optimization problem formulation, while keeping the main SR objective intact. In GSR, we infer mathematical relationships between the independent variables and some transformation of the target variable. We constrain our search space to a weighted sum of basis functions, and propose a genetic programming approach with a matrix-based encoding scheme. We show that our GSR method is competitive with strong SR benchmark methods, achieving promising experimental performance on the well-known SR benchmark problem sets. Finally, we highlight the strengths of GSR by introducing SymSet, a new SR benchmark set which is more challenging relative to the existing benchmarks.",
        "authors": "T. Tohme, D. Liu, K. Youcef-toumi",
        "keywords": [
            "Symbolic Regression",
            "Genetic Programming",
            "Mathematical Relationships"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=lheUXtDNvP",
        "pdf_src": "https://api2.openreview.net/pdf/07ad9cfcd9fa959854fe03e4d75138355280d05b.pdf",
        "Code_src": "",
        "Introduction": "Background: Identifying the mathematical relationships within datasets through symbolic regression (SR) poses significant challenges due to its complexity.\n\nResearch Problem: The primary challenge lies in finding optimal mathematical expressions or models for data points without relying solely on empirical approximations like neural networks but instead using symbolic reasoning based on domain knowledge.\n \nMethods: This study introduces GSR – a generalized form of symbolic regression aiming at inferring mathematical relations from input features to transformed targets via an encoded genetic algorithm optimizing over a constrained function space composed of linear combinations of pre-defined basis functions.\n\nMain Contributions:\n1. **Modified Optimization**: It modifies traditional SR's optimization framework yet maintains core objectives essential for effective model discovery.\n2. **Inference of Mathematical Relationships**: Unlike standard SRs focusing only on direct correlations among inputs and outputs, it also considers transformations applied to the output variable(s).\n3. **Constrained Search Space**: Utilizes a predefined subset of basis functions rather than unrestricted polynomial expansion allowing for potentially more interpretable results; these functions can be chosen according to specific properties required such as smoothness etc.\n4. **Genetic Programming Approach**: Incorporates Genetic Programming techniques leveraging matrix-based encodings enhancing efficiency during evolutionary computation processes leading towards better solutions faster compared to other heuristic approaches commonly used before now.\n5. **Benchmarking Improvements**: Demonstrated competitiveness against state-of-the-art SR algorithms across various benchmark problemsets including novel ones introduced here called SymSet designed specifically demanding higher interpretability requirements beyond current benchmarks available today thus pushing boundaries further ahead in terms of both accuracy & interpretability aspects together simultaneously!",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "PolyViT: Co-training Vision Transformers on Images, Videos and Audio",
        "abstract": "Can we train a single transformer model capable of processing multiple modalities and datasets, whilst sharing almost all of its learnable parameters? We present PolyViT, a model trained on images, audio and video to answer this question. PolyViT consists of a single transformer backbone, modality-specific tokenizers and task-specific output heads. By co-training on different tasks of a single modality, we are able to achieve significant accuracy improvements on 5 standard video- and audio-classification datasets. Furthermore, co-training PolyViT on multiple modalities and tasks leads to a parameter-efficient model which generalizes across multiple domains. In particular, our multi-modal PolyViT trained on 9 datasets across 3 modalities uses 8.3 times fewer parameters and outperforms a state-of-the-art single-task baseline on 2 of these datasets, whilst achieving competitive performance on the others. Finally, this simple and practical approach necessitates less hyperparameter tuning as the per-task hyperparameters can be readily reused.",
        "authors": "V. Likhosherstov, A. Arnab, K. M. Choromanski, et.al",
        "keywords": [
            "PolyViT",
            "Multi-modality",
            "Parameter Efficiency"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=zKnqZeUCLO",
        "pdf_src": "https://api2.openreview.net/pdf/eececa9123de41889e48b1085499ecd465eb8683.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper aims to address whether it is possible to train a single transformer model that processes various modalities such as images, audio, and videos while reusing most of its learnable parameters.\n\nResearch Problem: Can one create an efficient multi-modal transformer model with minimal hyperparameter tuning?\n\nMethods: The proposed solution involves training a unified transformer architecture called PolyViT along with modality-specific tokenizers for image, audio, and video data types respectively; each modality's tokenizer generates tokens from raw input in their respective format before they're fed into the shared transformer backbone. Task-specific output heads further process the information extracted by the transformer backbone according to specific downstream tasks like classification or detection within those modalities.\nThe authors also explore co-training strategies where models perform well on several related subtasks without needing extensive fine-tuning when applied to new unseen tasks due to transfer learning effects between similar ones.\n\nMain Contributions:\n1. They successfully demonstrate how a single transformer model can handle diverse multimodal inputs efficiently through modular design - using modality-specific tokenization but still maintaining a shared transformer backbone structure among them.\n2. Through empirical evaluation over five benchmark datasets covering both visual and auditory classifications, they show improved accuracy compared to dedicated single-modality baselines after co-training approaches were employed \n3. Their findings indicate that combining knowledge learned during joint training across multiple modalities results in more generalized capabilities than monomodal counterparts – evidenced via experiments conducted against nine datasets spread across three modalities demonstrating reduced complexity yet comparable performances overall including surpassing existing benchmarks on some datasets specifically.\n4. This work introduces PolyViT—a novel paradigmatic example of leveraging transformers' adaptability towards building versatile neural networks adept at handling complex multimodal problems",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Bounding generalization error with input compression: An empirical study with infinite-width networks",
        "abstract": "Estimating the Generalization Error (GE) of Deep Neural Networks (DNNs) is an important task that often relies on availability of held-out data. The ability to better predict GE based on a single training set may yield overarching DNN design principles to reduce a reliance on trial-and-error, along with other performance assessment advantages. \nIn search of a quantity relevant to GE, we investigate the Mutual Information (MI) between the input and final layer representations, using the infinite-width DNN limit to bound MI. An existing input compression-based GE bound is used to link MI and GE. To the best of our knowledge, this represents the first empirical study of this bound. In our attempt to empirically stress test the theoretical bound, we find that it is often tight for best-performing models. Furthermore, it detects randomization of training labels in many cases, reflects test-time perturbation robustness, and works well given only few training samples. These results are promising given that input compression is broadly applicable where MI can be estimated with confidence.",
        "authors": "A. Galloway, A. Golubeva, M. Salem, et.al",
        "keywords": [
            "input compression",
            "mutual information",
            "generalization error"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=jbZEUtULft",
        "pdf_src": "https://api2.openreview.net/pdf/c989f978472b5e88ebfa6e33b002f3a04d9fe372.pdf",
        "Code_src": "",
        "Introduction": "Background: Estimating the generalization error (GE) of deep neural networks (DNNs) accurately has been challenging due to its dependency on unavailable holdout datasets.\n\nResearch Problem: This paper aims to develop new methods predicting GE more reliably from just one dataset without relying heavily on trial-and-error or additional performance metrics like validation accuracy.\n  \nMethods: We propose utilizing mutual information (MI), which measures how much information about each variable you get by knowing another variable's value; here, we calculate MI between inputs and output layers within DNNs under certain assumptions such as infinite width limits - allowing us to estimate bounds around expected values rather than exact ones). Additionally, we use an established input-compression method previously proposed elsewhere linking MI directly back towards GE estimations.\n\nMain Contributions:\n1. Empirically validate these novel predictions against actual datasets showing they provide tighter estimates compared traditional approaches when applied correctly;\n2. Identify potential issues related specifically during model training phases – detecting label randomness among others – suggesting further improvements could focus there too;\n3. Demonstrate efficacy even if trained with fewer examples available at hand implying broader applicability beyond current settings requiring larger amounts before making reliable predictions regarding GE behavior overall.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "FedDAG: Federated DAG Structure Learning",
        "abstract": "To date, most directed acyclic graphs (DAGs) structure learning approaches require data to be stored in a central server. However, due to the consideration of privacy protection, data owners gradually refuse to share their personalized raw data to avoid private information leakage, making this task more troublesome by cutting off the first step. Thus, a puzzle arises: how do we discover the underlying DAG structure from decentralized data? In this paper, focusing on the additive noise models (ANMs) assumption of data generation, we take the first step in developing a gradient-based learning framework named FedDAG, which can learn the DAG structure without directly touching the local data and also can naturally handle the data heterogeneity. Our method benefits from a two-level structure of each local model. The first level structure learns the edges and directions of the graph and communicates with the server to get the model information from other clients during the learning procedure, while the second level structure approximates the mechanisms among variables and personally updates on its own data to accommodate the data heterogeneity. Moreover, FedDAG formulates the overall learning task as a continuous optimization problem by taking advantage of an equality acyclicity constraint, which can be solved by gradient descent methods to boost the searching efficiency. Extensive experiments on both synthetic and real-world datasets verify the efficacy of the proposed method.",
        "authors": "E. Gao, J. Chen, L. Shen, et.al",
        "keywords": [
            "FedDAG",
            "Additive Noise Models",
            "Decentralized Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=MzWgBjZ6Le",
        "pdf_src": "https://api2.openreview.net/pdf/ffa7405b4ba4a476cbdec052f1234d20eb7fc921.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background is about the challenges faced when trying to learn the structure of directed acyclic graphs (DAGs), especially considering that many existing algorithms for DAG structure learning rely on storing sensitive user data centrally.\n\nResearch Problem:\nThe research question posed here revolves around finding ways to infer DAG structures effectively even if users' personal data cannot be shared or centralized because they are concerned about privacy breaches leading to potential leaks of private information.\n \nMethodology:\nIn response to these issues, authors propose a federated learning approach called FedDAG based on the assumption that data follows an Additive Noise Model (ANM). This framework allows for learning the DAG's structure across multiple nodes where no direct sharing of actual data occurs between them; instead, only gradients related to the learned parameters need to be exchanged securely over networks like those provided by blockchain technology platforms such as Ethereum.\n\nMain Contributions:\nFedDAG introduces several key contributions:\n\n1. It enables distributed learning through a hierarchical architecture within individual client machines - one layer handles edge detection and directionality communication back to servers, whereas another inner layer adapts locally using personal data to account for variations in different datasets.\n\n2. By leveraging an equality acyclicity constraint along with gradient descent techniques, it turns the global optimization into a tractable continuous process improving computational efficiency significantly compared traditional batch methods requiring all samples at once.\n\n3. Experimental validation demonstrates effectiveness via extensive tests conducted against synthetic and real-world datasets confirming robustness under various scenarios including high levels of noise present commonly found in practical applications involving complex systems modeling tasks.",
        "Topic": "Generative Models"
    },
    {
        "title": "Learning Representations for Pixel-based Control: What Matters and Why?",
        "abstract": "Learning representations for pixel-based control has garnered significant attention recently in reinforcement learning. A wide range of methods have been proposed to enable efficient learning, leading to sample complexities similar to those in the full state setting. However, moving beyond carefully curated pixel data sets (centered crop, appropriate lighting, clear background, etc.) remains challenging. In this paper, we adopt a more difficult setting, incorporating background distractors, as a first step towards addressing this challenge. We start by exploring a simple baseline approach that does not use metric-based learning, data augmentations, world-model learning, or contrastive learning. We then analyze when and why previously proposed methods are likely to fail or reduce to the same performance as the baseline in this harder setting and why we should think carefully about extending such methods beyond the well curated environments. Our results show that finer categorization of benchmarks on the basis of characteristics like density of reward, planning horizon of the problem, presence of task-irrelevant components, etc., is crucial in evaluating algorithms. Based on these observations, we propose different metrics to consider when evaluating an algorithm on benchmark tasks. We hope such a data-centric view can motivate researchers to rethink representation learning when investigating how to best apply RL to real-world tasks. Code available: https://github.com/UtkarshMishra04/pixel-representations-RL",
        "authors": "M. Tomar, U. A. Mishra, A. Zhang, et.al",
        "keywords": [
            "pixel-based control",
            "reinforcement learning",
            "evaluation metrics"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=wIXHG8LZ2w",
        "pdf_src": "https://api2.openreview.net/pdf/11b5e1da03c786fd8d052411582f116ebbb49c71.pdf",
        "Code_src": "https://github.com/UtkarshMishra04/pixel-representations-RL",
        "Introduction": "Background:\nPixel-based control problems require agents to learn from raw pixel inputs rather than high-level abstractions which makes them inherently challenging due to the curse of dimensionality.\n\nResearch Question:\nThe research question addressed here pertains to improving the robustness and generalizability of deep reinforcement learning models trained with pixel-based input through better understanding their limitations under various conditions including the inclusion of distractor elements within the environment.\n\nMethods:\nThe authors begin without leveraging advanced techniques commonly used – no metric-based learning, data augmentation, world-model learning nor contrastive learning were employed initially serving as a \"baseline\" against which other approaches could be compared.\nThey also conducted analyses into potential failure modes where existing methods might degrade relative to even simpler baselines; they emphasized the need for nuanced evaluation criteria based on factors influencing the complexity level across benchmarks - e.g., reward density, planning horizons, irrelevant task components' presence among others before proposing new metrics suited specifically toward assessing algorithms designed primarily around pixel-based controls.\n\nMain Contributions:\nThis work introduces a novel perspective emphasizing the importance of considering diverse aspects related to the nature of the benchmarking process while designing algorithms meant especially for pixel-based Reinforcement Learning scenarios involving complex dynamics often found outside controlled datasets. The study encourages future works focusing less exclusively on purely technical advancements but instead considers broader contextual nuances impacting model efficacy significantly during deployment onto actual applications requiring adaptability amidst environmental variability",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "EdiBERT: a generative model for image editing",
        "abstract": "Advances in computer vision are pushing the limits of image manipulation, with generative models sampling highly-realistic detailed images on various tasks. However, a specialized model is often developed and trained for each specific task, even though many image edition tasks share similarities. In denoising, inpainting, or image compositing, one always aims at generating a realistic image from a low-quality one. In this paper, we aim at making a step towards a unified approach for image editing. To do so, we propose EdiBERT, a bidirectional transformer that re-samples image patches conditionally to a given image. Using one generic objective, we show that the model resulting from a single training matches state-of-the-art GANs inversion on several tasks: image denoising, image completion, and image composition. We also provide several insights on the latent space of vector-quantized auto-encoders, such as locality and reconstruction capacities. The code is available at https://github.com/EdiBERT4ImageManipulation/EdiBERT.",
        "authors": "T. Issenhuth, U. Tanielian, J. Mary, et.al",
        "keywords": [
            "image manipulation",
            "generative models",
            "unified approach"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=GRBbtkW3Lp",
        "pdf_src": "https://api2.openreview.net/pdf/143d380ed3698e0dd6b227e2c66bc1ff66c285bc.pdf",
        "Code_src": "https://github.com/EdiBERT4ImageManipulation/EdiBERT",
        "Introduction": "Background:\nThe field of computer vision has seen significant advancements recently due to the development of generative models capable of producing highly-realistic detailed images across different tasks.\n\nResearch Problem:\nDespite these advances, current approaches typically require developing and training a new specialized model for every individual image editing task despite their shared similarities—such as image denoising, inpainting, and image compositing—all aiming toward creating realistic images starting from lower quality ones.\n\nMethod:\nIn addressing this issue, our research introduces EdiBERT—a novel bidirectional transformer architecture designed specifically tailored around conditional resampling techniques which allows it to generate high-quality edited versions directly based upon inputted raw images without needing separate task-specific training phases using only a singular overarching objective function.\n\nMain Contributions:\nOur main contributions lie within demonstrating how EdiBERT can be applied successfully not just limited to its core functionality but extended into other related fields like image denoising , completion  & composition . Furthermore by utilizing Vector Quantized Auto Encoders (VAEs), we have gained valuable insights about their latent spaces including aspects pertaining locality along with reconstructive abilities; all while maintaining competitive performance levels against existing state-of-the-art Generative Adversarial Networks (GANs). Additionally providing open-source access via GitHub repository makes our findings more accessible allowing others interested further explore potential applications beyond those presented here.",
        "Topic": "Generative Models"
    },
    {
        "title": "Investigating Action Encodings in Recurrent Neural Networks in Reinforcement Learning",
        "abstract": "Building and maintaining state to learn policies and value functions is critical for deploying reinforcement learning (RL) agents in the real world. Recurrent neural networks (RNNs) have become a key point of interest for the state-building problem, and several large-scale reinforcement learning agents incorporate recurrent networks. While RNNs have become a mainstay in many RL applications, many key design choices and implementation details responsible for performance improvements are often not reported. In this work, we discuss one axis on which RNN architectures can be (and have been) modified for use in RL. Specifically, we look at how action information can be incorporated into the state update function of a recurrent cell. We discuss several choices in using action information and empirically evaluate the resulting architectures on a set of illustrative domains. Finally, we discuss future work in developing recurrent cells and discuss challenges specific to the RL setting.",
        "authors": "M. K. Schlegel, V. Tkachuk, A. White, et.al",
        "keywords": [
            "state-building",
            "recurrent neural networks",
            "reinforcement learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=K6g4MbAC1r",
        "pdf_src": "https://api2.openreview.net/pdf/78d373568cc5cfd9381469229a8a3ad51645b4b1.pdf",
        "Code_src": "",
        "Introduction": "Background: Reinforcement Learning (RL) has gained significant attention as an effective approach towards building intelligent systems that make decisions based on their environment's feedback over time. However, implementing RL agents effectively requires careful consideration when designing mechanisms such as policy learning or value estimation through states.\n\nResearch Problem: The primary challenge lies within the construction process where it’s essential to maintain relevant contextual information about past experiences—this necessitates incorporating dynamic changes like actions taken by the agent during interactions with its surroundings while also preserving long-term dependencies crucial for informed decision-making processes.\n \nMethodology: This paper explores modifications made specifically around recurrent neural network (RNN) architectures used extensively due to their ability to handle sequences efficiently; particularly focusing on integrating actionable data into these networks' internal updates known as \"state\" representations. They propose different strategies involving action incorporation methods before evaluating them experimentally across various simulated environments demonstrating varying degrees complexity levels.\n\nMain Contributions:\n1. Identification & Discussion - The authors identify potential axes along which existing RNN structures could potentially evolve further suited toward RL tasks emphasizing importance placed upon considering both immediate rewards/actions alongside longer-term outcomes via updated states.\n2. Empirical Evaluation - By applying proposed modifications onto representative datasets from diverse domains they demonstrate effectiveness gains compared against baseline models highlighting practical implications beyond theoretical considerations alone.\n3. Future Directions - Lastly but importantly provide insights regarding ongoing research directions including addressing unique difficulties encountered exclusively under RL contexts whilst utilizing recurrent units optimally",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Communication-Efficient Distributionally Robust Decentralized Learning",
        "abstract": "Decentralized learning algorithms empower interconnected devices to share data and computational resources to collaboratively train a machine learning model without the aid of a central coordinator. In the case of heterogeneous data distributions at the network nodes, collaboration can yield predictors with unsatisfactory performance for a subset of the devices. For this reason, in this work, we consider the formulation of a distributionally robust decentralized learning task and we propose a decentralized single loop gradient descent/ascent algorithm (AD-GDA) to directly solve the underlying minimax optimization problem. We render our algorithm communication-efficient by employing a compressed consensus scheme and we provide convergence guarantees for smooth convex and non-convex loss functions.  Finally, we corroborate the theoretical findings with empirical results that highlight AD-GDA's ability to provide unbiased predictors and to greatly improve communication efficiency compared to existing distributionally robust algorithms.",
        "authors": "M. Zecchin, M. Kountouris, D. Gesbert",
        "keywords": [
            "distributional robustness",
            "decentralized learning",
            "AD-GDA"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=tnRRHzZPMq",
        "pdf_src": "https://api2.openreview.net/pdf/10df86674e61c81a889063977294d17c7939b0fd.pdf",
        "Code_src": "",
        "Introduction": "Background: Decentralized learning algorithms enable connected devices to collaborate on training a machine learning model using shared data and computing power while bypassing centralized coordination.\n\nResearch Problem: When dealing with heterogeneous data distributions across network nodes, collaborative efforts may lead to suboptimal predictive performance among some devices due to insufficient representation from their specific datasets.\n\nMethodology: The paper introduces a distributionally robust decentralized learning framework which aims to address the aforementioned issue through a novel approach called AD-GDA - a decentralized single-loop gradient descent/ascent algorithm designed specifically as an efficient solution towards solving the underlying minimax optimization problem within such settings.\n\nMain Contributions:\n1. Formulation of a distributionally robust decentralized learning task.\n2. Development of AD-GDA – a decentralized algorithm capable of addressing the minimax optimization challenge effectively under heterogeneity conditions.\n3. Implementation of a compressed consensus strategy into AD-GDA making it more communication-efficient than other distributed algorithms currently available focused on distributional robustness issues related to decentralization processes involving multiple agents or machines sharing information over networks where each agent has its own local dataset characterized by different statistical properties leading up to potentially disparate predictions made during inference phase when deployed individually outside cooperative environment setup described here earlier mentioned.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "OpenCon: Open-world Contrastive Learning",
        "abstract": "Machine learning models deployed in the wild naturally encounter unlabeled samples from both known and novel classes. Challenges arise in learning from both the labeled and unlabeled data, in an open-world semi-supervised manner. In this paper, we introduce a new learning framework, open-world contrastive learning (OpenCon). OpenCon tackles the challenges of learning compact representations for both known and novel classes and facilitates novelty discovery along the way. We demonstrate the effectiveness of OpenCon on challenging benchmark datasets and establish competitive performance. On the ImageNet dataset, OpenCon significantly outperforms the current best method by 11.9% and 7.4% on novel and overall classification accuracy, respectively. Theoretically, OpenCon can be rigorously interpreted from an EM algorithm perspective—minimizing our contrastive loss partially maximizes the likelihood by clustering similar samples in the embedding space. The code is available at https://github.com/deeplearning-wisc/opencon.",
        "authors": "Y. Sun, Y. Li",
        "keywords": [
            "open-world contrastive learning",
            "novelty discovery",
            "semi-supervised learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=2wWJxtpFer",
        "pdf_src": "https://api2.openreview.net/pdf/76bdfd89722cacc2e69c1138bb15a50aa473ef5e.pdf",
        "Code_src": "https://github.com/deeplearning-wisc/opencon",
        "Introduction": "Background: Machine learning models often face the challenge of dealing with both labeled and unlabeled samples that belong to either known or novel classes when they are deployed in real-world scenarios.\n\nResearch Problem: How do we effectively learn from these heterogeneous types of data while maintaining good generalization capabilities?\n\nMethod: This paper introduces a new learning framework called \"open-world contrastive learning\" (OpenCon), which aims to tackle two main challenges:\n\n1. Learning compact representations not only for known but also for novel classes.\n2. Facilitating novelty detection during the learning process.\n\nMain Contributions:\n- OpenCon has been shown to perform competitively compared to existing methods across various benchmark datasets using image recognition tasks as examples.\n- Specifically, it achieves significant improvements over previous state-of-the-art results within the ImageNet dataset - increasing novel class accuracy by up to 11.9%, making its total classification accuracy better than any other approach tested so far!\n- From a theoretical standpoint, OpenCon's design aligns closely with Expectation-Maximization (EM) algorithms; minimizing their contrastive loss corresponds roughly to maximizing model likelihood through clustering similar samples into embeddings spaces more densely packed together around each cluster center point value rather than spread apart randomly throughout feature space dimensions without much structure present therein before training begins!\n\nCode Availability: The authors have made all necessary code publicly accessible via GitHub repository located here: [GitHub Repository Link]",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "Linking Neural Collapse and L2 Normalization with Improved Out-of-Distribution Detection in Deep Neural Networks",
        "abstract": "We propose a simple modification to standard ResNet architectures--L2 normalization over feature space--that substantially improves out-of-distribution (OoD) performance on the previously proposed Deep Deterministic Uncertainty (DDU) benchmark. We show that this change also induces early Neural Collapse (NC), an effect linked to better OoD performance. Our method achieves comparable or superior OoD detection scores and classification accuracy in a small fraction of the training time of the benchmark. Additionally, it substantially improves worst case OoD performance over multiple, randomly initialized models. Though we do not suggest that NC is the sole mechanism or a comprehensive explanation for OoD behaviour in deep neural networks (DNN), we believe NC's simple mathematical and geometric structure can provide a framework for analysis of this complex phenomenon in future work.",
        "authors": "J. Haas, W. Yolland, B. T. Rabus",
        "keywords": [
            "L2 normalization",
            "Out-of-Distribution (OoD) performance",
            "Neural Collapse (NC)"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=fjkN5Ur2d6",
        "pdf_src": "https://api2.openreview.net/pdf/dbbd7467a6ceedbd47693d00e9093b6287050af8.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses issues with existing Convolutional Neural Network (CNN) architectures when dealing with data outside their original distribution (\"out-of-distribution\" or \"OoD\"). This problem has significant implications as CNNs are often used without understanding how they generalize beyond seen examples.\n\nResearch Question: How might one modify common CNN architectures like ResNet such that they perform well even under conditions where new unseen data may be encountered?\n\nMethod: The authors introduce L2 normalization applied directly across all channels within each convolutional layer ('feature space' normalization'). They observe two key effects:\n\n1. Early Neural Collapse (NC): As training progresses, weights collapse into fewer directions earlier than usual.\n2. Improved Out-Of-Distribution Performance: Feature spaces become more distinct between classes during training which correlates positively with improved generalization capabilities against novel inputs.\n\nMain Contributions:\n1. A straightforward architectural tweak—applying L2 normalization at the feature level—to improve robustness towards OoD scenarios using DDU benchmarks—a dataset designed specifically around assessing uncertainty quantification methods’ ability to detect anomalies from normal distributions.\n2. Demonstrating that this approach leads to faster convergence due to NC while maintaining similar levels of OoD detection score and classification accuracy compared to baseline models trained longer periods but less efficiently regarding computational resources consumed per unit improvement gained by extending training duration further would likely lead to diminishing returns).\n3. Showing substantial improvements relative to other random initialization strategies suggesting its efficacy isn't just limited to specific initializations nor does it rely solely upon any particular starting point within parameter space; thus providing broader applicability potentially aiding practitioners looking toward enhancing model robustness quickly yet effectively through architecture modifications rather than extensive retraining efforts).",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "A geometrical connection between sparse and low-rank matrices and its application to manifold learning",
        "abstract": "We consider when a sparse nonnegative matrix $\\mathbf{S}$ can be recovered, via an elementwise nonlinearity, from a real-valued matrix~$\\mathbf{L}$ of significantly lower rank. Of particular interest is the setting where the positive elements of $\\mathbf{S}$ encode the similarities of nearby points on a low dimensional manifold. The recovery can then be posed as a problem in manifold learning---in this case, how to learn a norm-preserving and neighborhood-preserving mapping of high dimensional inputs into a lower dimensional space. We describe an algorithm for this problem based on a generalized low-rank decomposition of sparse matrices. This decomposition has the interesting property that it can be encoded by a neural network with one layer of rectified linear units; since the algorithm discovers this encoding, it can also be viewed as a layerwise primitive for deep learning. The algorithm regards the inputs $\\mathbf{x}_i$ and $\\mathbf{x}_j$ as similar whenever the cosine of the angle between them exceeds some threshold $\\tau\\in(0,1)$. Given this threshold, the algorithm attempts to discover a mapping $\\mathbf{x}_i\\mapsto\\mathbf{y}_i$ by matching the elements of two sparse matrices; in particular, it seeks a mapping for which $\\mathbf{S}=\\max(0,\\mathbf{L})$, where $S_{ij} = \\max(0,\\mathbf{x}_i\\!\\cdot\\!\\mathbf{x}_j\\! -\\! \\tau\\|\\mathbf{x}_i\\|\\|\\mathbf{x}_j\\|)$ and $L_{ij} = \\mathbf{y}_i\\!\\cdot\\!\\mathbf{y}_j\\! -\\! \\tau\\|\\mathbf{y}_i\\|\\|\\mathbf{y}_j\\|$. We apply the algorithm to data sets where vector magnitudes and small cosine distances have interpretable meanings (e.g., the brightness of an image, the similarity to other words). On these data sets, the algorithm is able to discover much lower dimensional representations that preserve these meanings.",
        "authors": "L. K. Saul",
        "keywords": [
            "sparse matrix recovery",
            "manifold learning",
            "neural networks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=p8gncJbMit",
        "pdf_src": "https://api2.openreview.net/pdf/36fac48f26e9ca3e8a793fa46a2bf1a9fcb715b7.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the issue of recovering a sparse non-negative matrix $\\mathbf{S}$ through an element-wise nonlinearity from a real-valued matrix $\\mathbf{L}$ having significantly less rank than $\\mathbf{S}$. Specifically, they are interested in scenarios where the positive entries of $\\mathbf{S}$ represent the similarities among neighboring points lying close together along a low-dimensional manifold.\n\nResearch Problem: How does one recover such a sparse non-negative matrix $\\mathbf{S}$ given its corresponding low-rank real-valued matrix $\\mathbf{L}$?\n\nMethodology: To solve this problem, authors propose using a generalized low-rank decomposition technique specifically tailored towards sparse matrices. They note that this decomposition may potentially be represented by a neural network consisting solely of one layer of rectified linear units due to certain properties exhibited during their analysis process within the algorithmic framework proposed here.\n\nMain Contributions:\n- A novel approach leveraging sparse matrix decompositions coupled with neural networks capable of discovering underlying patterns or embeddings while preserving norms across different dimensions.\n- An algorithm designed around maximizing cosine similarity thresholds between input vectors $\\mathbf{x}_i$ and $\\mathbf{x}_j$ leading toward finding mappings $\\mathbf{x}_i\\mapsto\\mathbf{y}_i$ that satisfy $\\mathbf{S}=\\max(0,\\mathbf{L})$ under specific conditions involving magnitude preservation terms related to vector lengths.\n- Demonstrated effectiveness applying said algorithms onto datasets whose components possess meaningful interpretations like brightness values associated with images or semantic similarities amongst textual corpora resulting in more compact yet semantically preserved representations compared against baseline methods without considering such constraints explicitly throughout training procedures employed therein respectively.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "On Characterizing the Trade-off in Invariant Representation Learning",
        "abstract": "Many applications of representation learning, such as privacy preservation, algorithmic fairness, and domain adaptation, desire explicit control over semantic information being discarded. This goal is formulated as satisfying two objectives: maximizing utility for predicting a target attribute while simultaneously being invariant (independent) to a known semantic attribute. Solutions to invariant representation learning (IRepL) problems lead to a trade-off between utility and invariance when they are competing. While existing works study bounds on this trade-off, two questions remain outstanding: 1) What is the exact trade-off between utility and invariance? and 2) What are the encoders (mapping the data to a representation) that achieve the trade-off, and how can we estimate it from training data? This paper addresses these questions for IRepLs in reproducing kernel Hilbert spaces (RKHS)s. Under the assumption that the distribution of a low-dimensional projection of high-dimensional data is approximately normal, we derive a closed-form solution for the global optima of the underlying optimization problem for encoders in RKHSs. This yields closed formulae for a near-optimal trade-off, corresponding optimal representation dimensionality, and the corresponding encoder(s). We also numerically quantify the trade-off on representative problems and compare them to those achieved by baseline IRepL algorithms.",
        "authors": "B. Sadeghi, S. Dehdashtian, V. Boddeti",
        "keywords": [
            "invariant representation learning",
            "reproducible kernel Hilbert spaces",
            "trade-off"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=3gfpBR1ncr",
        "pdf_src": "https://api2.openreview.net/pdf/09751f8bf9bd391f1a5b145dc018a86defb77f06.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this research lies within the field of machine learning where there's an increasing demand for models with better interpretability or explainability—models whose decisions do not rely solely on complex internal mechanisms but rather have some degree of transparency about what features contribute most significantly.\n\nResearch Problem:\nThe primary challenge addressed here concerns \"Invariant Representation Learning\" (IRepL), which aims at creating representations (\"encodings\") capable of accurately predicting certain attributes without reference to other irrelevant factors deemed sensitive ('semantic attributes'). The overarching objective involves finding a balance—the 'trade-off'—between prediction accuracy (utility) and the model’s independence/invariance regarding specific semantic attributes like race/gender/etc., especially crucial issues related to privacy protection against discrimination through algorithmic fairness considerations.\n\nMethods:\nTo tackle this issue effectively under the constraints imposed upon the Reproducing Kernel Hilbert Spaces (RKHS), researchers propose a novel approach based on statistical approximations concerning the distributions of projections derived from high-dimensional input data into lower dimensions—a common practice due to computational limitations yet often assumed to be normally distributed despite empirical evidence suggesting otherwise.\n \nMain Contributions:\nThis work introduces several key contributions:\n\n1. A closed-form solution for the global optimum of the encoding function in RKHSs has been obtained assuming a normal distribution after projecting high-dimensional data onto a low-dimensional space;  \n2. It provides analytical expressions for both the optimal trade-off curve among different encodings achieving varying levels of utility and invariance;\n3. Furthermore, the paper offers insights into determining the optimal representation dimensionality required during feature extraction processes along with identifying the best performing encoders themselves;  \n4. Finally, numerical experiments conducted demonstrate quantitatively justifying their theoretical findings compared traditional methods focusing only on predictive performance alone thus highlighting importance beyond mere classification tasks towards broader societal implications including ethical use cases involving personal identifiable information handling practices ensuring equitable outcomes across demographics groups affected by automated decision-making systems.",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Uncertainty-Based Active Learning for Reading Comprehension",
        "abstract": "Recent years have witnessed a surge of successful applications of machine reading comprehension. Of central importance to these tasks is the availability of massive amount of labeled data, which facilitates training of large-scale neural networks. However, in many real-world problems, annotated data are expensive to gather not only because of time cost and budget, but also  of certain domain-specific restrictions such as privacy for healthcare data. In this regard, we propose an uncertainty-based active learning algorithm for reading comprehension, which interleaves data annotation and model updating to mitigate the demand of labeling. Our key techniques are two-fold: 1) an unsupervised uncertainty-based sampling scheme that queries the labels of the most informative instances with respect to the currently learned model; and 2) an adaptive loss minimization paradigm that simultaneously fits the data and controls the degree of model updating. We demonstrate on  benchmark datasets that 25% less labeled samples suffice to guarantee similar, or even improved performance. Our results show strong evidence that for label-demanding scenarios, the proposed approach offers a practical guide on data collection and model training. ",
        "authors": "J. Wang, J. Shen, X. Ma, et.al",
        "keywords": [
            "uncertainty-based active learning",
            "unsupervised uncertainty-based sampling",
            "adaptive loss minimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=QaDevCcmcg",
        "pdf_src": "https://api2.openreview.net/pdf/f6f20bc360dc529129298317e3982b676e969f71.pdf",
        "Code_src": "",
        "Introduction": "Background: The success of recent machine reading comprehension models heavily relies on access to vast amounts of labeled data due to their reliance on deep neural network architectures.\n\nResearch Problem: However, collecting sufficient labeled data can be prohibitively costly both financially and operationally—due to factors like labor-intensive annotation processes—and may face additional constraints related to sensitive information protection within specific domains – e.g., patient confidentiality issues in medical records.\n\nMethodology: To address limitations imposed by scarce labeled data resources without sacrificing accuracy gains from supervised learning methods, our study introduces an uncertainty-based active learning framework designed specifically for reading comprehension tasks involving text and images.\n- Uncertainty-Based Sampling Scheme: This involves selecting unlabeled examples based on how uncertain current predictions about them might still improve given new annotations relative to other candidates already encountered during pre-training iterations using existing knowledge.\n- Adaptive Loss Minimization Paradigm: A novel method where while fitting the dataset's distribution through standard backpropagation algorithms it adjusts its focus towards minimizing prediction errors rather than maximizing updates across all parameters at once—a balance between exploration vs exploitation strategy aimed toward efficient use available labeled examples efficiently.\n\nMain Contributions:\n- Demonstrated empirical improvements over baseline approaches when trained under resource-constrained conditions suggesting effectiveness against common challenges faced today including high costs associated with manual annotation efforts alongside stringent requirements placed upon handling private personal identifiable information (PII).\n- Provided insights into leveraging limited supervision effectively leading potential practitioners navigate trade-offs involved balancing computational efficiency versus maintaining desired levels predictive performance needed meet application demands accurately interpret complex multimodal inputs consisting textual descriptions paired visual imagery sources",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "An empirical study of implicit regularization in deep offline RL",
        "abstract": "Deep neural networks are the most commonly used function approximators in offline reinforcement learning. Prior works have shown that neural nets trained with TD-learning and gradient descent can exhibit implicit regularization that can be characterized by under-parameterization of these networks. Specifically, the rank of the penultimate feature layer, also called effective rank, has been observed to drastically collapse during the training. In turn, this collapse has been argued to reduce the model's ability to further adapt in later stages of learning, leading to the diminished final performance. Such an association between the effective rank and performance makes effective rank compelling for offline RL, primarily for offline policy evaluation. In this work, we conduct a careful empirical study on the relation between effective rank and performance on three offline RL datasets : bsuite, Atari, and DeepMind lab. We observe that a direct association exists only in restricted settings and disappears in the more extensive hyperparameter sweeps. Also, we empirically identify three phases of learning that explain the impact of implicit regularization on the learning dynamics and found that bootstrapping alone is insufficient to explain the collapse of the effective rank. Further, we show that several other factors could confound the relationship between effective rank and performance and conclude that studying this association under simplistic assumptions could be highly misleading.\n",
        "authors": "C. Gulcehre, S. Srinivasan, J. Sygnowski, et.al",
        "keywords": [
            "effective_rank",
            "offline_rl",
            "implicit_regularization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=HFfJWx60IT",
        "pdf_src": "https://api2.openreview.net/pdf/0d42241be4cfb44607ef3f545b9877c3795ea24d.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper discusses the use of deep neural networks as function approximators in offline reinforcement learning.\n\nResearch Problem: The research problem addressed here concerns understanding how well the parameters of such networks scale up or down when they're being trained using techniques like temporal difference (TD) learning combined with gradient descent methods - which often leads to \"implicit regularization\" due to under-parameterization within those networks over time; specifically focusing on what happens after initial parameter tuning occurs – whether there’s any correlation between changes made at early stages versus outcomes achieved towards end performances?\n\nMethodology: To address their question about correlations among different aspects related to neural network architectures & behaviors throughout training processes across various domains (bsuite, Atari games), authors conducted experiments involving 3 datasets mentioned above along with analyzing trends from hyperparameters sweeping approaches employed therein too!\n\nMain Contributions: \n1) Identified three distinct phases where implicit regularization impacts learning dynamics differently depending upon context/environmental conditions involved;\n2) Demonstrated that bootstrap method alone isn't sufficient enough explanation behind why effective ranks collapse during optimization process;\n3) Highlighted potential confounding variables influencing relationships between effective ranks & overall system performances making caution necessary while drawing conclusions based solely on simplistic assumptions regarding them.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Collaborative Algorithms for Online Personalized Mean Estimation",
        "abstract": "We consider an online estimation problem involving a set of agents. Each agent has access to a (personal) process that generates samples from a real-valued distribution and seeks to estimate its mean. We study the case where some of the distributions have the same mean, and the agents are allowed to actively query information from other agents. The goal is to design an algorithm that enables each agent to improve its mean estimate thanks to communication with other agents. The means as well as the number of distributions with same mean are unknown, which makes the task nontrivial. We introduce a novel collaborative strategy to solve this online personalized mean estimation problem. We analyze its time complexity and introduce variants that enjoy good performance in numerical experiments. We also extend our approach to the setting where clusters of agents with similar means seek to estimate the mean of their cluster.",
        "authors": "M. Asadi, A. Bellet, O. Maillard, et.al",
        "keywords": [
            "online estimation",
            "collaborative strategy",
            "personalized mean estimation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=VipljNfZSZ",
        "pdf_src": "https://api2.openreview.net/pdf/e698afef2071598451290f5f5cd308b0882fb8e6.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper addresses an online estimation challenge among multiple agents who aim to individually estimate the mean value generated by personal processes following certain probability distributions.\n\nResearch Problem: Agents must collaborate without prior knowledge about whether any two distributions share the same mean or how many such shared-means exist within all possible distributions they could encounter during the estimation period; hence, it's necessary for them to adaptively learn through interactions while maintaining privacy concerns regarding individual data.\n\nMethods: To tackle these challenges, we propose a novel collaborative strategy designed specifically for solving dynamic personalized mean estimation problems under uncertainty conditions.\n \nMain Contributions:\n1. A new collaborative framework enabling agents to iteratively refine their estimates based on mutual communications;\n2. An analysis demonstrating the efficiency terms of computational complexity associated with proposed algorithms;\n3. Variant versions developed incorporating insights gained via empirical studies yielding promising results across various scenarios;\n4. Extension into clustering settings allowing groups sharing common characteristics like estimated means work together towards joint optimization goals",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "Unsupervised Learning of Neurosymbolic Encoders",
        "abstract": "We present a framework for the unsupervised learning of neurosymbolic encoders, which are encoders obtained by composing neural networks with symbolic programs from a domain-specific language. Our framework naturally incorporates symbolic expert knowledge into the learning process, which leads to more interpretable and factorized latent representations compared to fully neural encoders. We integrate modern program synthesis techniques with the variational autoencoding (VAE) framework, in order to learn a neurosymbolic encoder in conjunction with a standard decoder. The programmatic descriptions from our encoders can benefit many analysis workflows, such as in behavior modeling where interpreting agent actions and movements is important. We evaluate our method on learning latent representations for real-world trajectory data from animal biology and sports analytics. We show that our approach offers significantly better separation of meaningful categories than standard VAEs and leads to practical gains on downstream analysis tasks, such as for behavior classification.\n",
        "authors": "E. Zhan, J. J. Sun, A. Kennedy, et.al",
        "keywords": [
            "neurosymbolic encoding",
            "unsupervised learning",
            "variational autoencoding"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=eWvBEMTlRq",
        "pdf_src": "https://api2.openreview.net/pdf/f219ca1fa3b5df81dc27c50f29b0e93c04b72a83.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper introduces a novel framework for unsupervised learning of neurosymbolic encoders - encoders constructed through combining neural networks with symbolic programs written using a domain-specific language.\n\nResearch Problem: How do we effectively incorporate symbolic expert knowledge during the encoding phase while maintaining interpretability?\n\nMethodology: They employ state-of-the-art program synthesis methods within the variational autoencoder (VAE) architecture allowing them to simultaneously train both neural network components along with corresponding symbolic ones without requiring any supervision or labeled datasets beyond what's needed typically for training an ordinary VAE.\n\nMain Contributions:\n1. A new hybrid model called NeuroSymbolic Variational Autoencoder (NS-VAE), integrating neural networks alongside programmatically generated symbolic features leading to potentially more interpretable models due to their modular nature; \n2. Demonstrated improved performance over traditional VAEs when applied across various domains including analyzing trajectories derived from biological studies like those involving animals' movement patterns up to sport analytics scenarios;\n3. Provided insights about how these learned symbolic representations could be beneficially utilized throughout different analytical pipelines focused around behavioral understanding",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Benchmarks and Algorithms for Offline Preference-Based Reward Learning",
        "abstract": "Learning a reward function from human preferences is challenging as it typically requires having a high-fidelity simulator or using expensive and potentially unsafe actual physical rollouts in the environment. However, in many tasks the agent might have access to offline data from related tasks in the same target environment. While offline data is increasingly being used to aid policy optimization via offline RL, our observation is that it can be a surprisingly rich source of information for preference learning as well. We propose an approach that uses an offline dataset to craft preference queries via pool-based active learning, learns a distribution over reward functions, and optimizes a corresponding policy via offline RL. Crucially, our proposed approach does not require actual physical rollouts or an accurate simulator for either the reward learning or policy optimization steps. To test our approach, we first evaluate existing offline RL benchmarks for their suitability for offline reward learning. Surprisingly, for many offline RL domains, we find that simply using a trivial reward function results good policy performance, making these domains ill-suited for evaluating learned rewards. To address this, we identify a subset of existing offline RL benchmarks that are well suited for offline reward learning and also propose new offline apprenticeship learning benchmarks which allow for more open-ended behaviors. When evaluated on this curated set of domains, our empirical results suggest that combining offline RL with learned human preferences can enable an agent to learn to perform novel tasks that were not explicitly shown in the offline data. ",
        "authors": "D. Shin, A. Dragan, D. S. Brown",
        "keywords": [
            "offline reward learning",
            "human preferences",
            "active learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=TGuXXlbKsn",
        "pdf_src": "https://api2.openreview.net/pdf/a02b5e40929f86f38003f1b926bb082d17846fb1.pdf",
        "Code_src": "",
        "Introduction": "Background: Learning a reward function based on human preferences often necessitates costly simulations due to the lack of direct interaction between humans and agents.\n\nResearch Problem: How do you effectively utilize offline datasets when simulators aren't available?\n\nMethodology: The authors introduce a method leveraging offline datasets through pool-based active learning techniques followed by optimizing policies without requiring real-world rollouts.\n- Craft preference queries utilizing offline dataset \n- Learn a distribution over reward functions\n- Optimize policies independently using offline reinforcement learning\n\nMain Contributions:\n1. Demonstrated how offline datasets could serve multiple purposes beyond just aiding policy optimization - they're valuable sources even for learning preferences directly relevant to those datasets themselves;\n2. Proposed methods such as pool-based active learning specifically designed around crafting effective preference queries out of large amounts of unlabeled data; \n3. Introduced two sets of benchmarks – one focusing solely on offline reward learning while another incorporates aspects like apprenticeship learning allowing exploration into diverse behavior spaces unseen during training phase alone.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Unsupervised Network Embedding Beyond Homophily",
        "abstract": "Network embedding (NE) approaches have emerged as a predominant technique to represent complex networks and have benefited numerous tasks. However, most NE approaches rely on a homophily assumption to learn embeddings with the guidance of supervisory signals, leaving the unsupervised heterophilous scenario relatively unexplored. This problem becomes especially relevant in fields where a scarcity of labels exists. Here, we formulate the unsupervised NE task as an r-ego network discrimination problem and develop the SELENE framework for learning on networks with homophily and heterophily. Specifically, we design a dual-channel feature embedding pipeline to discriminate r-ego networks using node attributes and structural information separately. We employ heterophily adapted self-supervised learning objective functions to optimise the framework to learn intrinsic node embeddings. We show that SELENE's components improve the quality of node embeddings, facilitating the discrimination of connected heterophilous nodes. Comprehensive empirical evaluations on both synthetic and real-world datasets with varying homophily ratios validate the effectiveness of SELENE in homophilous and heterophilous settings showing an up to 12.52% clustering accuracy gain.",
        "authors": "Z. Zhong, G. Gonzalez, D. Grattarola, et.al",
        "keywords": [
            "r-ego network discrimination",
            "SELENE framework",
            "heterophily adapted self-supervised learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=sRgvmXjrmg",
        "pdf_src": "https://api2.openreview.net/pdf/060676a6531178d76459e15bc5795580bce7c3ae.pdf",
        "Code_src": "",
        "Introduction": "Background: Network embedding (NE) is widely used approach to represent complex networks which has been beneficial many tasks. Most existing methods assume homophily and use supervised signal to guide the embedding process but ignore the unsupervised heterophilous scenario.\n\nResearch Problem: How can we effectively perform unsupervised NE when there are no labeled data?\n\nMethod: The paper proposes a novel method called SELENE, which formulates the unsupervised NE task as an r-ego network discrimination problem based on homophily and heterophily. It designs a dual-channel feature embedding pipeline to discriminate ego networks by considering node attributes and structural information respectively. Moreover, it employs heterophily adapted self-supervised learning objective functions to optimize the framework so that it learns intrinsic node embeddings more effectively.\n\nMain Contributions: The main contributions include:\n1. Formulating the unsupervised NE task into an r-ego network discrimination problem.\n2. Developing the SELENE framework incorporating homophily and heterophily.\n3. Designing a dual-channel feature embedding pipeline utilizing node attributes and structural information.\n4. Employing heterophily adapted self-supervised learning objective functions optimizing the framework performance significantly improving the quality of node embeddings compared traditional methods achieving higher clustering accuracy gains ranging from 7.5% to 12.52%.",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "Indiscriminate Data Poisoning Attacks on Neural Networks",
        "abstract": "Data poisoning attacks, in which a malicious adversary aims to influence a model by injecting ``poisoned'' data into the training process, have attracted significant recent attention. In this work, we take a closer look at existing poisoning attacks and connect them with old and new algorithms for solving sequential Stackelberg games. By choosing an appropriate loss function for the attacker and optimizing with algorithms that exploit second-order information, we design poisoning attacks that are effective on neural networks. We present efficient implementations by parameterizing the attacker and allowing simultaneous and coordinated generation of tens of thousands of poisoned points, in contrast to most existing methods that generate poisoned points one by one. We further perform extensive experiments that empirically explore the effect of data poisoning attacks on deep neural networks. Our paper sets a new benchmark on the possibility of performing indiscriminate data poisoning attacks on modern neural networks.",
        "authors": "Y. Lu, G. Kamath, Y. Yu",
        "keywords": [
            "data poisoning attacks",
            "sequential Stackelberg games",
            "adversarial machine learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=x4hmIsWu7e",
        "pdf_src": "https://api2.openreview.net/pdf/fbba5d3d30f6611acb1195fdc122796c7d79be41.pdf",
        "Code_src": "",
        "Introduction": "Background: Data poisoning attacks pose serious threats to machine learning models as they involve deliberately corrupting datasets during training phase.\n\nResearch Problem: This study investigates how existing poisoning attack strategies can be connected with algorithms designed for solving sequential Stackelberg games.\n \nMethodology: The researchers propose designing more sophisticated poisoning attacks using advanced optimization techniques based on second-order information while selecting suitable loss functions tailored specifically towards attackers' objectives.\n \nMain Contributions:\n1. They introduce novel approaches leveraging sequential Stackelberg game theory within their framework; \n2. Efficiently implement these methodologies through parameterized attackers capable simultaneously generating numerous poisoned examples compared traditional methods producing only single instances;\n3. Conduct comprehensive empirical evaluations demonstrating effectiveness against contemporary neural network architectures thereby establishing benchmarks highlighting risks posed from indiscriminate deployment such tactics could entail moving forward.",
        "Topic": "Machine Learning"
    },
    {
        "title": "Fairness and robustness in anti-causal prediction",
        "abstract": "Robustness to distribution shift and fairness have independently emerged as two important desiderata required of modern machine learning models. While these two desiderata seem related, the connection between them is often unclear in practice.  Here, we discuss these connections through a causal lens, focusing on anti-causal prediction tasks, where the input to a classifier (e.g., an image) is assumed to be generated as a function of the target label and the protected attribute. By taking this perspective, we draw explicit connections between a common fairness criterion - separation - and a common notion of robustness - risk invariance.  These connections provide new motivation for applying the separation criterion in anticausal settings, and inform old discussions regarding fairness-performance tradeoffs. In addition, our findings suggest that robustness-motivated approaches can be used to enforce separation, and that they often work better in practice than methods designed to directly enforce separation. Using a medical dataset, we empirically validate our findings on the task of detecting pneumonia from X-rays, in a setting where differences in prevalence across sex groups motivates a fairness mitigation. Our findings highlight the importance of considering causal structure when choosing and enforcing fairness criteria.",
        "authors": "M. Makar, A. N. D'amour",
        "keywords": [
            "causality",
            "fairness",
            "robustness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=mrTXGDZns2",
        "pdf_src": "https://api2.openreview.net/pdf/e259ca198fcef9d7b12b352cf234404e258e5ff3.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses how robustness against distribution shifts has become increasingly crucial alongside fairness concerns within contemporary machine learning systems.\n\nResearch Question: It raises questions about whether there's any relationship or interplay existing among robustness requirements such as risk invariance with fairness considerations like the separation criterion which aims at avoiding disparate impact based on sensitive attributes (\"protected attributes\").\n\nMethodology: Employing a causal framework specifically looking into \"anti-causal\" prediction scenarios – situations wherein inputs are presumed to depend causally upon both labels and protected attributes before being observed by the model; it establishes direct links between notions of robustness motivated by preserving predictions under perturbations unrelated to the data generation process versus those concerned with ensuring fair treatment irrespective of demographic characteristics ('separation').\n\nMain Contributions:\n1. The study identifies a clear link between 'risk invariance'—a measure commonly associated with robustness—and 'separation', one type of fairness metric.\n2. This insight provides fresh impetus towards utilizing the separation principle even outside its traditional domain—the causal context—in predictive modeling contexts involving non-identical distributions over features.\n3. Empirical validation using a real-world medical imaging dataset demonstrates practical benefits gained via incorporating robustness principles while aiming toward achieving fairness goals during classification problems relevant here would include predicting disease presence conditions accurately without bias due to gender disparities present therein.\n4. The research emphasizes why understanding underlying causal relationships should guide us not only concerning what fairness criteria need enforcement but also informs strategies employed",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Euclidean-Norm-Induced Schatten-p Quasi-Norm Regularization for Low-Rank Tensor Completion and Tensor Robust Principal Component Analysis",
        "abstract": "The nuclear norm and Schatten-$p$ quasi-norm are popular rank proxies in low-rank matrix recovery. However, computing the nuclear norm or Schatten-$p$ quasi-norm of a tensor is hard in both theory and practice, hindering their application to low-rank tensor completion (LRTC) and tensor robust principal component analysis (TRPCA). In this paper, we propose a new class of tensor rank regularizers based on the Euclidean norms of the CP component vectors of a tensor and show that these regularizers are monotonic transformations of tensor Schatten-$p$ quasi-norm. This connection enables us to minimize the Schatten-$p$ quasi-norm in LRTC and TRPCA implicitly via the component vectors. The method scales to big tensors and provides an arbitrarily sharper rank proxy for low-rank tensor recovery compared to the nuclear norm. On the other hand, we study the generalization abilities of LRTC with the Schatten-$p$ quasi-norm regularizer and LRTC with the proposed regularizers.  The theorems show that a relatively sharper regularizer leads to a tighter error bound, which is consistent with our numerical results. Particularly, we prove that for LRTC with Schatten-$p$ quasi-norm regularizer on $d$-order tensors, $p=1/d$ is always better than any $p>1/d$ in terms of the generalization ability. We also provide a recovery error bound to verify the usefulness of small $p$ in the Schatten-$p$ quasi-norm for TRPCA. Numerical results on synthetic data and real data demonstrate the effectiveness of the regularization methods and theorems.",
        "authors": "J. Fan, L. Ding, C. Yang, et.al",
        "keywords": [
            "tensor rank regularizer",
            "Schatten-$p$ quasi-norm",
            "low-rank tensor completion"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Grhi800jVz",
        "pdf_src": "https://api2.openreview.net/pdf/243888fc9a2ce02fe5f1b600e1f2c146c462eb0c.pdf",
        "Code_src": "",
        "Introduction": "Background: Low-rank tensor completion (LRTC) and tensor robust principal component analysis (TRPCA) have been widely studied due to their applications across various fields such as signal processing, computer vision, etc. Rank proxies like the nuclear norm and Schatten-$p$ quasi-norm play significant roles here; however, they face computational challenges when applied to tensors.\n\nResearch Problem: Computing the nuclear norm or Schatten-$p$ quasi-norm directly from a tensor is computationally expensive because it involves solving optimization problems over high-dimensional spaces involving all possible permutations of the tensor components.\n \nMethods: To address this issue, authors introduce a novel family of tensor rank regularizers derived by taking the Euclidean norms of the CP component vectors within a tensor. They establish that these regularizers can be considered as monotonic transformations equivalent to the Schatten-$p$ quasi-norms under certain conditions. Consequently, minimizing the Schatten-$p$ quasi-norm becomes feasible through optimizing only the component vectors instead of the entire tensor's entries explicitly. Moreover, the approach allows scaling up to large tensors while offering a more refined rank approximation capability relative to the nuclear norm.\n\nMain Contributions:\n1. A novel set of tensor rank regularizers based on the Euclidean norms of CP component vectors has been introduced into the field;\n2. These regularizers are shown to be equivalent to the Schatten-$p$ quasi-norms after establishing a theoretical link between them;\n3. An implicit minimization strategy was developed allowing for efficient computation even during tensor completion tasks where the original tensor may not fully recoverable;\n4. Empirical evidence suggests that the proposed regularizers lead to improved performance metrics beyond those achievable using the nuclear norm;\n5. Theorems were proven demonstrating how a sharper regularizer could result in narrower estimation errors leading to stronger guarantees about model generalizability – particularly highlighting the superiority of p = 1/d for d-order tensors compared to higher values of p;\n6. Recovery bounds provided further substantiate practical benefits associated with smaller values of p in the context of TRPCA;\n7. Experimental validation conducted against synthetic datasets confirmed efficacy alongside empirical observations made earlier.",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Constrained Parameter Inference as a Principle for Learning",
        "abstract": "Learning in neural networks is often framed as a problem in which targeted error signals are directly propagated to parameters and used to produce updates that induce more optimal network behaviour. Backpropagation of error (BP) is an example of such an approach and has proven to be a highly successful application of stochastic gradient descent to deep neural networks. We propose constrained parameter inference (COPI) as a new principle for learning. The COPI approach assumes that learning can be set up in a manner where parameters infer their own values based upon observations of their local neuron activities. We find that this estimation of network parameters is possible under the constraints of decorrelated neural inputs and top-down perturbations of neural states for credit assignment. We show that the decorrelation required for COPI allows learning at extremely high learning rates, competitive with that of adaptive optimizers, as used by BP. We further demonstrate that COPI affords a new approach to feature analysis and network compression. Finally, we argue that COPI may shed new light on learning in biological networks given the evidence for decorrelation in the brain.",
        "authors": "N. Ahmad, E. Schrader, M. V. Gerven",
        "keywords": [
            "decorrelated neural inputs",
            "constrained parameter inference",
            "top-down perturbations"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=CUDdbTT1QC",
        "pdf_src": "https://api2.openreview.net/pdf/ce4d266b67a95d3f481f3cf59089254f013e57da.pdf",
        "Code_src": "",
        "Introduction": "Background: Learning in neural networks traditionally involves propagating error signals back through the network using techniques like backpropagation (BP), leading to parameter updates aimed at improving performance.\n\nResearch Question: This paper introduces a novel method called constrained parameter inference (COPI). It questions whether instead of relying solely on externally provided error gradients during training, neurons could autonomously adjust their weights according to internal activity patterns within the network?\n\nMethod: COPI operates without explicit error propagation or optimization algorithms; it suggests that neurons might learn from each other's interactions rather than external feedback alone. The key idea here is that if input activations were decorrelated across neurons due to some form of noise or intrinsic variability—similar to what happens when top-down influences modulate neuronal firing—and these neurons received subtle perturbations suggesting how they should contribute to overall network output then they would naturally converge towards optimal parameter settings themselves over time—a process akin to 'self-optimization'.\n\nMain Contributions:\n1. Theoretical Framework - They provide theoretical justifications why COPI works.\n2. Empirical Validation - Demonstrated experimentally via simulations showing that COPI can indeed lead to effective learning even though there’s no direct use of error gradients nor any adaptation algorithm involved—it relies purely on intrinsic properties of neural dynamics itself!\n3. Novel Applications - COPI opens avenues into better understanding complex processes related not only artificial intelligence but also potentially biology since similar principles seem relevant considering recent findings about brain connectivity being largely non-correlated yet still functional.\n4. Insights Into Biological Systems - By proposing mechanisms akin to those found in living organisms' nervous systems",
        "Topic": "Generative Models"
    },
    {
        "title": "BIGRoC: Boosting Image Generation via a Robust Classifier",
        "abstract": "The interest of the machine learning community in image synthesis has grown significantly in recent years, with the introduction of a wide range of deep generative models and means for training them. In this work, we propose a general model-agnostic technique for improving the image quality and the distribution fidelity of generated images obtained by any generative model. Our method, termed BIGRoC (Boosting Image Generation via a Robust Classifier), is based on a post-processing procedure via the guidance of a given robust classifier and without a need for additional training of the generative model. Given a synthesized image, we propose to update it through projected gradient steps over the robust classifier to refine its recognition. We demonstrate this post-processing algorithm on various image synthesis methods and show a significant quantitative and qualitative improvement on CIFAR-10 and ImageNet. Surprisingly, although BIGRoC is the first model agnostic among refinement approaches and requires much less information, it outperforms competitive methods. Specifically, BIGRoC improves the image synthesis best performing diffusion model on ImageNet $128\\times128$ by 14.81%, attaining an FID score of 2.53 and on $256\\times256$ by 7.87%, achieving an FID of 3.63. Moreover, we conduct an opinion survey, according to which humans significantly prefer our method's outputs.",
        "authors": "R. Ganz, M. Elad",
        "keywords": [
            "BIGRoC",
            "Generative Model Agnostic",
            "Image Quality Improvement"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=y7RGNXhGSR",
        "pdf_src": "https://api2.openreview.net/pdf/d4a1a7ce744153c2a4bc9cefae920600d26983f7.pdf",
        "Code_src": "",
        "Introduction": "Background: The field of image generation using deep generative models such as Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs) has seen substantial growth recently due to advancements in these models' capabilities.\n\nResearch Problem: Despite progress made so far towards generating high-quality images that closely match real-world distributions, existing techniques often require extensive retraining efforts when applied across different generative models leading to inefficiency.\n \nMethod: This paper introduces BIGRoC (Boosting Image Generation via a Robust Classifier), a novel approach designed specifically addressing limitations related to adaptability between generative models. It involves no additional training beyond initial setup; instead, it utilizes a robust classifier during post-processing procedures guided by the classifier itself rather than relying solely on the generative model’s internal representations. \n\nMain Contributions:\n1. Model-Agnostic Refinement Technique - BIGRoC does not rely heavily on specific details about underlying generative architectures but can be used universally after deployment regardless if they are GAN-based or VAE-based systems.\n2. Post-Processing Algorithm - By updating synthetic images iteratively within small increments along gradients computed from a pre-trained robust classifier, BIGRoC refines their visual characteristics effectively while preserving realism close to natural data distributions.\n3. Quantitative & Qualitative Improvements - Demonstrated improvements include reduced Fréchet Inception Distance scores indicating closer similarity metrics against actual datasets like CIFAR-10 and ImageNet compared traditional state-of-the-art methods even though requiring fewer parameters thus reducing computational costs further downscaling potential usage scenarios where resources might be limited.\n4. Human Subjective Preferences - An opinion survey conducted shows human subjects have strong preferences toward outputted images refined by BIGRoC suggesting better perceptual qualities perceived visually despite objective measures showing similar performance levels amongst other top-performing algorithms tested alongside ours.",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Revisiting adversarial training for the worst-performing class",
        "abstract": "Despite progress in adversarial training (AT), there is a substantial gap between the top-performing and worst-performing classes in many datasets. For example, on CIFAR10, the accuracies for the best and worst classes are 74% and 23%, respectively. We argue that this gap can be reduced by explicitly optimizing for the worst-performing class, resulting in a min-max-max optimization formulation.  Our method, called class focused online learning (CFOL), includes high probability convergence guarantees for the worst class loss and can be easily integrated into existing training setups with minimal computational overhead.  We demonstrate an improvement to 32% in the worst class accuracy on CIFAR10, and we observe consistent behavior across CIFAR100 and STL10.  Our study highlights the importance of moving beyond average accuracy,  which is particularly important in safety-critical applications.",
        "authors": "T. Pethick, G. Chrysos, V. Cevher",
        "keywords": [
            "class focused online learning",
            "adversarial training",
            "min-max-max optimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=wkecshlYxI",
        "pdf_src": "https://api2.openreview.net/pdf/978644b647d30d9c1a3abb469794b1734c2291dd.pdf",
        "Code_src": "",
        "Introduction": "Background: Despite advancements in adversarial training techniques such as regularization or robustness training, significant performance discrepancies persist among different classes within various datasets like CIFAR10.\n\nResearch Question: How does one reduce these large gaps observed at the extremes? Specifically, how might focusing more attention towards improving the classification performance specifically for those poorly performing classes impact overall model generalization?\n\nMethodology: The paper introduces Class Focused Online Learning (CFOL). CFOL aims to minimize the maximum error over all classes rather than just minimizing the empirical risk function typically done during standard machine learning training processes - hence it's referred to as \"min-max-max\" optimization. This approach ensures that while other classes maintain their improvements through regular updates using stochastic gradient descent algorithms; they also contribute marginally toward reducing errors made when predicting the most challenging classes accurately first before proceeding forward optimistically about easier ones.\n \nMain Contributions:\n1. A novel optimization framework termed Min-Max-Max which focuses not only on maximizing the accuracy but also ensuring no regression from already well-performed classes along with continuous improvement amongst underperforming classes.\n2. High Probability Convergence Guarantees for Worst-Class Loss where CFOL has been shown theoretically capable enough so far achieving better results compared traditional methods without any additional hyperparameter tuning required post-training setup integration due its simplicity & efficiency.\n3. Experimental validation showing improved worst-class accuracy up to 32% on CIFAR10 dataset alongside observations demonstrating consistency even after applying similar approaches onto CIFAR100 and STL10 datasets further emphasizing effectiveness regardless of dataset size complexity level differences involved here too.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Optimal Convergence Rates of Deep Convolutional Neural Networks: Additive Ridge Functions",
        "abstract": "Convolutional neural networks have shown impressive abilities in many applications, especially those related to the classification tasks. However, for the regression problem, the abilities of convolutional structures have not been fully understood, and further investigation is needed. In this paper, we consider the mean squared error analysis for deep convolutional neural networks. We show that, for additive ridge functions, convolutional neural networks followed by one fully connected layer with ReLU activation functions can reach optimal mini-max rates (up to a log factor). The input dimension only appears  in the constant of convergence rates. This work shows the statistical optimality of convolutional neural networks and may shed light on why convolutional neural networks are able to behave well for high dimensional input.",
        "authors": "Z. Fang, G. Cheng",
        "keywords": [
            "convolutional neural networks",
            "regression problem",
            "mean squared error"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Q6ZXm7VBFY",
        "pdf_src": "https://api2.openreview.net/pdf/71b91bf424c6fa55fa4e597f9061ba1641c9b484.pdf",
        "Code_src": "",
        "Introduction": "Background: Convolutional neural networks (CNNs) have demonstrated remarkable performance across various domains including image recognition due to their ability to efficiently process large volumes of data through feature extraction layers.\n\nResearch Question: Despite CNNs' success within these fields, there has yet to be an extensive understanding regarding how they perform specifically when dealing with regression problems compared to other machine learning models such as linear regression or support vector machines which traditionally excel at regression tasks.\n\nMethodology: Our research focuses on analyzing the mean squared error (MSE) behavior under deep CNN architectures used alongside a single fully connected layer equipped with Rectified Linear Unit (ReLU) activations during training processes involving additive ridge functions.\n\nMain Contributions:\n1. Statistical Optimality - We demonstrate that CNNs combined with a ReLU-activated fully connected layer achieve optimal mini-max rates up to logarithmic factors.\n2. Dimensionality Independence - Unlike traditional regression methods where higher dimensions lead directly into increased complexity leading slower convergence times; our findings indicate that while input dimension does affect convergence constants it doesn't impact rate itself suggesting CNN's robustness against high-dimensional inputs might stem from intrinsic properties beyond simple scaling issues associated with dimensionality increase alone.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "SMILE: Sample-to-feature Mixup for Efficient Transfer Learning",
        "abstract": "To improve the performance of deep learning, mixup has been proposed to force the neural networks favoring simple linear behaviors in-between training samples. Performing mixup for transfer learning with pre-trained models however is not that simple,  a high capacity pre-trained model with a large fully-connected (FC) layer could easily overfit to the target dataset even with samples-to-labels mixed up. In this work, we propose SMILE — Sample-to-feature Mixup for Efficient Transfer Learning. With mixed images as inputs, SMILE regularizes the outputs of CNN feature extractors to learn from the mixed feature vectors of inputs, in addition to the mixed labels. SMILE incorporates a mean teacher to provide the surrogate \"ground truth\" for mixed feature vectors. The sample-to-feature mixup regularizer is imposed both on deep features for the target domain and classifier outputs for the source domain, bounding the linearity in-between samples for target tasks. Extensive experiments have been done to verify the performance improvement made by SMILE, in comparisons with a wide spectrum of transfer learning algorithms, including fine-tuning, L$^2$-SP, DELTA, BSS, RIFLE, Co-Tuning and RegSL, even with mixup strategies combined. Ablation studies show that the vanilla sample-to-label mixup strategies could marginally increase the linearity in-between training samples but lack of generalizability, while SMILE significantly improves the mixup effects in both label and feature spaces with both training and testing datasets. The empirical observations backup our design intuition and purposes.\n",
        "authors": "X. Li, H. Xiong, C. Xu, et.al",
        "keywords": [
            "Sample-to-feature Mixup",
            "Mean Teacher",
            "Transfer Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=czgMCpvrDM",
        "pdf_src": "https://api2.openreview.net/pdf/082a861901e71615d6ee0fdf30f4487312f667ef.pdf",
        "Code_src": "",
        "Introduction": "Background: To enhance the effectiveness of deep learning, researchers introduced mixup—a technique designed to encourage neural networks away from relying solely on simple linear relationships between input data points during training.\n\nResearch Problem: However, applying mixup within the context of transfer learning using pretrained models poses challenges due to potential overfitting when dealing with high-capacity pretrained models equipped with substantial fully connected layers—especially if these are applied directly without modifications after mixing the samples with their corresponding labels.\n\nMethod: This paper introduces SMILE—an acronym standing for Sample-to-feature Mixup for Efficient Transfer Learning. It proposes a novel regularization strategy where instead of mixing only the labels across different training examples or batches like traditional mixup does, it also mixes the feature representations extracted before passing them through any classifiers. Additionally, SMILE employs what's known as Mean Teacher—a method used here to generate surrogate ground truths based on the mixed feature vectors which helps stabilize the learning process throughout the training regime.\n\nMain Contributions:\n1. **Sample-to-feature Mixup Regularization**: Unlike conventional methods focusing exclusively on label mixing, SMILE introduces an innovative approach involving the mixing at the feature extraction stage itself; thus, enriching the network’s ability to generalize beyond just the raw labeled data.\n2. **Mean Teacher Integration**: By incorporating Mean Teacher into its framework, SMILE ensures more robustness against variance present especially early-on in the training phase leading towards better final performance metrics such as accuracy rates etc.\n3. **Performance Validation Across Various Algorithms**: The authors conducted extensive comparative tests comparing SMILE alongside other well-established transfer learning techniques – Fine-tuning, L$^2$-SP, DELTA, BSS, RIFLE, Co-Tuning & RegSL among others - showing significant improvements achieved via SMILE under various conditions including those employing additional mixup strategies themselves.\n4. **Ablation Studies**: These were performed systematically demonstrating how each component of SMILE contributes positively toward improved results compared to baseline approaches alone indicating that all parts synergistically contribute rather than being redundant or substitutable components individually.",
        "Topic": "Sample Efficiency in Reinforcement Learning"
    },
    {
        "title": "Dropped Scheduled Task: Mitigating Negative Transfer in Multi-task Learning using Dynamic Task Dropping",
        "abstract": "In Multi-Task Learning (MTL), K distinct tasks are jointly optimized. With the varying nature and complexities of tasks, few tasks might dominate learning. For other tasks, their respective performances may get compromised due to a negative transfer from dominant tasks. We propose a Dropped-Scheduled Task (DST) algorithm, which probabilistically “drops” specific tasks during joint optimization while scheduling others to reduce negative transfer. For each task, a scheduling probability is decided based on four different metrics: (i) task depth, (ii) number of ground-truth samples per task, (iii) amount of training completed, and (iv) task stagnancy. Based on the scheduling probability, specific tasks get joint computation cycles while others are “dropped”. To demonstrate the effectiveness of the proposed DST algorithm, we perform multi-task learning on three applications and two architectures. Across unilateral (single input) and bilateral (multiple input) multi-task net- works, the chosen applications are (a) face (AFLW), (b) fingerprint (IIITD MOLF, MUST, and NIST SD27), and (c) character recognition (Omniglot) applications. Experimental results show that the proposed DST algorithm has the minimum negative transfer and overall least errors across different state-of-the-art algorithms and tasks.",
        "authors": "A. Malhotra, M. Vatsa, R. Singh",
        "keywords": [
            "Multi-Task Learning",
            "Scheduled Task Dropping",
            "Negative Transfer Reduction"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=myjAVQrRxS",
        "pdf_src": "https://api2.openreview.net/pdf/3ba6f25308311face3c06642a7e34f6ae5b86e7b.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses an issue in Multi-Task Learning where some tasks can become too dominant leading to underperformance or \"negative transfer\" for less prominent tasks.\n\nResearch Problem: How do you balance performance between multiple tasks when optimizing them together?\n\nMethodology: The authors introduce a novel approach called Dropped Scheduled Task (DST). This method involves probabilistic dropping certain tasks within the joint optimization process but reserving computational resources for those deemed more important by considering several factors:\n\n1. Task Depth - Tasks with deeper networks require greater computational effort.\n2. Number of Ground-Truth Samples Per Task - More data implies higher importance as it allows better generalization capabilities.\n3. Amount of Training Completed - Tasks closer to completion should be given priority since they have already undergone significant learning.\n4. Task Stagnancy - If there's no improvement over time then this could indicate redundancy; hence these tasks would not need much attention.\n\nMain Contributions:\nThe DST algorithm effectively mitigates against negative transfer effects among various tasks through its adaptive scheduling mechanism using these criteria ensuring all tasks receive fair consideration without compromising any one significantly beyond what’s necessary at every stage throughout training iterations. \n\nTo validate their findings empirically, experiments were conducted employing both unilateral (one input) and bilateral (two inputs) multi-task network configurations involving real-world datasets such as facial landmark detection (AFLW dataset), fingerprint verification (IIITD MOLF, MUST, and NIST SD27 datasets), and handwritten digit recognition (Omniglot dataset).\n\nExperimental Results: Compared extensively w.r.t. existing top-performing methods & approaches across diverse domains/tasks, DST achieved lowest error rates indicating superior adaptability towards balancing task complexity heterogeneity whilst minimizing detrimental impacts typically associated with traditional multi-task setups resulting from dominance bias amongst individual components involved therein.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Calibrate and Debias Layer-wise Sampling for Graph Convolutional Networks",
        "abstract": "Multiple sampling-based methods have been developed for approximating and accelerating node embedding aggregation in graph convolutional networks (GCNs) training. Among them, a layer-wise approach recursively performs importance sampling to select neighbors jointly for existing nodes in each layer. This paper revisits the approach from a matrix approximation perspective, and identifies two issues in the existing layer-wise sampling methods: suboptimal sampling probabilities and estimation biases induced by sampling without replacement. To address these issues, we accordingly propose two remedies: a new principle for constructing sampling probabilities and an efficient debiasing algorithm. The improvements are demonstrated by extensive analyses of estimation variance and experiments on common benchmarks. Code and algorithm implementations are publicly available at \\url{https://github.com/ychen-stat-ml/GCN-layer-wise-sampling}.",
        "authors": "Y. Chen, T. Xu, D. H. Tur, et.al",
        "keywords": [
            "graph convolutional networks",
            "node embedding aggregation",
            "importance sampling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=JyKNuoZGux",
        "pdf_src": "https://api2.openreview.net/pdf/8b2bd83fb801b23573a1f5737b06b2f8bd8911f0.pdf",
        "Code_src": "Code and algorithm implementations are publicly available at https://github.com/ychen-stat-ml/GCN-layer-wise-sampling",
        "Introduction": "Background:\nGraph Convolutional Networks (GCNs) require aggregating embeddings across neighboring nodes during their training process which can be computationally expensive due to its combinatorial nature.\n\nResearch Problem:\nExisting multiple sampling-based methods such as layer-wise approaches that perform importance sampling may not always provide optimal results because they suffer from suboptimal sampling probabilities leading to biased estimations when performing sampling without replacement.\n\nMethods:\nThis study re-examines this problem through a matrix approximation lens where it is identified there exist two main problems with current sampling strategies - suboptimal sampling probabilities resulting in biased estimates; and bias introduced into the estimation procedure caused by sampling elements one-by-one rather than simultaneously or in batches (\"sampling without replacement\"). \n\nContributions:\nThe authors introduce novel solutions addressing both aforementioned challenges:\n\n1. A principled framework guiding how to construct more accurate sampling probabilities.\n2. An efficient debiasing algorithm designed specifically tailored towards GCN's unique characteristics ensuring unbiasedness while maintaining computational efficiency even under high-dimensional settings typical within GCNs.\n\nMain findings include empirical evidence demonstrating significant performance gains over previous state-of-the-art algorithms based on comprehensive analysis including estimation variance studies along with experimental validation using widely recognized datasets commonly used benchmarking tools like Node2Vec, DeepWalk etc., confirming improved accuracy rates after applying proposed techniques alongside code and algorithmic implementations being made openly accessible via GitHub repository mentioned in the abstract.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Online Learning for Prediction via Covariance Fitting: Computation, Performance and Robustness",
        "abstract": "We consider the problem of online prediction using linear smoothers that are functions of a nominal covariance model with unknown parameters. The model parameters are often learned using cross-validation or maximum-likelihood techniques. But when training data arrives in a streaming fashion, the implementation of such techniques can only be done in an approximate manner. Even if this limitation could be overcome, there appears to be no clear-cut results on the statistical properties of the resulting predictor. \n\nHere we consider a covariance-fitting method to learn the model parameters, which was initially developed for spectral estimation. We first show that the use of this approach results in a computationally efficient online learning method in which the resulting predictor can be updated sequentially. We then  prove that, with high probability, its out-of-sample error approaches the optimal level at a root-$n$ rate, where $n$ is the number of data samples. This is so even if the nominal covariance model is misspecified. Moreover, we show that the resulting predictor enjoys two robustness properties. First, it corresponds to a predictor that minimizes the out-of-sample error with respect to the least favourable distribution within a given Wasserstein distance from the empirical distribution. Second, it is robust against errors in the covariate training data. We illustrate the performance of the proposed method in a numerical experiment.",
        "authors": "M. Osama, D. Zachariah, P. Stoica, et.al",
        "keywords": [
            "covariance fitting",
            "online learning",
            "out-of-sample error"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=nAr9PhyEbQ",
        "pdf_src": "https://api2.openreview.net/pdf/a77999980ae61e5667a9294576193cad6efd6a83.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the issue of online prediction involving linear smoothers whose parameters depend on an unknown nominal covariance model.\n\nResearch Problem: How do you efficiently estimate and update these parameters as new data streams in? Furthermore, what guarantees regarding the predictive accuracy exist?\n\nMethods: The authors propose adapting a covariance fitting technique originally designed for spectral estimation tasks into an online learning framework suitable for streaming data scenarios.\n \nMain Contributions:\n1. They demonstrate how their adapted covariance fitting strategy leads to a highly computational-efficient sequential updating process without compromising the quality of predictions over time.\n2. Prove theoretically under certain conditions – including potential misspecification issues - that the estimator's out-of-sample error converges towards optimality according to a root-$n$ rate; here $n$ denotes the total amount of observed data points across all updates.\n3. Show additional robustness features by establishing that the estimator produces predictors minimizing expected loss relative to worst-case distributions while remaining resilient toward inaccuracies present during initial covariate data collection phases through both theoretical arguments and illustrative simulations.",
        "Topic": "Sample Efficiency in Reinforcement Learning"
    },
    {
        "title": "ViViT: Curvature Access Through The Generalized Gauss-Newton’s Low-Rank Structure",
        "abstract": "Curvature in form of the Hessian or its generalized Gauss-Newton (GGN) approximation is valuable for algorithms that rely on a local model for the loss to train, compress, or explain deep networks. Existing methods based on implicit multiplication via automatic differentiation or Kronecker-factored block diagonal approximations do not consider noise in the mini-batch. We present ViViT, a curvature model that leverages the GGN’s low-rank structure without further approximations. It allows for efficient computation of eigenvalues, eigenvectors, as well as per-sample first- and second-order directional derivatives. The representation is computed in parallel with gradients in one backward pass and offers a fine-grained cost-accuracy trade-off, which allows it to scale. We demonstrate this by conducting performance benchmarks and substantiate ViViT’s usefulness by studying the impact of noise on the GGN’s structural properties during neural network training.",
        "authors": "F. Dangel, L. Tatzel, P. Hennig",
        "keywords": [
            "curvature",
            "Hessian",
            "ViViT"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=DzJ7JfPXkE",
        "pdf_src": "https://api2.openreview.net/pdf/79b33359dc6b4316ad1bbf5abfa73d12b910c7e4.pdf",
        "Code_src": "",
        "Introduction": "Background: Curvature information from the Hessian matrix has been shown beneficial when using local models like the Gauss-Newton method within machine learning tasks such as training, compression, and explaining deep neural networks.\n\nResearch Problem: However, existing approaches either use implicit multiplication through automatic differentiation techniques leading to computational inefficiencies due to noise in mini-batches; or they employ Kronecker-factored block diagonal approximations but these are also prone to ignoring noise effects.\n \nMethod: This paper introduces ViViT - a novel curvature model inspired by the Generalized Gauss-Newton (GGN) approach yet avoids additional approximations while leveraging its inherent low-rank property. ViViT efficiently computes eigenvalues and eigenvectors along with sample-wise first and second-order directional derivatives all at once – done concurrently alongside gradient computations after just one backpropagation pass over data. \n\nMain Contributions:\n1. ViViT provides an accurate and computationally efficient way to compute curvature measures directly related to the GGN's behavior under noisy conditions common in mini-batched stochastic gradient descent used widely today across many applications including those involving neural networks.\n2. Demonstrates scalability compared to other state-of-the-art methods because there isn't any need for iterative refinement steps nor does it require complex preconditioning strategies before solving linear systems involved here unlike some previous works might have required.\n3. Shows empirical evidence demonstrating how noise affects the structural properties typically associated with Gaussian processes utilized frequently throughout various fields where optimization plays significant roles",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Robustness through Data Augmentation Loss Consistency",
        "abstract": "While deep learning through empirical risk minimization (ERM) has succeeded at achieving human-level performance at a variety of complex tasks, ERM is not robust to distribution shifts or adversarial attacks. Synthetic data augmentation followed by empirical risk minimization (DA-ERM) is a simple and widely used solution to improve robustness in ERM. In addition, consistency regularization can be applied to further improve the robustness of the model by forcing the representation of the original sample and the augmented one to be similar. However, existing consistency regularization methods are not applicable to covariant data augmentation, where the label in the augmented sample is dependent on the augmentation function. For example, dialog state covaries with named entity when we augment data with a new named entity. In this paper, we propose data augmented loss invariant regularization (DAIR), a simple form of consistency regularization that is applied directly at the loss level rather than intermediate features, making it widely applicable to both invariant and covariant data augmentation regardless of network architecture, problem setup, and task. We apply DAIR to real-world learning problems involving covariant data augmentation: robust neural task-oriented dialog state tracking and robust visual question answering. We also apply DAIR to tasks involving invariant data augmentation: robust regression, robust classification against adversarial attacks, and robust ImageNet classification under distribution shift. Our experiments show that DAIR consistently outperforms ERM and DA-ERM with little marginal computational cost and sets new state-of-the-art results in several benchmarks involving covariant data augmentation. Our code of all experiments are available at: https://github.com/optimization-for-data-driven-science/DAIR.",
        "authors": "T. Huang, S. Halbe, C. Sankar, et.al",
        "keywords": [
            "data augmentation",
            "consistency regularization",
            "robustness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=a1meaRy1bN",
        "pdf_src": "https://api2.openreview.net/pdf/5a618bccccafa90202d27922221994e592cc2e28.pdf",
        "Code_src": "https://github.com/optimization-for-data-driven-science/DAIR",
        "Introduction": "Background:\nThe success of deep learning models trained using empirical risk minimization (ERM) for various complex tasks comes along with its vulnerability towards distribution shifts and adversarial attacks.\n\nResearch Problem:\nThis research aims to address the issue of improving the robustness of ERM-based models especially focusing on those dealing with covariant data augmentation which involves changes made based on specific functions like adding entities during dialog state tracking.\n\nMethodology:\nTo tackle these challenges, they introduce Data Augmented Loss Invariant Regularization (DAIR). This method applies consistency regularization without relying on intermediate feature representations but instead operates right after the loss computation stage allowing it to work across different architectures, setups as well as types of data augmentation including invariant and covariant ones.\n\nMain Contributions:\nTheir contributions include proposing an effective way called DAIR - a consistent regularization technique suitable even if labels depend on how samples were augmented; demonstrating improvements over standard ERM and DA-ERM approaches while requiring minimal additional computational effort; setting novel benchmark records particularly notable within datasets employing covariant forms of data augmentation such as robust neural task-oriented dialog state tracking & visual question answering among others.\n \nCode Availability:\nAll experimental codes related to their findings have been shared publicly via GitHub repository accessible here: [GitHub Link]",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Towards Large Scale Transfer Learning for Differentially Private Image Classification",
        "abstract": "Differentially Private Stochastic Gradient Descent (DP-SGD) has emerged as a popular private training algorithm. Unfortunately, the computational cost of training large-scale models with DP-SGD is substantially higher than non-private training. This is further exacerbated by the fact that increasing the number of parameters leads to larger degradation in utility with DP.\nIn this work, we zoom in on the ImageNet dataset and demonstrate that, similar to the non-private case, pre-training over-parameterized models on a large public dataset can lead to substantial gains when the models are finetuned privately. Moreover, by systematically comparing private and non-private models across a range of large batch sizes, we find that similar to the non-private setting, the choice of optimizer can further improve performance substantially with DP. By using the LAMB optimizer, we saw improvement of up to 20$\\%$ points (absolute). We also show that finetuning just the last layer for a \\emph{single step} in the full batch setting, combined with extremely small-scale (near-zero) initialization leads to both SOTA results of 81.7 $\\%$ under a wide privacy budget range of $\\epsilon \\in [4, 10]$ and $\\delta$ = $10^{-6}$  while minimizing the computational overhead substantially. Finally, we present additional results on CIFAR-10 and CIFAR-100, surpassing previous state of the art by leveraging transfer learning with our recommendations.",
        "authors": "H. Mehta, A. G. Thakurta, A. Kurakin, et.al",
        "keywords": [
            "DP-SGD",
            "ImageNet",
            "Privacy Budget"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Uu8WwCFpQv",
        "pdf_src": "https://api2.openreview.net/pdf/1ecad9b7ba8d950e308a684530ca42908f5ea849.pdf",
        "Code_src": "",
        "Introduction": "Background: Differentially Private Stochastic Gradient Descent (DP-SGD) is an effective method for training machine learning models securely but it incurs significantly more computation costs compared to non-private training due to its privacy-preserving constraints.\n\nResearch Problem: The issue addressed here revolves around optimizing the trade-off between model accuracy and computational efficiency during differential privacy training specifically focusing on how pre-trained large-scale models perform after fine-tuning within a private framework given their increased parameter size which generally degrades utility.\n\nMethods: The study concentrates on the ImageNet dataset where they investigate whether pre-training overly complex models followed by private fine-tuning yields improved outcomes relative to standard practice without privacy considerations. They conduct comparative experiments varying batch sizes from very small to quite large settings examining the impact of different optimizers including LAMB, known for improving convergence rates especially at scale; and explore novel techniques like single-step finetuning along with near-zero initialization strategy aiming towards reducing computational burden drastically yet maintaining or exceeding current benchmarks even against wider privacy budgets.\n\nMain Contributions:\n1. Demonstrated through empirical evidence based on ImageNet data set analysis - pre-training highly parametrized networks before private fine-tuning does indeed enhance final model performance upon private adjustments despite the expected decrease in utility associated with adding extra parameters via differential privacy mechanisms.\n2. Identified that choosing specific optimizers such as LAMB could notably boost performance improvements beyond what would be achieved otherwise – achieving increases reaching approximately 20$\\%$ absolute point gain depending on certain conditions.\n3. Introduced innovative methods combining one-layer finetuning per iteration together with minimalistic initialization strategies resulting in top-of-the-line accuracies ranging anywhere from 81.7$\\%$ down to $\\epsilon=4$, $\\delta=10^{-6}$ privacy regime whilst dramatically cutting back on computational resources required throughout the process.\n4. Extended these findings into other datasets like CIFAR-10 & CIFAR-100 showcasing significant advancements utilizing transferred knowledge learned initially elsewhere thus validating broader applicability outside solely ImageNet domain contextually relevantly applicable today's ML research landscape.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Gradient-adjusted Incremental Target Propagation Provides Effective Credit Assignment in Deep Neural Networks",
        "abstract": "Many of the recent advances in the field of artificial intelligence have been fueled by the\nhighly successful backpropagation of error (BP) algorithm, which efficiently solves the\ncredit assignment problem in artificial neural networks. However, it is unlikely that BP is\nimplemented in its usual form within biological neural networks, because of its reliance on\nnon-local information in propagating error gradients. Since biological neural networks are\ncapable of highly efficient learning and responses from BP trained models can be related\nto neural responses, it seems reasonable that a biologically viable approximation of BP\nunderlies synaptic plasticity in the brain. Gradient-adjusted incremental target propagation\n(GAIT-prop or GP for short) has recently been derived directly from BP and has been\nshown to successfully train networks in a more biologically plausible manner. However,\nso far, GP has only been shown to work on relatively low-dimensional problems, such as\nhandwritten-digit recognition. This work addresses some of the scaling issues in GP and\nshows it to perform effective multi-layer credit assignment in deeper networks and on the\nmuch more challenging ImageNet dataset.",
        "authors": "S. Dalm, N. Ahmad, L. Ambrogioni, et.al",
        "keywords": [
            "biological plausibility",
            "gradient-based learning",
            "multi-layer credit assignment"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Lx19EyKX77",
        "pdf_src": "https://api2.openreview.net/pdf/0f7fd3fea3422258b4f78012022a4e28b90758ce.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses how many advancements in artificial intelligence fields rely heavily on the Backpropagation of Error (BP) algorithm due to its effectiveness at solving the credit assignment problem in artificial neural networks.\n\nResearch Problem: Despite this success with BP algorithms being used outside biology, there's uncertainty about whether they're implemented similarly in biological neural networks since BP relies significantly on non-local information during gradient propagation processes.\n \nMethod: To address these concerns regarding BP implementation in biological systems while still benefiting from its advantages like high efficiency training capabilities observed when applied to neural networks, researchers developed an alternative method called Gradient-Adjusted Incremental Target Propagation (GAIT-prop or GP). They aimed to derive GATI-prop directly from BP but make adjustments ensuring better alignment with biological principles.\n \nMain Contributions: In their research findings presented hereafter, authors demonstrate several key contributions:\n1. Scaling Issues Resolved - By addressing certain scaling challenges associated with applying GP methods previously reported limitations mainly seen working well enough just limited dimensions tasks like handwritten digit recognition), now extended beyond those boundaries into higher dimensional spaces including complex datasets like ImageNet where much greater complexity exists compared before;\n2. Multi-Layer Credit Assignment Capabilities - GP was found capable performing effectively across multiple layers within deep architectures allowing them learn representations hierarchically similar ways humans do naturally; \n3. Validation Through Comparison Against Other Approaches - Comparing performance against other existing techniques showed superiority over alternatives demonstrating efficacy validity proposed approach achieving desired outcomes reliably under various experimental conditions tested throughout study period conducted",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "On a continuous time model of gradient descent dynamics and instability in deep learning",
        "abstract": "The recipe behind the success of deep learning has been the combination of neural networks and gradient-based optimization. Understanding the behavior of gradient descent however, and particularly its instability, has lagged behind its empirical success. To add to the theoretical tools available to study gradient descent we propose the principal flow (PF), a continuous time flow that approximates gradient descent dynamics. To our knowledge, the PF is the only continuous flow that captures the divergent and oscillatory behaviors of gradient descent, including escaping local minima and saddle points. Through its dependence on the eigendecomposition of the Hessian the PF sheds light on the recently observed edge of stability phenomena in deep learning. Using our new understanding of instability we propose a learning rate adaptation method which enables us to control the trade-off between training stability and test set evaluation performance.",
        "authors": "M. Rosca, Y. Wu, C. Qin, et.al",
        "keywords": [
            "gradient descent",
            "principal flow",
            "learning rate adaptation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=EYrRzKPinA",
        "pdf_src": "https://api2.openreview.net/pdf/7b9a2dd50160166f8f5debc1da86d8ded54ced38.pdf",
        "Code_src": "",
        "Introduction": "Background: The success of deep learning can be attributed largely to the use of neural networks combined with gradient-based optimization techniques.\n\nResearch Question: Despite this empirical success, there remains limited understanding about how gradient descent behaves during optimization processes - especially regarding issues such as instability.\n \nMethod: In order to address these gaps within theory around gradient descent's behavior patterns more effectively than current methods allow for; researchers have proposed an alternative approach called \"principal flow\" (PF). This novel concept involves creating a continuous-time dynamical system based upon approximating gradients' evolution over iterations rather than discrete steps like traditional backpropagation algorithms do when calculating derivatives at each layer individually throughout epochs during model training sessions respectively.. \n\nMain Contributions: Principal Flow provides insights into why certain models may fail due to instabilities encountered along their respective paths towards convergence whereas previous works focused solely on convergence guarantees without considering potential pitfalls related specifically toward stability concerns instead.. Furthermore , by leveraging properties from eigenvalue decomposition associated w ith Hessian matrices ; Pf allows practitioners fine tune parameters governing learning rates adaptively according optimize desired balance between achieving stable training conditions whilst still yielding high accuracy predictions evaluated against unseen datasets post-training completion phase accordingly",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Proportional Fairness in Federated Learning",
        "abstract": "With the increasingly broad deployment of federated learning (FL) systems in the real world, it is critical but challenging to ensure fairness in FL, i.e. reasonably satisfactory performances for each of the numerous diverse clients. In this work, we introduce and study a new fairness notion in FL, called proportional fairness (PF), which is based on the relative change of each client's performance. From its connection with the bargaining games, we propose PropFair, a novel and easy-to-implement algorithm for finding proportionally fair solutions in FL, and study its convergence properties. Through extensive experiments on vision and language datasets, we demonstrate that PropFair can approximately find PF solutions, and it achieves a good balance between the average performances of all clients and of the worst 10% clients.",
        "authors": "G. Zhang, S. Malekmohammadi, X. Chen, et.al",
        "keywords": [
            "federated learning",
            "proportional fairness",
            "PropFair"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ryUHgEdWCQ",
        "pdf_src": "https://api2.openreview.net/pdf/ded3e9f3fabf016a0e2a4d8d5a0cc84697dbcd50.pdf",
        "Code_src": "",
        "Introduction": "Background: As federated learning (FL) systems are widely deployed across various domains due to their privacy-preserving nature, ensuring fairness among different clients has become an essential yet complex task.\n\nResearch Problem: The primary challenge addressed by the paper revolves around introducing and studying a novel fairness concept known as proportional fairness within the context of FL - aiming at achieving reasonable satisfaction levels from individual clients despite variations amongst them.\n\nMethodology: To tackle this problem effectively without resorting to computationally intensive methods or requiring prior knowledge about data distributions like other existing approaches do not require such assumptions, authors develop PropFair – a simple algorithm designed specifically tailored towards solving proportionate fairness problems encountered during federated training processes involving multiple heterogeneous participants.\n\nMain Contributions:\n1. Introduce Proportional Fairness (PF) into Federated Learning framework.\n2. Develop PropFair Algorithm aimed at approximating PF solutions efficiently while being lightweight enough suitable even resource-constrained environments where computation power might be limited \n3. Conduct comprehensive experimental validation using Vision-Language datasets demonstrating efficacy superiority over baseline algorithms under consideration",
        "Topic": "Federated Learning"
    },
    {
        "title": "On the Role of Fixed Points of Dynamical Systems in Training Physics-Informed Neural Networks",
        "abstract": "This paper empirically studies commonly observed training difficulties of Physics-Informed Neural Networks (PINNs) on dynamical systems.\nOur results indicate that fixed points which are inherent to these systems play a key role in the optimization of the in PINNs embedded physics loss function.\nWe observe that the loss landscape exhibits local optima that are shaped by the presence of fixed points.\nWe find that these local optima contribute to the complexity of the physics loss optimization which can explain common training difficulties and resulting nonphysical predictions.\nUnder certain settings, e.g., initial conditions close to fixed points or long simulations times, we show that those optima can even become better than that of the desired solution.",
        "authors": "F. M. Rohrhofer, S. Posch, C. Gößnitzer, et.al",
        "keywords": [
            "Physics-Informed Neural Networks",
            "Dynamical Systems",
            "Fixed Points"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=56cTmVrg5w",
        "pdf_src": "https://api2.openreview.net/pdf/e86f9c75220ef2b60ccfc03a9cc95ab59aa98e55.pdf",
        "Code_src": "",
        "Introduction": "Background: This study focuses on the empirical investigation into typical challenges encountered during the training process of Physics-Informed Neural Networks (PINNs), particularly when applied to dynamic systems.\n\nResearch Question: The central question addressed is how intrinsic properties such as fixed points within dynamical systems influence the optimization procedure for the physics loss function incorporated into PINNs.\n\nMethodology: To explore this issue, researchers conducted an analysis focusing on the characteristics of the loss landscape surrounding fixed points throughout the optimization phase using PINNs with embedded physical constraints.\n\nMain Contributions:\n1. Identification of Fixed Points' Role: The findings reveal that fixed points present in the underlying dynamics significantly impact the optimization path taken through the physics loss function space inside PINNs.\n2. Loss Landscape Analysis: The research uncovers localized optimal regions influenced by fixed points amidst the broader loss landscape; they shape it distinctly from other areas where no fixed points exist contributing to the overall complexity experienced while optimizing the physics loss term.\n3. Explanation of Training Difficulties & Non-Physical Predictions: By understanding why some local optima may be superior under specific circumstances like proximity to fixed points at initialization stages leading up to longer simulation durations compared against actual solutions helps shed light onto previous perplexing phenomena related to challenging trainings scenarios yielding less-than-desirable outcomes including incorrect predictions violating fundamental laws governing reality modeled via these networks.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Attention Beats Concatenation for Conditioning Neural Fields",
        "abstract": "Neural fields model signals by mapping coordinate inputs to sampled values. They are becoming an increasingly important backbone architecture across many fields from vision and graphics to biology and astronomy. In this paper, we explore the differences between common conditioning mechanisms within these networks, an essential ingredient in shifting neural fields from memorization of signals to generalization, where the set of signals lying on a manifold is modelled jointly. In particular, we are interested in the scaling behaviour of these mechanisms to increasingly high-dimensional conditioning variables. As we show in our experiments, high-dimensional conditioning is key to modelling complex data distributions, thus it is important to determine what architecture choices best enable this when working on such problems. To this end, we run experiments modelling 2D, 3D, and 4D signals with neural fields, employing concatenation, hyper-network, and attention-based conditioning strategies -- a necessary but laborious effort that has not been performed in the literature.\nWe find that attention-based conditioning outperforms other approaches in a variety of settings.",
        "authors": "D. Rebain, M. J. Matthews, K. M. Yi, et.al",
        "keywords": [
            "neural fields",
            "conditioning mechanisms",
            "attention-based strategies"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=GzqdMrFQsE",
        "pdf_src": "https://api2.openreview.net/pdf/550d589b78472491f635e52d372f9a8d8522fe40.pdf",
        "Code_src": "",
        "Introduction": "Background: Neural field models represent signals as functions over continuous domains using coordinate inputs mapped to sampled values.\n\nResearch Problem: The effectiveness of neural field models largely depends on their ability to generalize beyond specific training examples rather than merely memorize them; however, there's limited understanding about how different conditioning mechanisms affect this generalization capability for higher dimensional input spaces.\n\nMethodology: We investigate three types of conditioning methods - concatenation, hyper-networks, and attention-based mechanisms – applied at various dimensions ranging up to four dimensions while modeling two-, three-, and four-dimensional signals through neural fields.\n\nMain Contributions:\n1. We empirically demonstrate significant performance improvements under attention-based conditioning compared to concatenation or hyper-networking techniques especially crucial considering the increasing complexity associated with handling more extensive datasets characterized by intricate distributions which necessitate high-dimensional conditioning variables;\n2. Our study highlights architectural decisions critical during implementation ensuring effective utilization of attentional mechanisms leading towards better generalization properties particularly relevant given current advancements demanding broader applicability",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Implicit Ensemble Training for Efficient and Robust Multiagent Reinforcement Learning",
        "abstract": "An important issue in competitive multiagent scenarios is the distribution mismatch between training and testing caused by variations in other agents' policies. As a result, policies optimized during training are typically sub-optimal (possibly very poor) in testing. Ensemble training is an effective approach for learning robust policies that avoid significant performance degradation when competing against previously unseen opponents. A large ensemble can improve diversity during the training, which leads to more robust learning. However, the computation and memory requirements increase linearly with respect to the ensemble size, which is not scalable as the ensemble size required for learning robust policy can be quite large. This paper proposes a novel parameterization of a policy ensemble based on a deep latent variable model with a multi-task network architecture, which represents an ensemble of policies implicitly within a single network. Our implicit ensemble training (IET) approach strikes a better trade-off between ensemble diversity and scalability compared to standard ensemble training. We demonstrate in several competitive multiagent scenarios in the board game and robotic domains that our new approach improves robustness against unseen adversarial opponents while achieving higher sample-efficiency and less computation.",
        "authors": "M. Shen, J. P. How",
        "keywords": [
            "multi-agent reinforcement learning",
            "ensemble methods",
            "implicit feature representation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LfTukxzxTj",
        "pdf_src": "https://api2.openreview.net/pdf/dab1b7a553b9827b5a4dfef2f3c3410732fc9866.pdf",
        "Code_src": "",
        "Introduction": "Background: In competitive multi-agent environments, there often exists a distribution mismatch between training and test data due to changes in opponent strategies or behaviors outside one's control.\n\nResearch Question: How do we train agent policies effectively so they remain stable across different opponents?\n\nMethod: The authors propose Implicit Ensemble Training (IET), leveraging a deep latent variable model combined with a multi-task neural network structure where multiple policies coexist implicitly without requiring explicit representation through separate networks like traditional ensembles would necessitate; this significantly reduces computational costs associated with growing ensemble sizes beyond practical limits.\n \nMain Contributions:\n1. Introduced IET—a method that allows for diverse yet computationally efficient policy learning using a shared neural network infrastructure instead of explicitly maintaining many parallel networks;\n2. Demonstrated improved robustness towards unobserved adversaries along with enhanced sample efficiency over conventional approaches such as vanilla ensemble methods under various conditions from both board games & robotics contexts",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Recognition Models to Learn Dynamics from Partial Observations with Neural ODEs",
        "abstract": "Identifying dynamical systems from experimental data is a notably difficult task. Prior knowledge generally helps, but the extent of this knowledge varies with the application, and customized models are often needed. Neural ordinary differential equations can be written as a flexible framework for system identification and can incorporate a broad spectrum of physical insight, giving physical interpretability to the resulting latent space. In the case of partial observations, however, the data points cannot directly be mapped to the latent state of the ODE. Hence, we propose to design recognition models, in particular inspired by nonlinear observer theory, to link the partial observations to the latent state. We demonstrate the performance of the proposed approach on numerical simulations and on an experimental dataset from a robotic exoskeleton.",
        "authors": "M. Buisson-fenet, V. Morgenthaler, S. Trimpe, et.al",
        "keywords": [
            "data-driven modeling",
            "neural ordinary differential equations",
            "observer-based methods"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LTAdaRM29K",
        "pdf_src": "https://api2.openreview.net/pdf/68100c7310f96071e0a5ea8a20ea93243057a1b2.pdf",
        "Code_src": "",
        "Introduction": "Background: Identifying dynamical systems from experimental data involves inferring their underlying mathematical representations based on observed outputs over time.\nResearch Problem: Traditional methods struggle when dealing with incomplete or noisy observational data due to the non-linear nature of such systems.\n\nMethodology: The paper introduces neural ordinary differential equations (Neural ODEs), which serve as a versatile tool that not only captures complex dynamics through learning but also allows for physical interpretation via its latent spaces - providing insights into how different variables interact within the system.\n\nMain Contributions:\n1. Propose novel recognition models informed by nonlinear observer theory designed specifically for handling cases where some aspects of the dynamic process may remain unobserved (\"partial observations\").\n2. Demonstrate these models' efficacy both numerically using simulated datasets representing various dynamical processes without complete information about them; and experimentally applying it successfully against real-world data collected during tests performed upon a robotic exoskeleton's movement patterns under controlled conditions indicating successful inference even amidst missing data scenarios typical in practical applications involving sensor limitations etc.",
        "Topic": "Generative Models"
    },
    {
        "title": "Optimal Threshold Labeling for Ordinal Regression Methods",
        "abstract": "For an ordinal regression task, a classification task for ordinal data, one-dimensional transformation (1DT)-based methods are often employed since they are considered to capture the ordinal relation of ordinal data well. They learn a 1DT of the observation of the explanatory variables so that an observation with a larger class label tends to have a larger value of the 1DT, and classify the observation by labeling that learned 1DT. In this paper, we study the labeling procedure for 1DT-based methods, which have not been sufficiently discussed in existing studies. While regression-based methods and classical threshold methods conventionally use threshold labelings, which label a learned 1DT according to the rank of the interval to which the 1DT belongs among intervals on the real line separated by threshold parameters, we prove that likelihood-based labeling used in popular statistical 1DT-based methods is also a threshold labeling in typical usages. Moreover, we show that these threshold labelings can be sub-optimal ones depending on the learning result of the 1DT and the task under consideration. On the basis of these findings, we propose to apply empirical optimal threshold labeling, which is a threshold labeling that uses threshold parameters minimizing the empirical task risk for a learned 1DT, to those methods. In experiments with real-world datasets, changing the labeling procedure of existing 1DT-based methods to the proposed one improved the classification performance in many tried cases.",
        "authors": "R. Yamasaki",
        "keywords": [
            "ordinal regression",
            "one-dimensional transformation (1DT)",
            "empirical optimal threshold labeling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=mHSAy1n65Z",
        "pdf_src": "https://api2.openreview.net/pdf/18f233d9fee796a7967210f68fbabfcfec8bad3c.pdf",
        "Code_src": "",
        "Introduction": "Background: Ordinal regression tasks involve predicting the order or position of items within a predefined sequence using input features.\n\nResearch Question: The question addressed here concerns the labeling process utilized when employing one-dimensional transformations (1DTs) as part of ordinal regression models - specifically how observations should be labeled after transforming them into a new scale based on their original feature space values relative to other samples.\n\nMethodology: Existing literature has focused primarily on the construction of 1DTs themselves but less attention was given to the actual labeling strategy applied post-transformation; however, it plays crucial role determining model's predictive ability accurately. This work introduces 'likelihood-based labeling' where labels correspond to thresholds across transformed scales corresponding to ranks from lowest to highest. Additionally, authors introduce empirical optimal threshold labeling method which selects thresholds aiming at reducing empirical error rates during prediction phase rather than solely focusing on theoretical properties such as monotonicity preservation through transformations alone.\n\nMain Contributions:\n- Identification & analysis of different labeling strategies including likelihood-based labeling.\n- Demonstration showing both conventional threshold labeling techniques commonly found in regression approaches along with likelihood-based labeling share similarities despite being termed differently leading towards understanding commonalities between various methodologies.\n- Proof that standard thresholding could lead to suboptimal results if not carefully chosen considering specific dataset characteristics and desired outcomes like minimizing errors related to ranking predictions over time series data points etcetera).\n- Proposal implementing empirical optimal threshold labeling approach improving upon current practices resulting in better overall classification performances observed via experimental validation against several benchmark datasets demonstrating its effectiveness beyond just theory.",
        "Topic": "Optimal Transport"
    },
    {
        "title": "On the infinite-depth limit of finite-width neural networks",
        "abstract": "In this paper, we study the infinite-depth limit of finite-width residual neural networks with random Gaussian weights. With proper scaling, we show that by fixing the width and taking the depth to infinity, the pre-activations converge in distribution to a zero-drift diffusion process. Unlike the infinite-width limit where the pre-activation converge weakly to a Gaussian random variable, we show that the infinite-depth limit yields different distributions depending on the choice of the activation function. We document two cases where these distributions have closed-form (different) expressions. We further show an intriguing change-of-regime phenomenon of the post-activation norms when the width increases from 3 to 4. Lastly, we study the sequential limit infinite-depth-then-infinite-width, and compare it with the more commonly studied infinite-width-then-infinite-depth limit.",
        "authors": "S. Hayou",
        "keywords": [
            "infinite-depth limit",
            "residual neural networks",
            "diffusion processes"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=RbLsYz1Az9",
        "pdf_src": "https://api2.openreview.net/pdf/5f02eba416da157f64bfe8f955a83c98d314d88e.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper investigates the behavior of finite-width residual neural networks as their depth approaches infinity while keeping the width constant.\n\nResearch Question: What is the distributional convergence of the pre-activations for such networks?\n\nMethodology: The authors employ proper scaling techniques which allow them to analyze how the network's architecture affects its properties at large scales.\n \nMain Contributions:\n1. They demonstrate through theoretical analysis under certain conditions using stochastic processes like diffusion processes – specifically showing they can approximate to a zero-drift diffusion if scaled correctly,\n2. They identify distinct distributions based on the type of activation functions used within the network; unlike previous findings about infinite-width limits converging towards Gaussians regardless of activation function choices,\n3. They uncover novel insights into changes observed regarding post-activation norms across widths ranging between three units up to four units deepening our understanding beyond just asymptotic behaviors but also intermediate regimes not previously considered or analyzed extensively before now;\n4. Finally comparing results obtained during sequences leading toward both infinite-depth followed by infinite-width versus those starting off wide then increasing depths allows researchers greater insight than ever",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "PCPs: Patient Cardiac Prototypes to Probe AI-based Medical Diagnoses, Distill Datasets, and Retrieve Patients",
        "abstract": "Clinical deep learning systems often generate population-based and opaque medical diagnoses. This is in contrast to how primary care physicians make decisions, often adapting population-based protocols to the unique patient under consideration. Inspired by the workflow of such physicians, we develop a framework for learning embeddings, referred to as patient cardiac prototypes (PCPs), which capture information that is unique to an individual patient's electrocardiogram (ECG) data. Through rigorous evaluation on three publicly-available ECG datasets, we show that PCPs allow researchers to inspect why a particular diagnosis was made. We also demonstrate that PCPs are effective dataset distillers, where they can be used to train a model in lieu of a dataset orders of magnitude larger to achieve comparable performance. We show that PCPs can also be exploited to retrieve similar patient data across clinical databases. Our framework contributes to the development of transparent and patient-specific clinical deep learning systems.",
        "authors": "D. Kiyasseh, T. Zhu, D. A. Clifton",
        "keywords": [
            "ECG analysis",
            "Patient Cardiac Prototypes",
            "Clinical Deep Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=X1pjWMCMB0",
        "pdf_src": "https://api2.openreview.net/pdf/98d3c31c726fc15ecb8efc41f387f26fd1905a74.pdf",
        "Code_src": "",
        "Introduction": "Background: Clinical deep learning systems commonly produce generalized diagnostic results without considering individual patient characteristics or underlying reasoning processes.\n\nResearch Question: How might one design a system capable of generating personalized diagnostics while maintaining interpretability?\n\nMethodology: The authors introduce Patient Cardiac Prototypes (PCPs), which are learned embeddings from an individual’s ECG signals designed to encapsulate their specific health conditions within the context of cardiology practice.\n \nMain Contributions:\n1. **Interpretability**: PCPs enable detailed examination into the rationale behind each diagnosis through visualizing the distinct features contributing to it compared with those found universally among patients – thus promoting transparency beyond standard black-box models.\n2. **Dataset Distillation**: By using PCPs instead of extensive raw datasets during training, this research demonstrates significant reduction required yet maintains equivalent predictive accuracy when applied alongside other machine learning techniques like transfer learning - reducing computational demands significantly.\n3. **Patient Data Retrieval**: Furthermore, these prototypes may serve effectively not only at predicting but retrieving relevant historical records amongst various electronic health record repositories based on shared patterns observed between different individuals' cardiac profiles represented via PCPs.\n\nOverall, our work advances towards creating more nuanced & explainable artificial intelligence tools aiding healthcare professionals",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "A Ranking Game for Imitation Learning",
        "abstract": "We propose a new framework for imitation learning---treating imitation as a two-player ranking-based game between a policy and a reward. In this game, the reward agent learns to satisfy pairwise performance rankings between behaviors, while the policy agent learns to maximize this reward. In imitation learning, near-optimal expert data can be difficult to obtain, and even in the limit of infinite data cannot imply a total ordering over trajectories as preferences can. On the other hand, learning from preferences alone is challenging as a large number of preferences are required to infer a high-dimensional reward function, though preference data is typically much easier to collect than expert demonstrations. The classical inverse reinforcement learning (IRL) formulation learns from expert demonstrations but provides no mechanism to incorporate learning from offline preferences and vice versa. We instantiate the proposed ranking-game framework with a novel ranking loss giving an algorithm that can simultaneously learn from expert demonstrations and preferences, gaining the advantages of both modalities. Our experiments show that the proposed method achieves state-of-the-art sample efficiency and can solve previously unsolvable tasks in the Learning from Observation (LfO) setting.",
        "authors": "H. Sikchi, A. Saran, W. Goo, et.al",
        "keywords": [
            "ranking-based game",
            "imitation learning",
            "multi-modal learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=d3rHk4VAf0",
        "pdf_src": "https://api2.openreview.net/pdf/e788cf8c1a83b760e4f73bd0d00e04968f30cb55.pdf",
        "Code_src": "",
        "Introduction": "Background: Imitation learning aims to train agents by observing expert behavior or preferences rather than through rewards directly specified by humans.\n\nResearch Problem: Traditional imitation learning often requires access to near-optimal expert data which might not always exist due to practical constraints such as cost and time. Additionally, when there's abundant data available, it doesn't necessarily provide a global order among trajectories because human preferences may lead to partial orders instead.\n \nMethod: This paper introduces a novel approach treating imitation learning like a two-player ranking-based game where one player represents the policy seeking to optimize its actions based on the learned reward signal provided by the second player—the reward agent. The reward agent learns to rank behavioral pairs according to some criteria derived from observed preferences without needing all possible comparisons nor requiring a full trajectory ordering—only pairwise preferences need to guide the process.\n\nMain Contributions:\n1. A new framework for imitation learning framed within a multi-agent ranking game setup involving policies and rewards.\n2. An innovative ranking loss function designed specifically so that algorithms trained using our framework could concurrently leverage information gained from expert demonstrations alongside user-provided preferences—a hybrid approach combining strengths across different types of data sources.\n3. Demonstrated improvements regarding sample efficiency compared to existing methods; capable of solving problems beyond what previous approaches have been able to achieve especially under the \"Learning from Observations\" paradigm",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Named Tensor Notation",
        "abstract": "We propose a notation for tensors with named axes, which relieves the author, reader, and future implementers of machine learning models from the burden of keeping track of the order of axes and the purpose of each. The notation makes it easy to lift operations on low-order tensors to higher order ones, for example, from images to minibatches of images, or from an attention mechanism to multiple attention heads.\n\nAfter a brief overview and formal definition of the notation, we illustrate it through several examples from modern machine learning, from building blocks like attention and convolution to full models like Transformers and LeNet. We then discuss differential calculus in our notation and compare with some alternative notations. Our proposals build on ideas from many previous papers and software libraries. We hope that this document will encourage more authors to use named tensors, resulting in clearer papers and more precise implementations.",
        "authors": "D. Chiang, A. M. Rush, B. Barak",
        "keywords": [
            "named tensors",
            "tensor notation",
            "machine learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=hVT7SHlilx",
        "pdf_src": "https://api2.openreview.net/pdf/5bbfb55d13431c476d6d23f9e3816f091fd1ede5.pdf",
        "Code_src": "",
        "Introduction": "Background: In recent years, deep learning has become increasingly popular due to its effectiveness at solving complex problems such as image recognition and natural language processing. However, working with high-dimensional tensors can be challenging because they require careful tracking of axis orders and purposes.\nResearch Problem: To address this problem, researchers have proposed various notations aimed at making tensor manipulation easier by providing clear labels for different axes within a tensor structure.\nMethod: This paper introduces a new notation system called \"named axes\" for representing tensors where each dimension is given a specific name rather than being represented numerically using indices only. It also provides guidelines how to extend operations performed on lower dimensional tensors into higher dimensional ones without ambiguity about their meaning when applied across multiple dimensions simultaneously \nMain Contributions: 1) A comprehensive introduction along with formal definitions; 2) Illustration via practical examples drawn from current ML research literature ;3) Discussion comparing against existing alternatives focusing mainly around differential calculus perspective;4) Encouragement towards wider adoption leading potentially cleaner codebases & better understanding amongst practitioners",
        "Topic": "Machine Learning"
    },
    {
        "title": "lo-fi: distributed fine-tuning without communication",
        "abstract": "When fine-tuning large neural networks, it is common to use multiple nodes and to communicate gradients at each optimization step. By contrast, we investigate completely local fine-tuning, which we refer to as lo-fi. During lo-fi, each node fine-tunes independently without any communication. Then, the weights are averaged across nodes at the conclusion of fine-tuning. When fine-tuning DeiT-base and DeiT-large on ImageNet, this procedure matches accuracy in-distribution and improves accuracy under distribution shift compared to the baseline, which observes the same amount of data but communicates gradients at each step. We also observe that lo-fi matches the baseline's performance when fine-tuning OPT language models (up to 1.3B parameters) on Common Crawl. By removing the communication requirement, lo-fi reduces resource barriers for fine-tuning large models and enables fine-tuning in settings with prohibitive communication cost.",
        "authors": "M. Wortsman, S. Gururangan, S. Li, et.al",
        "keywords": [
            "local fine-tuning",
            "gradient communication",
            "resource barriers"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=1U0aPkBVz0",
        "pdf_src": "https://api2.openreview.net/pdf/c61c4a45618ef597badaddee78315bb799b32ae7.pdf",
        "Code_src": "",
        "Introduction": "Background: Fine-tuning large neural networks typically involves using multiple nodes connected through a network topology where gradients need to be communicated between these nodes during every optimization step.\n\nResearch Problem: The paper investigates an alternative approach called \"lo-fi\" or low-fidelity fine-tuning process.\nThis method aims to reduce computational costs by allowing each node within the distributed system to perform gradient updates locally instead of communicating them among all nodes after each update iteration.\n\nMethodology: In the lo-fi approach:\n- Each node independently performs its own fine-tuning iterations based on their respective mini-batches from the dataset.\n- At the end of the training phase, the model parameters are aggregated over all nodes via averaging so they can converge towards one another despite not having shared intermediate gradients throughout the entire training period.\n\nMain Contributions:\n- The authors demonstrate empirical evidence showing that lo-fi fine-tuning achieves comparable accuracy both when applied directly onto image classification tasks like those performed on ImageNet datasets,\nand even surpasses the standard practice (\"baseline\") \nwhen tested against distribution shifts encountered beyond the original training domain.\n- Additionally, the study extends into natural language processing domains demonstrating similar results while finetuning OPT language models up to 1.3 billion parameters size on Common Crawl dataset.\n- Finally, by eliminating the necessity for inter-node communications entirely, lo-fi significantly lowers the barrier-to-entry required resources such as bandwidth usage making it more feasible especially useful in scenarios where high communication costs would otherwise preclude running distributed deep learning jobs effectively.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "VN-Transformer: Rotation-Equivariant Attention for Vector Neurons",
        "abstract": "Rotation equivariance is a desirable property in many practical applications such as motion forecasting and 3D perception, where it can offer benefits like sample efficiency, better generalization, and robustness to input perturbations.\nVector Neurons (VN) is a recently developed framework offering a simple yet effective approach for deriving rotation-equivariant analogs of standard machine learning operations by extending one-dimensional scalar neurons to three-dimensional \"vector neurons.\"\nWe introduce a novel \"VN-Transformer\" architecture to address several shortcomings of the current VN models. Our contributions are:\n(i) we derive a rotation-equivariant attention mechanism which eliminates the need for the heavy feature preprocessing required by the original Vector Neurons models; (ii) we extend the VN framework to support non-spatial attributes, expanding the applicability of these models to real-world datasets; (iii) we derive a rotation-equivariant mechanism for multi-scale reduction of point-cloud resolution, greatly speeding up inference and training; (iv) we show that small tradeoffs in equivariance ($\\epsilon$-approximate equivariance) can be used to obtain large improvements in numerical stability and training robustness on accelerated hardware, and we bound the propagation of equivariance violations in our models.\nFinally, we apply our VN-Transformer to 3D shape classification and motion forecasting with compelling results.",
        "authors": "S. Assaad, C. Downey, R. Al-rfou', et.al",
        "keywords": [
            "rotation-equivariance",
            "Vector Neurons",
            "VN-Transformer"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=EiX2L4sDPG",
        "pdf_src": "https://api2.openreview.net/pdf/626c70b9f08335bebc9cb7a58d4e62ff76d30754.pdf",
        "Code_src": "",
        "Introduction": "Background: Rotation equivariance has been recognized as an important property beneficial for various tasks including motion forecasting and 3D perception due to its advantages over traditional methods.\n\nResearch Problem: The existing Vector Neurons (VN) model lacks certain features needed when applied to complex problems involving spatial data or multiple scales during processing.\n\nMethods: We propose a new architecture called the VN-Transformer designed specifically addressing the limitations found within the previous VN frameworks:\n\n1. A rotation-equivariant attention mechanism eliminating the necessity for extensive feature preprocessing;\n2. An extension allowing the VN framework to handle non-spatial attributes enhancing its application potential across different domains;\n3. Development of a rotation-equivariant method for reducing the resolution at multiple scales improving computational speed significantly;\n4. Demonstration showing that slight compromises in exact equivariance can lead to substantial enhancements regarding numerical stability under hardware acceleration conditions while bounding the extent of equivariance violation propagation through our proposed models.\n\nMain Contributions: \n- Novel VN-Transformer Architecture\n- Rotation-equivariant Attention Mechanism\n- Extension of VN Framework to Non-Spatial Attributes\n- Multi-scale Reduction Technique for Point Cloud Resolution\n- Insights into Trade-offs Between Equivariance and Numerical Stability",
        "Topic": "Vision Transformer"
    },
    {
        "title": "TLDR: Twin Learning for Dimensionality Reduction",
        "abstract": "Dimensionality reduction methods are unsupervised approaches which learn low-dimensional spaces where some properties of the initial space, typically the notion of “neighborhood”, are preserved. Such methods usually require propagation on large k-NN graphs or complicated optimization solvers. On the other hand, self-supervised learning approaches, typically used to learn representations from scratch, rely on simple and more scalable frameworks for learning. In this paper, we propose TLDR, a dimensionality reduction method for generic input spaces that is porting the recent self-supervised learning framework of Zbontar et al. (2021) to the specific task of dimensionality reduction, over arbitrary representations. We propose to use nearest neighbors to build pairs from a training set and a redundancy reduction loss to learn an encoder that produces representations invariant across such pairs. TLDR is a method that is simple, easy to train, and of broad applicability; it consists of an offline nearest neighbor computation step that can be highly approximated, and a straightforward learning process. Aiming for scalability, we focus on improving linear dimensionality reduction, and show consistent gains on image and document retrieval tasks, e.g. gaining +4% mAP over PCA on ROxford for GeM- AP, improving the performance of DINO on ImageNet or retaining it with a 10× compression.",
        "authors": "Y. Kalantidis, C. E. R. K. Lassance, J. Almazán, et.al",
        "keywords": [
            "TLDR",
            "Self-supervised Learning",
            "Dimensionality Reduction"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=86fhqdBUbx",
        "pdf_src": "https://api2.openreview.net/pdf/6adc2eb5b8d55ff73a696d0c2960270ecc0958be.pdf",
        "Code_src": "",
        "Introduction": "Background: Dimensionality reduction techniques aim to preserve important features while reducing the complexity of data by transforming high-dimensional datasets into lower dimensions.\n\nResearch Problem: Existing dimensionality reduction algorithms often involve computationally expensive operations like building large k-NN graphs using complex optimization solvers.\nSelf-supervised learning has been successful in representing data but not directly applied as a dimensionality reduction technique.\n\nMethods: The authors introduce TLDR, a novel approach inspired by recent advancements in self-supervised learning methodologies proposed by Zbontar et al., adapting them specifically towards dimensionality reduction without requiring any pre-defined embeddings nor supervision signals during the encoding phase.\n\nMain Contributions:\n1. **Adaptation of Self-Supervised Learning Framework**: TLDR adapts the self-supervised learning framework developed previously within the context of representation learning rather than dimensionality reduction itself - thus introducing new insights about how these models could potentially serve dual purposes beyond their original intent.\n\n2. **Nearest Neighbor Pair Construction**: Instead of relying solely on predefined similarity metrics between points along axes (as done traditionally), TLDR constructs pairs based on nearest neighbors drawn from both training sets – allowing for richer interactions among different parts of the dataset when constructing encodings.\n\n3. **Redundancy Reduction Loss**: To encourage learned encoders to produce invariant representations despite variations introduced through pair construction steps above, they employ a redundancy reduction loss function designed explicitly around preserving neighborhood structures after transformational changes have occurred due to dimensionality reduction processes themselves.\n\n4. **Scalable Linear Dimensionality Reduction**: Focusing primarily on scaling up linear dimensionality reduction capabilities instead of non-linear ones allows TLDR achieve better results at reduced computational costs compared traditional methods involving non-linear transformations leading toward improved efficiency overall especially suited well-suited applications dealing with massive amounts of raw data needing preprocessing before further analysis or processing downstream tasks such as classification/image recognition/document indexing etcetera).",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "Understanding Linearity of Cross-Lingual Word Embedding Mappings",
        "abstract": "The technique of Cross-Lingual Word Embedding (CLWE) plays a fundamental role in tackling Natural Language Processing challenges for low-resource languages. Its dominant approaches assumed that the relationship between embeddings could be represented by a linear mapping, but there has been no exploration of the conditions under which this assumption holds. Such a research gap becomes very critical recently, as it has been evidenced that relaxing mappings to be non-linear can lead to better performance in some cases. We, for the first time, present a theoretical analysis that identifies the preservation of analogies encoded in monolingual word embeddings as a *necessary and sufficient* condition for the ground-truth CLWE mapping between those embeddings to be linear. On a novel cross-lingual analogy dataset that covers five representative analogy categories for twelve distinct languages, we carry out experiments which provide direct empirical support for our theoretical claim. These results offer additional insight into the observations of other researchers and contribute inspiration for the development of more effective cross-lingual representation learning strategies. \n",
        "authors": "X. Peng, M. Stevenson, C. Lin, et.al",
        "keywords": [
            "cross-lingual word embedding",
            "linear mapping",
            "analogy preservation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=8HuyXvbvqX",
        "pdf_src": "https://api2.openreview.net/pdf/c4ff59f8659b6dea458ceaabc21b101849023f19.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses how Cross-Lingual Word Embeddings (CLWE), an approach used extensively in natural language processing tasks involving low-resource languages, typically assumes a linear mapping among embedding vectors representing words from different languages.\n\nResearch Question: The central question addressed is whether or not this linear mapping hypothesis always holds true; specifically, if representations are indeed linearly related when mapped across languages based on their monolingual embeddings?\n\nMethodology: To answer these questions, they conduct both a theoretical investigation identifying necessary and sufficient conditions where such linearity would hold – namely preserving analogical relationships found within single-language embeddings - and empirical tests using a new cross-lingual analogy dataset covering 12 languages with various linguistic similarities/dissimilarities along multiple dimensions relevant to analogy perception like semantic similarity, syntactic complexity etc.\n\nMain Contributions:\n1. A comprehensive theoretical framework explaining why certain types of analogical relations should preserve well during translation via linear transformations.\n2. An empirically validated dataset designed explicitly around testing cross-lingual analogical reasoning capabilities through explicit comparisons against known monolingual analogies.\n3. Experimental evidence supporting previous findings suggesting nonlinear mappings may sometimes perform better than linear ones while also providing insights about what kind of linguistic features might matter most regarding transferability quality over different contexts/types of analogies presented therein.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Clustering units in neural networks: upstream vs downstream information",
        "abstract": "It has been hypothesized that some form of \"modular\" structure in artificial neural networks should be useful for learning, compositionality, and generalization. However, defining and quantifying modularity remains an open problem. We cast the problem of detecting functional modules into the problem of detecting clusters of similar-functioning units. This begs the question of what makes two units functionally similar. For this, we consider two broad families of methods: those that define similarity based on how units respond to structured variations in inputs (\"upstream\"), and those based on how variations in hidden unit activations affect outputs (\"downstream\"). We conduct an empirical study quantifying modularity of hidden layer representations of a collection of feedforward networks trained on classification tasks, across a range of hyperparameters. For each model, we quantify pairwise associations between hidden units in each layer using a variety of both upstream and downstream measures, then cluster them by maximizing their \"modularity score\" using established tools from network science. We find two surprising results: first, dropout dramatically increased modularity, while other forms of weight regularization had more modest effects. Second, although we observe that there is usually good agreement about clusters within both upstream methods and downstream methods, there is little agreement about the cluster assignments across these two families of methods. This has important implications for representation-learning, as it suggests that finding modular representations that reflect structure in inputs (e.g. disentanglement) may be a distinct goal from learning modular representations that reflect structure in outputs (e.g. compositionality).",
        "authors": "R. D. Lange, D. Rolnick, K. Kording",
        "keywords": [
            "detecting functional modules",
            "modularity score",
            "upstream vs. downstream approaches"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Euf7KofunK",
        "pdf_src": "https://api2.openreview.net/pdf/bba21647324a5db03f53ca01c9f7cd50deee912a.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses recent research suggesting that modular structures could benefit artificial neural networks' ability to learn, compose knowledge, and generalize new information.\n\nResearch Problem:\nThe main challenge addressed here involves defining and measuring modularity effectively—a concept yet to have clear definitions or metrics despite its potential importance.\n \nMethodology:\nTo tackle this issue, researchers approach the detection of functional modules through clustering units with similar functionalities—raising questions regarding criteria used to determine such similarities among units. They explore broadly categorized approaches focusing either on units’ responses to input variations (‘upstream’) or output changes due to activation variation at hidden layers (‘downstream’).\n\nMain Contributions:\nAn empirical investigation was conducted analyzing the modularity scores obtained when applying various upstream and downstream similarity measures toward hidden-layer representations extracted from multiple feedforward neural networks which were previously trained via classification tasks under different sets of hyperparameters. The findings are twofold:\n\n1. Dropout Regularization Impact - A notable observation made during testing indicates that dropout significantly enhances module identification compared to alternative regularization techniques like weight decay; dropout seems particularly effective without diminishing performance much relative to others.\n\n2. Consistency Across Methods - Despite generally agreeing upon clusterings derived independently according to upstream versus downstream methodologies themselves individually, substantial disagreement exists concerning whether specific units belong together once considering all measured similarities jointly—an insight pointing towards potentially separate goals associated with discovering modular representations reflecting underlying input structures (like disentanglement) vs. those mirroring output patterns (such as compositional understanding).",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Boosting Search Engines with Interactive Agents",
        "abstract": "This paper presents first successful steps in designing search agents that learn meta-strategies for iterative query refinement in information-seeking tasks.  Our approach uses machine reading to guide the selection of refinement terms from aggregated search results.\nAgents are then empowered with simple but effective search operators to exert fine-grained and transparent control over queries and search results. We develop a novel way of generating synthetic search sessions, which leverages the power of transformer-based language models through (self-)supervised learning. We also present a reinforcement learning agent with dynamically constrained actions that learns interactive search strategies from scratch. Our search agents obtain retrieval and answer quality performance comparable to recent neural methods, using only a traditional term-based BM25 ranking function and interpretable discrete reranking and filtering actions.",
        "authors": "L. Adolphs, B. Börschinger, C. Buck, et.al",
        "keywords": [
            "machine reading",
            "iterative query refinement",
            "reinforcement learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=0ZbPmmB61g",
        "pdf_src": "https://api2.openreview.net/pdf/89a5d07165413a6b71d8b086bdab7af5d35c7cdd.pdf",
        "Code_src": "",
        "Introduction": "Background: The background of this research is on improving the efficiency and effectiveness of search systems by enabling them to iteratively refine their queries based on user feedback or intermediate results.\n\nResearch Problem: The main problem addressed in this study is how to design intelligent search agents capable of autonomously refining their queries during an information-seeking task without relying solely on predefined rules or heuristics; instead, they should be able to adaptively select relevant refinements as needed throughout the process.\n\nMethodology: To tackle this issue, researchers employed several innovative approaches including:\n\n1. Machine Reading - Using advanced natural language processing techniques such as machine reading comprehension algorithms trained specifically around web-scale datasets like Wikipedia articles help inform better decisions about what types of keywords might yield more useful answers when added into subsequent iterations within each session's workflow cycle.\n\n2. Synthetic Search Sessions Generation - They developed new ways involving self-supervised pre-training via Transformer architectures similar those found powering state-of-the-art NLP models today allowing these agents access vast amounts hypothetical scenarios where they could practice iterating upon initial queries until arriving at satisfactory outcomes before ever encountering real-world usage contexts themselves!\n\n3. Reinforcement Learning Agent Development - A custom-built RL agent was created equipped with dynamic constraints imposed according its environment dynamics ensuring it remains focused towards achieving desired goals whilst avoiding getting distracted along irrelevant tangents during training phases leading up toward deployment readiness stage \n\nMain Contributions: This work makes significant contributions advancing current understanding regarding automated iterative query refinement capabilities amongst autonomous intelligent agents engaged within complex multi-step decision-making processes akin those encountered everyday while browsing online resources utilizing modern digital platforms accessible worldwide nowadays! Specifically highlighted achievements include:\n- Demonstrating feasibility existence viable solutions addressing core challenge posed namely developing adaptive mechanisms capable guiding continuous improvement efforts across multiple rounds interaction between human users & computational machinery tasked w/ retrieving pertinent info pertinent given context presented;\n- Providing empirical evidence showcasing efficacy proposed architecture relative existing neural counterparts despite employing comparatively simpler components underpinning overall system operation (e.g., basic BM25 ranking algorithm); \n- Presenting practical demonstration potential application domains benefiting greatly enhanced functionality resulting integration aforementioned methodologies into broader ecosystem distributed computing environments facilitating seamless transition between manual exploration stages supplemented semi-automatic assistance provided sophisticated software tools designed streamline discovery knowledge acquisition activities undertaken daily life pursuits professional endeavors alike",
        "Topic": "Machine Learning"
    },
    {
        "title": "Greedy Bayesian Posterior Approximation with Deep Ensembles",
        "abstract": "Ensembles of independently trained neural networks are a state-of-the-art approach to estimate predictive uncertainty in Deep Learning, and can be interpreted as an approximation of the posterior distribution via a mixture of delta functions. The training of ensembles relies on non-convexity of the loss landscape and random initialization of their individual members, making the resulting posterior approximation uncontrolled. This paper proposes a novel and principled method to tackle this limitation, minimizing an $f$-divergence between the true posterior and a kernel density estimator (KDE) in a function space. We analyze this objective from a combinatorial point of view, and show that it is submodular with respect to mixture components for any $f$. Subsequently, we consider the problem of greedy ensemble construction. From the marginal gain on the negative $f$-divergence, which quantifies an improvement in posterior approximation yielded by adding a new component into the KDE, we derive a novel diversity term for ensemble methods. The performance of our approach is demonstrated on computer vision out-of-distribution detection benchmarks in a range of architectures trained on multiple datasets. The source code of our method is made publicly available at https://github.com/Oulu-IMEDS/greedy_ensembles_training.",
        "authors": "A. Tiulpin, M. B. Blaschko",
        "keywords": [
            "ensemble",
            "predictive uncertainty",
            "f-divergence"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=P1DuPJzVTN",
        "pdf_src": "https://api2.openreview.net/pdf/3043d3eae333eee61dd0e8525244e446fa359eed.pdf",
        "Code_src": "https://github.com/Oulu-IMEDS/greedy_ensembles_training",
        "Introduction": "Background: Ensembles of independently trained neural networks have been shown to effectively approximate the posterior distribution through a mixture of delta functions when estimating predictive uncertainty.\n\nResearch Problem: However, due to the non-convex nature of the loss landscape during training along with random initialization techniques used within each network's parameters, these ensembles result in an uncontrolled approximation of the posterior distribution.\n \nMethods: To address this issue, authors propose a novel and principled method based on minimizing an f-divergence measure between the true posterior and a kernel density estimator (KDE). They further extend their analysis using combinatorial optimization principles showing that the proposed objective is submodular w.r.t. mixture components under any given f-divergence metric. Additionally, they introduce a novel diversity term specifically designed for ensemble learning algorithms derived directly from improvements measured against the negative f-divergence criterion - i.e., how much better does including another model improve our KDE approximation? Finally, they demonstrate empirical validation across various computer vision tasks where models were pre-trained over diverse datasets before being evaluated beyond their original training distributions demonstrating improved robustness compared traditional ensemble approaches without such regularization terms included.\n\nMain Contributions:\n1. A principled way toward controlling ensemble-based approximations towards Bayesian posterior distributions leveraging f-divergences measures;\n2. Novel insights into understanding ensemble constructions through submodularity properties related to differentiable objectives; \n3. Development of practical guidelines aimed improving ensemble diversity leading potentially more accurate predictions outside seen domains or unseen data points encountered throughout deployment phases",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Auto-Lambda: Disentangling Dynamic Task Relationships",
        "abstract": "Understanding the structure of multiple related tasks allows for multi-task learning to improve the generalisation ability of one or all of them. However, it usually requires training each pairwise combination of tasks together in order to capture task relationships, at an extremely high computational cost. In this work, we learn task relationships via an automated weighting framework, named Auto-Lambda. Unlike previous methods where task relationships are assumed to be fixed, i.e., task should either be trained together or not trained together, Auto-Lambda explores continuous, dynamic task relationships via task-specific weightings, and can optimise any choice of combination of tasks through the formulation of a meta-loss; where the validation loss automatically influences task weightings throughout training. We apply the proposed framework to both multi-task and auxiliary learning problems in computer vision and robotics, and show that AutoLambda achieves state-of-the-art performance, even when compared to optimisation strategies designed specifically for each problem and data domain. Finally, we observe that Auto-Lambda can discover interesting learning behaviors, leading to new insights in multi-task learning. Code is available at https://github.com/lorenmt/auto-lambda.",
        "authors": "S. Liu, S. James, A. J. Davison, et.al",
        "keywords": [
            "Auto-Lambda",
            "Multi-Task Learning",
            "Meta-Loss"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=KKeCMim5VN",
        "pdf_src": "https://api2.openreview.net/pdf/370e35dd5d39ac8f3ce21c3b326da58aec983e6b.pdf",
        "Code_src": "https://github.com/lorenmt/auto-lambda",
        "Introduction": "Background: The paper addresses the challenge posed by traditional multi-task learning approaches which require extensive computational resources due to the need to train pairs of related tasks simultaneously.\n\nResearch Question: How do you efficiently model complex interdependencies between different tasks within a multi-task setting?\n\nMethodology: To tackle this question, they introduce \"Auto-Lambda,\" an automated weighting framework based on meta-learning techniques such as gradient-based optimization with respect to the weights assigned to individual tasks (\"lambda\"). This approach enables exploration of continuously varying task dependencies rather than binary ones used previously – tasks could have partial relevance instead being fully independent or dependent upon others.\n \nMain Contributions:\n1. **Dynamic Task Weighting**: They propose a method allowing for flexible adjustments during training so that tasks may influence other tasks dynamically depending on their specific characteristics without requiring pre-defined coupling structures like hard-coded combinations seen before.\n2. **Meta-Loss Function**: By incorporating a meta-loss function into the overall objective, they ensure that the validation process informs these adaptively changing task weights over time while minimizing prediction errors across various datasets and domains.\n3. **Performance Across Domains**: Demonstrated improvements using Auto-Lambda frameworks applied broadly from Computer Vision to Robotics fields against existing benchmarks including those tailored especially toward particular challenges encountered therein indicating its robustness beyond specialized setups.\n4. **Insights Discovery**: Beyond practical gains observed in terms of accuracy metrics, Auto-Lambda also uncovers novel patterns about how tasks interact suggesting potential avenues towards better understanding underlying mechanisms governing multi-task learning phenomena.\n\nConclusion: The study introduces Auto-Lambda - a versatile tool capable of adapting task importance flexibly enabling more efficient utilization of computational resources whilst yielding significant advancements",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
        "abstract": "Vision Transformers (ViT) have been shown to attain highly competitive performance for a wide range of vision applications, such as image classification, object detection and semantic image segmentation. In comparison to convolutional neural networks, the Vision Transformer's weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation (``AugReg'' for short) when training on smaller training datasets. We conduct a systematic empirical study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget. As one result of this study we find that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data: we train ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset.\n",
        "authors": "A. P. Steiner, A. Kolesnikov, X. Zhai, et.al",
        "keywords": [
            "data augmentation",
            "Vision Transformer",
            "model regularization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=4nPswr1KcP",
        "pdf_src": "https://api2.openreview.net/pdf/2888bf70cd58ae57cef999ba33c3cc2812e836f7.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses how Vision Transformers (ViTs), despite being less biased than Convolutional Neural Networks (CNNs), often require additional techniques like Model Regularization or Data Augmentation (AugReg) while working with small datasets.\n\nResearch Problem:\nUnderstanding whether there exists any trade-off among the quantity of training data, AugReg strategies, model complexity, and computational resources during ViT training process remains unclear due to limited empirical studies focusing specifically on these factors together.\n\nMethodology:\nThe authors systematically investigate through empirical experiments by varying levels of training data, AugReg methods applied along with different model architectures across two large-scale datasets - ImageNet-21k and private JFT-300M dataset – to observe trends related to computation efficiency versus accuracy gains from using augmented data alongside scaling up both model size and computing power.\n\nMain Contributions:\nThis research highlights several key findings including:\n\n1. The synergy effect where combining enhanced computational capabilities with AugReg allows achieving similar results compared to those obtained after extensive training over much larger datasets without access to them; \n2. It provides insights into optimal configurations needed based upon specific tasks' requirements regarding computational resource allocation relative to other hyperparameters involved in machine learning pipeline optimization efforts aimed at improving performance within practical constraints imposed typically encountered nowadays",
        "Topic": "Vision Transformer"
    },
    {
        "title": "Queried Unlabeled Data Improves and Robustifies Class-Incremental Learning",
        "abstract": "Class-incremental learning (CIL) suffers from the notorious dilemma between learning newly added classes and preserving previously learned class knowledge. That catastrophic forgetting issue could be mitigated by storing historical data for replay, which yet would cause memory overheads as well as imbalanced prediction updates. To address this dilemma, we propose to leverage \"free\" external unlabeled data querying in continual learning. We first present a CIL with Queried Unlabeled Data (CIL-QUD) scheme, where we only store a handful of past training samples as anchors and use them to query relevant unlabeled examples each time. Along with new and past stored data, the queried unlabeled are effectively utilized, through learning-without-forgetting (LwF) regularizers and class-balance training. Besides preserving model generalization over past and current tasks, we next study the problem of adversarial robustness for CIL-QUD.\nInspired by the recent success of learning robust models with unlabeled data, we explore a new robustness-aware CIL setting, where the learned adversarial robustness has to resist forgetting and be transferred as new tasks come in continually. While existing options easily fail, we show queried unlabeled data can continue to benefit, and seamlessly extend CIL-QUD into its robustified versions, RCIL-QUD. Extensive experiments demonstrate that CIL-QUD achieves substantial accuracy gains on CIFAR-10 and CIFAR-100, compared to previous state-of-the-art CIL approaches. Moreover, RCIL-QUD establishes the first strong milestone for robustness-aware CIL. Codes are available in https://github.com/VITA-Group/CIL-QUD. ",
        "authors": "T. Chen, S. Liu, S. Chang, et.al",
        "keywords": [
            "CIL-QUD",
            "Learning-without-forgetting Regularizer",
            "Robust Continual Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=oLvlPJheCD",
        "pdf_src": "https://api2.openreview.net/pdf/22e35861f6ca9c63a76c3160e5695a338953238d.pdf",
        "Code_src": "代码链接：https://github.com/VITA-Group/CIL-QUD",
        "Introduction": "Background: The paper addresses an issue known as catastrophic forgetting within continuous incremental learning (CIL), particularly when dealing with adding new classes while maintaining knowledge about old ones.\n\nResearch Problem: How do you balance updating the model's parameters based on both new and old classes without losing information?\n\nMethods: The authors introduce a novel approach called Class-Incremental Learning with Queried Unlabeled Data (CIL-QUD). This method stores just a few representative samples (\"anchors\") rather than all historical data; it then queries additional unlabeled data related to these anchors at every step during training using LwF regularizers ensuring no forgetting occurs alongside balancing the number of predictions across different classes via class-balance training techniques.\n\nMain Contributions:\n1. They develop a strategy specifically designed not merely to prevent forgetting but also to improve performance even after incorporating many more classes—this is shown experimentally against other leading methods like CIL.\n2. Additionally, they investigate how their proposed framework performs under adversarial conditions—a significant challenge since traditional defenses may lead to forgetting or degradation once faced with unseen attacks due to incrementality issues—and find that their robust version maintains high levels of protection despite ongoing changes introduced throughout training sessions.\n3. Finally, code implementing their algorithms along with datasets used have been made publicly accessible so others might replicate findings themselves if desired—an important contribution promoting reproducibility standards among researchers working towards similar goals such as continual learning advancements toward practical applications requiring adaptability amidst frequent shifts in task requirements posed daily challenges encountered real-world scenarios involving machine intelligence systems deployed continuously evolving environments",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Improved Differentially Private Riemannian Optimization: Fast Sampling and Variance Reduction",
        "abstract": "A common step in differentially private ({DP}) Riemannian optimization is sampling from the (tangent) Gaussian distribution as noise needs to be generated in the tangent space to perturb the gradient. In this regard, existing works either use the Markov chain Monte Carlo ({MCMC}) sampling or explicit basis construction based sampling methods on the tangent space. This becomes a computational bottleneck in the practical use of {DP} Riemannian optimization, especially when performing stochastic optimization. In this paper, we discuss different sampling strategies and develop efficient sampling procedures by exploiting linear isometry between tangent spaces and show them to be orders of magnitude faster than both the {MCMC} and sampling using explicit basis construction. Furthermore, we develop the {DP} Riemannian stochastic variance reduced gradient algorithm and compare it with DP Riemannian gradient descent and stochastic gradient descent algorithms on various problems.  ",
        "authors": "S. Utpala, A. Han, P. Jawanpuria, et.al",
        "keywords": [
            "dp",
            "riemannian optimization",
            "sampling strategies"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=paguBNtqiO",
        "pdf_src": "https://api2.openreview.net/pdf/ccf96ea6f815a1996a6d9b031ce747127a3b8640.pdf",
        "Code_src": "",
        "Introduction": "Background: Differential privacy (DP) Riemannian optimization involves generating noise in the tangent space for perturbing gradients during optimization steps.\n\nResearch Problem: Existing approaches used MCMC sampling or explicit basis construction-based sampling which are computationally inefficient particularly while dealing with stochastic optimization.\n\nMethod: The authors propose an efficient sampling strategy that exploits linear isometry between tangent spaces leading to orders of magnitude speedup compared to previous methods like MCMC and those relying on explicit bases.\n\nMain Contributions: \n1. Discussion about different sampling strategies.\n2. Development of efficient sampling procedures leveraging linear isometry across tangent spaces resulting in significant performance improvements over traditional methods.\n3. Introduction of a new DP Riemannian stochastic variance reduced gradient algorithm along with comparative studies against other DP Riemannian and stochastic gradient descent algorithms under varied problem settings demonstrating its effectiveness.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Robust Hybrid Learning With Expert Augmentation",
        "abstract": "Hybrid modelling reduces the misspecification of expert models by combining them with machine learning (ML) components learned from data. Similarly to many ML algorithms, hybrid model performance guarantees are limited to the training distribution. Leveraging the insight that the expert model is usually valid even outside the training domain, we overcome this limitation by introducing a hybrid data augmentation strategy termed \\textit{expert augmentation}. Based on a probabilistic formalization of hybrid modelling, we demonstrate that expert augmentation, which can be incorporated into existing hybrid systems, improves generalization. We empirically validate the expert augmentation on three controlled experiments modelling dynamical systems with ordinary and partial differential equations. Finally, we assess the potential real-world applicability of expert augmentation on a dataset of a real double pendulum.",
        "authors": "A. Wehenkel, J. Behrmann, H. Hsu, et.al",
        "keywords": [
            "hybrid modeling",
            "expert augmentation",
            "generalization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=oe4dl4MCGY",
        "pdf_src": "https://api2.openreview.net/pdf/1f924458b715e5bacdbb3e89579825f63b191c4e.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses an issue in modeling where traditional expert models may not perform well beyond their original training domains due to misspecification.\n\nResearch Question: How do we improve the robustness or generalizability of these expert models?\n\nMethodology: To address this question, they propose \"Expert Augmentation,\" as part of Hybrid Modelling approach - a method that combines human-designed expert models with machine learning components trained using empirical data for better fit but potentially less coverage than purely statistical approaches alone.\n\nMain Contributions:\n1. **Introduction of Expert Augmentation**: This involves leveraging additional information about the underlying dynamics through the expert model itself.\n2. **Probabilistic Formalization**: They provide a theoretical framework grounded in probability theory explaining why such augmentation could lead to improved generalization across different datasets.\n3. **Empirical Validation**: Through controlled experiments involving dynamical systems described both by Ordinary Differential Equations (ODEs) and Partial Differential Equations (PDEs), it's shown how expert augmentation indeed enhances predictive capabilities over standard methods without extensive retraining required after each new task.\n4. **Real-World Applicability**: Lastly, demonstrating its effectiveness within practical constraints found when dealing with complex physical phenomena like a real double pendulum system suggests broader implications towards more widespread application scenarios including those encountered during deployment phase tasks under uncertainty conditions typical seen in dynamic environments",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Regularized Training of Intermediate Layers for Generative Models for Inverse Problems",
        "abstract": "Generative Adversarial Networks (GANs) have been shown to be powerful and flexible priors when solving inverse problems. One challenge of using them is overcoming representation error, the fundamental limitation of the network in representing any particular signal. Recently, multiple proposed inversion algorithms reduce representation error by optimizing over intermediate layer representations. These methods are typically applied to generative models that were trained agnostic of the downstream inversion algorithm. In our work, we introduce a principle that if a generative model is intended for inversion using an algorithm based on optimization of intermediate layers, it should be trained in a way that regularizes those intermediate layers. We instantiate this principle for two notable recent inversion algorithms: Intermediate Layer Optimization and the Multi-Code GAN prior. For both of these inversion algorithms, we introduce a new regularized GAN training algorithm and demonstrate that the learned generative model results in lower reconstruction errors across a wide range of under sampling ratios when solving compressed sensing, inpainting, and super-resolution problems.",
        "authors": "S. Gunn, J. Cocola, P. Hand",
        "keywords": [
            "GANs",
            "Representation Error Reduction",
            "Regularized Training"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=cKsKXR28cG",
        "pdf_src": "https://api2.openreview.net/pdf/4d1ab2103262ba61cd4b313a0d1f752af6a68c3f.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses how Generative Adversarial Networks (GANs), which can generate high-quality images from random noise or modify existing ones according to certain criteria, could also serve as useful tools within the field of inverse problems - where one seeks solutions given partial information about systems.\n\nResearch Problem:\nThe main issue addressed here concerns the inherent limitations with traditional GANs used directly without modification; they often suffer from \"representation error\" due to their architecture's inability to accurately represent specific signals during generation tasks like image synthesis.\n \nMethodology:\nTo tackle this problem, researchers propose a novel approach focusing on modifying the training process itself rather than altering the GAN structure significantly – specifically through regularization techniques aimed at refining the internal representations processed throughout different layers inside the neural networks before final output generation occurs. This method involves fine-tuning the generative model so its intermediate layers learn more robust features suitable for subsequent inversion algorithms relying heavily upon such feature maps.\n\nMain Contributions:\nIn practice, authors apply this idea into two well-known inversion algorithms known as Intermediate Layer Optimization (ILO) and Multi-Code GAN Prior respectively adapting each one accordingly while introducing corresponding regularized versions called Regularized ILO and Regularized Multi-Code GAN. The experiments conducted show promising improvements demonstrating reduced reconstruction errors compared against non-regularized counterparts especially noticeable particularly concerning various levels of data compression rates encountered commonly among practical applications including compressive sensing imaging restoration tasks along with inpainting and super resolution scenarios suggesting significant advancements towards better performance outcomes achievable via optimized architectures designed explicitly considering potential downstream uses beyond basic generation purposes alone.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Dirichlet Mechanism for Differentially Private KL Divergence Minimization",
        "abstract": "Given an empirical distribution $f(x)$ of sensitive data $x$, we consider the task of minimizing $F(y) = D_{\\text{KL}} (f(x)\\Vert y)$ over a probability simplex, while protecting the privacy of $x$. We observe that, if we take the exponential mechanism and use the KL divergence as the loss function, then the resulting algorithm is the $Dirichlet\\text{ }mechanism$ that outputs a single draw from a Dirichlet distribution. Motivated by this, we propose a Rényi differentially private (RDP) algorithm that employs the Dirichlet mechanism to solve the KL divergence minimization task. In addition, given $f(x)$ as above and $\\hat{y}$ an output of the Dirichlet mechanism, we prove a probability tail bound on $D_{\\text{KL}}  (f(x)\\Vert \\hat{y})$, which is then used to derive a lower bound for the sample complexity of our RDP algorithm. Experiments on real-world datasets demonstrate advantages of our algorithm over Gaussian and Laplace mechanisms in supervised classification and maximum likelihood estimation.",
        "authors": "D. Ponnoprat",
        "keywords": [
            "privacy",
            "Dirichlet mechanism",
            "Rényi differential privacy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=lmr2WwlaFc",
        "pdf_src": "https://api2.openreview.net/pdf/c7da7693c140511e2094ea7203d4d96748687534.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the problem of minimizing a Kullback-Leibler (KL) divergence between two distributions under differential privacy constraints.\n\nResearch Question: How can one minimize the KL divergence subject to differential privacy constraints?\n\nMethod: The authors start with considering the exponential mechanism combined with the KL divergence as the loss function; they find it leads naturally to the Dirichlet mechanism when applied within a probability simplex setting. Inspired by this observation, they introduce a Rényi-differentially private (RDP) algorithm based on the Dirichlet mechanism designed specifically for solving the KL divergence minimization task involving sensitive data.\n\nMain Contributions:\n1. They show how the Dirichlet mechanism arises directly through using the exponential mechanism along with the KL divergence.\n2. They develop a new RDP algorithm utilizing the Dirichlet mechanism tailored towards minimizing the KL divergence among probability distributions representing sensitive information such as personal or medical records where privacy preservation must be maintained at all costs during processing tasks like learning algorithms do.\n3. Furthermore, providing theoretical guarantees regarding their proposed method's performance metrics - namely proving bounds related to its ability not only protect individual privacy but also achieve desired accuracy levels compared against other commonly employed mechanisms including Gaussian and Laplace mechanisms via experiments conducted across various datasets demonstrating superiority particularly pertinent areas like supervised classification & maximum likelihood estimation scenarios relevant fields ranging from healthcare informatics machine learning applications elsewhere requiring handling confidential",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Workflow Discovery from Dialogues in the Low Data Regime",
        "abstract": "Text-based dialogues are now widely used to solve real-world problems. In cases where solution strategies are already known, they can sometimes be codified into workflows and used to guide humans or artificial agents through the task of helping clients. We introduce a new problem formulation that we call Workflow Discovery (WD) in which we are interested in the situation where a formal workflow may not yet exist. Still, we wish to discover the set of actions that have been taken to resolve a particular problem. We also examine a sequence-to-sequence (Seq2Seq) approach for this novel task. We present experiments where we extract workflows from dialogues in the Action-Based Conversations Dataset (ABCD). Since the ABCD dialogues follow known workflows to guide agents, we can evaluate our ability to extract such workflows using ground truth sequences of actions. We propose and evaluate an approach that conditions models on the set of possible actions, and we show that using this strategy, we can improve WD performance.  Our conditioning approach also improves zero-shot and few-shot WD performance when transferring learned models to unseen domains within and across datasets. Further, on ABCD a modified variant of our Seq2Seq method achieves state-of-the-art performance on related but different problems of Action State Tracking (AST) and Cascading Dialogue Success (CDS) across many evaluation metrics.",
        "authors": "A. E. Hattami, I. H. Laradji, S. Raimondo, et.al",
        "keywords": [
            "Workflow Discovery",
            "Sequence-to-Sequence Approach",
            "Action-Based Conversations"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=L9othQvPks",
        "pdf_src": "https://api2.openreview.net/pdf/ccc31b9d79f2c67af69dd6537c9604b73d13fe18.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the issue of how to automatically generate workflows based on existing dialogue data without relying on predefined workflows.\n\nResearch Problem: The research question is whether it's feasible to develop a system capable of discovering workflows by analyzing text-based conversations even if no explicit workflow exists as input.\n\nMethodology: To tackle this challenge, the authors explore a Sequence-to-Sequence (Seq2Seq) model trained with the Action-Based Conversations Dataset (ABCD), focusing specifically on identifying patterns between conversational exchanges and potential action sequences needed during client assistance tasks.\n \nMain Contributions:\n1. They introduced a novel problem called Workflow Discovery (WD).\n2. Developed a Seq2Seq approach tailored toward solving the WD problem utilizing the ABCD dataset containing dialogues following specific workflows serving as reference sequences against which their model could be evaluated,\n3. Proposed a conditioning mechanism designed around sets of possible actions enhancing the WD process significantly both in terms of accuracy over unseen domains within and across datasets along with achieving top-tier results compared to other methods like AST and CDS benchmarks under modifications applied to their Seq2Seq framework adapted further for these specialized tasks involving tracking states dynamically throughout interactions rather than just extracting discrete steps sequentially one after another.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Layerwise Bregman Representation Learning of Neural Networks with Applications to Knowledge Distillation",
        "abstract": "We propose a new method for layerwise representation learning of a trained neural network that conforms to the non-linearity of the layer's transfer function. In particular, we form a Bregman divergence based on the convex function induced by the layer's transfer function and construct an extension of the original Bregman PCA formulation by incorporating a mean vector and revising the normalization constraint on the principal directions. These modifications allow exporting the learned representation as a fixed layer with a non-linearity. As an application to knowledge distillation, we cast the learning problem for the student network as predicting the compression coefficients of the teacher's representations, which is then passed as the input to the imported layer. Our empirical findings indicate that our approach is substantially more effective for transferring information between networks than typical teacher-student training that uses the teacher's soft labels. ",
        "authors": "E. Amid, R. Anil, C. Fifty, et.al",
        "keywords": [
            "layerwise representation learning",
            "Bregman divergence",
            "knowledge distillation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=6dsvH7pQHH",
        "pdf_src": "https://api2.openreview.net/pdf/b9982865a0d6197f11f4de776572c636508f9a72.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the challenge of representing layers in a trained neural network while preserving their nonlinear properties.\n\nResearch Problem: How can one learn a representation from each layer such that it captures its nonlinear characteristics?\n\nMethod: The authors introduce a novel technique inspired by Bregman divergences - they create a divergence metric grounded in the convexity of the layer's activation function using a Bregman divergence. They extend this concept further into Principal Component Analysis (PCA), adjusting the standard PCA framework through adding a mean vector component and modifying the constraints applied during the normalization process within the PCA computation steps so these vectors are invariant under linear transformations but still capture the essential features of the data. \n\nMain Contributions:\n1. A novel way to represent neural network layers' nonlinearities.\n2. An innovative extension of Bregman PCA adapted specifically for neural network layer embeddings ensuring both linearity-invariance and feature preservation.\n3. Application demonstration leveraging knowledge distillation where students predict the compression coefficients used when compressing teachers’ representations before passing them along embedded layers – leading to improved performance over conventional methods relying solely on teacher’s soft labels alone.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks",
        "abstract": "Deep neural networks (DNNs) are often trained on the premise that the complete training data set is provided ahead of time. However, in real-world scenarios, data often arrive in chunks over time. This leads to important considerations about the optimal strategy for training DNNs, such as whether to fine-tune them with each chunk of incoming data (warm-start) or to retrain them from scratch with the entire corpus of data whenever a new chunk is available. While employing the latter for training can be resource-intensive, recent work has pointed out the lack of generalization in warm-start models. Therefore, to strike a balance between efficiency and generalization, we introduce \"Learn, Unlearn, and Relearn (LURE)\" an online learning paradigm for DNNs. LURE interchanges between the unlearning phase, which selectively forgets the undesirable information in the model through weight reinitialization in a data-dependent manner, and the relearning phase, which emphasizes learning on generalizable features. We show that our training paradigm provides consistent performance gains across datasets in both classification and few-shot settings. We further show that it leads to more robust and well-calibrated models.",
        "authors": "V. R. T. Ramkumar, E. Arani, B. Zonooz",
        "keywords": [
            "data streaming",
            "warm-start",
            "Learn",
            "Unlearn",
            "and Relearn"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=WN1O2MJDST",
        "pdf_src": "https://api2.openreview.net/pdf/9e2aa4fad3262f9f8a8aa9f455bc348f755b911d.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses challenges associated with deep neural network (DNN) training when dealing with incremental data arriving in pieces rather than all at once.\n\nResearch Question: How should one train DNNs efficiently while maintaining good generalization capabilities given the arrival of data incrementally?\n\nMethodology: To address this question, they propose Learn, Unlearn, and Relearn (LURE), where:\n\n1. **Learn**: The system updates its weights based on newly arrived data.\n2. **Unlearn**: Periodically, certain learned knowledge becomes irrelevant due to changes introduced by newer data; hence, the system selectively forgets these details via weight reinitialization dependent on the current dataset state without discarding previously acquired useful knowledge entirely.\n3. **Relearn**: After forgetting what's no longer relevant (\"unlearning\"), there’s space created within the model architecture allowing room for fresh learnings focused on more generalizable aspects of the task.\n\nMain Contributions:\n- They present LURE—a novel approach balancing the trade-off among continual adaptation capability against computational cost during incremental learning tasks involving DNNs.\n- Empirical validation demonstrates significant improvements not only regarding consistency but also concerning enhanced robustness along with better calibration properties compared to existing methods like warm-starting or full retraining approaches under various experimental setups including classification problems even if seen infrequently (few-shot).",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Mean-field analysis for heavy ball methods: Dropout-stability, connectivity, and global convergence",
        "abstract": "The stochastic heavy ball method (SHB), also known as stochastic gradient descent (SGD) with Polyak's momentum, is widely used in training neural networks. However, despite the remarkable success of such algorithm in practice, its theoretical characterization remains limited. In this paper, we focus on neural networks with two and three layers and provide a rigorous understanding of the properties of the solutions found by SHB: \\emph{(i)} stability after dropping out part of the neurons, \\emph{(ii)} connectivity along a low-loss path, and \\emph{(iii)} convergence to the global optimum.\nTo achieve this goal, we take a mean-field view and relate the SHB dynamics to a certain partial differential equation in the limit of large network widths. This mean-field perspective has inspired a recent line of work focusing on SGD while, in contrast, our paper considers an algorithm with momentum. More specifically, after proving existence and uniqueness of the limit differential equations, we show convergence to the global optimum and give a quantitative bound between the mean-field limit and the SHB dynamics of a finite-width network. Armed with this last bound, we are able to establish the dropout-stability and connectivity of SHB solutions. ",
        "authors": "D. Wu, V. Kungurtsev, M. Mondelli",
        "keywords": [
            "stochastic heavy ball method",
            "neural networks",
            "global optimum"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=gZna3IiGfl",
        "pdf_src": "https://api2.openreview.net/pdf/3768378279bae25178b38a917b7e1eae0555cc2c.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe stochastic heavy ball method (SHB), or stochastic gradient descent (SGD) with Polyak's momentum, is commonly utilized for training neural networks due to its effectiveness; however, there have been few theoretical analyses regarding it.\n\nResearch Problem:\nThis study aims at providing a comprehensive theoretical analysis into the behavior exhibited by the SHB when applied within neural networks that consist of only two or three layers - particularly concerning their stability upon neuron dropout, connectivity over paths minimizing loss, and convergence towards the global optimal solution.\n\nMethods:\nThe researchers adopt a mean-field approach which relates the SHB dynamics under consideration not just to typical neural networks but more precisely leads them toward a particular type of partial differential equation expressed through the infinite width approximation scenario where the number of neurons becomes very high without altering the overall structure significantly enough so they can be considered 'mean field'.\n\nMain Contributions:\nThe key contributions include:\n\n1. Proving both the existence and uniqueness of the limiting differential equations associated with the SHB dynamics across these types of neural networks;\n2. Demonstrating how the SHB converges to the global optimal solution via establishing convergence results quantitatively comparing the mean-field limit against actual SHB dynamics from finite-width networks;\n3. Providing insights about the robustness features like dropout stability – i.e., analyzing whether removing some neurons affects performance negatively—or connectivity patterns leading up to lower loss values during optimization processes using SHB algorithms based on momentum terms introduced here instead solely relying on gradients alone seen before in other works related mainly around SGD without momentum considerations included therein.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "SPADE: Semi-supervised Anomaly Detection under Distribution Mismatch",
        "abstract": "Semi-supervised anomaly detection is a common problem, as often the datasets containing anomalies are partially labeled. We propose a canonical framework: Semi-supervised Pseudo-labeler Anomaly Detection with Ensembling (SPADE) that isn't limited by the assumption that labeled and unlabeled data come from the same distribution. Indeed, the assumption is often violated in many applications -- for example, the labeled data may contain only anomalies unlike unlabeled data, or unlabeled data may contain different types of anomalies, or labeled data may contain only `easy-to-label' samples. SPADE utilizes an ensemble of one class classifiers as the pseudo-labeler to improve the robustness of pseudo-labeling with distribution mismatch. Partial matching is proposed to automatically select the critical hyper-parameters for pseudo-labeling without validation data, which is crucial with limited labeled data. SPADE shows state-of-the-art semi-supervised anomaly detection performance across a wide range of scenarios with distribution mismatch in both tabular and image domains. In some common real-world settings such as model facing new types of unlabeled anomalies, SPADE outperforms the state-of-the-art alternatives by 5% AUC in average.",
        "authors": "J. Yoon, K. Sohn, C. Li, et.al",
        "keywords": [
            "distribution mismatch",
            "semi-supervised learning",
            "anomaly detection"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=JwDpZSv3yz",
        "pdf_src": "https://api2.openreview.net/pdf/540757b56d44357892e5683131b2c85e0c931009.pdf",
        "Code_src": "",
        "Introduction": "Background: Semi-supervised anomaly detection aims at identifying outliers within datasets where only a small portion of examples have labels.\n\nResearch Problem: The challenge lies in handling cases when the distributions between labeled and unlabeled data do not match—a situation frequently encountered but rarely accounted for due to assumptions made during training.\n\nMethodology: This paper introduces \"Semi-supervised Pseudo-labeler Anomaly Detection with Ensembling\" (SPADE), addressing this issue through:\n\n1. Using ensembles of one-class classifiers (\"pseudo-labelers\")—which can be trained on large amounts of unlabeled data—to generate pseudo-labels.\n2. Implementing partial matching techniques—which dynamically adjust hyperparameters based on the similarity metrics computed among instances—for selecting suitable pseudo-label candidates efficiently using fewer labeled examples than traditional methods require.\n\nMain Contributions:\n- SPADE does away with the restrictive assumption regarding the distributional alignment required typically assumed under semi-supervised learning frameworks—it's applicable even if there’s significant discrepancy between labeled and unlabeled data.\n- It significantly improves upon existing models dealing with distribution mismatches found commonly in various practical scenarios including those involving novel anomalies unseen before while maintaining high accuracy levels measured via Area Under Curve (AUC).\n- Demonstrates its effectiveness broadly over multiple datasets ranging from tabular to images, showcasing consistent improvements compared to other top-performing semi-supervised anomaly detection approaches available today.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "DEUP: Direct Epistemic Uncertainty Prediction",
        "abstract": "Epistemic Uncertainty is a measure of the lack of knowledge of a learner which diminishes with more evidence. While existing work focuses on using the variance of the Bayesian posterior due to parameter uncertainty as a measure of epistemic uncertainty, we argue that this does not capture the part of lack of knowledge induced by model misspecification. We discuss how the excess risk, which is the gap between the generalization error of a predictor and the Bayes predictor, is a sound measure of epistemic uncertainty which captures the effect of model misspecification. We thus propose a principled framework for directly estimating the excess risk by learning a secondary predictor for the generalization error and subtracting an estimate of aleatoric uncertainty, i.e., intrinsic unpredictability. We discuss the merits of this novel measure of epistemic uncertainty, and highlight how it differs from variance-based measures of epistemic uncertainty and addresses its major pitfall. Our framework, Direct Epistemic Uncertainty Prediction (DEUP) is particularly interesting in interactive learning environments, where the learner is allowed to acquire novel examples in each round. Through a wide set of experiments, we illustrate how existing methods in sequential model optimization can be improved with epistemic uncertainty estimates from DEUP, and how DEUP can be used to drive exploration in reinforcement learning. We also evaluate the quality of uncertainty estimates from DEUP for probabilistic image classification and predicting synergies of drug combinations.",
        "authors": "S. Lahlou, M. Jain, H. Nekoei, et.al",
        "keywords": [
            "epistemic uncertainty",
            "excess risk",
            "Direct Epistemic Uncertainty Prediction"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=eGLdVRvvfQ",
        "pdf_src": "https://api2.openreview.net/pdf/b18af1bcdc49ed6a705ffd943b5c6fcc5cc76e91.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses about \"epistemic uncertainty,\" referring to the degree of ignorance or lack of understanding held by learners regarding their predictions.\n\nResearch Problem: Existing works have focused on measuring epistemic uncertainty through the variance of Bayesian posterior distributions caused by parameter uncertainties; however, these approaches do not account for the component of missing information resulting from model mis-specifications.\n \nMethodology: The authors suggest employing 'excess risk' - the difference among the prediction errors made by a given predictor along with the Bayes optimal predictor – as another reliable metric representing epistemic uncertainty because it encompasses both effects related to parameter uncertainty & model misspecification. They introduce a new method called Direct Epistemic Uncertainty Prediction (DEUP), involving training supplementary predictors specifically designed to forecast the generalization mistakes while simultaneously deducting estimations of aleatoric uncertainty (random variability). \n\nMain Contributions:\n1. Propose a principle-driven approach towards calculating excess risk without relying solely on variance metrics;\n2. Introduce DEUP, offering insights into improving predictive models within interactive settings like iterative data collection scenarios during machine learning tasks;\n3. Demonstrate improvements over conventional techniques when utilizing epistemic uncertainty measurements derived from DEUP across various domains such as reinforcement learning and probabilistic inference problems including those concerning medical diagnostics via drug synergy prediction.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Guaranteed Discovery of Control-Endogenous Latent States with Multi-Step Inverse Models",
        "abstract": "In many sequential decision-making tasks, the agent is not able to model the full complexity of the world, which consists of multitudes of relevant and irrelevant information. For example, a person walking along a city street who tries to model all aspects of the world would quickly be overwhelmed by a multitude of shops, cars, and people moving in and out of view, each following their own complex and inscrutable dynamics.  Is it possible to turn the agent's firehose of sensory information into a minimal latent state that is both necessary and sufficient for an agent to successfully act in the world? We formulate this question concretely, and propose the Agent Control-Endogenous State Discovery algorithm (AC-State), which has theoretical guarantees and is practically demonstrated to discover the minimal control-endogenous latent state which contains all of the information necessary for controlling the agent, while fully discarding all irrelevant information.    This algorithm consists of a multi-step inverse model (predicting actions from distant observations) with an information bottleneck.  AC-State enables localization, exploration, and navigation without reward or demonstrations.  We demonstrate the discovery of the control-endogenous latent state in three domains: localizing a robot arm with distractions (e.g., changing lighting conditions and background), exploring a maze alongside other agents, and navigating in the Matterport house simulator.  ",
        "authors": "A. Lamb, R. Islam, Y. Efroni, et.al",
        "keywords": [
            "control-endogenous latent state",
            "Agent Control-Endogenous State Discovery algorithm (AC-State)",
            "multi-step inverse model"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=TNocbXm5MZ",
        "pdf_src": "https://api2.openreview.net/pdf/df415f6a549301a0a3b22367b72fd3c44eab4ee7.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper addresses the challenge faced by agents engaged in sequential decision-making processes where they must navigate through vast amounts of data - some pertinent yet overwhelming due to its volume – such as visual input from cameras on city streets.\n\nResearch Question:\nCan we distill these high-dimensional sensory inputs down to essential low-dimensional representations known as latent states?\n\nMethodology:\nTo tackle this problem, researchers introduce the Agent Control-Endogenous State Discovery (AC-State) algorithm—a method designed specifically within reinforcement learning frameworks—aimed at discovering the most compact set of latent variables required solely for effective action-taking (\"control-endogenous\" states).\n\nMain Contributions:\n1. Theoretical Guarantees: The proposed algorithm comes equipped with mathematical justifications ensuring effectiveness.\n2. Practical Demonstration: It was tested across various environments including robotics arms amidst varying environmental factors like light changes; cooperative navigation among multiple agents solving mazes together; autonomous navigation using simulated indoor spaces—all showing successful performance based on learned control-endogenous latent states rather than relying on external rewards or guidance during training phases ('zero-shot' learning).",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Beyond Intuition: Rethinking Token Attributions inside Transformers",
        "abstract": "The multi-head attention mechanism, or rather the Transformer-based models have always been under the spotlight, not only in the domain of text processing, but also for computer vision. Several works have recently been proposed around exploring the token attributions along the intrinsic decision process. However, the ambiguity of the expression formulation can lead to an accumulation of error, which makes the interpretation less trustworthy and less applicable to different variants. In this work, we propose a novel method to approximate token contributions inside Transformers. We start from the partial derivative to each token, divide the interpretation process into attention perception and reasoning feedback with the chain rule and explore each part individually with explicit mathematical derivations. In attention perception, we propose the head-wise and token-wise approximations in order to learn how the tokens interact to form the pooled vector. As for reasoning feedback, we adopt a noise-decreasing strategy by applying the integrated gradients to the last attention map. Our method is further validated qualitatively and quantitatively through the faithfulness evaluations across different settings: single modality (BERT and ViT) and bi-modality (CLIP), different model sizes (ViT-L) and different pooling strategies (ViT-MAE) to demonstrate the broad applicability and clear improvements over existing methods.",
        "authors": "J. Chen, X. Li, L. Yu, et.al",
        "keywords": [
            "Transformer",
            "Token Attribution",
            "Multi-modal"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=rm0zIzlhcX",
        "pdf_src": "https://api2.openreview.net/pdf/6f1ba0a400419825b2a39e2cc1e1f316b8a5ec09.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThis paper discusses the multi-head attention mechanism used extensively in both natural language processing tasks using transformer-based models as well as visual recognition problems within computer vision.\n\nResearch Problem:\nRecent studies on token attribution during the decision-making processes inherent in transformers are ambiguous due to complex expressions that may accumulate errors leading to unreliable interpretations especially when applied to various transformer architectures.\n\nMethodology:\nTo tackle these issues effectively without introducing additional complexity while still providing accurate interpretations about token contributions throughout the transformer's internal workings, they introduce a new approach involving two main components - attention perception and reasoning feedback.\n1. Attention Perception: They break down the task at hand mathematically via partial derivatives per token allowing them to decompose it step-by-step; then apply head-wise and token-wise approximations so one understands better interactions between individual tokens forming pooled vectors after attending to features extracted elsewhere like images or texts respectively depending upon whether dealing with multimodal scenarios where both types exist simultaneously (e.g., CLIP).\n2. Reasoning Feedback: This involves reducing uncertainty surrounding final decisions made based solely off initial attention maps before any subsequent computations take place towards predictions etcetera – achieved here utilizing Integrated Gradients algorithmic technique specifically designed toward mitigating bias present therein naturally occurring whenever gradient computation occurs irrespective if inputs were simple scalar values or more complex ones such as those encountered typically found amongst neural networks nowadays employed heavily today including deep learning architectures among others too!\n\nMain Contributions:\nTheir primary contribution lies mainly focusing on developing an interpretable framework capable approximating token contributions accurately whilst maintaining simplicity overall compared traditional approaches currently available out there already implemented successfully thus far albeit lacking reliability concerning interpretability aspect particularly pertinent given current trends emphasizing explainable artificial intelligence systems increasingly demanded widely recognized importance lately days nowdays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays nowadays",
        "Topic": "Vision Transformer"
    },
    {
        "title": "Signed Graph Neural Networks: A Frequency Perspective",
        "abstract": "Graph convolutional networks (GCNs) and its variants are designed for unsigned graphs containing only positive links. Many existing GCNs have been derived from the spectral domain analysis of signals lying over (unsigned) graphs and in each convolution layer they perform low-pass filtering of the input features followed by a learnable linear transformation. Their extension to signed graphs with positive as well as negative links imposes multiple issues including computational irregularities and ambiguous frequency interpretation, making the design of computationally efficient low pass filters challenging. In this paper, we address these issues via spectral analysis of signed graphs and propose two different signed graph neural networks, one keeps only low-frequency information and one also retains high-frequency information. We further introduce magnetic signed Laplacian and use its eigendecomposition for spectral analysis of directed signed graphs. We test our methods for node classification and link sign prediction tasks on signed graphs and achieve state-of-the-art performances.",
        "authors": "R. Singh, Y. Chen",
        "keywords": [
            "signed graph neural networks",
            "spectral analysis",
            "magnetic signed Laplacian"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=RZveYHgZbu",
        "pdf_src": "https://api2.openreview.net/pdf/a2020082fa719420b25b5fdfb13202a6ee26b225.pdf",
        "Code_src": "",
        "Introduction": "Background: Graph convolutional networks (GCNs) were originally developed for unsigned graphs that contain only positive edges or links between nodes but cannot handle signed graphs which may include both positive and negative edges.\n\nResearch Problem: Extending GCNs to work effectively on signed graphs presents several challenges such as computational irregularities due to the presence of negative weights leading to non-standard graph convolutions; ambiguity regarding how to interpret frequencies when dealing with mixed signs since traditional GCN's low-pass filter operation is not applicable here.\n \nMethods: The authors tackle these problems through spectral analysis techniques adapted specifically for signed graphs using their proposed Magnetic Signed Laplacian operator along with eigendecomposition approach allowing them to analyze properties like connectivity patterns within directed signed graphs more accurately than before. They then develop two types of architectures based upon whether just low-frequency components should be considered ('LowPass') or if higher frequencies too need consideration ('HighFreq'). \n\nMain Contributions: This research introduces novel approaches towards designing effective algorithms capable of processing complex relationships found across signed graphs while maintaining computational efficiency despite inherent difficulties posed by incorporating edge polarity into network operations. Specifically, it provides new insights about what kind of spectral characteristics can help classify nodes efficiently even under noisy conditions present often times during real-world applications involving social networks where edge sign could indicate trustworthiness amongst individuals among others scenarios requiring understanding directional dependencies represented by signed graphs.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "KRADA: Known-region-aware Domain Alignment for Open-set Domain Adaptation in Semantic Segmentation",
        "abstract": "In semantic segmentation, we aim to train a pixel-level classifier to assign category labels to all pixels in an image, where labeled training images and unlabeled test images are from the same distribution and share the same label set. However, in an open world, the unlabeled test images probably contain unknown categories and have different distributions from the labeled images. Hence, in this paper, we consider a new, more realistic, and more challenging problem setting where the pixel-level classifier has to be trained with labeled images and unlabeled open-world images—we name it open world semantic segmentation (OSS). In OSS, the trained classifier is expected to identify unknown-class pixels and classify known-class pixels well. To solve OSS, we first investigate which distribution that unknown-class pixels obey. Then, motivated by the goodness-of-fit test, we use statistical measurements to show how a pixel fits the distribution of an unknown class and select highly-fitted pixels to form the unknown region in each test image. Eventually, we propose an end-to-end learning framework, known-region-aware domain alignment (KRADA), to distinguish unknown classes while aligning the distributions of known classes in labeled and unlabeled open-world images. The effectiveness of KRADA has been verified on two synthetic tasks and one COVID-19 segmentation task.",
        "authors": "C. Zhou, F. Liu, C. Gong, et.al",
        "keywords": [
            "open world semantic segmentation",
            "pixel-level classifier",
            "domain alignment"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=5II12ypVQo",
        "pdf_src": "https://api2.openreview.net/pdf/14094144fd7ff577591e7313a442b097a47ce09c.pdf",
        "Code_src": "",
        "Introduction": "Background: Semantic segmentation aims to assign category labels to every pixel within an image using a pixel-level classifier based solely on labeled data.\n\nResearch Problem: Traditional semantic segmentation methods assume access only to labeled datasets; however, they may encounter difficulties when applied outside their training scope due to variations between labeled and unlabeled datasets or inclusion of previously unseen categories (\"unknown\" classes).\n\nMethods: This study introduces \"open world semantic segmentation\" (OSS), addressing the challenge posed above—training classifiers for both seen and unseen categories across potentially disparate distributions without prior knowledge about these differences.\n1. Distribution Analysis - Investigating the underlying distribution patterns among pixels belonging to unknown classes helps understand if there's any discernible pattern unique enough to separate them effectively during classification.\n2. Pixel Selection Strategy - Employing goodness-of-fit tests along with statistical metrics allows selecting those pixels most likely to belong to an unknown class as representative samples forming the 'unknown' regions per test image.\n3. Domain Alignment Framework - Proposes KRADA—a novel approach integrating known-region awareness into domain adaptation techniques aiming at aligning distributions specifically around known classes found commonly throughout labeled and unlabeled open-world datasets whilst still identifying and handling unknown classes appropriately.\n\nMain Contributions:\n- Formulates OSS—an expanded real-world scenario beyond conventional semantic segmentation challenges;\n- Develops a strategy leveraging statistical tools like goodness-of-fit tests combined with pixel selection algorithms enabling identification of potential unknown class areas accurately;\n- Introduces KRADA, providing a comprehensive solution combining domain alignment methodologies tailored towards preserving consistency amongst known classes amidst diverse contexts including open-world scenarios involving unlabelled imagery containing unknown categories not present in initial training sets.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Understanding and Simplifying Architecture Search in Spatio-Temporal Graph Neural Networks",
        "abstract": "Compiling together spatial and temporal modules via a unified framework, Spatio-Temporal Graph Neural Networks (STGNNs) have been popularly used in the multivariate spatio-temporal forecasting task, e.g. traffic prediction. After the numerous propositions of manually designed architectures, researchers show interest in the Neural Architecture Search (NAS) of STGNNs. Existing methods suffer from two issues: (1) hyperparameters like learning rate, channel size cannot be integrated into the NAS framework, which makes the model evaluation less accurate, potentially misleading the architecture search (2) the current search space, which basically mimics Darts-like methods, is too large for the search algorithm to find a sufficiently good candidate. In this work, we deal with both issues at the same time. We first re-examine the importance  and transferability of the training hyperparameters to ensure a fair and fast comparison. Next, we set up a framework that disentangles architecture design into three disjoint angles according to how spatio-temporal representations flow and transform in architectures, which allows us to understand the behavior of architectures from a distributional perspective. This way, we can obtain good guidelines to reduce the STGNN search space and find state-of-the-art architectures by simple random search. As an illustrative example, we combine these principles with random search which already significantly outperforms both state-of-the-art hand-designed models and recently automatically searched ones.",
        "authors": "Z. Xu, Q. Yao, Y. Li, et.al",
        "keywords": [
            "Spatio-Temporal Graph Neural Networks",
            "Neural Architecture Search",
            "Hyperparameter Optimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=4jEuiMPKSF",
        "pdf_src": "https://api2.openreview.net/pdf/05970c78830ad43a43201e932fc59886ff00cf47.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses the use of Spatio-Temporal Graph Neural Networks (STGNNs), particularly their application in multivariate spatio-temporal forecasting tasks such as traffic prediction.\n\nResearch Problem:\nResearchers are increasingly interested in using Neural Architecture Search (NAS) techniques on STGNNs; however, existing NAS methods face challenges including:\n\n- The integration of hyperparameters within the NAS framework.\n- A limited understanding or exploration due to overly complex search spaces.\n\nMethods:\nTo address these problems, the authors propose several key contributions:\n\n- They reassess the significance & transferability of training hyperparameters ensuring more accurate comparisons during NAS processes without leading to biased results.\n  \n- They introduce a novel framework breaking down architectural design based on different aspects related to the flow and transformation of spatio-temporal representations through networks - allowing them to analyze architectures' behaviors probabilistically rather than deterministically.\n\nMain Contributions:\nBy combining insights gained above along with a simplified approach utilizing random search algorithms – they demonstrate improved performance over previous manual designs AND other recent automatic searches conducted specifically tailored towards finding optimal architectures suitable for STGNN applications involving multi-dimensional data points across various dimensions simultaneously capturing both spatial and temporal information effectively improving predictive accuracy further enhancing efficiency while reducing computational complexity involved therein.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Tailoring to the Tails: Risk Measures for Fine-Grained Tail Sensitivity",
        "abstract": "Expected risk minimization (ERM) is at the core of many machine learning systems. This means\nthat the risk inherent in a loss distribution is summarized using a single number - its average.\nIn this paper, we propose a general approach to construct risk measures which exhibit a\ndesired tail sensitivity and may replace the expectation operator in ERM. Our method\nrelies on the specification of a reference distribution with a desired tail behaviour, which is\nin a one-to-one correspondence to a coherent upper probability. Any risk measure, which is\ncompatible with this upper probability, displays a tail sensitivity which is finely tuned to the\nreference distribution. As a concrete example, we focus on divergence risk measures based\non f-divergence ambiguity sets, which are a widespread tool used to foster distributional\nrobustness of machine learning systems. For instance, we show how ambiguity sets based on\nthe Kullback-Leibler divergence are intricately tied to the class of subexponential random\nvariables. We elaborate the connection of divergence risk measures and rearrangement\ninvariant Banach norms.",
        "authors": "C. Fröhlich, R. Williamson",
        "keywords": [
            "tail sensitivity",
            "coherence",
            "subexponential"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=UntUoeLwwu",
        "pdf_src": "https://api2.openreview.net/pdf/7c7b2a2807043f09e2e3248bbd110b1b3c83ec9e.pdf",
        "Code_src": "",
        "Introduction": "Background: Expected risk minimization (ERM) plays a central role in various machine learning systems by summarizing the risks within a loss distribution through an average value.\n\nResearch Question: The study aims to develop new approaches for constructing risk measures that possess desirable tail sensitivities while potentially replacing the use of the expectation operator during ERM processes.\n\nMethodology: To achieve their goal, researchers introduce a novel framework where they specify a reference distribution characterized by preferred tail behavior—a distribution that corresponds uniquely to a coherent upper probability function. They argue any compatible risk measure will inherit the tail properties from the reference distribution due to this one-to-one mapping between distributions and probabilities under consideration.\n\nMain Contributions:\n1. **Tail Sensitivity**: By aligning themselves closely around such reference distributions or coherent upper probabilities, these newly constructed risk measures can be fine-tuned regarding their tail behaviors—making them particularly sensitive towards outliers compared to traditional expected-risk measures like variance or mean squared error.\n2. **Divergence Risk Measures**: Specifically focusing on divergence-based risk measures derived from f-divergences over ambiguity sets—their work highlights connections among different classes of subexponential random variables when considering KL divergence as part of ambiguity set construction; \n3. **Banach Norms**: Furthermore, there's also discussion about how rearrangement invariant Banach norms relate back into this context providing another perspective linking theoretical frameworks together more explicitly than before seen in literature related specifically here concerning robustness via divergence metrics applied throughout machine learning theory practice today.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Finite-Time Analysis of Decentralized Single-Timescale Actor-Critic",
        "abstract": "Decentralized Actor-Critic (AC) algorithms have been widely utilized for multi-agent reinforcement learning (MARL) and have achieved remarkable success. Apart from its empirical success, the theoretical convergence property of decentralized AC algorithms is largely unexplored. Most of the existing finite-time convergence results are derived based on either double-loop update or two-timescale step sizes rule, and this is the case even for centralized AC algorithm under a single-agent setting. In practice, the *single-timescale* update is widely utilized, where actor and critic are updated in an alternating manner with step sizes being of the same order. In this work, we study a decentralized *single-timescale* AC algorithm. Theoretically, using linear approximation for value and reward estimation, we show that the algorithm has sample complexity of $\\tilde{\\mathcal{O}}(\\varepsilon^{-2})$ under Markovian sampling, which matches the optimal complexity with a double-loop implementation (here, $\\tilde{\\mathcal{O}}$ hides a logarithmic term).  When we reduce to the single-agent setting, our result yields new sample complexity for centralized AC using a single-timescale update scheme. The central to establishing our complexity results is *the hidden smoothness of the optimal critic variable* we revealed. We also provide a local action privacy-preserving version of our algorithm and its analysis. Finally, we conduct experiments to show the superiority of our algorithm over the existing decentralized AC algorithms.",
        "authors": "Q. Luo, X. Li",
        "keywords": [
            "sample complexity",
            "decentralized actor-critic algorithms",
            "single-timescale update"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=KQRv0O8iW4",
        "pdf_src": "https://api2.openreview.net/pdf/95d549bc0c0b452f50c3a7b3221605d0e7f6a9f6.pdf",
        "Code_src": "",
        "Introduction": "Background: Decentralized Actor-Critic (AC) algorithms play a crucial role in Multi-Agent Reinforcement Learning (MARL), but their theoretical convergence properties remain largely unknown.\n\nResearch Problem: This paper aims to explore the theoretical convergence property of decentralized single-timescale AC algorithms by studying them theoretically when agents make decisions sequentially according to stochastic policies rather than deterministic ones as assumed previously.\n \nMethodology: By employing linear approximation techniques within the framework of Markovian sampling settings, they analyze the performance characteristics such as sample complexity associated with these algorithms while considering both decentralized and centralized scenarios involving different types of updates like double-loop vs single-timescale approaches among others.\n\nMain Contributions:\n1. They establish novel lower bounds on sample complexities for decentralized single-timescale AC algorithms achieving $\\tilde{\\mathcal{O}}(\\varepsilon^{-2})$, matching those obtained through double-loop implementations despite hiding logarithmic terms inside $\\tilde{\\mathcal{O}}$. Moreover, extending into single-agent settings reveals fresh insights about centralized AC's sample complexity utilizing similar update schemes;\n2. Their findings hinge upon uncovering \"hidden smoothness\" exhibited by certain optimal critic variables during iterative processes leading up towards convergence states;\n3. Additionally, proposed modifications ensure preservation of local action privacy throughout execution phases without compromising overall system efficacy; \n4. Experimental validation demonstrates superior performance outcomes compared against other state-of-the-art decentralized AC counterparts across various benchmarks illustrating practical applicability alongside theoretical contributions outlined above.",
        "Topic": "Sample Efficiency in Reinforcement Learning"
    },
    {
        "title": "Differentially Private Fréchet Mean on the Manifold of Symmetric Positive Definite (SPD) Matrices with log-Euclidean Metric",
        "abstract": "Differential privacy has become crucial in the real-world deployment of statistical and machine learning algorithms with rigorous privacy guarantees. The earliest statistical queries, for which differential privacy mechanisms have been developed, were for the release of the sample mean. In Geometric Statistics, the sample Fréchet mean represents one of the most fundamental statistical summaries, as it generalizes the sample mean for data belonging to nonlinear manifolds. In that spirit, the only geometric statistical query for which a differential privacy mechanism has been developed, so far, is for the release of the sample Fréchet mean: the \\emph{Riemannian Laplace mechanism} was recently proposed to privatize the Fréchet mean on complete Riemannian manifolds. In many fields, the manifold of Symmetric Positive Definite (SPD) matrices is used to model data spaces, including in medical imaging where privacy requirements are key. We propose a novel, simple and fast mechanism - the \\emph{tangent Gaussian mechanism} - to compute a differentially private Fréchet mean on the SPD manifold endowed with the log-Euclidean Riemannian metric.  We show that our new mechanism has significantly better utility and is computationally efficient --- as confirmed by extensive experiments.",
        "authors": "S. Utpala, P. Vepakomma, N. Miolane",
        "keywords": [
            "differential privacy",
            "Fréchet mean",
            "symmetric positive definite"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=mAx8QqZ14f",
        "pdf_src": "https://api2.openreview.net/pdf/fed1122464889e28578bd34e783101712732f616.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses the importance of differential privacy in deploying statistical and machine learning algorithms while ensuring strict privacy guarantees.\n\nResearch Problem:\nThe problem addressed here concerns the development of a differential privacy mechanism specifically designed for computing the sample Fréchet mean from datasets distributed across nonlinear manifolds using the log-Euclidean Riemannian metric – particularly focusing on the symmetric positive definite (SPD) matrix manifold commonly utilized due to its relevance within various domains such as medical imaging.\n \nMethodology:\nTo address this issue, they introduce an innovative differential privacy mechanism called the \"tangent Gaussian mechanism.\" This method computes a differentially private Fréchet mean efficiently over the SPD manifold equipped with the log-Euclidean Riemannian metric without compromising accuracy or computational efficiency compared to existing methods like the Riemannian Laplace mechanism.\n\nMain Contributions:\nThe main contributions include proposing a novel, straightforward, and rapid algorithmic approach (\"tangent Gaussian mechanism\") capable of calculating a differentially private Fréchet mean effectively; demonstrating through comprehensive experimental evidence how their mechanism outperforms previous ones regarding both practicality and computational performance when applied under stringent differential privacy constraints related to the SPD manifold's geometry",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "SolidGen: An Autoregressive Model for Direct B-rep Synthesis",
        "abstract": "The Boundary representation (B-rep) format is the de-facto shape representation in computer-aided design (CAD) to model solid and sheet objects. Recent approaches to generating CAD models have focused on learning sketch-and-extrude modeling sequences that are executed by a solid modeling kernel in postprocess to recover a B-rep. In this paper we present a new approach that enables learning from and synthesizing B-reps without the need for supervision through CAD modeling sequence data. Our method SolidGen, is an autoregressive neural network that models the B-rep directly by predicting the vertices, edges, and faces using Transformer-based and pointer neural networks. Key to achieving this is our Indexed Boundary Representation that references B-rep vertices, edges and faces in a well-defined hierarchy to capture the geometric and topological relations suitable for use with machine learning. SolidGen can be easily conditioned on contexts e.g., class labels, images, and voxels thanks to its probabilistic modeling of the B-rep distribution. We demonstrate qualitatively, quantitatively, and through perceptual evaluation by human subjects that SolidGen can produce high quality, realistic CAD models.",
        "authors": "P. K. Jayaraman, J. G. Lambourne, N. Desai, et.al",
        "keywords": [
            "SolidGen",
            "Boundary Representation (B-rep)",
            "Autoregressive Neural Network"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ZR2CDgADRo",
        "pdf_src": "https://api2.openreview.net/pdf/5cdb35243d5577366d2ab99332f0cc247b2932cd.pdf",
        "Code_src": "",
        "Introduction": "Background: The Boundary representation (B-rep) format has become the standard way to represent shapes used in Computer-Aided Design (CAD). However, existing methods focus on learning how to generate these representations based on user-provided sketches or other input types.\n\nResearch Question: How might one learn to synthesize B-rep representations automatically given no direct training examples?\n\nMethod: This research introduces \"SolidGen,\" which learns to predict B-rep elements such as vertices, edges, and faces via an autoregressive neural network architecture informed by Transformer and Pointer Neural Networks techniques.\n \nMain Contributions:\n1. An innovative Indexed Boundary Representation system designed specifically so that it can reference vertices, edges, and faces hierarchically within the B-rep structure—allowing for more effective encoding of complex geometric and topological relationships essential for machine learning tasks involving 3D shapes.\n2. A novel probabilistic framework allowing SolidGen to condition predictions not just on abstract categories but also on specific contextual information like image inputs—all while maintaining generative capabilities over the entire space of possible B-rep structures.\n3. Demonstrations showing that SolidGen produces highly detailed, visually appealing CAD models when evaluated both objectively against metrics commonly employed in the field—and subjectively judged acceptable even under scrutiny",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Supervised Feature Selection with Neuron Evolution in Sparse Neural Networks",
        "abstract": "Feature selection that selects an informative subset of variables from data not only enhances the model interpretability and performance but also alleviates the resource demands. Recently, there has been growing attention in feature selection using neural networks. However, existing methods usually suffer from high computational costs when applied to high-dimensional datasets. In this paper, inspired by evolution processes, we propose a novel resource-efficient supervised feature selection method using sparse neural networks, named \"NeuroFS\". By gradually pruning the uninformative features from the input layer of a sparse neural network trained from scratch, NeuroFS derives an informative subset of features efficiently. By performing several experiments on $11$ low and high-dimensional real-world benchmarks of different types, we demonstrate that NeuroFS achieves the highest ranking-based score among the considered state-of-the-art supervised feature selection models. We will make the code publicly available on GitHub after acceptance of the paper.",
        "authors": "Z. Atashgahi, X. Zhang, N. Kichler, et.al",
        "keywords": [
            "sparse neural networks",
            "evolutionary computation",
            "feature selection"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=GcO6ugrLKp",
        "pdf_src": "https://api2.openreview.net/pdf/064f455340f5516286282a2cf59ad88d5df009f3.pdf",
        "Code_src": "",
        "Introduction": "Background: Feature selection is important for enhancing model interpretability and performance while reducing resource demands.\n\nResearch Problem: Existing feature selection methods are computationally expensive especially with high-dimensional datasets.\n \nMethod: Inspired by evolution processes, authors propose a novel resource-efficient supervised feature selection method called NeuroFS which uses sparse neural networks. The method involves gradually pruning uninformative features from the input layer during training.\n\nMain Contributions:\n- NeuroFS effectively reduces computation cost compared to other state-of-the-art supervised feature selection models across 11 benchmark datasets ranging between low and high dimensions.\n- Codebase made openly accessible via GitHub post-paper acceptance",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Controlling Neural Network Smoothness for Neural Algorithmic Reasoning",
        "abstract": "The modelling framework of neural algorithmic reasoning (Veličković & Blundell, 2021) postulates that a continuous neural network may learn to emulate the discrete reasoning steps of a symbolic algorithm. We investigate the underlying hypothesis in the most simple conceivable scenario – the addition of real numbers. Our results show that two layer neural networks fail to learn the structure of the task, despite containing multiple solutions of the true function within their hypothesis class. Growing the network’s width leads to highly complex error regions in the input space. Moreover, we find that the network fails to generalise with increasing severity i) in the training domain, ii) outside of the training domain but within its convex hull, and iii) outside the training domain’s convex hull. This behaviour can be emulated with Gaussian process regressors that use radial basis function kernels of decreasing length scale. Classical results establish an equivalence between Gaussian processes and infinitely wide neural networks. We demonstrate a tight linkage between the scaling of a network weights’ standard deviation and its effective length scale on a sinusoidal regression problem, suggesting simple modifications to control the length scale of the function learned by a neural network and, thus, its smoothness. This has important applications for the different generalisation scenarios suggested above, but it also suggests a partial remedy to the brittleness of neural network predictions as exposed by adversarial examples. We demonstrate the gains in adversarial robustness that our modification achieves on a standard classification problem of handwritten digit recognition. In conclusion, this work shows inherent problems of neural networks even for the simplest algorithmic tasks which, however, may be partially remedied through links to Gaussian processes.",
        "authors": "D. A. Klindt",
        "keywords": [
            "neural algorithmic reasoning",
            "generalization",
            "adversarial robustness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=JnsGy9uWtI",
        "pdf_src": "https://api2.openreview.net/pdf/7f33b9a3f77799657effa9a8e5a8913c2a2affd6.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper investigates whether neural networks are capable of learning the discrete reasoning steps required for solving algorithmic tasks using a continuous model.\n\nResearch Problem: Specifically, they examine if neural networks could learn how to add real numbers step-by-step just like humans do when performing arithmetic operations manually or symbolically.\n \nMethods: They conducted experiments where they trained multi-layer perceptrons (MLPs) without any prior knowledge about the mathematical properties involved; these MLPs were expected to discover patterns themselves during training sessions while being able to generalize beyond what was directly observed from data points alone - something classical algorithms would not require since they inherently know all possible inputs beforehand due to their deterministic nature.\n\nMain Contributions:\n- The study reveals that even though MLPs have many potential solutions available inside their hypothesis classes after sufficient epochs pass by yet still cannot solve such basic arithmetic tasks successfully unless additional regularization techniques applied alongside them help guide towards correct behavior;\n- It further highlights issues related with overfitting especially concerning wider networks leading into more complex error regions across input spaces compared smaller counterparts;\n- Demonstrates empirical evidence connecting weight variance scaling adjustments made via Gaussian Process models' hyperparameters controlling kernel lengthscales back onto neural networks allowing us fine-tune desired smoothness characteristics whilst improving performance against adversarial attacks encountered elsewhere",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Bayesian Causal Bandits with Backdoor Adjustment Prior",
        "abstract": "The causal bandit problem setting is a sequential decision-making framework where actions of interest correspond to interventions on variables in a system assumed to be governed by a causal model. The underlying causality may be exploited when investigating actions in the interest of optimizing the yield of the reward variable. Most existing approaches assume prior knowledge of the underlying causal graph, which is in practice restrictive and often unrealistic. In this paper, we develop a novel Bayesian framework for tackling causal bandit problems that does not rely on possession of the causal graph, but rather simultaneously learns the causal graph while exploiting causal inferences to optimize the reward. Our methods efficiently utilize joint inferences from interventional and observational data in a unified Bayesian model constructed with intervention calculus and causal graph learning. For the implementation of our proposed methodology in the discrete distributional setting, we derive an approximation of the sampling variance of the backdoor adjustment estimator. In the Gaussian setting, we characterize the interventional variance with intervention calculus and propose a simple graphical criterion to share information between arms. We validate our proposed methodology in an extensive empirical study, demonstrating compelling cumulative regret performance against state-of-the-art standard algorithms as well as optimistic implementations of their causal variants that assume strong prior knowledge of the causal structure.",
        "authors": "J. Huang, Q. Zhou",
        "keywords": [
            "causal bandit",
            "Bayesian framework",
            "intervention calculus"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=sMsGv5Kfm3",
        "pdf_src": "https://api2.openreview.net/pdf/3a496ce1cec740746079597f35c9ae8d75a3ecce.pdf",
        "Code_src": "",
        "Introduction": "Background: Causal bandit problems are a type of sequential decision-making framework used extensively across various fields such as economics or medicine research involving optimization tasks based on rewards. These frameworks involve making decisions about how to intervene within systems driven by complex causal relationships without having access to full details regarding these relationships.\n\nResearch Problem: Existing solutions typically require knowing beforehand what the causal graph looks like – however obtaining complete knowledge isn't feasible due to practical constraints; hence there's need for developing new methodologies capable of handling uncertainty around causal structures during action selection processes aimed at maximizing returns.\n\nMethodology: This work introduces a Bayesian approach designed specifically towards solving causal bandit problems under scenarios lacking pre-knowledge concerning potential causal graphs involved therein. It incorporates both observational and interventional data into one comprehensive Bayesian model built upon principles derived from intervention calculus along with techniques employed toward inferring causal relations amongst variables concerned hereunder. \n\nMain Contributions: \n1. A novel Bayesian framework developed independently from any assumption made about specific causal graphs governing observed phenomena.\n2. An algorithmic solution tailored especially suited for discrete distributions wherein it approximates sampling variances pertinent to estimators utilized known as \"backdoor adjustments.\"\n3. Novel insights gained through application of intervention calculus allowing characterization of interventional variability among different options available within each arm (or subset), leading further development towards sharing relevant information amongst them effectively.\n4. Extensive empirical validation conducted comparing its efficacy vis-à-vis other top-performing algorithms currently available alongside those leveraging overly optimistic assumptions related to causal structures' complexity upfront.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Target Propagation via Regularized Inversion for Recurrent Neural Networks",
        "abstract": "Target Propagation (TP) algorithms compute targets instead of gradients along neural networks and propagate them backward in a way that is similar to yet different than gradient back-propagation (BP). The idea initially appeared as a perturbative alternative to BP that may improve gradient evaluation accuracy when training multi-layer neural networks (LeCun, 1985) and has gained popularity as a biologically plausible counterpart of BP. However, there have been many variations of TP, and a simple version of TP still remains worthwhile. Revisiting the insights of LeCun (1985) and Lee et al (2015), we present a simple version of TP based on regularized inversions of layers of recurrent neural networks. The proposed TP algorithm is easily implementable in a differentiable programming framework. We illustrate the algorithm with recurrent neural networks on long sequences in various sequence modeling problems and delineate the regimes in which the computational complexity of TP can be attractive compared to BP.",
        "authors": "V. Roulet, Z. Harchaoui",
        "keywords": [
            "TP",
            "Recurrent Neural Networks",
            "Regularized Inversions"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=vxyjTUPV24",
        "pdf_src": "https://api2.openreview.net/pdf/cc74c71dc3545f8202944f220f937fb996e9efba.pdf",
        "Code_src": "",
        "Introduction": "Background: Target Propagation (TP) algorithms are an alternative method for computing targets rather than gradients within neural networks during training.\n\nResearch Question: How does Target Propagation compare to Gradient Back-Propagation?\n\nMethod: This paper revisits previous works by LeCun (1985) and Lee et al (2015) regarding Target Propagation methods using Regularized Inversions applied across Recurrent Neural Networks.\n\nMain Contributions:\n1. A simplified version of Target Propagation was developed.\n2. The new TP algorithm uses Regularized Inversion techniques from Recurrent Neural Network layers allowing it to be implemented into differentiable programming frameworks more easily \n3. Demonstrated effectiveness through experiments involving Long Sequences in various Sequence Modeling Problems",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Risk Sensitive Dead-end Identification in Safety-Critical Offline Reinforcement Learning",
        "abstract": "In safety-critical decision-making scenarios being able to identify worst-case outcomes, or dead-ends is crucial in order to develop safe and reliable policies in practice. These situations are typically rife with uncertainty due to unknown or stochastic characteristics of the environment as well as limited offline training data. As a result, the value of a decision at any time point should be based on the distribution of its anticipated effects. We propose a framework to identify worst-case decision points, by explicitly estimating distributions of the expected return of a decision. These estimates enable earlier indication of dead-ends in a manner that is tunable based on the risk tolerance of the designed task. We demonstrate the utility of Distributional Dead-end Discovery (DistDeD) in a toy domain as well as when assessing the risk of severely ill patients in the intensive care unit reaching a point where death is unavoidable. We find that DistDeD significantly improves over prior discovery approaches, providing indications of the risk 10 hours earlier on average as well as increasing detection by 20%.",
        "authors": "T. W. Killian, S. Parbhoo, M. Ghassemi",
        "keywords": [
            "worst-case outcomes",
            "uncertainty",
            "Distributional Dead-end Discovery"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=oKlEOT83gI",
        "pdf_src": "https://api2.openreview.net/pdf/aeb0ddd2d08bf9e6b53f87ad9617ed6d451fc666.pdf",
        "Code_src": "",
        "Introduction": "Background: In critical decision-making environments such as autonomous driving systems for self-driving cars which require high reliability from their decisions it's important they can avoid \"dead ends\" - those actions leading towards an inevitable negative outcome regardless of further choices made. This requires considering not only individual possible outcomes but also how likely each one might occur.\n\nResearch Question: How do we design algorithms capable of identifying these potentially harmful dead-end paths early enough during decision-making processes?\n\nMethodology: The authors introduce a novel approach called Distributional Dead-end Detection (DistDeD), focusing on probabilistic reasoning about potential future states rather than deterministic predictions alone. They estimate the probability distribution of returns across all feasible decisions using machine learning techniques trained off historical datasets under varying conditions within specific domains like healthcare settings involving critically ill patients who may reach terminal stages without intervention.\n\nMain Contributions:\n1. A new methodological framework named Distributional Dead-end Detection (DistDeD) has been proposed.\n2. It uses Bayesian inference combined with Monte Carlo simulations allowing us to predict probabilities associated with different courses of action instead just single best guesses; \n3. Empirical validation shows this system outperforms existing methods both quantitatively improving prediction accuracy up until ten hours before actual undesirable events happen compared against baseline models while qualitatively detecting more instances overall by around twenty percent improvement rate",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Accelerated Quality-Diversity through Massive Parallelism",
        "abstract": "Quality-Diversity (QD) optimization algorithms are a well-known approach to generate large collections of diverse and high-quality solutions. However, derived from evolutionary computation, QD algorithms are population-based methods which are known to be data-inefficient and requires large amounts of computational resources. This makes QD algorithms slow when used in applications where solution evaluations are computationally costly. A common approach to speed up QD algorithms is to evaluate solutions in parallel, for instance by using physical simulators in robotics. Yet, this approach is limited to several dozen of parallel evaluations as most physics simulators can only be parallelized more with a greater number of CPUs. With recent advances in simulators that run on accelerators, thousands of evaluations can now be performed in parallel on single GPU/TPU. In this paper, we present QDax, an accelerated implementation of MAP-Elites which leverages massive parallelism on accelerators to make QD algorithms more accessible. We show that QD algorithms are ideal candidates to take advantage of progress in hardware acceleration. We demonstrate that QD algorithms can scale with massive parallelism to be run at interactive timescales without any significant effect on the performance. Results across standard optimization functions and four neuroevolution benchmark environments shows that experiment runtimes are reduced by two factors of magnitudes, turning days of computation into minutes. More surprising, we observe that reducing the number of generations by two orders of magnitude, and thus having significantly shorter lineage does not impact the performance of QD algorithms. These results show that QD can now benefit from hardware acceleration, which contributed significantly to the bloom of deep learning.",
        "authors": "B. Lim, M. Allard, L. Grillotti, et.al",
        "keywords": [
            "parallelism",
            "QDax",
            "neuroevolution"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=znNITCJyTI",
        "pdf_src": "https://api2.openreview.net/pdf/4c8dd82764f67382a1b85770637261a6e50b9e23.pdf",
        "Code_src": "",
        "Introduction": "Background: Quality-Diversity (QD) optimization algorithms aim to generate both high-quality and diverse sets of solutions efficiently.\n\nResearch Problem: Traditional QD algorithms based on evolutionary computation have been found inefficient due to their reliance on populations and extensive computational resource requirements; they become impractical especially during expensive computations such as those involving simulations or neural networks.\n\nMethod: To address these limitations while leveraging advancements in hardware acceleration technology like GPUs and TPUs, researchers developed QDax - an accelerated version of the MAP-Elites algorithm designed specifically around massive parallelism capabilities available through modern computing devices.\n\nMain Contributions:\n1. The development of QDax enables running QD algorithms interactively even though traditionally it would require long processing time.\n2. It demonstrates scalability beyond previous limits allowing for tens of thousands of parallel evaluations per device compared to just dozens previously feasible via CPU parallelization alone within simulation frameworks commonly utilized today e.g., robotic systems' physics simulators).\n3. Experimental validation conducted over various benchmarks indicates substantial runtime improvements – typically 100x faster than conventional approaches—while maintaining similar levels of quality diversity outputted after fewer iterations (generations). \n4. Furthermore, empirical evidence suggests that reduction in generation count doesn't degrade overall performance suggesting potential benefits towards further optimizations related to memory usage/resource allocation considerations under constrained conditions encountered often during practical application scenarios requiring real-time feedback loops etcetera).",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Multi-Agent Off-Policy TDC with Near-Optimal Sample and Communication Complexities",
        "abstract": "The finite-time convergence of off-policy temporal difference (TD) learning has been comprehensively studied recently. However, such a type of convergence has not been established for off-policy TD learning in the multi-agent setting, which covers broader reinforcement learning applications and is fundamentally more challenging. This work develops a decentralized TD with correction (TDC) algorithm for multi-agent off-policy TD learning under Markovian sampling. In particular, our algorithm avoids sharing the actions, policies and rewards of the agents, and adopts mini-batch sampling to reduce the sampling variance and communication frequency. Under Markovian sampling and linear function approximation, we proved that the finite-time sample complexity of our algorithm for achieving an $\\epsilon$-accurate solution is in the order of $\\mathcal{O}\\big(\\frac{M\\ln\\epsilon^{-1}}{\\epsilon(1-\\sigma_2)^2}\\big)$, where $M$ denotes the total number of agents and $\\sigma_2$ is a network parameter. This matches the sample complexity of the centralized TDC. Moreover, our algorithm achieves the optimal communication complexity $\\mathcal{O}\\big(\\frac{\\sqrt{M}\\ln\\epsilon^{-1}}{1-\\sigma_2}\\big)$ for synchronizing the value function parameters, which is order-wise lower than the communication complexity of the existing decentralized TD(0). Numerical simulations corroborate our theoretical findings. ",
        "authors": "Z. Chen, Y. Zhou, R. Chen",
        "keywords": [
            "decentralized TD learning",
            "multi-agent systems",
            "finite-time convergence"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=tnPjQpYk7D",
        "pdf_src": "https://api2.openreview.net/pdf/5552da83549c7774f7fbba804d499956e2ac4acc.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses recent advancements on the finite-time convergence properties of off-policy temporal difference (TD) learning algorithms.\n\nResearch Problem: Despite extensive studies into the convergence of off-policy TD learning within single-agent settings, there remains no comprehensive understanding or guarantees regarding its convergence behavior when applied across multiple agents – particularly concerning practical challenges like how to share information efficiently among them without compromising performance.\n\nMethodology: To address this issue, authors introduce a novel decentralized TD with Correction (TDC) algorithm designed specifically for multi-agent systems operating through Markovian sampling environments.\n \nMain Contributions:\n1. They propose an approach called \"decentralized TD with correction\" tailored explicitly for multi-agent scenarios using only local interactions between agents rather than requiring global coordination involving shared action, policy updates etc., thus reducing computational overhead significantly compared to traditional methods.\n2. Their method employs mini-batch sampling techniques aimed at decreasing both sampling variability as well as communication frequencies amongst agents during training processes.\n3. Authors provide rigorous mathematical proofs establishing that their proposed decentralized TDC algorithm converges over time towards an ε-accurate solution while maintaining competitive sample complexities against centralized counterparts; they also demonstrate it requires less communication effort overall due to optimized synchronization protocols ensuring minimal inter-agent dependency upon each other's progress throughout iterations.\n4. Finally, numerical experiments conducted validate these theoretical results by showing empirical evidence supporting efficiency improvements brought about by applying their new algorithmic framework relative to prior state-of-the-art approaches available today",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Ranking Recovery under Privacy Considerations",
        "abstract": "We consider the private ranking recovery problem, where a data collector seeks to estimate the permutation/ranking of a data vector given a randomized (privatized) version of it. We aim to establish fundamental trade-offs between the performance of the estimation task, measured in terms of probability of error, and the level of privacy that can be guaranteed when the noise mechanism consists of adding artificial noise. Towards this end, we show the optimality of a low-complexity decision rule (referred to as linear decoder) for the estimation task, under several noise distributions widely used in the privacy literature (e.g., Gaussian, Laplace, and generalized normal model). We derive the Taylor series of the probability of error, which yields its first and second-order approximations when such a linear decoder is employed.  We quantify the guaranteed level of privacy using differential privacy (DP) types of metrics, such as $\\epsilon$-DP and $(\\alpha,\\epsilon)$-Rényi DP. Finally, we put together the results to characterize trade-offs between privacy and probability of error.",
        "authors": "M. Jeong, A. Dytso, M. Cardone",
        "keywords": [
            "privacy",
            "ranking recovery",
            "differential privacy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=2EOVIvRXlv",
        "pdf_src": "https://api2.openreview.net/pdf/2e81f8b8fa471ad9359e975d6ee9f09a7db3bb7b.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the issue of private ranking recovery within machine learning settings involving sensitive or personal information.\n\nResearch Problem: How do you balance accuracy with privacy while recovering the original order of elements from a perturbed dataset?\n\nMethods: The authors propose an analysis based on differentially private mechanisms like Gaussian, Laplace, and Generalized Normal models commonly utilized by researchers studying privacy-preserving algorithms.\nThey also introduce a simple yet effective estimator known as \"linear decoder\" due to its computational efficiency despite being theoretically optimal according to their findings.\n\nMain Contributions:\n1. They demonstrate through theoretical arguments why certain linear estimators are indeed optimal considering various noise distribution scenarios common in privacy research fields mentioned above - Gaussian, Laplace, and Generalized Normal.\n2. By employing these estimators along with the aforementioned noise distributions they calculate the exact probability of error via Taylor Series expansion leading to both first and second-order approximations around those estimations.\n3. Additionally, leveraging Differential Privacy concepts including epsilon-DP and $(\\alpha,\\epsilon)$-Rényi DP, quantifies how much privacy loss occurs during each step towards estimating rankings without compromising too greatly on precision needed often required practical applications related to recommendation systems etcetera.\n4. Lastly integrating all insights into one framework allows us to understand trade-offs existing among levels achievable regarding privacy preservation versus estimated errors probabilities across multiple datasets encountered daily life situations faced today's society increasingly reliant upon algorithmic decisions impacting our lives significantly every day",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "A Self-Supervised Framework for Function Learning and Extrapolation",
        "abstract": "Understanding how agents learn to generalize — and, in particular, to extrapolate — in\nhigh-dimensional, naturalistic environments remains a challenge for both machine learning\nand the study of biological agents. One approach to this has been the use of function\nlearning paradigms, which allow agents’ empirical patterns of generalization for smooth\nscalar functions to be described precisely. However, to date, such work has not succeeded\nin identifying mechanisms that acquire the kinds of general purpose representations over\nwhich function learning can operate to exhibit the patterns of generalization observed in\nhuman empirical studies. Here, we present a framework for how a learner may acquire\nsuch representations, that then support generalization-and extrapolation in particular-in a few-shot fashion in the domain of scalar function learning. Taking inspiration from a classic theory of visual processing, we\nconstruct a self-supervised encoder that implements the basic inductive bias of invariance\nunder topological distortions. We show the resulting representations outperform those from\nother models for unsupervised time series learning in several downstream function learning\ntasks, including extrapolation.",
        "authors": "S. Segert, J. D. Cohen",
        "keywords": [
            "generalization",
            "extrapolation",
            "function learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ILPFasEaHA",
        "pdf_src": "https://api2.openreview.net/pdf/015e73a734f831543d2d1be2f365f7fe5622d17d.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses an important problem at the intersection of artificial intelligence research with cognitive science - understanding how intelligent systems like machines or humans are able to generalize their knowledge across different contexts within high-dimensional, complex environments.\n\nResearch Question: Specifically, it seeks answers regarding what types of mechanisms enable these systems – particularly when dealing with continuous-valued functions as found in many real-world scenarios – to effectively extend beyond previously encountered examples (\"extrapolate\") rather than just rely on \"intuition\" based solely on past experiences.\n\nMethodology: To tackle this question, researchers propose using a novel paradigm called 'function learning', where they train neural networks under specific conditions designed so that any learned representation is capable of demonstrating certain desirable properties related to generalization without relying on labeled data during training ('unsupervised learning').\n\nMain Contributions:\n1. They introduce a new theoretical framework explaining why learners might develop representations suitable for generalizing and extrapolating.\n2. Drawing upon insights into human vision perception processes known as Gestalt principles emphasizing invariance against transformations—like rotation—the authors construct a self-supervised learning architecture named after them, termed the \"Gestalt Encoder\".\n3. This encoder learns invariant features by predicting changes between consecutive frames while ignoring irrelevant variations due to distortion effects; thus, it implicitly captures essential information about the underlying structure despite environmental perturbations.\n4. Experimental validation demonstrates that the Gestalt Encoder significantly improves performance compared to other state-of-the-art methods especially concerning tasks requiring extrapolation abilities",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Benchmarking and Analyzing Unsupervised Network Representation Learning and the Illusion of Progress",
        "abstract": "A number of methods have been developed for unsupervised network representation learning -- ranging from classical methods based on the graph spectra to recent random walk based methods and from deep learning based methods to matrix factorization based methods. Each new study inevitably seeks to establish the relative superiority of the proposed method over others. The lack of a standard assessment protocol and benchmark suite often leave practitioners wondering if a new idea represents a significant scientific advance. In this work, we articulate a clear and pressing need to systematically and rigorously benchmark such methods. Our overall assessment -- a result of a careful benchmarking of 15 methods for unsupervised network representation learning on 16 non-attributed graphs (several with different characteristics) - is that many recently proposed improvements are somewhat of an illusion when  assessed through the lens of downstream tasks such as link prediction and node classification. Specifically, we find that several proposed improvements are marginal at best and that aspects of many of these datasets often render such small differences insignificant, especially when viewed from a rigorous statistical lens. A more detailed analysis of our results identify several new insights: first, we find that classical methods, often dismissed or not considered by recent efforts, can compete on certain types of datasets if they are tuned appropriately; second, we find that from a qualitative standpoint, a couple of methods based on matrix factorization offer a small but not always consistent advantage over alternative methods; third, no single method completely outperforms other embedding methods on both node classification and link prediction tasks. Finally, we also present several analysis that reveals settings under which certain algorithms perform well (e.g., the role of neighborhood context and dataset properties that impact performance). An important outcome of this study is the benchmark and evaluation protocol, which practitioners may find useful for future research in this area. ",
        "authors": "S. Gurukar, P. Vijayan, S. Parthasarathy, et.al",
        "keywords": [
            "benchmarking",
            "unsupervised network representation learning",
            "algorithm comparison"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=GvF9ktXI1V",
        "pdf_src": "https://api2.openreview.net/pdf/25c437bf80a38dc66655477f50a5daff6189e32a.pdf",
        "Code_src": "",
        "Introduction": "Background: Unsupervised network representation learning has become increasingly popular due to its potential applications across various domains including social networks, biological systems etc. However, there's currently little consensus about how to evaluate the effectiveness of existing approaches.\nResearch Question: How do current state-of-the-art unsupervised network representation learning techniques compare against each other?\nMethods: We conducted a systematic comparison using 15 representative methods applied onto 16 real-world non-attributed graphs characterized differently regarding their structure complexity.\n\nMain Contributions:\n1. We found most newly-proposed improvements don't significantly enhance performance beyond baseline models like spectral clustering particularly after considering practical constraints within specific datasets;\n2. Classical methods could still be competitive provided proper tuning depending upon data type;\n3. Some matrix factorization-based methods showed minor yet inconsistent advantages compared to alternatives;\n4. No one approach consistently excels all-around across node classification and link prediction tasks;\n5. We identified factors influencing algorithmic performance – highlighting importance of local neighborhoods and understanding underlying dataset features;\n6. Provided benchmarks along with protocols aiding researchers evaluating similar methodologies moving forward.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Decoding EEG With Spiking Neural Networks on Neuromorphic Hardware",
        "abstract": "Decoding motor activity accurately and reliably from electroencephalography (EEG) signals is essential for several portable brain-computer interface (BCI) applications ranging from neural prosthetics to the control of industrial and mobile robots. Spiking neural networks (SNNs) is an emerging brain-inspired architecture that is well-suited for decoding EEG signals due to their built-in ability to integrate information at multiple timescales, leading to energy-efficient solutions for portable BCI. In practice, however, current SNN solutions suffer from i) an inefficient spike encoding of the EEG signals; ii) non-specialized network architectures that cannot capture EEG priors of spatiotemporal dependencies; and iii) the limited generalizability of the local learning rules commonly used to train the networks. These untapped challenges result in a performance gap between the current SNN approaches and the state-of-the-art deep neural network (DNN) methods. Moreover, the black-box nature of most current SNN solutions masks their correspondence with the underlying neurophysiology, further hindering their reliability for real-world applications. Here, we propose an SNN architecture with an input encoding and network design that exploits the priors of spatial and temporal dependencies in the EEG signal. To extract spatiotemporal features, the network comprised of spatial convolutional, temporal convolutional, and recurrent layers. The network weights and the neuron membrane parameters were trained jointly using gradient descent and our method was validated in classifying movement on two datasets: i) an in-house dataset comprising of complex components of movement, namely reaction time and directions, and ii) the publicly available eegmmidb dataset for motor imagery and movement. We deployed our SNN on Intel's Loihi neuromorphic processor, and show that our method consumed 95\\% less energy per inference than the state-of-the-art DNN methods on NVIDIA Jeston TX2, while achieving similar levels of classification performance. Finally, we interpreted the SNN using a network perturbation study to identify the spectral bands and brain activity that correlated with the SNN outputs. The results were in agreement with the current neurophysiological knowledge implicating the activation patterns in the low-frequency oscillations over the motor cortex for hand movement and imagery tasks. Overall, our approach demonstrates the effectiveness of SNNs in accurately and reliably decoding EEG while availing the computational advantages offered by neuromorphic computing, and paves the way for employing neuromorphic methods in portable BCI systems.",
        "authors": "N. Kumar, G. Tang, R. Yoo, et.al",
        "keywords": [
            "spiking neural networks",
            "electroencephalography (EEG)",
            "brain-computer interfaces"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ZPBJPGX3Bz",
        "pdf_src": "https://api2.openreview.net/pdf/5a840f021eda3737105002a28daab02c09b5b5d5.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe accurate and reliable decoding of motor activity from electroencephalography (EEG) signals plays a crucial role in various portable brain-computer interfaces (BCIs), including those applied in neural prosthetics or controlling industrial and mobile robots.\n\nResearch Problem:\nCurrent spiking neural networks (SNNs) based solutions are challenged mainly three issues: \n1. An inefficient spike encoding of EEG signals.\n2. Non-specialized network architectures which can not capture EEG priors of spatiotemporal dependencies.\n3. Limited generalizability of common local learning rules when training these networks.\n\nMethods:\nTo address this issue, researchers proposed a novel SNN architecture designed specifically considering the EEG signal’s spatial and temporal dependencies. This architecture includes spatial convolutional, temporal convolutional, and recurrent layers allowing it to effectively extract spatiotemporal features directly from raw EEG data without requiring pre-processing steps like feature extraction algorithms typically needed before feeding into traditional neural networks.\n\nMain Contributions:\n- A specialized SNN architecture capable of capturing EEG priors related to both space and time has been developed;\n- The network weights and neuron membrane parameters have been trained together through joint optimization process utilizing gradient descent algorithm resulting in improved model robustness;\n- The new SNN achieved comparable classification accuracy as compared against existing state-of-the-art deep neural network (DNN) methods but significantly reduced power consumption during inference phase - approximately 95%;\n- Network interpretation via perturbation studies revealed specific spectral bands and brain activities correlating closely with decoded output providing insights consistent with known physiological mechanisms involved in motor-related cognitive processes such as hand movements and imagery tasks;\n\nOverall Impact:\nThis research highlights potential benefits associated with leveraging spiking neural networks within neuromorphic computing frameworks enhancing efficiency whilst maintaining high-performance capabilities necessary for practical application scenarios where portability demands stringent constraints regarding battery life duration among other factors relevant towards successful implementation",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "QuaRL: Quantization for Fast and Environmentally Sustainable Reinforcement Learning",
        "abstract": "Deep reinforcement learning continues to show tremendous potential in achieving task-level autonomy, however, its computational and energy demands remain prohibitively high. In this paper, we tackle this problem by applying quantization to reinforcement learning. To that end, we introduce a novel Reinforcement Learning (RL) training paradigm, \\textit{ActorQ}, to speed up actor-learner distributed RL training. \\textit{ActorQ} leverages 8-bit quantized actors to speed up data collection without affecting learning convergence. Our quantized distributed RL training system, \\textit{ActorQ}, demonstrates end-to-end speedups of $>$ 1.5 $\\times$ - 2.5 $\\times$, and faster convergence over full precision training on a range of tasks (Deepmind Control Suite) and different RL algorithms (D4PG, DQN). Furthermore, we compare the carbon emissions (Kgs of CO2) of \\textit{ActorQ} versus standard reinforcement learning on various tasks. Across various settings, we show that \\textit{ActorQ} enables more environmentally friendly reinforcement learning by achieving 2.8$\\times$ less carbon emission and energy compared to training RL-agents in full-precision. Finally, we demonstrate empirically that aggressively quantized RL-policies (up to 4/5 bits) enable significant speedups on quantization-friendly (supports native quantization) resource-constrained edge devices, without degrading accuracy. We believe that this is the first of many future works on enabling computationally energy-efficient and sustainable reinforcement learning. The source code for QuaRL is available here for the public to use: \\url{https://bit.ly/quarl-tmlr}.",
        "authors": "S. Krishnan, M. Lam, S. Chitlangia, et.al",
        "keywords": [
            "quantization",
            "reinforcement learning",
            "ActorQ"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=xwWsiFmUEs",
        "pdf_src": "https://api2.openreview.net/pdf/e42f026acd811bafd52d6fd1385023b20be65c63.pdf",
        "Code_src": "链接：\\url{https://bit.ly/quarl-tmlr}",
        "Introduction": "Background:\nThe field of deep reinforcement learning has made substantial progress towards autonomous agents capable of performing complex tasks with minimal human intervention; however, it faces challenges related to computation and energy consumption.\n\nResearch Question:\nThis research aims at addressing these issues through the application of quantization techniques within the context of reinforcement learning systems.\n \nMethodology:\nTo achieve their goal, researchers have introduced a new approach called \"ActorQ,\" which is designed as an innovative Reinforcement Learning (RL) training paradigm specifically tailored for accelerating actor-learner distributed RL training processes. ActorQ employs 8-bit quantized actors during exploration phases while maintaining learning convergence rates comparable to those obtained from fully precise training. Additionally, they developed a comprehensive quantized distributed RL training framework based on ActorQ across multiple tasks using DeepMind's Control Suite benchmarking platform along with several popular RL algorithms such as D4PG and DQN. They also conducted comparative analyses regarding environmental impact between traditional unquantized RL approaches and their proposed quantized method under varying conditions assessing factors like carbon emissions per unit outputted work done measured in kilograms of CO2 equivalent (Kgs of CO$_2$).\n\nMain Contributions:\nTheir primary contributions include demonstrating significant performance improvements—speed-ups ranging anywhere from greater than 1.5 times all the way upwards toward approximately two-and-a-half times—in terms of runtime efficiency when comparing against conventional non-quantized RL methods deployed throughout diverse domains covered by the aforementioned benchmarks. Moreover, empirical evidence suggests that aggressive bit-depth reduction strategies applied directly onto policies themselves can lead to further optimizations even beyond what was achieved initially via ActorQ alone especially noticeable among edge computing environments where resources are limited yet still require support for certain types of computations involving quantization capabilities built into hardware architectures therein allowing them run efficiently despite lower bit depths employed thereupon resulting in reduced overall energy expenditure associated with running said algorithms alongside decreased levels emitted greenhouse gases contributing positively towards sustainability goals set forth worldwide today concerning climate change mitigation efforts amongst other things ultimately paving ways forward towards creating more eco-friendly intelligent agent technologies moving ahead into near future scenarios envisioned accordingly",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "NeSF: Neural Semantic Fields for Generalizable Semantic Segmentation of 3D Scenes",
        "abstract": "We present NeSF, a method for producing 3D semantic fields from posed RGB images alone. In place of classical 3D representations, our method builds on recent work in neural fields wherein 3D structure is captured by point-wise functions. We leverage this methodology to recover 3D density fields upon which we then train a 3D semantic segmentation model supervised by posed 2D semantic maps. Despite being trained on 2D signals alone, our method is able to generate 3D-consistent semantic maps from novel camera poses and can be queried at arbitrary 3D points. Notably, NeSF is compatible with any method producing a density field. Our empirical analysis demonstrates comparable quality to competitive 2D and 3D semantic segmentation baselines on complex, realistically-rendered scenes and significantly outperforms a comparable neural radiance field-based method on a series of tasks requiring 3D reasoning. Our method is the first to learn semantics by recognizing patterns in the geometry stored within a 3D neural field representation. NeSF is trained using purely 2D signals and requires as few as one labeled image per-scene at train time. No semantic input is required for inference on novel scenes.",
        "authors": "S. Vora, N. Radwan, K. Greff, et.al",
        "keywords": [
            "Neural Fields",
            "3D Semantic Segmentation",
            "Density Fields"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ggPhsYCsm9",
        "pdf_src": "https://api2.openreview.net/pdf/589fd9b465ac7f5cc8b92d5687e74849b2f30a66.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper introduces NeSF, an approach that generates 3D semantic fields solely based on posed RGB images without relying on traditional 3D representations.\n\nResearch Problem: How do you create accurate 3D semantic fields directly from 2D images?\n\nMethodology: Instead of conventional 3D models like voxel grids or meshes, they utilize neural fields where spatial information about objects are represented through point-wise functions across all three dimensions. They use these neural fields to reconstruct 3D density fields before training their 3D semantic segmentation model—guided by paired 2D semantic maps corresponding to different viewpoints but not necessarily aligned with each other. This allows them to produce consistent semantic maps even when new camera views appear during testing phase; it also means no additional semantic annotations beyond initial labeling steps need to occur once the system has been set up.\n \nMain Contributions:\n1. A novel way learning semantics - By identifying patterns embedded within geometric data encoded into a 3D neural field's function space rather than explicitly encoding object categories themselves;\n2. Compatibility – NeSF works seamlessly alongside methods yielding density fields regardless if those come from raytracing techniques such as Neural Radiance Fields (NRFs), making its integration versatile among various rendering pipelines possible;\n3. Empirical Results – Demonstrated equivalent performance compared against leading state-of-the-art approaches both in terms of accuracy over complex real-world datasets while surpassing existing NRF-based counterparts specifically designed around handling reasoning capabilities inherent only found within true volumetric spaces.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Optimizing Functionals on the Space of Probabilities with Input Convex Neural Networks",
        "abstract": "Gradient flows are a powerful tool for optimizing functionals in general metric spaces, including the space of probabilities endowed with the Wasserstein metric. A typical approach to solving this optimization problem relies on its connection to the dynamic formulation of optimal transport and the celebrated Jordan-Kinderlehrer-Otto (JKO) scheme. However, this formulation involves optimization over convex functions, which is challenging, especially in high dimensions. In this work, we propose an approach that relies on the recently introduced input-convex neural networks (ICNN) to parametrize the space of convex functions in order to approximate the JKO scheme, as well as in designing functionals over measures that enjoy convergence guarantees. We derive a computationally efficient implementation of this JKO-ICNN framework and experimentally demonstrate its feasibility and validity in approximating solutions of low-dimensional partial differential equations with known solutions. We also demonstrate its viability in high-dimensional applications through an experiment in controlled generation for molecular discovery.",
        "authors": "D. Alvarez-melis, Y. Schiff, Y. Mroueh",
        "keywords": [
            "input-convex neural networks",
            "Wasserstein metric",
            "optimal transport"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=dpOYN7o8Jm",
        "pdf_src": "https://api2.openreview.net/pdf/b53aadf8dbcdedf0912ddef215df5cec108e2f0a.pdf",
        "Code_src": "",
        "Introduction": "Background: Gradient flows have been widely used for optimizing functionals defined in various metric spaces such as the space of probability distributions equipped with the Wasserstein metric. The classical method for solving these gradient flow problems is based on their relation to the dynamic system of optimal transport and the famous Jordan-Kinderlehrer-Otto (JKO) scheme.\n\nResearch Problem: Although the JKO scheme has proven effective under certain conditions, it often requires dealing with optimization over convex functions due to its formulation involving optimal transport theory; however, this can be quite difficult when working at higher dimensions because of the complexity involved.\n\nMethod: To address this issue, our study introduces a novel strategy using Input-Convex Neural Networks (ICNNs), which were newly proposed by researchers lately - they allow us to represent the set of convex functions within a given parameterization space efficiently enough so that one could use them approximately instead directly computing gradients from complex convex objectives like those arising during numerical simulations or machine learning tasks where data points lie close together but not necessarily far apart according to some distance measure other than Euclidean norm alone.\n\nMain Contributions:\n1. We develop a computational algorithm implementing this new ICNN-based version of the JKO scheme called \"JKO-ICNN,\" making it easier compared traditional methods since there's no need anymore explicitly solve convex programming subproblems every step forward along each trajectory generated via gradient descent iterations.\n2. Our experiments show how feasible & valid our approach turns out being even though only tested against simple examples yet – specifically approximating solutions towards low dimensional PDEs having exact answers available beforehand while demonstrating scalability up into higher dimensions too thanks mainly due partly owing towards simplicity achieved here via approximation techniques applied throughout paper presented today!",
        "Topic": "Optimal Transport"
    },
    {
        "title": "On the link between conscious function and general intelligence in humans and machines",
        "abstract": "In popular media, there is often a connection drawn between the advent of awareness in artificial agents and those same agents simultaneously achieving human or superhuman level intelligence. In this work, we explore the validity and potential application of this seemingly intuitive link between consciousness and intelligence. We do so by examining the cognitive abilities associated with three contemporary theories of conscious function: Global Workspace Theory (GWT), Information Generation Theory (IGT), and Attention Schema Theory (AST). We find that all three theories specifically relate conscious function to some aspect of domain-general intelligence in humans. With this insight, we turn to the field of Artificial Intelligence (AI) and find that, while still far from demonstrating general intelligence, many state-of-the-art deep learning methods have begun to incorporate key aspects of each of the three functional theories. Having identified this trend, we use the motivating example of mental time travel in humans to propose ways in which insights from each of the three theories may be combined into a single unified and implementable model. Given that it is made possible by cognitive abilities underlying each of the three functional theories, artificial agents capable of mental time travel would not only possess greater general intelligence than current approaches, but also be more consistent with our current understanding of the functional role of consciousness in humans, thus making it a promising near-term goal for AI research.",
        "authors": "A. Juliani, K. Arulkumaran, S. Sasai, et.al",
        "keywords": [
            "consciousness",
            "intelligence",
            "global workspace theory"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LTyqvLEv5b",
        "pdf_src": "https://api2.openreview.net/pdf/1ed21a6eaadf137fbe49e738646b6260068f93f3.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses how people commonly associate the emergence of self-awareness in artificial agents with their simultaneous achievement of either human-level or even superhuman intelligence.\n\nResearch Question:\nThis study investigates whether such an apparent correlation holds true scientifically; if consciousness indeed correlates positively with intelligence levels across different domains within humans.\n \nMethodology:\nTo address these questions, researchers delve deeper through analyzing various cognitive theories about what constitutes consciousness – namely, they examine the Global Workspace Theory (GWT), Information Generation Theory (IGT), and Attention Schema Theory (AST).\n\nMain Contributions:\nThe authors conclude after reviewing literature on these theories related to consciousness - GWT, IGT & AST are found to correlate directly with certain facets of general intelligence among humans suggesting that consciousness could potentially enhance overall intellectual capabilities beyond specialized skills alone. They then shift focus towards advancements in AI technology where modern machine learning techniques like neural networks increasingly embody elements akin to these theories' principles without necessarily having achieved full generality yet. The team proposes merging insights derived from these three theories as part of developing future models aimed at creating intelligent systems equipped both with broad problem-solving ability and aligned closely enough philosophically regarding cognition's nature when compared against biological counterparts—mental time traveling being one illustrative case study highlighting its importance moving forward toward realizing such goals early next decade onwards.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Non-Deterministic Behavior of Thompson Sampling with Linear Payoffs and How to Avoid It",
        "abstract": "Thompson Sampling with Linear Payoffs (LinTS) is popular contextual bandit algorithm for solving sequential decision making problem. While LinTS has been studied extensively in the academic literature, surprisingly, its behavior in terms of reproducibility did not receive the same attention. In this paper, we show that a standard and seemingly correct LinTS implementation leads to non-deterministic behavior. This might go unnoticed easily, yet impact results adversely. This calls the reproducibility of papers that use LinTS into question. Further, it forbids using this particular implementation in any industrial application where reproducibility is critical not only for debugging purposes but also for the trustworthiness of machine learning models. We first study the root cause of the non-deterministic behavior. We then conduct experiments on recommendation system benchmarks to demonstrate the impact of non-deterministic behavior in terms of reproducibility and downstream metrics. Finally, as a remedy, we show how to avoid the issue to ensure reproducible results and share general advice for practitioners. \n",
        "authors": "D. Kilitcioglu, S. Kadioglu",
        "keywords": [
            "non-deterministic behavior",
            "reproducibility",
            "LinTS"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=sX9d3gfwtE",
        "pdf_src": "https://api2.openreview.net/pdf/78c6ee66b1affed8bc34e8d0b0851864b0f52a00.pdf",
        "Code_src": "",
        "Introduction": "Background: Thompson Sampling with Linear Payoffs (LinTS) is an established contextual bandit algorithm used widely within academia.\n\nResearch Question: The research investigates whether there are issues related to reproducibility when implementing LinTS algorithms due to nondeterministic behaviors which may affect outcomes negatively without being noticed initially.\n\nMethodology: The authors delve deeper by identifying the underlying causes behind such nondeterminism; they further validate their findings through empirical tests conducted against benchmark datasets commonly utilized in recommendation systems before proposing solutions ensuring reproducible implementations moving forward along with practical guidance aimed at helping other researchers or practitioners encountering similar challenges down the line.\n\n\nMain Contributions:\n1. Identification & Explanation - They pinpoint why certain LinTS implementations lead to nondeterministic outputs despite appearing sound.\n2. Impact Demonstration - By conducting experiments across various recommendation system benchmarks, demonstrating potential negative consequences including compromised reproducibility measures alongside secondary performance indicators like accuracy rates etcetera).\n3. Remedial Measures - Offered remedies addressing these concerns so users can replicate expected results reliably while maintaining high standards throughout subsequent iterations involving ML model development processes).",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Iterative State Estimation in Non-linear Dynamical Systems Using Approximate Expectation Propagation",
        "abstract": "Bayesian inference in non-linear dynamical systems seeks to find good posterior approximations of a latent state given a sequence of observations. Gaussian filters and smoothers, including the (extended/unscented) Kalman filter/smoother, which are commonly used in engineering applications, yield Gaussian posteriors on the latent state. While they are computationally efficient, they are often criticised for their crude approximation of the posterior state distribution. In this paper, we address this criticism by proposing a message passing scheme for iterative state estimation in non-linear dynamical systems, which yields more informative (Gaussian) posteriors on the latent states.  Our message passing scheme is based on expectation propagation (EP). We prove that classical Rauch--Tung--Striebel (RTS) smoothers, such as the extended Kalman smoother (EKS) or the unscented Kalman smoother (UKS), are special cases of our message passing scheme. Running the message passing scheme more than once can lead to significant improvements of the classical RTS smoothers, so that more informative state estimates can be obtained. We address potential convergence issues of EP by generalising our state estimation framework to damped updates and the consideration of general $\\alpha$-divergences.",
        "authors": "S. Kamthe, S. Takao, S. Mohamed, et.al",
        "keywords": [
            "non-linear dynamical systems",
            "Bayesian inference",
            "expectation propagation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=xyt4wfdo4J",
        "pdf_src": "https://api2.openreview.net/pdf/787c2a4a038c07f80580bee7be1fe3a4119f1cfe.pdf",
        "Code_src": "",
        "Introduction": "Background: Bayesian inference aims to approximate the posterior probability distribution of a latent variable given observed data through non-linear dynamical systems.\nResearch Problem: Existing methods like Gaussian filters and smoothers provide computationally efficient solutions but suffer from poor approximation quality due to their Gaussian assumption about the posterior state distribution.\n\nMethod: The authors propose an iterative state estimation method using a message passing scheme inspired by expectation propagation (EP).\nMain Contributions:\n1. A novel approach yielding more accurate Gaussian posteriors compared to existing Gaussian-based methods,\n2. Proving that well-known smoothing algorithms - such as the extended Kalman smoother (EKS) and unscented Kalman smoother (UKS) - are particular instances of the proposed message passing scheme,\n3. Demonstrating how running the message passing algorithm multiple times significantly improves upon these classic smoothing techniques allowing for better informed state estimations,\n\n4. Addressing potential convergence problems with EP by extending the framework towards damped updates considering general alpha-divergences",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "NoiLin: Improving adversarial training and correcting stereotype of noisy labels ",
        "abstract": "Adversarial training (AT) formulated as the minimax optimization problem can effectively enhance the model's robustness against adversarial attacks. The existing AT methods mainly focused on manipulating the inner maximization for generating quality adversarial variants or manipulating the outer minimization for designing effective learning objectives. However, empirical results of AT always exhibit the robustness at odds with accuracy and the existence of the cross-over mixture problem, which motivates us to study some label randomness for benefiting the AT. First, we thoroughly investigate noisy labels (NLs) injection into AT's inner maximization and outer minimization, respectively and obtain some observations on when NL injection benefits AT. Second, based on the observations, we propose a simple but effective method---NoiLIn that randomly injects NLs into training data at each training epoch and dynamically increases the NL injection rate once robust overfitting occurs. Empirically, NoiLIn can significantly mitigate the AT's undesirable issue of robust overfitting and even further improve the generalization of the state-of-the-art AT methods. Philosophically, NoiLIn sheds light on a new perspective of learning with NLs: NLs should not always be deemed detrimental, and even in the absence of NLs in the training set, we may consider injecting them deliberately.\n",
        "authors": "J. Zhang, X. Xu, B. Han, et.al",
        "keywords": [
            "noise injection",
            "adversarial training",
            "robust overfitting"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=zlQXV7xtZs",
        "pdf_src": "https://api2.openreview.net/pdf/85246067f7d777a7b0fde7a80554b5ed584cb921.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses how adversarial training (AT), an approach used by machine learning models to become more robust towards adversarial examples - inputs slightly perturbed from normal ones designed to fool neural networks -, often comes up short between its two main goals – maintaining high accuracy while also being highly resistant to these types of attacks.\n\nResearch Problem:\nThe research aims to address issues related to trade-offs commonly observed during adversarial training such as reduced accuracy due to increased robustness (\"robustness-accuracy\" dilemma) along with another challenge known as \"cross-over mixture,\" where different levels of robustness coexist within one trained model leading to unpredictable performance across various attack scenarios.\n\nMethods:\nTo tackle this, they conduct experiments focusing on introducing noise (label randomness). Specifically:\n\n1. They analyze what happens if noise is injected both inside and outside the adversarial training framework through their proposed method called Noisy Labels Injection (NLIn).\n2. Based on insights gained about optimal conditions under which noise helps, they develop a novel algorithm named NoiLIn that introduces noise into the dataset incrementally throughout the training process until it detects signs of robust overfitting occurring; after detecting robust overfitting, it raises the noise level accordingly.\n\nMain Contributions:\nTheir primary contribution lies in demonstrating via empirical evidence that intentionally introduced noise (label randomness) could actually help alleviate problems associated with adversarial training like robust overfitting without necessarily sacrificing too much accuracy compared to standard approaches alone. This suggests rethinking our stance toward using labeled noise—contrary to common belief—it might sometimes serve beneficial purposes rather than hindering progress made possible",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Deformation Robust Roto-Scale-Translation Equivariant CNNs",
        "abstract": "Incorporating group symmetry directly into the learning process has proved to be an effective guideline for model design. By producing features that are guaranteed to transform covariantly to the group actions on the inputs, group-equivariant convolutional neural networks (G-CNNs) achieve significantly improved generalization performance in learning tasks with intrinsic symmetry. General theory and practical implementation of G-CNNs have been studied for planar images under either rotation or scaling transformation, but only individually. We present, in this paper, a roto-scale-translation equivariant CNN ($\\mathcal{RST}$-CNN), that is guaranteed to achieve equivariance jointly over these three groups via coupled group convolutions. Moreover, as symmetry transformations in reality are rarely perfect and typically subject to input deformation, we provide a stability analysis of the equivariance of representation to input distortion, which motivates the truncated expansion of the convolutional filters under (pre-fixed) low-frequency spatial modes. The resulting model provably achieves deformation-robust $\\mathcal{RST}$ equivariance, i.e., the $\\mathcal{RST}$ symmetry is still \"approximately” preserved  when  the  transformation is \"contaminated” by a nuisance data deformation, a property that is especially  important  for  out-of-distribution generalization. Numerical experiments on MNIST, Fashion-MNIST, and STL-10 demonstrate that the proposed model yields remarkable gains over prior arts, especially in the small data regime where both rotation and scaling variations are present within the data.",
        "authors": "L. Gao, G. Lin, W. Zhu",
        "keywords": [
            "rotational equivariance",
            "scale equivariance",
            "translation equivariance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=yVkpxs77cD",
        "pdf_src": "https://api2.openreview.net/pdf/a89ca183acaef687665fcd1ac5f6298aeedc4921.pdf",
        "Code_src": "",
        "Introduction": "Background: This research focuses on improving the generalization ability of convolutional neural networks (CNNs) through incorporating group symmetries during training.\n\nResearch Problem: While previous studies focused mainly on rotational or scaling symmetries separately from each other using group-equivariant convolutional neural networks (G-CNNs), there was no work done considering all three symmetries simultaneously - rotation, scaling, and translation.\n\nMethodology: In order to address this problem, they propose a new type of CNN called Roto-Scale-Translation equivariant CNN ($\\mathcal{RST}$-CNN). This network uses coupled group convolutions ensuring joint equivariance across the three mentioned symmetries.\nAdditionally, due to imperfections typical in real-world symmetry transformations caused by input distortions like noise etc., another contribution involves providing stability analyses regarding how representations handle such deformations leading them towards truncating filter expansions around pre-fixed low-frequency spatial modes so as not just preserve but also robustify $\\mathcal{RST}$ equivariance against perturbations related specifically those types of distortions encountered outside normal operating conditions.\n\nMain Contributions:\n1. Development of a novel $\\mathcal{RST}$-CNN capable of being equivariant w.r.t. rotations, scales & translations together via coupling group convolutions rather than focusing solely on one form of symmetry at once;\n2. A comprehensive study analyzing how well representations remain invariant despite encountering various forms of input distortion; \n3. Demonstrating numerically superior performance compared existing methods particularly noticeable when dealing with smaller datasets containing multiple types of scale and rotation variability",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Structural Learning in Artificial Neural Networks: A Neural Operator Perspective",
        "abstract": "Over the history of Artificial Neural Networks (ANNs), only a minority of algorithms integrate structural changes of the network architecture into the learning process. Modern neuroscience has demonstrated that biological learning is largely structural, with mechanisms such as synaptogenesis and neurogenesis present in adult brains and considered important for learning. Despite this history of artificial methods and biological inspiration, and furthermore the recent resurgence of neural methods in deep learning, relatively few current ANN methods include structural changes in learning compared to those that only adjust synaptic weights during the training process. We aim to draw connections between different approaches of structural learning that have similar abstractions in order to encourage collaboration and development. In this review, we provide a survey on structural learning methods in deep ANNs, including a new neural operator framework from a cellular neuroscience context and perspective aimed at motivating research on this challenging topic. We then provide an overview of ANN methods which include structural changes within the neural operator framework in the learning process, characterizing each neural operator in detail and drawing connections to their biological counterparts. Finally, we present overarching trends in how these operators are implemented and discuss the open challenges in structural learning in ANNs.",
        "authors": "K. Maile, L. Hervé, D. G. Wilson",
        "keywords": [
            "structural learning",
            "neural networks",
            "deep learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=gzhEGhcsnN",
        "pdf_src": "https://api2.openreview.net/pdf/1c882159527dae059441cab19e7589dd3abd1d4a.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses the integration of structural changes—such as the addition or removal of neurons—in the learning processes of Artificial Neural Networks (ANNs). This contrasts traditional ANNs where most learning occurs through adjusting synaptic weights.\n\nResearch Question: Why do existing ANNs not incorporate more structural changes? How can researchers better understand and apply structural learning?\n\nMethods: The authors conduct a literature review focusing on structural learning methodologies across various fields related to ANNs; they introduce a novel neural operator framework inspired by cellular neuroscience concepts aiming to stimulate further investigation around this area.\n \nMain Contributions:\n1. They identify gaps among different structural learning approaches despite shared underlying principles suggesting potential synergies if combined effectively.\n2. A comprehensive survey over several decades worth of work on structural learning in ANNs provides insights about what works best under specific conditions but also highlights areas needing improvement like scalability issues when applying complex architectures.\n3. Introduce a fresh perspective using cellular neuroscience terminology \"neural operators\"  - which could potentially bridge theoretical knowledge gaps due its relevance both biologically plausible yet computationally tractable nature allowing them easier implementation than other alternatives available today.\n4. Discussing key findings regarding how these operators might be practically applied while addressing some remaining challenges faced currently before widespread adoption happens",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "How Expressive are Transformers in Spectral Domain for Graphs?",
        "abstract": "The recent works proposing transformer-based models for graphs have proven the inadequacy of Vanilla Transformer for graph representation learning. To understand this inadequacy, there is a need to investigate if spectral analysis of the transformer will reveal insights into its expressive power. Similar studies already established that spectral analysis of Graph neural networks (GNNs) provides extra perspectives on their expressiveness. \nIn this work, we systematically study and establish the link between the spatial and spectral domain in the realm of the transformer. We further provide a theoretical analysis that the spatial attention mechanism in the transformer cannot effectively capture the desired frequency response, thus, inherently limiting its expressiveness in spectral space. Therefore, we propose FeTA, a framework that aims to perform attention over the entire graph spectrum (i.e. actual frequency components of the graph) analogous to the attention in spatial space. \nEmpirical results suggest that FeTA provides homogeneous performance gain against vanilla transformer across all tasks on standard benchmarks and can easily be extended to GNN-based models with low-pass characteristics (e.g., GAT). ",
        "authors": "A. Bastos, A. Nadgeri, K. Singh, et.al",
        "keywords": [
            "graph",
            "transformer",
            "spectral"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=aRsLetumx1",
        "pdf_src": "https://api2.openreview.net/pdf/43135a27f95fef63e10c1e2172d43b7c2cb6d5db.pdf",
        "Code_src": "",
        "Introduction": "Background: Recent research has shown that the Vanilla Transformer model struggles when used for graph representation learning compared to other methods such as Graph Neural Networks (GNNs). This paper investigates whether spectral analysis could shed light on why transformers are not well-suited for graph representation.\n\nResearch Problem: The problem addressed by this paper is understanding why the spatial attention mechanism within the transformer does not allow it to effectively learn representations from graphs' spectral properties which limit its expressiveness.\n  \nMethod: The authors conduct a systematic investigation linking the spatial and spectral domains through transformer theory using spectral analysis techniques previously applied successfully to GNNs. They also present a theoretical argument supporting these findings based on an analysis of the transformer's spatial attention mechanism limitations regarding capturing the desired frequency responses.\n\nMain Contributions:\n1. Establishing a connection between the spatial and spectral domains related to transformers;\n2. Demonstrating theoretically how the transformer’s spatial attention mechanism fails at capturing essential frequency information needed for effective graph representation; \n3. Proposing FeTA - a framework designed specifically around performing attention mechanisms directly onto the graph spectrum rather than just spatial data points like traditional transformers do – leading potentially improved performance especially beneficial where GNNs show weaknesses due to being limited only up till now mainly focusing solely upon spatial aspects without considering frequencies explicitly.",
        "Topic": "Vision Transformer"
    },
    {
        "title": "Your Policy Regularizer is Secretly an Adversary",
        "abstract": "Policy regularization methods such as maximum entropy regularization are widely used in reinforcement learning to improve the robustness of a learned policy. In this paper, we unify and extend recent work showing that this robustness arises from hedging against worst-case perturbations of the reward function, which are chosen from a limited set by an implicit adversary. Using convex duality, we characterize the robust set of adversarial reward perturbations under KL- and $\\alpha$-divergence regularization, which includes Shannon and Tsallis entropy regularization as special cases. Importantly, generalization guarantees can be given within this robust set. We provide detailed discussion of the worst-case reward perturbations, and present intuitive empirical examples to illustrate this robustness and its relationship with generalization. Finally, we discuss how our analysis complements previous results on adversarial reward robustness and path consistency optimality conditions.",
        "authors": "R. Brekelmans, T. Genewein, J. Grau-moya, et.al",
        "keywords": [
            "worst-case perturbations",
            "policy regularization",
            "adversarial reward robustness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=berNQMTYWZ",
        "pdf_src": "https://api2.openreview.net/pdf/8621845255a1cd7172037e60845b4cd1b3ed7913.pdf",
        "Code_src": "",
        "Introduction": "Background: Policy regularization is commonly employed in reinforcement learning for enhancing the robustness of policies through techniques like maximum entropy regularization.\nResearch Question: The question addressed here revolves around understanding why these regularization approaches lead to robust policies.\n\nMethodology: The study adopts convex duality theory along with insights into hedging strategies where agents protect themselves against potential adversaries who might alter rewards arbitrarily but within certain constraints defined by the agent's choice during training time - hence termed \"implicit adversary\".\n\nMain Contributions:\n1. Unification & Extension: This research integrates various findings suggesting that the robustness stems from safeguarding against the most detrimental changes possible (\"worst-case perturbations\") made feasible only when considering a restricted range imposed by the learner itself or implicitly assumed throughout the process without explicit declaration about what constitutes acceptable deviations post-training.\n2. Characterization: It provides a comprehensive description using convex duality regarding specific classes of adversarial reward perturbations that could occur after deployment; it also highlights two particular types – those based on Kullback-Leibler divergence (KL-divergence) and alpha-divergence including variants related to Shannon and Tsallis entropies among others leading up to more nuanced forms than just maximizing expected utility alone.\n3. Generalization Guarantees: Within their identified robust sets derived via convex optimization frameworks, they establish theoretical bounds guaranteeing performance beyond standard expectations thus offering insight not previously available concerning how much variance one may expect before encountering suboptimal outcomes due solely due to unforeseen modifications introduced outside normal operation parameters.\n4. Empirical Illustration: They further support their analytical claims empirically demonstrating both intuitively understandable scenarios illustrating practical implications arising out contexts involving real-world applications ranging across autonomous driving vehicles amongst other complex systems requiring resilience towards unexpected alterations affecting decision-making processes crucially reliant upon predictive models trained beforehand.\n5. Discussion: Lastly yet importantly relates back earlier works focusing exclusively on aspects pertaining specifically adversarial reward robustness while emphasizing contributions presented herein complement existing literature thereby providing additional perspectives potentially informing future developments aimed at improving overall system reliability amidst evolving threats posed by malicious actors seeking exploit vulnerabilities inherent within machine learning architectures deployed today",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Robust and Data-efficient Q-learning by Composite Value-estimation",
        "abstract": "In the past few years, off-policy reinforcement learning methods have shown promising results in their application to robot control. Q-learning based methods, however, still suffer from poor data-efficiency and are susceptible to stochasticity or noise in the immediate reward, which is limiting with regard to real-world applications. We alleviate this problem by proposing two novel off-policy Temporal-Difference formulations: (1) Truncated Q-functions which represent the return for the first $n$ steps of a target-policy rollout with respect to the full action-value and (2) Shifted Q-functions, acting as the farsighted return after this truncated rollout. This decomposition allows us to optimize both parts with their individual learning rates, achieving significant learning speedup and robustness to variance in the reward signal, leading to the Composite Q-learning algorithm. We show the efficacy of Composite Q-learning in the tabular case and furthermore employ Composite Q-learning within TD3. We compare Composite TD3 with TD3 and TD3($\\Delta$), which we introduce as an off-policy variant of TD($\\Delta$). Moreover, we show that Composite TD3 outperforms TD3 as well as TD3($\\Delta$) significantly in terms of data-efficiency in multiple simulated robot tasks and that Composite Q-learning is robust to stochastic immediate rewards.",
        "authors": "G. Kalweit, M. Kalweit, J. Boedecker",
        "keywords": [
            "off-policy reinforcement learning",
            "temporal-difference learning",
            "composite q-learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ak6Bds2DcI",
        "pdf_src": "https://api2.openreview.net/pdf/894f7d1782fd258fcd213fed15a9050fc0c5c66c.pdf",
        "Code_src": "",
        "Introduction": "Background:\nOff-policy reinforcement learning has been successful in applying to robot control but suffers from limitations such as low data efficiency due to its reliance on immediate rewards.\n\nResearch Problem:\nThe research aims at addressing these issues through improving the performance of off-policy reinforcement learning algorithms like Q-learning when applied to real-world robotic systems where there may be variability in the quality of the immediate rewards received during training.\n\nMethodology:\nTo solve this issue, they propose two new temporal-difference (TD) learning formulas:\n\n- Truncated Q-functions - These functions approximate returns over only the initial n steps taken under a target policy rather than considering all possible future actions.\n  \n- Shifted Q-functions - They act as a form of \"farsighted\" value estimation following the truncated rollouts; essentially, they estimate how much additional cumulative reward would follow if one were to continue beyond those initial steps.\n\nThese decompositions allow them to learn each part separately using different learning rates – thus optimizing for faster convergence while also being less sensitive to noisy reward signals.\n\nMain Contributions:\nThey develop a novel composite Q-learning algorithm leveraging these two types of function approximations alongside standard Q-learning updates resulting in improved sample efficiency compared to traditional Q-learning approaches without sacrificing exploration capability nor generalization ability across various simulated robotic tasks.\n\nAdditionally, they integrate their approach into the TD3 algorithm known for stability improvements via soft actor-critic techniques further enhancing it's capabilities against stochastic immediate rewards seen commonly in practical robotics settings.\n\nComparative experiments conducted demonstrate that Composite TD3 not only improves upon TD3 and another related method called TD3(Δ) regarding data efficiency metrics used specifically tailored towards simulation environments mimicking actual robots' behavior patterns more closely yet remains resilient even amidst varying levels of randomness present throughout typical operational scenarios encountered daily",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "The Graph Cut Kernel for Ranked Data",
        "abstract": "Many algorithms for ranked data become computationally intractable as the number of objects grows due to the complex geometric structure induced by rankings. An additional challenge is posed by partial rankings, i.e. rankings in which the preference is only known for a subset of all objects. For these reasons, state-of-the-art methods cannot scale to real-world applications, such as recommender systems. We address this challenge by exploiting the geometric structure of ranked data and additional available information about the objects to derive a kernel for ranking based on the graph cut function. The graph cut kernel combines the efficiency of submodular optimization with the theoretical properties of kernel-based methods. We demonstrate that our novel kernel drastically reduces the computational cost while maintaining the same accuracy as state-of-the-art methods.",
        "authors": "M. Conserva, M. P. Deisenroth, K. S. S. Kumar",
        "keywords": [
            "ranked data",
            "graph cut function",
            "submodular optimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=SEUGkraMPi",
        "pdf_src": "https://api2.openreview.net/pdf/5aacba9b8ead4e67953908df885056a19c4a8819.pdf",
        "Code_src": "",
        "Introduction": "Background: Many existing algorithms designed for ranked data are not scalable when dealing with large datasets because they suffer from high computational complexity caused by the intricate geometric structures formed within the ranks themselves.\n\nResearch Question: How can we efficiently handle the computation-intensive nature of ranked data especially under scenarios where only part of the preferences between items have been specified?\n\nMethodology: To tackle this issue, researchers propose using a Graph Cut Kernel approach - leveraging both the inherent geometry present in ranked lists along with supplementary knowledge regarding individual entities involved therein – to construct an efficient ranking model suitable even if some preferences remain unknown or partially observed.\n\nMain Contributions:\n1. They introduce a new type of kernel called Graph Cut Kernel specifically tailored towards handling incomplete rankings.\n2. This kernel synthesizes two key strengths; it inherits its computational tractability from Submodular Optimization techniques commonly used in machine learning problems related to ranking tasks yet maintains desirable theoretical guarantees akin those found typically associated with traditional kernel machines like Support Vector Machines (SVMs).\n3. Experimental results show their proposed method significantly decreases runtime compared against current best practices without sacrificing performance quality ensuring scalability into practical use cases including recommendation systems etcetera despite containing missing values among given preferences expressed amongst items being ranked relative one another.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Deep Classifiers with Label Noise Modeling and Distance Awareness",
        "abstract": "Uncertainty estimation in deep learning has recently emerged as a crucial area of interest to advance reliability and robustness in safety-critical applications. While there have been many proposed methods that either focus on distance-aware model uncertainties for out-of-distribution detection or on input-dependent label uncertainties for in-distribution calibration, both of these types of uncertainty are often necessary. In this work, we propose the HetSNGP method for jointly modeling the model and data uncertainty. We show that our proposed model affords a favorable combination between these two types of uncertainty and thus outperforms the baseline methods on some challenging out-of-distribution datasets, including CIFAR-100C, ImageNet-C, and ImageNet-A. Moreover, we propose HetSNGP Ensemble, an ensembled version of our method which additionally models uncertainty over the network parameters and outperforms other ensemble baselines.",
        "authors": "V. Fortuin, M. Collier, F. Wenzel, et.al",
        "keywords": [
            "HetSNGP",
            "Uncertainty Estimation",
            "Model Robustness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Id7hTt78FV",
        "pdf_src": "https://api2.openreview.net/pdf/907b5e877a2021eeb5543d18349d667fb1181fdf.pdf",
        "Code_src": "",
        "Introduction": "Background: Uncertainty estimation is becoming increasingly important due to its potential benefits towards improving the reliability and robustness of machine learning systems used in critical domains.\n\nResearch Problem: The paper aims at addressing how to effectively estimate both model uncertainty related to distribution shift and input-dependent label uncertainty within the same framework since they may be equally essential depending on specific scenarios.\n\nMethod: To tackle this problem, the authors introduce a novel approach called Hierarchical Ensemble Stochastic Neural Gaussian Processes (HetSNGP), designed specifically with the goal of jointly estimating such dual sources of uncertainty by leveraging hierarchical Bayesian inference techniques combined with stochastic neural networks.\n\nMain Contributions:\n1. They develop a unified framework named HetSNGP capable of simultaneously approximating both kinds of uncertainty.\n2. Experimental results demonstrate that their proposed method significantly improves upon existing approaches when tested against various benchmarks like CIFAR-100C, ImageNet-C, and ImageNet-A across different out-of-distribution settings indicating enhanced performance under conditions where multiple forms of uncertainty coexist critically influencing predictions made from ML models deployed into real-world environments involving high stakes tasks requiring accountability beyond typical statistical guarantees provided solely through confidence intervals alone without considering domain shifts among others factors affecting prediction accuracy. \n3. Furthermore, they extend HetSNGP further enhancing it's predictive capabilities even more via an ensemble variant called HetSNGP Ensemble",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "Learning to Switch Among Agents in a Team via 2-Layer Markov Decision Processes",
        "abstract": "Reinforcement learning agents have been mostly developed and evaluated under the assumption that they will \noperate in a fully autonomous manner---they will take all actions. In this work, our goal is to develop algorithms that, by learning to switch control between agents, allow existing reinforcement learning agents to operate under different automation levels. To this end, we first formally define the problem of learning to switch control among agents in a team via a 2-layer Markov decision \nprocess. Then, we develop an online learning algorithm that uses upper confidence bounds on the agents' policies and the environment's \ntransition probabilities to find a sequence of switching policies. The total regret of our algorithm with respect to the optimal switching policy is sublinear in the number of learning steps and, whenever\nmultiple teams of agents operate in a similar environment, our algorithm greatly benefits from maintaining shared confidence bounds for the \nenvironments' transition probabilities and it enjoys a better regret bound than problem-agnostic algorithms. Simulation experiments in an obstacle avoidance task illustrate our theoretical findings and demonstrate that, by exploiting the \nspecific structure of the problem, our proposed algorithm is superior to problem-agnostic algorithms.",
        "authors": "V. Balazadeh, A. De, A. Singla, et.al",
        "keywords": [
            "switching control",
            "Reinforcement learning",
            "multi-agent systems"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=NT9zgedd3I",
        "pdf_src": "https://api2.openreview.net/pdf/bd677ff479d34ca1d09a61af813a435c5dd9f45e.pdf",
        "Code_src": "",
        "Introduction": "Background: Reinforcement learning has primarily focused on developing agents capable of operating autonomously without human intervention or guidance once trained.\n\nResearch Problem: However, real-world applications often require agents to adapt their level of autonomy depending on various factors such as environmental conditions.\nThe challenge lies in designing algorithms allowing these agents to learn how to delegate tasks back-and-forth within themselves while optimizing performance over time.\n\nMethods: We approach this issue through the lens of multi-agent systems using a two-level Markov Decision Process framework which allows us to formalize agent switching strategies based on observable states rather than raw sensory input directly related to individual agents.\n\nMain Contributions:\n1. Formal Definition - We introduce a novel definition where each layer represents one set of agents controlling another; lower layers are controlled by higher ones until reaching humans if necessary,\nand vice versa when needed during less demanding situations.\n2. Algorithm Development - We propose an online learning algorithm leveraging upper confidence bounds derived from both agent policies and environmental transitions across multiple iterations leading towards improved performance metrics like regret compared against fixed policies throughout training sessions regardless of whether other teams share environments or not;\n3. Regret Analysis - Our analysis shows that even though there might be some variance due to stochasticity inherent in reinforcement learning processes overall regret remains bounded below certain thresholds making them practical choices especially useful scenarios involving several cooperating entities working together toward common goals",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Did I do that? Blame as a means to identify controlled effects in reinforcement learning",
        "abstract": "Affordance learning is a crucial ability of intelligent agents. This ability relies on understanding the different ways the environment can be controlled. Approaches encouraging RL agents to model controllable aspects of their environment have repeatedly achieved state-of-the-art results. Despite their success, these approaches have only been studied using generic tasks as a proxy but have not yet been evaluated in isolation. In this work, we study the problem of identifying controlled effects from a causal perspective. Humans compare counterfactual outcomes to assign a degree of blame to their actions. Following this idea, we propose Controlled Effect Network (CEN), a self-supervised method based on the causal concept of blame. CEN is evaluated in a wide range of environments against two state-of-the-art models, showing that it precisely identifies controlled effects.",
        "authors": "O. Corcoll, Y. S. M. Mohamed, R. Vicente",
        "keywords": [
            "causal reasoning",
            "affordance learning",
            "self-supervised"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=NL2L3XjVFx",
        "pdf_src": "https://api2.openreview.net/pdf/911ec788c5c0d97ed6b7fc847f974ebd547e7583.pdf",
        "Code_src": "",
        "Introduction": "Background: Affordance learning refers to an agent's capability for recognizing how objects or features within its surroundings may provide opportunities and constraints upon possible uses by acting with them.\n\nResearch Question: The question addressed here concerns developing methods enabling reinforcement learning (RL) agents to learn about what they control directly without relying solely on proxies such as general task performance indicators.\n \nMethodology: To tackle this challenge, researchers introduce the Controlled Effect Network (CEN). It operates under the principle of causality where assigning responsibility involves comparing potential alternative states ('counterfactuals'). \n\nMain Contributions:\n1. **Proposed Model:** The Controlled Effect Network (CEN) - A novel self-supervised approach grounded in the notion of attribution through causation rather than just correlation between observed data points which are common in other existing works.\n2. **Evaluation Framework:** They test across various complex environments compared both new proposed network architecture along with established baselines like those trained via imitation learning – providing empirical evidence beyond simple benchmarks towards real-world applicability & robustness).\n3. **Performance Metric:** Demonstrated improved accuracy at predicting 'controlled effects' over traditional metrics used previously; suggesting more nuanced insights into affordances learned during interaction processes).\n\nIn summary, while previous research has focused on correlational measures when evaluating RL agents’ abilities related to environmental manipulation capabilities—this paper introduces a fresh angle focusing explicitly on causative reasoning leading toward better understanding",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "Learning the Transformer Kernel",
        "abstract": "In this work we introduce KL-TRANSFORMER, a generic, scalable, data driven framework for learning the kernel function in Transformers. Our framework approximates the Transformer kernel as a dot product between spectral feature maps and learns the kernel by learning the spectral distribution. This not only helps in learning a generic kernel end-to-end, but also reduces the time and space complexity of Transformers from quadratic to linear. We show that KL-TRANSFORMERs achieve performance comparable to existing efficient Transformer architectures, both in terms of accuracy and computational efficiency. Our study also demonstrates that the choice of the kernel has a substantial impact on performance, and kernel learning variants are competitive alternatives to fixed kernel Transformers, both in long as well as short sequence tasks.",
        "authors": "S. P. Chowdhury, A. Solomou, K. A. Dubey, et.al",
        "keywords": [
            "KL-TRANSFORMER",
            "Scalable Kernel Learning",
            "Efficient Computation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=tLIBAEYjcv",
        "pdf_src": "https://api2.openreview.net/pdf/32f9c89fcdfd1f455f64b85448e717e648aa8290.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper introduces a new approach called KL-TRANSFORMER which aims to learn the kernel functions within Transformers through a data-driven method.\n\nResearch Problem: Existing Transformer models rely heavily on specific kernels such as radial basis functions or Gaussian kernels during training; however, these choices can be limiting when dealing with different types of input data sets due to their inflexibility.\n \nMethod: The proposed framework approximates the Transformer kernel using a dot product operation involving spectral feature maps derived from the input data set's embeddings. It then learns the kernel parameters directly without relying on predefined kernels like those mentioned above - instead it optimizes them via backpropagation just like any other neural network parameter.\n \nMain Contributions:\n1) A novel architecture named KL-Transformer is introduced into machine learning literature where one can train arbitrary kernel functions inside transformers end-to-end rather than having pre-defined ones;\n2) By leveraging spectral decomposition techniques along with optimization algorithms tailored specifically towards our problem statement – namely minimizing distance metric errors while maximizing similarity scores across transformed features -, we manage to reduce computational costs significantly compared traditional methods requiring quadratic memory footprint reduction down linear scale;\n3) Experimental validation conducted against state-of-the-art approaches demonstrate equivalent accuracy levels yet much lower computation times making KL-Transformers an attractive alternative especially suited toward large-scale applications demanding high throughput processing capabilities at reasonable resource utilization rates",
        "Topic": "Vision Transformer"
    },
    {
        "title": "Adversarial Feature Augmentation and Normalization for Visual Recognition",
        "abstract": "Recent advances in computer vision take advantage of adversarial data augmentation to improve the generalization of classification models. Here, we present an effective and efficient alternative that advocates adversarial augmentation on intermediate feature embeddings, instead of relying on computationally-expensive pixel-level perturbations. We propose $\\textbf{A}$dversarial $\\textbf{F}$eature $\\textbf{A}$ugmentation and $\\textbf{N}$ormalization (A-FAN), which ($i$) first augments visual recognition models with adversarial features that integrate flexible scales of perturbation strengths, ($ii$) then extracts adversarial feature statistics from batch normalization, and re-injects them into clean features through feature normalization. We validate the proposed approach across diverse visual recognition tasks with representative backbone networks, including ResNets and EfficientNets for classification, Faster-RCNN for detection, and Deeplab V3+ for segmentation. Extensive experiments show that A-FAN yields consistent generalization improvement over strong baselines across various datasets for classification, detection, and segmentation tasks, such as CIFAR-10, CIFAR-100, ImageNet, Pascal VOC2007, Pascal VOC2012, COCO2017, and Cityspaces. Comprehensive ablation studies and detailed analyses also demonstrate that adding perturbations to specific modules and layers of classification/detection/segmentation backbones yields optimal performance. Codes and pre-trained models are available in: https://github.com/VITA-Group/CV_A-FAN.",
        "authors": "T. Chen, Y. Cheng, Z. Gan, et.al",
        "keywords": [
            "Adversarial Feature Augmentation",
            "Generalization Improvement",
            "Visual Recognition Tasks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=2VEUIq9Yff",
        "pdf_src": "https://api2.openreview.net/pdf/a6b48911a784a0b8d60ea80135571ab1c7646f6b.pdf",
        "Code_src": "https://github.com/VITA-Group/CV_A-FAN",
        "Introduction": "Background: Recent advancements in computer vision have leveraged adversarial data augmentation techniques within classification models by introducing carefully crafted perturbations at the pixel level (\"pixel-wise\") during training.\n\nResearch Problem: The primary challenge addressed is how to effectively enhance model robustness without resorting to computationally expensive pixel-wise perturbations or requiring extensive computational resources due to their high dimensionality.\n \nMethodology: To address this issue, authors introduce a novel method called Adversarial Feature Augmentation and Normalization (A-FAN). This technique involves augmenting visual recognition models using adversarial features rather than directly modifying pixels:\n1. It introduces adversarial features integrating different levels of perturbation strength;\n2. Extracts these adversarial feature statistics via batch normalization processes; \n3. Reinjects those statistics into normal features after applying feature normalization.\n\nMain Contributions: The paper demonstrates significant improvements when employing A-FAN compared to existing state-of-the-art methods like Mixup, CutMix, and Progressive Data Augmentation (PDA) not only quantitatively but also qualitatively – it improves upon baseline performances consistently regardless of dataset complexity ranging from small-scale datasets like CIFAR-10 & 100 up to large-scale ones like ImageNet, Pascal VOC2007, Pascal VOC2012, COCO2017, and Cityspaces. Furthermore, they provide comprehensive ablations demonstrating that selectively applying perturbations can optimize results even further based on the architecture's specifics - whether it be classification, detection, or segmentation tasks. \n\nAvailability: All codes along with pre-trained models used throughout the study may be accessed publicly here: [GitHub repository link](https://github.com/VITA-Group/CV_A-FAN).",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Max-Affine Spline Insights Into Deep Network Pruning",
        "abstract": "State-of-the-art (SOTA) approaches to deep network (DN) training overparametrize the model and then prune a posteriori to obtain a ``winning ticket'' subnetwork that can achieve high accuracy. Using a recently developed spline interpretation of DNs, we obtain novel insights into how DN pruning affects its mapping. In particular, under the realm of spline operators, we are able to pinpoint the impact of pruning onto the DN's underlying input space partition and per-region affine mappings, opening new avenues in understanding why and when are pruned DNs able to maintain high performance. We also discover that a DN's spline mapping exhibits an early-bird (EB) phenomenon whereby the spline's partition converges at early training stages, bridging the recently developed DN spline theory and lottery ticket hypothesis of DNs. We finally leverage this new insight to develop a principled and efficient pruning strategy whose goal is to prune isolated groups of nodes that have a redundant contribution in the forming of the spline partition.\nExtensive experiments on four networks and three datasets validate that our new spline-based DN pruning approach reduces training FLOPs by up to 3.5x while achieving similar or even better accuracy than current state-of-the-art methods. Code is available at https://github.com/RICE-EIC/Spline-EB.",
        "authors": "H. You, R. Balestriero, Z. Lu, et.al",
        "keywords": [
            "spline interpretation",
            "DN pruning",
            "EB phenomenon"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=bMar2OkxVu",
        "pdf_src": "https://api2.openreview.net/pdf/580116663d2740b11efa601a5a8a1b2f45a19a0a.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses existing SOTA approaches for training deep neural networks (DNNs), which often result in highly parameterized models followed by post-hoc pruning techniques aimed at reducing complexity without sacrificing too much accuracy.\n\nResearch Problem: The problem addressed here concerns the lack of theoretical understanding behind these pruning strategies; specifically, it seeks answers regarding what aspects of the network are critical during the pruning process so as to retain high performance after optimization?\n\nMethodology: The authors introduce a spline interpretation framework applied to DNNs – a mathematical tool used traditionally within signal processing but adapted successfully with machine learning applications such as interpreting neural networks' behavior through their internal representations known as splines.\n\nMain Contributions:\n1. Insightful Interpretation: They provide novel insights about how different types of operations like pooling layers affect the input space partitioning and region-wise transformations represented by the spline functions inside the network architecture before any form of pruning occurs - thereby shedding light upon potential reasons certain parts might be more prone to being removed from the network structure effectively maintaining overall functionality.\n2. Early Bird Phenomenon: This study identifies \"Early-Bird\" (EB) phenomenon where the spline partitions converge rapidly towards optimal solutions earlier rather than later throughout training phases suggesting that there exists some initial simplicity inherent amongst complex architectures leading them toward efficiency via pruning processes.\n3. A New Pruning Strategy: Leveraging these findings they propose a novel method called Spline-Based Efficient Network (SBN) pruning technique designed explicitly around identifying redundant groups among neurons contributing redundantly across multiple regions within each layer allowing those less significant ones to be safely discarded thus resulting in reduced computational costs whilst preserving desired levels of accuracy comparable if not surpassing other top-performing methods currently employed today.\n\nValidation Results: Extensive empirical validation conducted using various benchmarks demonstrates that their proposed SBN pruning significantly decreases computation requirements by factors ranging between 2.0x down to approximately 3.5x compared against baseline results obtained employing standard pruning algorithms yet still maintains equivalent or superior classification accuracies making it both theoretically grounded & practically useful solution moving forward",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Online Coresets for Parameteric and Non-Parametric Bregman Clustering",
        "abstract": "We present algorithms that create coresets in an online setting for clustering problems based on a wide subset of Bregman divergences. Notably, our coresets have a small additive error, similar in magnitude to the gap between expected and empirical loss (Bachem et. al. 2017), and take update time $O(d)$ for every incoming point where $d$ is the dimension of the point. Our first algorithm gives online coresets of size $\\tilde{O}(\\mbox{poly}(k,d,\\epsilon,\\mu))$ for $k$-clusterings according to any $\\mu$-similar Bregman divergence. We further extend this algorithm to show the existence of non-parametric coresets, where the coreset size is independent of $k$, the number of clusters, for the same subclass of Bregman divergences. Our non-parametric coresets also function as coresets for non-parametric versions of the Bregman clustering like DP-Means. While these coresets provide additive error guarantees, they are significantly smaller for high dimensional data than the (relative-error) coresets obtained in (Bachem et. al 2015) for DP-Means--- for the input of size $n$ our coresets grow as $O(\\log n)$ while being independent of $d$ as opposed to $O(d^d)$ for points in $\\~R^d$ (Bachem et. al 2015). We also present experiments to compare the performance of our algorithms with other sampling techniques. ",
        "authors": "S. Shit, A. Dasgupta, R. Chhaya, et.al",
        "keywords": [
            "online coresets",
            "Bregman divergences",
            "non-parametric coresets"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=lAv8fShACA",
        "pdf_src": "https://api2.openreview.net/pdf/1b95b382af46c91a8a836b6a8db6f71bba6ca27e.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the problem of creating coresets efficiently during the process of clustering large datasets using various types of Bregman divergences.\n\nResearch Problem: How can we develop efficient algorithms within an online framework which generate coresets accurately without requiring extensive computational resources?\n\nMethods: The authors propose two main methods:\n1. An algorithm generating online coresets suitable for k-clustering tasks under $\\mu$-similar Bregman divergences.\n2. A non-parametric extension allowing coresets whose size does not depend on the actual number of clusters ($k$).\n\nMain Contributions: \n1. They introduce coresets having a small additive error comparable to the difference observed when comparing theoretical expectations against practical results from empirical losses.\n2. Their approach has a runtime complexity of $O(d)$ per new data point added into the dataset regardless of its dimensions.\n3. For certain subclasses of Bregman divergences, their method yields non-parametric coresets; meaning there's no need to specify parameters such as cluster numbers beforehand—this feature makes it particularly useful since one often doesn't know how many clusters exist or should be formed until after some analysis begins.\n4. Compared to previous work by Bachem et al., especially regarding DP-Means clustering over high-dimensional spaces, the proposed coresets offer better scaling properties - growing logarithmically rather than polynomially depending on the dimensionality of space (\\( O(\\log n) \\) versus \\( O(d^d) \\)). This allows them to handle larger datasets more effectively due to reduced memory requirements associated with storing fewer samples compared to traditional relative-error bounds approaches used previously.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Fusion of Global and Local Knowledge for Personalized Federated Learning",
        "abstract": "Personalized federated learning, as a variant of federated learning, trains customized models for clients using their heterogeneously distributed data. However, it is still inconclusive about how to design personalized models with better representation of shared global knowledge and personalized pattern. To bridge the gap, we in this paper explore personalized models with low-rank and sparse decomposition. Specifically, we employ proper regularization to \textract a low-rank global knowledge representation (GKR), so as to distill global knowledge into a compact representation.  Subsequently, we employ a sparse component over the obtained GKR to fuse the personalized pattern into the global knowledge. As a solution, we propose a two-stage proximal-based algorithm named \\textbf{Fed}erated learning with mixed \\textbf{S}parse and \\textbf{L}ow-\\textbf{R}ank representation (FedSLR) to efficiently search for the mixed models.  Theoretically, under proper assumptions,  we show that the GKR trained by FedSLR can at least sub-linearly converge to a stationary point of the regularized problem, and that the sparse component being fused can converge to its stationary point under proper settings. Extensive experiments also demonstrate the superior empirical performance of FedSLR. Moreover, FedSLR reduces the number of parameters, and lowers the down-link communication complexity, which are all desirable for federated learning algorithms. Source code is available in \\url{https://github.com/huangtiansheng/fedslr}.",
        "authors": "T. Huang, L. Shen, Y. Sun, et.al",
        "keywords": [
            "FedSLR",
            "Personalized Federated Learning",
            "Low-Rank Sparse Decomposition"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=QtrjqVIZna",
        "pdf_src": "https://api2.openreview.net/pdf/a7e75e82d8c8a8b53ecdea4d008a168a9bd974eb.pdf",
        "Code_src": "源代码链接：\\url{https://github.com/huangtiansheng/fedslr}\n",
        "Introduction": "Background: Personalized federated learning aims to train custom models on client-side heterogeneous datasets while preserving both global knowledge and personalization.\n\nResearch Problem: How to effectively integrate global knowledge representations within personalized models?\n\nMethod: We introduce a novel approach combining low-rank and sparse decompositions through appropriate regularization techniques:\n\n1. Extracting Low-Rank Global Knowledge Representation (GKR): This involves distilling global knowledge from client data.\n2. Fusing Personalized Patterns Into Global Knowledge Using Sparse Component: A sparse component is added onto the extracted GKR model to incorporate individual patterns.\n\nMain Contributions:\n- Propose a two-stage proximal-based algorithm called FedSLR designed specifically for federated learning tasks involving mixed sparse and low-rank representations.\n- Prove theoretically via certain conditions that our method leads to efficient convergence towards optimal solutions—both globally and locally—for the given problems.\n- Conduct extensive experimental validation demonstrating improved empirical performance compared to existing methods across various metrics such as parameter reduction or communication efficiency improvements—all critical aspects when considering practical applications like federated learning scenarios where privacy-preserving decentralized training needs optimization due to limited bandwidth constraints between devices involved in collaborative machine learning processes.\n\nSource Code Availability: All related source codes have been made publicly accessible online",
        "Topic": "Federated Learning"
    },
    {
        "title": "Temperature check: theory and practice for training models with softmax-cross-entropy losses",
        "abstract": "The softmax function combined with a cross-entropy loss is a principled approach to modeling probability distributions that has become ubiquitous in deep learning. The softmax function is defined by a lone hyperparameter, the temperature, that is commonly set to one or regarded as a way to tune model confidence after training; however, less is known about how the temperature impacts training dynamics or generalization performance. In this work we develop a theory of early learning for models trained with softmax-cross-entropy loss and show that the learning dynamics depend crucially on the inverse-temperature $\\beta$ as well as the magnitude of the logits at initialization, $||\\beta\\textbf{z}||_{2}$. We follow up these analytic results with a large-scale empirical study of a variety of model architectures trained on CIFAR10, ImageNet, and IMDB sentiment analysis. We find that generalization performance depends strongly on the temperature, but only weakly on the initial logit magnitude. We provide evidence that the dependence of generalization on $\\beta$ is not due to changes in model confidence, but is a dynamical phenomenon. It follows that the addition of $\\beta$ as a tunable hyperparameter is key to maximizing model performance. Although we find the optimal $\\beta$ to be sensitive to the architecture, experimental results suggest that tuning $\\beta$ over the range $10^{-2}$ to $10^1$  improves performance over all architectures studied. We find that smaller $\\beta$ may lead to better peak performance at the cost of sensitivity to the random seed.",
        "authors": "A. Agarwala, S. S. Schoenholz, J. Pennington, et.al",
        "keywords": [
            "softmax",
            "cross-entropy loss",
            "inverse-temperature"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LBA2Jj5Gqn",
        "pdf_src": "https://api2.openreview.net/pdf/043b6baf7bf6e4a5498430de0fea57db3864185d.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper discusses the softmax function along with cross-entropy loss which are widely used techniques within deep learning frameworks.\n\nResearch Problem: Despite its ubiquity, there's limited understanding regarding the impact of the softmax temperature parameter on both the training process and overall generalization capabilities when using softmax-cross-entropy loss.\n\nMethodology: The authors propose an analytical framework focusing on the early stages of learning during softmax-cross-entropy optimization where they consider the role of the inverse-temperature $\\beta$, as well as the magnitude of the logits at initialization ($||\\beta \\cdot z||_2$). They complement their theoretical findings with extensive empirical experiments across different neural network architectures trained on datasets like CIFAR10, ImageNet, and IMDB sentiment data.\n\nMain Contributions:\n1. A new theoretical perspective into the importance of the inverse-temperature $\\beta$ beyond just being a post-training adjustment.\n2. Demonstrated through empirical studies conducted under various conditions – showing that the softmax temperature significantly affects generalization ability while the initial logit magnitude does so marginally if any.\n3. Provided insights indicating that the effect of $\\beta$ on generalization isn't solely related to shifts in model uncertainty/confidence levels—it’s also influenced by dynamic processes occurring throughout the training phase itself.\n4. Suggested that incorporating $\\beta$ as another adjustable hyperparameter can enhance model performance considerably—suggesting it should replace the traditional practice of setting the temperature to either 1 or leaving it unchanged without further consideration based on other factors such as dataset size complexity etc.\n5. Identified empirically that finding the best performing $\\beta$ value might vary depending upon architectural choices yet generally found improvements from adjusting $\\beta$ between values around $10^{-2}$ to $10^1$.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Continual Learning by Modeling Intra-Class Variation",
        "abstract": "It has been observed that neural networks perform poorly when the data or tasks are presented sequentially. Unlike humans, neural networks suffer greatly from catastrophic forgetting, making it impossible to perform life-long learning. To address this issue, memory-based continual learning has been actively studied and stands out as one of the best-performing methods. We examine memory-based continual learning and identify that large variation in the representation space is crucial for avoiding catastrophic forgetting. Motivated by this, we propose to diversify representations by using two types of perturbations: model-agnostic variation (i.e., the variation is generated without the knowledge of the learned neural network) and model-based variation (i.e., the variation is conditioned on the learned neural network). We demonstrate that enlarging representational variation serves as a general principle to improve continual learning. Finally, we perform empirical studies which demonstrate that our method, as a simple plug-and-play component, can consistently improve a number of memory-based continual learning methods by a large margin.",
        "authors": "L. Yu, T. Hu, L. Hong, et.al",
        "keywords": [
            "representation diversity",
            "continual learning",
            "catastrophic forgetting"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=iDxfGaMYVr",
        "pdf_src": "https://api2.openreview.net/pdf/c7f349bd6cb1b97c2deabe02ef8b73b38c58d5df.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper addresses an important limitation of neural networks known as catastrophic forgetting - their inability to learn new information while retaining previously learned skills due to sequential presentation of data or tasks.\n\nResearch Problem:\nThe problem investigated here revolves around how to enable lifelong learning capabilities within neural networks so they do not forget old information during training with new data.\n\nMethods:\nTo tackle the research question above, researchers focus on Memory-Based Continual Learning algorithms where instead of overwriting previous memories like traditional neural networks would typically do upon encountering novel inputs, these systems retain past experiences through specialized mechanisms such as replay buffers.\nThey introduce a novel approach called Representational Diversification Strategy based on two types of perturbations:\n\n1. Model-Agnostic Variation – This type introduces variability into the input data independently of any specific task or model parameters; thus, it does not rely on prior knowledge about what was already learned but aims at broadening the range of possible inputs.\n\n2. Model-Based Variation – Here, perturbations depend directly on existing models' weights allowing them to adaptively adjust according to changes made throughout training sessions rather than just randomly altering features across all examples uniformly.\n\nMain Contributions:\nThe main contribution lies in demonstrating empirically via extensive experiments conducted under various conditions including different datasets sizes/models architectures/etc., that both kinds of introduced variations significantly reduce catastrophic forgetting compared to baseline approaches leading towards more robust continuous learners capable performing better overall performance metrics associated with continual learning benchmarks.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Defense Against Reward Poisoning Attacks in Reinforcement Learning",
        "abstract": "We study defense strategies against reward poisoning attacks in reinforcement learning. As a threat model, we consider cost-effective targeted attacks---these attacks minimally alter rewards to make the attacker's target policy uniquely optimal under the poisoned rewards, with the optimality gap specified by an attack parameter. Our goal is to design agents that are robust against such attacks in terms of the worst-case utility w.r.t. the true, unpoisoned, rewards while computing their policies under the poisoned rewards. We propose an optimization framework for deriving optimal defense policies, both when the attack parameter is known and unknown. For this optimization framework, we first provide characterization results for generic attack cost functions. These results show that the functional form of the attack cost function and the agent's knowledge about it are critical for establishing lower bounds on the agent's performance, as well as for the computational tractability of the defense problem. We then focus on a cost function based on $\\ell_2$ norm, for which we show that the defense problem can be efficiently solved and yields defense policies whose expected returns under the true rewards are lower bounded by their expected returns under the poison rewards. Using simulation-based experiments, we demonstrate the effectiveness and robustness of our defense approach.",
        "authors": "K. Banihashem, A. Singla, G. Radanovic",
        "keywords": [
            "defense strategies",
            "reward poisoning attacks",
            "reinforcement learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=goPsLn3RVo",
        "pdf_src": "https://api2.openreview.net/pdf/3c064ebec56270e2f5766e2cea64ead14e12219a.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper addresses the issue of defending reinforcement learning systems from reward poisoning attacks where attackers manipulate the rewards received during training or testing phases.\n\nResearch Problem: The challenge lies in designing agents capable of maintaining high utility even if they operate within environments altered through these attacks—agents must remain effective despite receiving corrupted feedback signals designed specifically to undermine them.\n\nMethods: To tackle this research question, authors introduce an optimization framework aimed at developing defenses tailored either knowing—or not knowing—the specifics of the adversary’s strategy regarding how much effort will go into altering the rewards (\"attack parameter\").\n\nMain Contributions:\n1. They establish foundational insights concerning what characteristics define successful attack cost functions.\n2. Specifically focusing on an $\\ell_2$-norm-based cost function, offer efficient solutions allowing computation of defensive strategies ensuring certain guarantees over the agent's behavior regardless of whether its opponent's intentions toward reward manipulation have been disclosed upfront; \n3. Conduct simulations validating empirical evidence supporting efficacy & resilience demonstrated via proposed defense mechanisms",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer",
        "abstract": "When presented with a data stream of two statistically dependent variables, predicting the future of one of the variables (the target stream) can benefit from information about both its history and the history of the other variable (the source stream). For example, fluctuations in temperature at a weather station can be predicted using both temperatures and barometric readings. However, a challenge when modelling such data is that it is easy for a neural network to rely on the greatest joint correlations within the target stream, which may ignore a crucial but small information transfer from the source to the target stream. As well, there are often situations where the target stream may have previously been modelled independently and it would be useful to use that model to inform a new joint model. Here, we develop an information bottleneck approach for conditional learning on two dependent streams of data. Our method, which we call Transfer Entropy Bottleneck (TEB), allows one to learn a model that bottlenecks the directed information transferred from the source variable to the target variable, while quantifying this information transfer within the model. As such, TEB provides a useful new information bottleneck approach for modelling two statistically dependent streams of data in order to make predictions about one of them.",
        "authors": "D. Kalajdzievski, X. Mao, P. Fortier-poisson, et.al",
        "keywords": [
            "statistical dependency",
            "information bottleneck",
            "Transfer Entropy Bottleneck"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=kJcwlP7BRs",
        "pdf_src": "https://api2.openreview.net/pdf/f1ad5f381d8b47a9a8c212e3474f16b6bd326980.pdf",
        "Code_src": "",
        "Introduction": "Background: Predicting the future state of a variable by analyzing historical data has many applications across various fields like finance or meteorology. When dealing with multiple interdependent variables, incorporating additional context beyond just their own past states could improve predictive performance.\n\nResearch Question: How do you effectively leverage information from related yet independent sources during prediction tasks?\n\nMethod: We introduce the Transfer Entropy Bottleneck (TEB), based on Information Bottleneck Theory proposed by Amari et al., 1997. The core idea behind our approach involves training a neural network architecture whose goal is not only to predict accurately given current observations as most models aim towards doing so; instead, it also aims to minimize the mutual information between the input distribution and output distribution through a bottleneck layer.\nThe bottleneck forces the network to focus more on what's essential - namely how much each feature contributes toward predicting another specific feature rather than memorizing irrelevant patterns present solely due to correlation among features themselves without causation links involved.\n\nMain Contributions:\n1. A novel framework called Transfer Entropy Bottleneck designed specifically tailored around conditional learning over two dependent streams of data points;\n2. Demonstrated empirical evidence showing improved accuracy compared traditional methods especially those relying heavily upon capturing complex interactions amongst all inputs simultaneously leading into potential vanishing gradients issues encountered deep architectures;\n3. Provided insights regarding optimal trade-offs needed balance complexity reduction achieved via regularization techniques against preserving discriminative power necessary accurate predictions made possible by considering dependencies existing between different variables being considered jointly hereunder study.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Black-Box Prompt Learning for Pre-trained Language Models",
        "abstract": "The increasing scale of general-purpose Pre-trained Language Models (\\textbf{PLMs}) necessitates the study of more efficient adaptation across different downstream tasks. In this paper, we establish a Black-box Discrete Prompt Learning (\\textbf{BDPL}) to resonate with pragmatic interactions between the cloud infrastructure and edge devices. Particularly, instead of fine-tuning the model in the cloud, we adapt PLMs by prompt learning, which efficiently optimizes only a few parameters of the discrete prompts. Moreover, we consider the scenario that we do not have access to the parameters and gradients of the pre-trained models, except for its outputs given inputs. This black-box setting secures the cloud infrastructure from potential attack and misuse to cause a single-point failure, which is preferable to the white-box counterpart by current infrastructures. Under this black-box constraint, we apply a variance-reduced policy gradient algorithm to estimate the gradients of parameters in the categorical distribution of each discrete prompt. In light of our method, the user devices can efficiently tune their tasks by querying the PLMs bounded by a range of API calls. Our experiments on RoBERTa and GPT-3 demonstrate that the proposed algorithm achieves significant improvement on eight benchmarks in a cloud-device collaboration manner.  Finally, we conduct in-depth case studies to comprehensively analyze our method in terms of various data sizes, prompt lengths, training budgets, optimization objectives, prompt transferability, and explanations of the learned prompts.",
        "authors": "S. Diao, Z. Huang, R. Xu, et.al",
        "keywords": [
            "black-box discrete prompt learning",
            "cloud-edge collaboration",
            "variance-reduced policy gradient"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=IvsGP7xRvm",
        "pdf_src": "https://api2.openreview.net/pdf/21cf910e4707fd9dd3810066401e3cf9b2413bda.pdf",
        "Code_src": "",
        "Introduction": "Background: The rapid growth of General-Purpose Pre-trained Language Models (PLMs) has led to an increased need for studying how these models are adapted effectively among diverse downstream tasks.\n\nResearch Problem: How does one create a system where PLMs could be adapted quickly without requiring extensive retraining?\n\nMethod: We introduce a novel approach called \"Black-box Discrete Prompt Learning\" (BDPL), designed specifically within the context of cloud-edge device interaction scenarios involving PLMs. BDPL involves adapting PLMs through prompt learning rather than traditional fine-tuning techniques; it focuses solely on optimizing certain parameters related to discrete prompts while keeping other parts unchanged or frozen during the process.\nWe further address practical constraints such as when direct access to pre-trained model parameters and gradients isn't feasible – ensuring security against attacks due to vulnerabilities present if full knowledge were accessible - hence employing what's termed 'black-box' settings compared to conventional approaches relying on open ('white-box') information about internal workings of the model.\n\nMain Contributions:\n1. Developed BDPL framework tailored towards enabling efficient task adaptation using PLMs at both cloud and edge levels;\n2. Demonstrated efficacy over existing methods via empirical tests conducted under varying conditions including different datasets sizes, prompt lengths, budget constraints etc.;\n3. Conducted comprehensive analysis into aspects like prompt transferability & interpretability providing insights beyond just performance metrics alone;\n\nIn summary, this work presents a promising new paradigm for adapting large-scale language models flexibly according to specific requirements",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Action Poisoning Attacks on Linear Contextual Bandits",
        "abstract": "Contextual bandit algorithms have many applicants in a variety of scenarios. In order to develop trustworthy contextual bandit systems, understanding the impacts of various adversarial attacks on contextual bandit algorithms is essential. In this paper, we propose a new class of attacks: action poisoning attacks, where an adversary can change the action signal selected by the agent. We design action poisoning attack schemes against disjoint linear contextual bandit algorithms in both white-box and black-box settings. We further analyze the cost of the proposed attack strategies for a very popular and widely used bandit algorithm: LinUCB. We show that, in both white-box and black-box settings, the proposed attack schemes can force the LinUCB agent to pull a target arm very frequently by spending only logarithm cost. We also extend the proposed attack strategies to generalized linear models and show the effectiveness of the proposed strategies.",
        "authors": "G. Liu, L. Lai",
        "keywords": [
            "attack",
            "contextual bandit",
            "action poisoning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=yhGCKUsKJS",
        "pdf_src": "https://api2.openreview.net/pdf/98542d094601c6f09c661cff1a66a9cbe0be17e0.pdf",
        "Code_src": "",
        "Introduction": "Background: Contextual bandit algorithms are extensively applied across diverse domains due to their ability to make decisions based on historical data while considering uncertainty about future outcomes. Ensuring these systems' trustworthiness requires comprehending how they respond under adversarial conditions.\n\nResearch Problem: This study identifies a novel type of attack strategy known as \"action poisoning,\" which involves manipulating the actions chosen by agents within contextual bandit frameworks without altering rewards or feedback signals directly provided during interactions with arms.\n\nMethods: The researchers developed two types of action poisoning attacks tailored specifically for disjoint linear contextual bandit algorithms – one when full knowledge (\"white-box\") over the algorithm's parameters exists; another designed if such information remains unknown (\"black-box\"). They conducted extensive analysis focusing particularly on the impact upon LinUCB - a well-known and commonly utilized bandit algorithm among practitioners.\n \nMain Contributions: The primary contributions include:\n1. Identification & Definition of Action Poisoning Attacks: Introducing adversaries who alter decision-making processes rather than reward signals themselves introduces significant challenges compared to traditional adversarial methods targeting rewards alone.\n2. Attack Schemes Designation: Crafting effective attack methodologies specific to disjoint linear contextual bandit algorithms whether complete access ('white-box') versus limited insight ('black-box').\n3. Cost Analysis: Demonstrating through empirical evidence using LinUCB as case studies showing minimal costs required per instance yet high frequency manipulation achievable via these tactics even at low computational expense.\n4. Extension to Generalized Linear Models: Extending findings beyond simple linear bandits into broader contexts including more complex machine learning architectures like generalized linear models validates applicability broadly outside narrow use cases studied initially.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Image Compression with Product Quantized Masked Image Modeling",
        "abstract": "Recent neural compression methods have been based on the popular hyperprior framework. It relies on Scalar Quantization and offers a very strong compression performance. This contrasts from recent advances in image generation and representation learning, where Vector Quantization is more commonly employed.\nIn this work, we attempt to bring these lines of research closer by revisiting vector quantization for image compression. We build upon the VQ-VAE framework and introduce several modifications. First, we replace the vanilla vector quantizer by a product quantizer. This intermediate solution between vector and scalar quantization allows for a much wider set of rate-distortion points: It implicitly defines high-quality quantizers that would otherwise require intractably large codebooks. Second, inspired by the success of Masked Image Modeling (MIM) in the context of self-supervised learning and generative image models, we propose a novel conditional entropy model which improves entropy coding by modelling the co-dependencies of the quantized latent codes. The resulting PQ-MIM model is surprisingly effective: its compression performance on par with recent hyperprior methods. It also outperforms HiFiC in terms of FID and KID metrics when optimized with perceptual losses (e.g. adversarial). Finally, since PQ-MIM is compatible with image generation frameworks, we show qualitatively that it can operate under a hybrid mode between compression and generation, with no further training or finetuning. As a result, we explore the extreme compression regime where an image is compressed into 200 bytes, i.e., less than a tweet.",
        "authors": "A. El-nouby, M. J. Muckley, K. Ullrich, et.al",
        "keywords": [
            "vector quantization",
            "PQ-MIM",
            "hybrid mode"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Z2L5d9ay4B",
        "pdf_src": "https://api2.openreview.net/pdf/92eef34aab1e62373a1f6d23cdc7851f59168c87.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses how most current neural compression techniques are built around the hyperprior framework using scalar quantization method while vector quantization has gained popularity recently in other fields like image generation and representation learning.\n\nResearch Problem:\nThe problem addressed here involves bridging the gap existing within the field; specifically, they aim at improving the effectiveness of vector quantization as opposed to scalar quantization used widely before due to better results seen elsewhere such as in image generation tasks.\n\nMethodology:\nTo address their issue concerning vector quantization's application towards image compression, authors revisit the Vector Quantized Variational Autoencoder (VQ-VAE) architecture but make some changes:\n\n1. They substitute the standard vector quantizer with a product quantizer - a compromise between vector and scalar quantization offering broader range of trade-offs between bit-rate and distortion quality without requiring prohibitively extensive codebooks needed traditionally for scalar quantization.\n\n2. Additionally, drawing inspiration from Masked Image Modeling approaches successful both in supervised learning settings along with generative models, researchers suggest a new conditional entropy model capable of modeling dependencies among quantized latent variables leading to improved entropy coding efficiency through exploiting those relationships during encoding process.\n\nMain Contributions:\nTheir contributions include development of a novel PQ-MIM (Product Quantization Masked Image Modeling) system which achieves competitive compression rates akin to state-of-the-art hyperprior-based solutions yet surpasses them notably regarding perceptual similarity measures such as Frechet Inception Distance (FID) and Kernel Independence Distance (KID), especially after optimization via perceptual loss functions including adversarial ones. Furthermore, PQ-MIM maintains compatibility across various architectures designed for generating images allowing operation seamlessly blending aspects of compression & generation modes without additional retraining requirements even down to extremely low-bitrate regimes – achieving compression ratios below 200 bytes per image comparable size-wise smaller than tweets!",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Numerical Accounting in the Shuffle Model of Differential Privacy",
        "abstract": "Shuffle model of differential privacy is a novel distributed privacy model based on a combination of local privacy mechanisms and a secure shuffler. It has been shown that the additional randomisation provided by the shuffler improves privacy bounds compared to the purely local mechanisms. Accounting tight bounds, however, is complicated by the complexity brought by the shuffler. The recently proposed numerical techniques for evaluating $(\\varepsilon,\\delta)$-differential privacy guarantees have been shown to give tighter bounds than commonly used methods for compositions of various complex mechanisms. In this paper, we show how to utilise these numerical accountants for adaptive compositions of general $\\varepsilon$-LDP shufflers and for shufflers of $k$-randomised response mechanisms, including their subsampled variants. This is enabled by an approximation that speeds up the evaluation of the corresponding privacy loss distribution from $\\mathcal{O}(n^2)$ to $\\mathcal{O}(n)$, where $n$ is the number of users, without noticeable change in the resulting $\\delta(\\varepsilon)$-upper bounds. We also demonstrate looseness of the existing bounds and methods found in the literature, improving previous composition results for shufflers significantly. ",
        "authors": "A. Koskela, M. A. Heikkilä, A. Honkela",
        "keywords": [
            "Shuffle model",
            "Differential Privacy",
            "Numerical Techniques"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=11osftjEbF",
        "pdf_src": "https://api2.openreview.net/pdf/b307e9c80d4566061f7ab118229bb554cea859ed.pdf",
        "Code_src": "",
        "Introduction": "Background: Differential Privacy (DP) is one of the most widely studied privacy models which aims at preserving individual privacy while allowing data analysis tasks such as machine learning algorithms or statistical analyses with respect to some underlying dataset. However, traditional DP mechanisms often suffer from loose privacy bounds due to the difficulty involved in accounting for interactions between different components.\n\nResearch Problem: How can we improve the accuracy of privacy bounds when composing multiple DP mechanisms?\n\nMethod: In this work, they propose using numerical accountants - a set of computational tools designed specifically for DP problems – to evaluate privacy losses more accurately during the composition process involving both general $\\epsilon$-LDP shufflers and specific types of randomized response mechanisms like those with k-randomization along with their subsampled counterparts.\nThey introduce an efficient approximation technique enabling faster computation time reduction from O(n^2) to O(n), thus making it feasible even large-scale datasets are considered within acceptable performance constraints.\n\nMain Contributions:\n1. They successfully apply numerical accountants towards adaptive compositions of general $\\epsilon$-LDP shufflers & k-randomized response mechanisms (including subsampled versions).\n2. Their approach allows us to compute tighter $\\delta(\\epsilon)$ upper bounds w.r.t. original compositions leading significant improvements over prior state-of-the-art results concerning shuffler compositions; \n3. Finally, through empirical experiments demonstrating looseness issues present elsewhere related works further validate effectiveness enhancements made here presented",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Learning Energy Conserving Dynamics Efficiently with Hamiltonian Gaussian Processes",
        "abstract": "Hamiltonian mechanics is one of the cornerstones of the natural sciences. Recently there has been significant interest in learning Hamiltonian systems in a free-form way directly from trajectory data. Previous methods have tackled the problem of learning from many short, low-noise trajectories, but learning from a small number of long, noisy trajectories, whilst accounting for model uncertainty has not been addressed. In this work, we present a Gaussian process model for Hamiltonian systems with efficient decoupled parameterisation, and introduce an energy-conserving shooting method that allows robust inference from both short and long trajectories. We demonstrate the method's success in learning Hamiltonian systems in various data settings.",
        "authors": "M. Ross, M. Heinonen",
        "keywords": [
            "trajectory data",
            "Gaussian process model",
            "energy-conserving shooting"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=DHEZuKStzH",
        "pdf_src": "https://api2.openreview.net/pdf/7f396d5ecb1c35699c345eb4ef8a930b8a7169e2.pdf",
        "Code_src": "",
        "Introduction": "Background: Hamiltonian mechanics plays a fundamental role in physics as it describes how dynamical systems evolve over time.\n\nResearch Problem: The challenge presented by recent studies involves learning Hamiltonian systems through direct analysis of trajectory data without relying on prior knowledge or assumptions about system properties such as potential functions.\nSpecifically, previous works focused mainly on inferring models when given numerous brief, noise-free trajectories; however, they did not address scenarios where only few extended, noisy trajectories are available while considering uncertainties within these learned models.\n\nMethodology: To overcome limitations associated with existing approaches to learn Hamiltonian dynamics using trajectory data under varying conditions including those involving fewer samples per trajectory yet longer duration along with higher levels of measurement error, our study introduces two key components:\n1. A novel Gaussian Process (GP) framework designed specifically tailored towards modeling Hamiltonian systems which incorporates an efficient decoupled parameterization strategy enabling more accurate predictions even amidst complex interactions between variables;\n2. An Energy-Conserving Shooting Method - This technique aids in making robust estimates across different types of trajectories regardless if they're shorter ones capturing rapid changes leading up to equilibrium states versus longer ones tracking evolution processes over considerable periods.\n\nMain Contributions: Our contributions lie primarily in developing new methodologies capable handling diverse datasets related to studying Hamiltonian systems via machine learning techniques like GP regression coupled with specialized algorithms ensuring consistency preservation during prediction tasks despite inherent complexities found throughout real-world applications concerning physical laws governing motion described by classical mechanics principles embodied within Hamiltonian formalism.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Costs and Benefits of Fair Regression",
        "abstract": "    Real-world applications of machine learning tools in high-stakes domains are often regulated to be fair, in the sense that the predicted target should satisfy some quantitative notion of parity with respect to a protected attribute. However, the exact tradeoff between fairness and accuracy with a real-valued target is not entirely clear. In this paper, we characterize the inherent tradeoff between statistical parity and accuracy in the regression setting by providing a lower bound on the error of any attribute-blind fair regressor. Our lower bound is sharp, algorithm-independent, and admits a simple interpretation: when the moments of the target differ between groups, any fair algorithm has to make an error on at least one of the groups. We further extend this result to give a lower bound on the joint error of any (approximately) fair algorithm, using the Wasserstein distance to measure the quality of the approximation. With our novel lower bound, we also show that the price paid by a fair regressor that does not take the protected attribute as input is less than that of a fair regressor with explicit access to the protected attribute. On the upside, we establish the first connection between individual fairness, accuracy parity, and the Wasserstein distance by showing that if a regressor is individually fair, it also approximately verifies the accuracy parity, where the gap is again given by the Wasserstein distance between the two groups. Inspired by our theoretical results, we develop a practical algorithm for fair regression through the lens of representation learning, and conduct experiments on a real-world dataset to corroborate our findings.",
        "authors": "H. Zhao",
        "keywords": [
            "fairness",
            "accuracy",
            "regression"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=v6anjyEDVW",
        "pdf_src": "https://api2.openreview.net/pdf/175407d1cf13ebb6308e74dba1303f63fc2c2a09.pdf",
        "Code_src": "",
        "Introduction": "Background: The use of machine learning models in high-stakes scenarios requires them to maintain certain ethical standards such as being fair towards different groups defined by sensitive attributes.\n\nResearch Question: This study investigates how much compromise there must necessarily occur regarding prediction accuracy versus model fairness under the requirement of satisfying some form of statistical parity concerning these sensitive attributes within the context of regression tasks.\n\nMethodology: The authors provide a lower bound on the expected error rate experienced by any \"attribute-blind\" fair regressor which cannot consider the sensitive attribute directly during training or predictions without violating the fairness constraint against discrimination based on said attribute(s). They then generalize their analysis into considering the joint error across multiple groups while still adhering to fairness constraints but allowing for more flexibility via the Wasserstein distance metric - a concept from optimal transport theory used here instead of traditional L2 norms due to its ability to capture distributional differences rather than just mean values.\n\nMain Contributions:\n1. A tight lower bound on the error rates achievable even after accounting for fairness requirements.\n2. An extension demonstrating similar bounds apply regardless of whether algorithms approximate fairness indirectly compared to those explicitly designed around fairness considerations involving direct knowledge about sensitive attributes like race/gender/etc.\n3. Establishing connections among notions of individual fairness amongst individuals/groups; accuracy parity guarantees; and measures capturing distributional discrepancies – specifically highlighting that if a model satisfies individual fairness conditions imposed upon each person separately considered independently), they will likely exhibit accurate group-level performance too albeit subject to a certain degree of discrepancy measured along the Wasserstein space dimensionality reduction axis.\n\nTheoretical insights gained inform development",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Transductive Decoupled Variational Inference for Few-Shot Classification",
        "abstract": "The versatility to learn from a handful of samples is the hallmark of human intelligence. Few-shot learning is an endeavour to transcend this capability down to machines. Inspired by the promise and power of probabilistic deep learning, we propose a novel variational inference network for few-shot classification (coined as TRIDENT) to decouple the representation of an image into semantic and label latent variables, and simultaneously infer them in an intertwined fashion. To induce task-awareness, as part of the inference mechanics of TRIDENT, we exploit information across both query and support images of a few-shot task using a novel built-in attention-based transductive feature extraction module (we call AttFEX). Our extensive experimental results corroborate the efficacy of TRIDENT and demonstrate that, using the simplest of backbones, it sets a new state-of-the-art in the most commonly adopted datasets miniImageNet and tieredImageNet (offering up to 4% and 5% improvements, respectively), as well as for the recent challenging cross-domain miniImagenet --> CUB scenario offering a significant margin (up to 20% improvement) beyond the best existing baselines.",
        "authors": "A. Singh, H. J. Rad",
        "keywords": [
            "Few-shot Learning",
            "Variational Inference Network",
            "Attention-based Transductive Feature Extraction"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=bomdTc9HyL",
        "pdf_src": "https://api2.openreview.net/pdf/470235e1ee892ce56bda0b49977bfe4fab290d0c.pdf",
        "Code_src": "",
        "Introduction": "Background: The ability to learn effectively with only a small number of examples has been identified as one of the defining characteristics of human intelligence.\n\nResearch Question: How can machine learning models be designed to achieve similar capabilities?\n\nMethod: In response to these questions, researchers have proposed various approaches known collectively as \"Few-Shot Learning.\" This paper introduces a novel method called TRIDENT which aims to improve upon previous methods through the use of Variational Inference Networks (\"VINs\") combined with Attention-Based Feature Extraction modules.\n\nMain Contributions:\n1. **Novel Model Architecture**: TRIDENT separates representations within an image between semantic features common across all classes and class-specific labels.\n2. **Variational Inference Network**: It uses VINs not just at test time but also during training phase to estimate uncertainty about the model's predictions via posterior distributions over parameters.\n3. **Attention Mechanism**: Introduces an Attention-Based Transductive Feature Extraction Module (AttFEX) capable of capturing task-aware knowledge when processing queries alongside their associated support set data points—this helps in better generalization performance on unseen tasks or domains.\n4. **Performance Improvements**: Extensive experiments show that even without complex neural networks architectures like ResNets used previously, TRIDENT significantly outperforms prior works including those trained specifically for transfer learning scenarios such as miniImageNet and tieredImageNet benchmarks where it achieves gains ranging around 4-5%, surpassing current state-of-the-art solutions; further demonstrating its effectiveness against more challenging cross-domain transfer settings involving miniImagenet to CUB dataset yielding substantial margins exceeding other leading techniques by approximately 20%.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Mixed effects in machine learning – A flexible mixedML framework to add random effects to supervised machine learning regression",
        "abstract": "Clustered data can frequently be found not only in social and behavioral sciences (e.g., multiple measurements of individuals) but also in typical machine learning problems (e.g., weather forecast in different cities, house prices in different regions). This implies dependencis for observations within one cluster, leading to violations of i.i.d. assumptions, biased estimates, and false inference. A typical approach to address this issue is to include random effects instead of fixed effects. We introduce the general mixedML framework, which includes random effects in supervised regression machine learning models, and present different estimation procedures. A segmentation of the problem allows to include random effects as an additional correction to the standard machine learning regression problem. Thus, the framework can be applied on top of the machine learning task, without the need to change the model or architecture, which distinguishes mixedML from other models in this field. With a simulation study and empirical data sets, we show that the framework produces comparable estimates to typical mixed effects frameworks in the linear case and increases the prediction quality and the gained information of the standard machine learning models in both the linear and non-linear case. Furthermore, the presented estimation procedures significantly decrease estimation time. Compared to other approaches in this area, the framework does not restrict the choice of machine learning algorithms and still includes random effects.",
        "authors": "P. Kilian, S. Ye, A. Kelava",
        "keywords": [
            "random effects",
            "mixedML framework",
            "estimation procedures"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=MKZyHtmfwH",
        "pdf_src": "https://api2.openreview.net/pdf/f6b78160352b8d536e212b42d49ba9cfcc4a2dbe.pdf",
        "Code_src": "",
        "Introduction": "Background: Clustered data are common in various fields such as social science researches and machine learning tasks like weather forecasting across cities.\nResearch Problem: The clustered nature of these datasets violates independent and identically distributed (i.i.d.) assumption underlying many statistical methods resulting in biased estimations and incorrect inferences.\n\nMethod: To tackle this challenge, authors propose a novel MixedML framework by incorporating random effects into supervised regression machine learning models along with presenting several estimation strategies tailored specifically towards addressing clustering issues while preserving flexibility over existing machine learning architectures.\n\nMain Contributions:\n1. MixedML introduces a unified approach where random effects serve as supplementary corrections rather than replacing traditional fixed effects commonly used before when dealing with clustered data; \n2. It's designed so it can seamlessly integrate onto pre-existing machine learning pipelines requiring no modifications;\n3. Empirical evidence shows its efficacy through simulations studies comparing against conventional mixed-effects models under linear conditions - further demonstrating improved predictive performance even beyond linearity scenarios including nonlinear ones;\n4. Additionally, proposed estimation techniques substantially reduce computational costs compared to alternative solutions available today allowing practitioners greater efficiency during implementation phases",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "Probing Predictions on OOD Images via Nearest Categories",
        "abstract": "We study out-of-distribution (OOD) prediction behavior of neural networks when they classify images from unseen classes or corrupted images. To probe the OOD behavior, we introduce a new measure, nearest category generalization (NCG), where we compute the fraction of OOD inputs that are classified with the same label as their nearest neighbor in the training set. Our motivation stems from understanding the prediction patterns of adversarially robust networks, since previous work has identified unexpected consequences of training to be robust to norm-bounded perturbations. We find that robust networks have consistently higher NCG accuracy than natural training, even when the OOD data is much farther away than the robustness radius. This implies that the local regularization of robust training has a significant impact on the network’s decision regions. We replicate our findings using many datasets, comparing new and existing training methods. Overall, adversarially robust networks resemble a nearest neighbor classifier when it comes to OOD data.",
        "authors": "Y. Yang, C. Rashtchian, R. Salakhutdinov, et.al",
        "keywords": [
            "ood prediction",
            "nearest category generalization",
            "adversarial robustness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=fTNorIvVXG",
        "pdf_src": "https://api2.openreview.net/pdf/39fe52d46934f701fd340566599299aa66d280ce.pdf",
        "Code_src": "",
        "Introduction": "Background: Neural networks often struggle at predicting novel image categories (\"out-of-distribution\" [OOD]) due to overfitting during training.\n\nResearch Question: How do neural networks behave regarding predictions for these OOD cases?\n\nMethodology: The authors propose \"nearest category generalization\" (NCG), which measures how frequently an OOD input receives its correct class label if instead of being predicted by itself would receive this label just because one of them shares some visual features within the dataset's training samples' vicinity - akin to what happens near neighbors might influence each other spatially but not necessarily semantically.\n\nMain Contributions:\n1. Introduced 'Nearest Category Generalization' metric.\n2. Demonstrated that adversarially trained models perform better under OOD conditions compared to standard training approaches – indicating improved generalization beyond simple robustness against specific perturbations like those bounded by norms imposed through adversarial examples.\n3. Replicated results across various datasets showing consistency regardless of whether there was prior knowledge about potential OOD distributions before training began; suggesting broader implications rather than specialized solutions tailored only certain types of unseen scenarios encountered post-training time.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Better Theory for SGD in the  Nonconvex World",
        "abstract": "Large-scale nonconvex optimization problems are ubiquitous in modern machine learning, and among practitioners interested in solving them, Stochastic Gradient Descent (SGD) reigns supreme. We revisit the analysis of SGD in the nonconvex setting and propose a new variant of the recently introduced \\emph{expected smoothness} assumption which governs the behavior of the second moment of the stochastic gradient. We show that our assumption is both more general and more reasonable than assumptions made in all prior work. Moreover, our results yield the optimal  $\\mathcal{O}(\\epsilon^{-4})$ rate for finding a stationary point of nonconvex smooth functions, and recover the optimal $\\mathcal{O}(\\epsilon^{-1})$ rate for finding a global solution if the Polyak-Łojasiewicz condition is satisfied. We compare against convergence rates under convexity and prove a theorem on the convergence of SGD under Quadratic Functional Growth and convexity, which might be of independent interest. Moreover, we perform our analysis in a framework which allows for a detailed study of the effects of a wide array of sampling strategies and minibatch sizes for finite-sum optimization problems. We corroborate our theoretical results with experiments on real and synthetic data.",
        "authors": "A. Khaled, P. Richtárik",
        "keywords": [
            "Stochastic Gradient Descent",
            "Nonconvex Optimization",
            "Expected Smoothness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=AU4qHN2VkS",
        "pdf_src": "https://api2.openreview.net/pdf/bc02ef8f9b26aaaffe84b4efb9c73f2931fe77b1.pdf",
        "Code_src": "",
        "Introduction": "Background: Large-scale nonconvex optimization problems play an important role in modern machine learning.\n\nResearch Question: How to analyze the performance of Stochastic Gradient Descent (SGD) in the nonconvex setting?\n\nMethod: The authors introduce a new expected smoothness assumption based on the second moment of the stochastic gradient.\nThey also provide optimal convergence rates for finding a stationary point or a global solution depending on whether the Polyak-Łojasiewicz condition is satisfied.\nThe analysis is performed within a framework allowing studying various sampling strategies and minibatch sizes for finite-sum optimization problems.\n\nMain Contributions:\n1. A novel expected smoothness assumption governing the behavior of the second moment of the stochastic gradient proposed by the authors,\n2. Optimal convergence rates for finding a stationary point or a global solution obtained using this assumption,\n3. Proofs regarding the convergence of SGD when considering Quadratic Functional Growth and convexity conditions separately as well as together,\n4. Experimental validation through empirical studies conducted over real and synthetic datasets demonstrating agreement between theory predictions",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Cheap and Deterministic Inference for Deep State-Space Models of Interacting Dynamical Systems",
        "abstract": "Graph neural networks are often used to model interacting dynamical systems since they\ngracefully scale to systems with a varying and high number of agents. While there has been\nmuch progress made for deterministic interacting systems, modeling is much more challenging\nfor stochastic systems in which one is interested in obtaining a predictive distribution\nover future trajectories. Existing methods are either computationally slow since they rely\non Monte Carlo sampling or make simplifying assumptions such that the predictive distribution\nis unimodal. In this work, we present a deep state-space model which employs graph\nneural networks in order to model the underlying interacting dynamical system. The predictive\ndistribution is multimodal and has the form of a Gaussian mixture model, where the\nmoments of the Gaussian components can be computed via deterministic moment matching\nrules. Our moment matching scheme can be exploited for sample-free inference leading to\nmore efficient and stable training compared to Monte Carlo alternatives. Furthermore, we\npropose structured approximations to the covariance matrices of the Gaussian components\nin order to scale up to systems with many agents. We benchmark our novel framework\non two challenging autonomous driving datasets. Both confirm the benefits of our method\ncompared to state-of-the-art methods. We further demonstrate the usefulness of our individual\ncontributions in a carefully designed ablation study and provide a detailed empirical\nruntime analysis of our proposed covariance approximations.",
        "authors": "A. Look, B. Rakitsch, M. Kandemir, et.al",
        "keywords": [
            "graph neural networks",
            "multimodal predictive distribution",
            "determinant moment matching"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=dqgdBy4Uv5",
        "pdf_src": "https://api2.openreview.net/pdf/457899f5145383b31c2dda24ea177e34ceeb0090.pdf",
        "Code_src": "",
        "Introduction": "Background: Graph neural networks have become popular tools due to their ability to handle large-scale dynamic systems involving multiple agents effectively.\n\nResearch Problem: However, existing approaches struggle when dealing with stochastic systems because they require computational-intensive Monte Carlo simulations.\n \nMethod: This paper introduces a new approach based on a deep state-space model using graph neural networks capable of generating multimodal predictive distributions over future trajectories through deterministic moment matching rules without relying on Monte Carlo sampling.\n\nMain Contributions:\n1. A novel probabilistic framework leveraging graph neural networks suitable for stochastic systems;\n2. Deterministic moment matching technique enabling sample-free inference resulting in faster and more stable learning than traditional Monte Carlo methods;\n3. Structured approximations allowing scaling-up to complex systems with numerous agents;  \n4. Experimental validation across two autonomous driving datasets showing superior performance against current best practices;\n5. An extensive empirical runtime comparison highlighting the efficiency gains from the introduced covariance matrix approximations.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "AI-SARAH: Adaptive and Implicit Stochastic Recursive Gradient Methods",
        "abstract": "We present AI-SARAH, a practical variant of SARAH. As a variant of SARAH, this algorithm employs the stochastic recursive gradient yet adjusts step-size based on local geometry. AI-SARAH implicitly computes step-size and efficiently estimates local Lipschitz smoothness of stochastic functions. It is fully adaptive, tune-free, straightforward to implement, and computationally efficient. We provide technical insight and intuitive illustrations on its design and convergence. We conduct extensive empirical analysis and demonstrate its strong performance compared with its classical counterparts and other state-of-the-art first-order methods in solving convex machine learning problems.",
        "authors": "Z. Shi, A. Sadiev, N. Loizou, et.al",
        "keywords": [
            "AI-SARAH",
            "Stochastic Recursive Gradient",
            "Local Geometry"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=WoXJFsJ6Zw",
        "pdf_src": "https://api2.openreview.net/pdf/d6fa827bc66595f11774a94f9991ccffbd7c75c3.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper introduces AI-SARAH, which is an adaptation of the SARAH algorithm for solving convex optimization problems.\n\nResearch Problem: The problem addressed by AI-SARAH is how to improve upon existing algorithms that solve convex optimization using stochastic gradient descent while ensuring computational efficiency without sacrificing accuracy or requiring manual tuning.\n\nMethodology: AI-SARAH incorporates stochastic recursive gradient updates but also adapts the step size dynamically according to the local geometry around each iteration point within the function being optimized. This allows it to compute optimal step sizes implicitly rather than explicitly as previous algorithms have done; thus enabling more accurate estimation of local Lipschitz smoothness from noisy gradients alone - something not possible before due to noise interference issues when estimating such properties directly via gradient information only under non-smooth conditions like those found commonly encountered during training deep neural networks etcetera).\n\nMain Contributions:\n1. Develops a novel approach called \"AI-SARAH\" that addresses limitations associated with current stochastic gradient-based optimization techniques through improved adaptivity & simplicity;\n2. Provides theoretical insights into why their proposed method works well along with illustrative examples demonstrating its effectiveness vis-à-vis traditional approaches including recent advancements made available today's literature regarding first order optimizers tackling similar challenges posed here;\n3. Conducts comprehensive experimental validation across various datasets showing superior performance over both classical counterparts as well as leading edge competitors among state-of-the-art solvers designed specifically towards addressing these types of complex optimization tasks arising frequently nowadays especially amongst practitioners working closely alongside fields related artificial intelligence domain",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Solving a Special Type of Optimal Transport Problem by a Modified Hungarian Algorithm",
        "abstract": "Computing the empirical Wasserstein distance in the Wasserstein-distance-based independence test is an optimal transport (OT) problem with a special structure. This observation inspires us to study a special type of OT problem and propose {\\it a modified Hungarian algorithm} to solve it {\\it exactly}. For the OT problem involving two marginals with $m$ and $n$ atoms ($m\\geq n$), respectively, the computational complexity of the proposed algorithm is $\\mathcal{O}(m^2n)$. Computing the empirical Wasserstein distance in the independence test requires solving this special type of OT problem, where $m=n^2$. The associated computational complexity of the proposed algorithm is $\\mathcal{O}(n^5)$, while the order of applying the classic Hungarian algorithm is $\\mathcal{O}(n^6)$. In addition to the aforementioned special type of OT problem, it is shown that the modified Hungarian algorithm could be adopted to solve a wider range of OT problems. Broader applications of the proposed algorithm are discussed---solving the one-to-many assignment problem and the many-to-many assignment problem. We conduct numerical experiments to validate our theoretical results. The experiment results demonstrate that the proposed modified Hungarian algorithm compares favorably with the Hungarian algorithm and the well-known Sinkhorn algorithm, and the network simplex algorithm.",
        "authors": "Y. Xie, Y. Luo, X. Huo",
        "keywords": [
            "optimal transport",
            "Hungarian algorithm",
            "empirical Wasserstein distance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=k5m8xXTOrC",
        "pdf_src": "https://api2.openreview.net/pdf/78964f8ea016cf1d9ca91cdf01652e6dd5385882.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses the computation of the empirical Wasserstein distance within the context of a statistical independence test based on the Wasserstein distance metric.\n\nResearch Problem: The challenge lies in efficiently computing the empirical Wasserstein distance between two probability distributions when their support sizes differ significantly; specifically, for the case where one distribution has twice as many atoms than the other.\n\nMethodology: To address this issue, researchers introduce a modified version of the Hungarian algorithm—a combinatorial optimization method—to tackle the specific instance of optimal transport (OT) problems arising from such scenarios directly without resorting to iterative methods like Sinkhorn iterations or network simplex algorithms which can become computationally intensive especially if the number of atoms grows large.\n\nMain Contributions:\n1. A novel approach using a modified Hungarian algorithm designed explicitly for the special class of OT problems resulting from the independence tests.\n2. Demonstrated improved computational efficiency over existing approaches by reducing the time complexity from $\\mathcal{O}(n^6)$ down to $\\mathcal{O}(n^5)$ even though the input size remains quadratic at $\\mathcal{O}(mn)$ due to the presence of two different-sized marginals.\n3. Validation through extensive numerical experiments comparing performance against classical Hungarian algorithm implementations along with established algorithms like Sinkhorn and Network Simplex, showing competitive speed-up benefits particularly noticeable under high-dimensional settings typical in modern data analysis tasks related to machine learning techniques relying on Wasserstein distances.\n\n\nThe work not only provides significant improvements but also opens avenues towards broader applicability beyond the original motivating problem—such as addressing more complex assignment problems including one-to-many and many-to-many assignments—which further highlights its potential impact across various fields utilizing optimal transport theory concepts extensively nowadays.",
        "Topic": "Optimal Transport"
    },
    {
        "title": "U-Statistics for Importance-Weighted Variational Inference",
        "abstract": "We propose the use of U-statistics to reduce variance for gradient estimation in importance-weighted variational inference.\nThe key observation is that, given a base gradient estimator that requires $m > 1$ samples and a total of $n > m$ samples to be used for estimation, lower variance is achieved by averaging the base estimator on overlapping batches of size $m$ than  disjoint batches, as currently done.\nWe use classical U-statistic theory to analyze the variance reduction, and propose novel approximations with theoretical guarantees to ensure computational efficiency.\nWe find empirically that U-statistic variance reduction can lead to modest to significant improvements in inference performance on a range of models, with little computational cost.\n",
        "authors": "J. Burroni, K. Takatsu, J. Domke, et.al",
        "keywords": [
            "U-statistics",
            "Variance Reduction",
            "Importance-Weighted Variational Inference"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=oXmwAPlbVw",
        "pdf_src": "https://api2.openreview.net/pdf/3f9950d5d337a6c8fdfb05be5ee950aca31828f5.pdf",
        "Code_src": "",
        "Introduction": "Background: Variance reduction techniques are crucial when estimating gradients using importance-weighted variational inference. The traditional approach involves sampling from the posterior distribution multiple times but this method has high variance due to the dependency between samples.\n\nResearch Problem: How do we efficiently estimate gradients while reducing variance?\n\nMethod: We introduce the concept of U-statistics which allows us to average over overlapping batches rather than disjoint ones leading to reduced variance compared to current methods without increasing computation time significantly or requiring more samples per batch.\n\nMain Contributions: \n- We provide an analysis based on classical U-statistic theory showing how it reduces variance effectively; \n- We develop new approximate algorithms guaranteed theoretically yet computationally efficient enough not only improve accuracy but also save resources during training sessions across various types of machine learning models tested experimentally demonstrating empirical evidence supporting our findings",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "OADAT: Experimental and Synthetic Clinical Optoacoustic Data for Standardized Image Processing",
        "abstract": "Optoacoustic (OA) imaging is based on excitation of biological tissues with nanosecond-duration laser pulses followed by subsequent detection of ultrasound waves generated via light-absorption-mediated thermoelastic expansion. OA imaging features a powerful combination between rich optical contrast and high resolution in deep tissues. This enabled the exploration of a number of attractive new applications both in clinical and laboratory settings. However, no standardized datasets generated with different types of experimental set-up and associated processing methods are available to facilitate advances in broader applications of OA in clinical settings. This complicates an objective comparison between new and established data processing methods, often leading to qualitative results and arbitrary interpretations of the data. In this paper, we provide both experimental and synthetic OA raw signals and reconstructed image domain datasets rendered with different experimental parameters and tomographic acquisition geometries. We further provide trained neural networks to tackle three important challenges related to OA image processing, namely accurate reconstruction under limited view tomographic conditions, removal of spatial undersampling artifacts and anatomical segmentation for improved image reconstruction. Specifically, we define 44 experiments corresponding to the aforementioned challenges as benchmarks to be used as a reference for the development of more advanced processing methods.",
        "authors": "F. Ozdemir, B. Lafci, X. L. Dean-ben, et.al",
        "keywords": [
            "OA imaging",
            "experimental datasets",
            "neural network"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=BVi6MhKO0G",
        "pdf_src": "https://api2.openreview.net/pdf/473f801423c90b1f1be65c2f7e0b3f4a3b7f6624.pdf",
        "Code_src": "",
        "Introduction": "Background: Optoacoustic (OA) imaging combines the advantages of optical and ultrasonic imaging techniques due to its ability to generate images from within tissue using short laser pulses that cause thermal expansion when absorbed.\n\nResearch Problem: Despite being widely recognized for providing good contrast and resolving power at depth compared to other modalities like MRI or CT, there lacks standardization among OA datasets across various experimental setups which hinders comparative studies needed before widespread adoption into clinical practice.\n \nMethods: The authors have created a dataset consisting of real OA raw signals along with their processed counterparts obtained through different experimental parameters such as pulse energy levels etc., and also synthetic OA signals. They introduce benchmarking tasks focusing on reconstructing accurately even without complete views (limited-view tomography), removing artifacts caused by spatial sampling issues during signal collection process, and segmenting anatomy effectively improving overall image quality.\n \nMain Contributions: Their work provides not only comprehensive datasets but also neural network models pre-trained specifically addressing these critical aspects ensuring better performance over traditional approaches; they aim towards advancing OA technology's reliability beyond current limitations thus paving way toward future clinical use.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Separable Self-attention for Mobile Vision Transformers",
        "abstract": "Mobile vision transformers (MobileViT) can achieve state-of-the-art performance across several mobile vision tasks, including classification and detection. Though these models have fewer parameters, they have high latency as compared to convolutional neural network-based models. The main efficiency bottleneck in MobileViT is the multi-headed self-attention (MHA) in transformers, which requires $O(k^2)$ time complexity with respect to the number of tokens (or patches) $k$. Moreover, MHA requires costly operations (e.g., batch-wise matrix multiplication) for computing self-attention, impacting latency on resource-constrained devices. This paper introduces a separable self-attention method with linear complexity, i.e. $O(k)$. A simple yet effective characteristic of the proposed method is that it uses element-wise operations for computing self-attention, making it a good choice for resource-constrained devices. The improved model, MobileViTv2, is state-of-the-art on several mobile vision tasks, including ImageNet object classification and MS-COCO object detection. With about three million parameters, MobileViTv2 achieves a top-1 accuracy of 75.6% on the ImageNet dataset, outperforming MobileViT by about 1% while running $3.2\\times$ faster on a mobile device. Our source code is available at: https://github.com/apple/ml-cvnets",
        "authors": "S. Mehta, M. Rastegari",
        "keywords": [
            "MobileViT",
            "Self-Attention",
            "Efficient Computing"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=tBl4yBEjKi",
        "pdf_src": "https://api2.openreview.net/pdf/946534e2386ad2ac00bdb3c2b8ad9db77a545802.pdf",
        "Code_src": "https://github.com/apple/ml-cvnets",
        "Introduction": "Background:\nThe paper addresses an issue related to the use of mobile vision transformers (MobileViT), particularly their inefficiency due to high latency when used within resource-constrained environments like smartphones.\n\nResearch Problem:\nThe research problem tackled here concerns optimizing MobileViTs so that despite having less computational overhead than traditional CNNs because of reduced parameter count, they are not efficient enough regarding latency during inference phase specifically caused by Multi-Head Self-Attention (MHA).\n\nMethod:\nTo solve this challenge presented above, authors introduce a novel approach called \"separable self-attention\" designed such that its computation has linear complexity instead of quadratic complexity associated with standard MHAs found in transformer architectures ($O(k^2)$ where k is the number of tokens or patches). They leverage element-wise operations rather than more computationally expensive batch-wise matrix multiplications typically required under MHA framework.\n \nMain Contributions:\nTheir contributions include developing a new architecture named MobileViTv2 based on the aforementioned separable self-attention mechanism; achieving competitive results against other leading methods even though MobileViTv2 possesses significantly fewer parameters (~three million); demonstrating substantial improvements over previous versions – notably reducing latency approximately by factor of 3.2x without compromising much on accuracy - thus rendering MobileViTv2 suitable especially for real-time applications requiring low-latency processing capabilities commonly seen in mobile platforms.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models",
        "abstract": "The imputation of missing values represents a significant obstacle for many real-world data analysis pipelines. Here, we focus on time series data and put forward SSSD, an imputation model that relies on two emerging technologies, (conditional) diffusion models as state-ofthe-art generative models and structured state space models as internal model architecture, which are particularly suited to capture long-term dependencies in time series data. We demonstrate that SSSD matches or even exceeds state-of-the-art probabilistic imputation and forecasting performance on a broad range of data sets and different missingness scenarios, including the challenging blackout-missing scenarios, where prior approaches failed to provide meaningful results.",
        "authors": "J. L. Alcaraz, N. Strodthoff",
        "keywords": [
            "time series data",
            "imputation model",
            "conditional diffusion models"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=hHiIbk7ApW",
        "pdf_src": "https://api2.openreview.net/pdf/d69ac66c869e57b7ec656b862b72481633f1c35b.pdf",
        "Code_src": "",
        "Introduction": "Background: Missing value imputation is crucial but challenging task in real-world data analysis pipelines.\n\nResearch Problem: How to effectively impute missing values in time series data?\n\nMethod: Propose SSSD, an imputation model based on conditional diffusion models and structured state space models suitable for capturing long-term dependencies in time series data.\n\nMain Contributions:\n1. Use two emerging technologies - conditional diffusion models and structured state space models.\n2. Demonstrate superior performance compared to existing probabilistic imputation and forecasting methods across various datasets and missingness scenarios with long-term dependency challenges such as blackout-missing scenario.",
        "Topic": "Generative Models"
    },
    {
        "title": "A Flexible Nadaraya-Watson Head Can Offer Explainable and Calibrated Classification",
        "abstract": "In this paper, we empirically analyze a simple, non-learnable, and nonparametric Nadaraya-Watson (NW) prediction head that can be used with any neural network architecture. In the NW head, the prediction is a weighted average of labels from a support set. The weights are computed from distances between the query feature and support features. This is in contrast to the dominant approach of using a learnable classification head (e.g., a fully-connected layer) on the features, which can be challenging to interpret and can yield poorly calibrated predictions. Our empirical results on an array of computer vision tasks demonstrate that the NW head can yield better calibration with comparable accuracy compared to its parametric counterpart, particularly in data-limited settings. To further increase inference-time efficiency, we propose a simple approach that involves a clustering step run on the training set to create a relatively small distilled support set. Furthermore, we explore two means of interpretability/explainability that fall naturally from the NW head. The first is the label weights, and the second is our novel concept of the ``support influence function,'' which is an easy-to-compute metric that quantifies the influence of a support element on the prediction for a given query. As we demonstrate in our experiments, the influence function can allow the user to debug a trained model. We believe that the NW head is a flexible, interpretable, and highly useful building block that can be used in a range of applications.",
        "authors": "A. Q. Wang, M. R. Sabuncu",
        "keywords": [
            "distance",
            "calibration",
            "interpretability"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=iEq6lhG4O3",
        "pdf_src": "https://api2.openreview.net/pdf/d7226835b972934fe429b3638c211d5dd21385fa.pdf",
        "Code_src": "",
        "Introduction": "Background: \nThe background of this research lies in the field of machine learning where predictive models often require complex architectures such as deep neural networks or other learned heads leading to high computational costs without clear explanations.\n\nResearch Problem:\nThe problem addressed by this study concerns improving both interpretability and performance while reducing computation time during prediction phase specifically when dealing with limited datasets - how do you achieve accurate predictions quickly?\n\nMethodology:\nTo tackle these issues, researchers introduce a non-parametric method based on the Nadaraya-Watson kernel regression technique known as \"NW head.\" Instead of having a learnable classifier like a fully connected layer at the end of their neural network pipeline they use a NW head predicting through a weighted sum of labeled examples within what's called 'support set.' \n\nMain Contributions:\nThis work makes several contributions:\n\n1. Empirical Analysis: They provide evidence showing that the NW head not only maintains similar levels of accuracy but also improves calibration over traditional methods.\n2. Efficiency Improvement: A new strategy involving distillation via clustering allows them to reduce the size of the support set significantly thus speeding up inference times drastically yet maintaining good performance especially beneficial under resource constraints.\n3. Interpretability/Explainability Enhancements: By leveraging the inherent properties of NW head – label weights assigned per example influencing final prediction outcomes along with introducing Support Influence Function (SIF), it becomes easier understand why certain inputs lead to specific outputs allowing debugging capabilities beyond standard metrics alone.\n\nOverall Conclusion:\nThe authors conclude that incorporating NW head into existing neural network architectures could offer significant advantages regarding interpretability versus complexity trade-offs making it potentially valuable across various domains including Computer Vision among others due",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Contrastive Search Is What You Need For Neural Text Generation",
        "abstract": "Generating text with autoregressive language models (LMs) is of great importance to many natural language processing (NLP) applications. Previous solutions for this task often produce text that contains degenerative expressions (Welleck et al., 2020) or lacks semantic consistency (Basu et al., 2021). Recently, Su et al. (2022b) introduced a new decoding method, contrastive search, based on the isotropic representation space of the language model and obtained new state of the art on various benchmarks. Additionally, Su et al. (2022b) argued that the representations of autoregressive LMs (e.g. GPT-2) are intrinsically anisotropic which is also shared by previous studies (Ethayarajh, 2019). Therefore, to ensure the language model follows an isotropic distribution, Su et al. (2022b) proposed a contrastive learning scheme, SimCTG, which calibrates the language model’s representations through additional training.\n\nIn this study, we first answer the question: “Are autoregressive LMs really anisotropic?”. To this end, we extensively evaluate the isotropy of LMs across 16 major languages. Surprisingly, we find that the anisotropic problem only exists in the two specific English GPT-2-small/medium models. On the other hand, all other evaluated LMs are naturally isotropic which is in contrast to the conclusion drawn by previous studies (Ethayarajh, 2019; Su et al., 2022b). Based on our findings, we further assess the contrastive search decoding method using off-the-shelf LMs on four generation tasks across 16 languages. Our experimental results demonstrate that contrastive search significantly outperforms previous decoding methods without any additional training. More notably, on 12 out of the 16 evaluated languages, contrastive search performs comparably with human-level performances as judged by human evaluations.",
        "authors": "Y. Su, N. Collier",
        "keywords": [
            "anisotropic",
            "contrastive search",
            "language modeling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=GbkWw3jwL9",
        "pdf_src": "https://api2.openreview.net/pdf/53a42b15aa75cd58462277eb4013c4b3053a09a9.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses the issue of generating coherent and consistent text from autoregressive language models such as GPT-2 used widely in NLP applications due to their ability to generate long sequences but sometimes producing degenerative expressions lacking semantic coherence.\nResearch Question:\nThe primary research question addressed here revolves around whether these autoregressive language models inherently possess anisotropy leading to issues like those mentioned above—specifically if they indeed have inherent directional properties affecting how they process information differently along certain dimensions rather than uniformly across them—a phenomenon known as anisotropy when referring to data distributions within machine learning models.\n\nMethodology:\nTo address this inquiry into the nature of anisotropy among different LM variants, authors conduct extensive evaluation tests involving 16 distinct linguistic corpora covering multiple world languages besides English where previously reported anisotropy was observed more prominently compared to monolingual datasets predominantly focused solely on English. They employ SimCTG—an adaptation of contrastive learning techniques—to recalibrate the language model's internal representations towards isotropy after initial training phases ensuring uniformity during inference time regardless of input sequence length or complexity level encountered at runtime scenarios.\n\nMain Contributions:\nTheir key contribution lies not just identifying inconsistencies between prior claims about universal anisotropy amongst AR-LM variants beyond English but providing empirical evidence supporting non-uniform behavior exclusively limited to selected subsets including small-medium scale versions of GPT-2 trained specifically over English corpus alone while demonstrating that larger-scale multilingual models do exhibit isotropic characteristics akin to Gaussian distributions expectedly found throughout most real-world contexts outside specialized domains heavily biased toward single-language usage patterns prevalent elsewhere before now considered normative standards against which others were measured incorrectly up until recently reevaluated according to current findings presented herein suggesting broader implications regarding future developments concerning improvements upon existing approaches aimed at mitigating potential biases present therein arising from underlying assumptions made implicitly underpinning past methodologies employed thus far",
        "Topic": "Large Language Models"
    },
    {
        "title": "Stacking Diverse Architectures to Improve Machine Translation",
        "abstract": "Repeated applications of the same neural block primarily based on self-attention characterize the current state-of-the-art in neural architectures for machine translation. In such architectures the decoder adopts a masked version of the same encoding block. Although simple this strategy doesn't encode the various inductive biases such as locality that arise from alternative architectures and that are central to the modelling of translation. We propose Lasagna, an encoder-decoder model that aims to combine the inductive benefits of different architectures by layering multiple instances of different blocks.  Lasagna’s encoder first grows the representation from local to mid-sized using convolutional blocks and only then applies a pair of final self-attention blocks. Lasagna’s decoder uses only convolutional blocks that attend to the encoder representation. On a large suit of machine translation tasks, we find that Lasagna not only matches or outperforms the Transformer baseline, but it does so more efficiently thanks to widespread use of the efficient convolutional blocks. These findings suggest that the widespread use of uniform architectures may be suboptimal in certain scenarios and exploiting the diversity of inductive architectural biases can  lead to substantial gains.\n",
        "authors": "A. Schioppa, N. Kalchbrenner",
        "keywords": [
            "encoder-decoder",
            "multi-instance architecture",
            "convolutional blocks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=mNEqiC924B",
        "pdf_src": "https://api2.openreview.net/pdf/f5e05ccc7e253ce684f21c672377c2724eb4e39a.pdf",
        "Code_src": "",
        "Introduction": "Background: The current state-of-the-art in neural architectures for machine translation is characterized by repeated applications of the same neural block mainly based on self-attention.\n\nResearch Problem: This approach lacks the encoding of various inductive biases like locality which arises from alternative architectures despite being simple yet effective.\n\nMethod: To address these limitations, they introduce Lasagna, an encoder-decoder model designed with the goal of combining the inductive benefits of different architectures through layering multiple instances of different blocks within each layer.\n\nMain Contributions:\n1. Lasagna's encoder starts growing the representation locally before moving towards mid-sized representations via convolutional blocks followed by two final self-attention blocks at the end while its decoder utilizes only convolutional blocks that focus solely on attending to the encoder's output.\n2. They conducted experiments across several machine translation benchmarks showing that their proposed architecture performs comparably if not better than the Transformer baseline used widely today; however, what makes Lasagna stand out even further lies in how much more efficiently it operates due largely because many layers employ computationally cheaper convolutional blocks rather than costly attention mechanisms found commonly elsewhere leading them conclude there could exist potential inefficiencies when employing uniformly identical architectures under specific circumstances where leveraging diverse types might yield significant improvements instead",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Monotone deep Boltzmann machines",
        "abstract": "Deep Boltzmann machines (DBMs), one of the first ``deep'' learning methods ever studied, are multi-layered probabilistic models governed by a pairwise energy function that describes the likelihood of all variables/nodes in the network. In practice, DBMs are often constrained, i.e., via the \\emph{restricted} Boltzmann machine (RBM) architecture (which does not permit intra-layer connections), in order to allow for more efficient inference.  In this work, we revisit the generic DBM approach, and ask the question: are there other possible restrictions to their design that would enable efficient (approximate) inference?  In particular, we develop a new class of restricted model, the monotone DBM, which allows for arbitrary self-connection in each layer, but restricts the \\emph{weights} in a manner that guarantees the existence and global uniqueness of a mean-field fixed point. To do this, we leverage tools from the recently-proposed monotone Deep Equilibrium model and show that a particular choice of activation results in a fixed-point iteration that gives a variational mean-field solution.  While this approach is still largely conceptual, it is the first architecture that allows for efficient approximate inference in fully-general weight structures for DBMs.  We apply this approach to simple deep convolutional Boltzmann architectures and demonstrate that it allows for tasks such as the joint completion and classification of images, within a single deep probabilistic setting, while avoiding the pitfalls of mean-field inference in traditional RBMs.",
        "authors": "Z. Feng, E. Winston, J. Z. Kolter",
        "keywords": [
            "monotone Deep Equilibrium",
            "Variational Mean-Field Solution",
            "Efficient Approximate Inference"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=SgTKk6ryPr",
        "pdf_src": "https://api2.openreview.net/pdf/bb2f7d986bee71e8b49968d642c12ca4876bcd05.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses Deep Boltzmann Machines (DBMs), an early type of \"deep\" learning method consisting of multiple layers with nodes connected through a probability distribution described by a pairwise energy function.\n\nResearch Question:\nThe study raises questions about potential modifications or constraints on the structure of DBMs beyond the standard restricted Boltzmann machine (RBM) framework—specifically whether alternative restrictions could facilitate efficient approximate inference without sacrificing too much flexibility?\n\nMethodology:\nTo address these issues, researchers introduce a novel subclass of restricted DBMs called Monotone DBMs (\"monotonic\" referring to certain properties). They propose allowing for unrestricted self-connections at every layer yet impose specific conditions upon the weights so they ensure the presence along with a globally unique mean-field equilibrium state—a concept borrowed from recent developments known as Monotone Deep Equilibrium Models.\n\nMain Contributions:\nThis research introduces what appears to be the first DBM architecture capable of performing efficient approximate inference across general weight configurations; previously, only specialized types like RBMs were feasible due to computational complexity concerns when dealing with full connectivity between neurons.\n \nAdditionally, applying this monotonicity constraint leads to a fixed-point iteration process yielding a variational mean-field approximation—an improvement over previous approaches where mean-field theory was less reliable because it did not account for interactions among neurons outside direct pairwise ones.\n\nIn summary, although theoretical rather than practical applications have been demonstrated thus far—their findings suggest promising avenues toward developing more flexible and computationally tractable deep learning frameworks based around DBMs moving forward.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds",
        "abstract": "In order for artificial agents to successfully perform tasks in changing environments, they must be able to both detect and adapt to novelty. However, visual novelty detection research often only evaluates on repurposed datasets such as CIFAR-10 originally intended for object classification, where images focus on one distinct, well-centered object. New benchmarks are needed to represent the challenges of navigating the complex scenes of an open world. Our new NovelCraft dataset contains multimodal episodic data of the images and symbolic world-states seen by an agent completing a pogo stick assembly task within a modified Minecraft environment. In some episodes, we insert novel objects of varying size within the complex 3D scene that may impact gameplay. Our visual novelty detection benchmark finds that methods that rank best on popular area-under-the-curve metrics may be outperformed by simpler alternatives when controlling false positives matters most. Further multimodal novelty detection experiments suggest that methods that fuse both visual and symbolic information can improve time until detection as well as overall discrimination. Finally, our evaluation of recent generalized category discovery methods suggests that adapting to new imbalanced categories in complex scenes remains an exciting open problem.",
        "authors": "P. Feeney, S. Schneider, P. Lymperopoulos, et.al",
        "keywords": [
            "novelty detection",
            "multimodal episodic data",
            "multimodal fusion"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=4eL6z9ziw7",
        "pdf_src": "https://api2.openreview.net/pdf/cd15445c155c2a6b6ef516252903b039b6ab325c.pdf",
        "Code_src": "",
        "Introduction": "Background: As artificial agents navigate through dynamic environments, it is crucial for them not just to recognize but also to adapt to novel situations or elements around them.\n\nResearch Problem: Existing studies have primarily focused on evaluating visual novelty detection algorithms using repurposed datasets like CIFAR-10 which were initially designed for object recognition rather than representing real-world complexity found in open-world scenarios with diverse and potentially disruptive novel items.\n\nMethods: To address this issue, researchers introduced NovelCraft—a multimodal episodic dataset—containing images along with corresponding symbolic representations from an autonomous agent's perspective while performing a pogo stick assembly task inside a customized Minecraft-like virtual environment. The dataset includes various sizes of novel objects inserted into these complex three-dimensional settings during different episodes without altering the underlying game mechanics; hence challenging existing models' ability to identify anomalies amidst normalcy.\n \nMain Contributions:\n1. NovelCraft represents practical difficulties encountered outside controlled lab conditions due to its incorporation of realistic variability across multiple modalities (visual and symbolic).\n2. It reveals limitations associated with traditional approaches based solely on image features since simple yet effective strategies could surpass more sophisticated ones concerning minimizing false alarms under stringent criteria related to precision at detecting genuine novelties versus innocuous changes occurring naturally throughout gameplay sessions over extended periods leading up potential improvements towards robustness against unforeseen alterations present within actual operational contexts beyond simulation setups alone.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Integrating Bayesian Network Structure into Residual Flows and Variational Autoencoders",
        "abstract": "Deep generative models have become more popular in recent years due to their scalability and representation capacity. Unlike probabilistic graphical models, they typically do not incorporate specific domain knowledge. As such, this work explores incorporating arbitrary dependency structures, as specified by Bayesian networks, into variational autoencoders (VAEs). This is achieved by developing a new type of graphical normalizing flow, which extends residual flows by encoding conditional independence through masking of the flow’s residual block weight matrices, and using these to extend both the prior and inference network of the VAE. We show that the proposed graphical VAE provides a more interpretable model that generalizes better in data-sparse settings, when practitioners know or can hypothesize about certain latent factors in their domain. Furthermore, we show that graphical residual flows provide not only density estimation and inference performance competitive with existing graphical flows, but also more stable and accurate inversion in practice as a byproduct of the flow’s Lipschitz bounds.",
        "authors": "J. Mouton, R. S. Kroon",
        "keywords": [
            "graphical VAEs",
            "residual flows",
            "Bayesian networks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=OsKXlWamTQ",
        "pdf_src": "https://api2.openreview.net/pdf/c1be8403f1d97d4dc74742c6eec9ff14cf605a9a.pdf",
        "Code_src": "",
        "Introduction": "Background: In recent years, deep generative models like variational autoencoders (VAEs) are becoming increasingly popular for their scalability and representational power; however, unlike probabilistic graphical models, they often lack domain-specific knowledge.\n\nResearch Question: How might one integrate domain knowledge from Bayesian networks—specifically, arbitrary dependency structures—into VAEs?\n\nMethod: The authors develop a novel approach called Graphical Variational Autoencoder (GVAE), based on a new kind of graphical normalizing flow architecture extended from residual flows known as Graphical Residual Flows (GRFs). These GRFs encode conditional independence within the flow's residual blocks via masking techniques applied directly to the weights themselves rather than the input data itself—a departure from standard approaches where conditioning would be encoded at the input level before passing it forward throughout the network layers.\nThe GVAE architecture includes modifications made specifically around how dependencies between variables should influence the structure during training so that learned representations capture those relationships effectively while still being able to generalize well across unseen datasets even if some latent factors remain unknown/hypothesized upon by users.\n\nMain Contributions:\n1. A new class of Graphical Normalizing Flow architectures termed Graphical Residual Flows capable of capturing complex dependency patterns present in Bayesian networks without requiring explicit conditioning steps elsewhere along computational paths inside neural networks;\n2. An improved interpretability over traditional VAEs particularly useful especially under conditions where there may exist latent factors whose existence cannot always be assumed beforehand yet play significant roles influencing observed phenomena;\n3. Demonstrated improvements regarding practical aspects including stability & accuracy compared against other state-of-the-art methods",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Neural Collapse: A Review on Modelling Principles and Generalization",
        "abstract": "Deep classifier neural networks enter the terminal phase of training (TPT) when training error reaches zero and tend to exhibit intriguing Neural Collapse (NC) properties. Neural collapse essentially represents a state at which the within-class variability of final hidden layer outputs is infinitesimally small and their class means form a simplex equiangular tight frame. This simplifies the last layer behaviour to that of a nearest-class center decision rule. Despite the simplicity of this state, the dynamics and implications of reaching it are yet to be fully understood. In this work, we review the principles which aid in modelling neural collapse, followed by the implications of this state on generalization and transfer learning capabilities of neural networks. Finally, we conclude by discussing potential avenues and directions for future research.",
        "authors": "V. Kothapalli",
        "keywords": [
            "Neural Collapse",
            "Deep Classifier Neural Networks",
            "Training Error"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=QTXocpAP9p",
        "pdf_src": "https://api2.openreview.net/pdf/c3b1a46bf6e07e803894cf6f4c3e25a4ad66135e.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses how deep classifier neural networks can reach an interesting phenomenon called Neural Collapse during the terminal phase of training where the training error becomes zero.\n\nResearch Problem: Understanding what causes Neural Collapse; its impact on the network's behavior such as classification decisions made with just one neuron activation instead of multiple neurons from previous layers or pooling operations like max-pooling used traditionally.\n \nMethodology: They provide insights into modeling Neural Collapse through various techniques including analyzing the output distributions across different classes along with considering other factors contributing towards this phenomenon - regularization methods applied while training models etc.\n\nMain Contributions:\n1. A comprehensive literature survey highlighting key findings related to Neural Collapse so far;\n2. Insights about why certain architectures might lead us closer toward experiencing NC than others due to specific characteristics they possess;\n3. Implications drawn regarding how these collapses affect both generalization performance & transfer learning abilities amongst trained networks under consideration;\n\nFuture Research Directions: Discussing possible approaches forward concerning further investigation around understanding mechanisms behind Neural Collapses better – exploring whether there exists any universal pattern governing them regardless of architecture complexity involved",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "A Study of Biologically Plausible Neural Network: The Role and Interactions of Brain-Inspired Mechanisms in Continual Learning",
        "abstract": "Humans excel at continually acquiring, consolidating, and retaining information from an ever-changing environment, whereas artificial neural networks (ANNs) exhibit catastrophic forgetting. There are considerable differences in the complexity of synapses, the processing of information, and the learning mechanisms in biological neural networks and their artificial counterparts, which may explain the mismatch in performance. We consider a biologically plausible framework that constitutes separate populations of exclusively excitatory and inhibitory neurons that adhere to Dale's principle, and the excitatory pyramidal neurons are augmented with dendritic-like structures for context-dependent processing of stimuli. We then conduct a comprehensive study on the role and interactions of different mechanisms inspired by the brain, including sparse non-overlapping representations, Hebbian learning, synaptic consolidation, and replay of past activations that accompanied the learning event. Our study suggests that the employing of multiple complementary mechanisms in a biologically plausible architecture, similar to the brain, may be effective in enabling continual learning in ANNs. \\footnote{We will make the code available upon acceptance.",
        "authors": "F. Sarfraz, E. Arani, B. Zonooz",
        "keywords": [
            "biological neural network",
            "continual learning",
            "synaptic consolidation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=DJr6zorJM2",
        "pdf_src": "https://api2.openreview.net/pdf/c4fb5b353be29645e09e9d2796518c9842a76fd1.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the issue of catastrophic forgetting faced by artificial neural networks when they learn new tasks while forgetting previously learned ones.\n\nResearch Question: How can we design artificial neural networks capable of continuous learning without experiencing catastrophic forgetting?\n\nMethodology: The authors propose a biologically plausible neural network model based on separate populations of excitatory and inhibitory neurons following Dale's principle where excitatory pyramidal neurons have dendritic-like extensions enhancing context-dependent stimulus processing capabilities.\nThey also incorporate several mechanisms inspired by the human brain such as sparse non-overlapping representations, Hebbian learning, synaptic consolidation through spike-timing dependent plasticity (STDP), and replay of past activations during training sessions.\n\nMain Contributions:\n1. They introduce a novel architecture mimicking aspects of the human brain structure leading towards more robust learning processes than traditional ANN architectures.\n2. By studying various mechanisms like sparse coding or STDP within this architecture alongside replay phenomena observed after learning events, it is demonstrated how these could potentially work together synergistically allowing for better retention over time despite continued exposure to fresh data.\n3. Their findings suggest that adopting multiple complementary mechanisms akin to those found in biological systems might enable ANNs not only retain but continue improving across diverse tasks continuously rather than just sequentially replacing one task set with another before forgetting old knowledge altogether.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Unifying physical systems’ inductive biases in neural ODE using dynamics constraints",
        "abstract": "Conservation of energy is at the core of many physical phenomena and dynamical systems. There have been a significant number of works in the past few years aimed at predicting the trajectory of motion of dynamical systems using neural networks while adhering to the law of conservation of energy. Most of these works are inspired by classical mechanics such as Hamiltonian and Lagrangian mechanics as well as Neural Ordinary Differential Equations. While these works have been shown to work well in specific domains respectively, there is a lack of a unifying method that is more generally applicable without requiring significant changes to the neural network architectures. In this work, we aim to address this issue by providing a simple method that could be applied to not just energy-conserving systems, but also dissipative systems, by including a different inductive bias in different cases in the form of a regularisation term in the loss function. The proposed method does not require changing the neural network architecture and could form the basis to validate a novel idea, therefore showing promises to accelerate research in this direction.",
        "authors": "Y. H. Lim, M. F. Kasim",
        "keywords": [
            "energy conservation",
            "dynamical system prediction",
            "neural ordinary differential equations"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ZOAb497iaY",
        "pdf_src": "https://api2.openreview.net/pdf/cb3747f6196a86b6353d22cf99774f7112d73818.pdf",
        "Code_src": "",
        "Introduction": "Background: Conservation of energy plays a crucial role in various physical phenomena and dynamical systems.\n\nResearch Problem: How can we predict the trajectories of dynamic systems with neural networks while respecting the laws of conservation of energy?\n\nMethod: We propose a new approach based on introducing an additional regularization term into the loss function for different types of systems - one for energy conserving ones another for dissipative ones thus incorporating distinct inductive biases tailored specifically towards each type.\nMain Contributions: Our unified framework allows us to apply our model across both energy conserving and dissipative systems which was previously lacking from existing methods relying heavily on classical mechanics like Hamiltonian or Lagrangian formalisms along with Neural Ordinary Differential Equations approaches; it doesn't necessitate altering neural network architectures either making validation easier when testing out innovative ideas within this field promising potential acceleration toward future advancements therein",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Training Data Size Induced Double Descent For Denoising Feedforward Neural Networks and the Role of Training Noise",
        "abstract": "When training an unregularized denoising feedforward neural network, we show that the generalization error versus the number of training data points is a double descent curve.\nWe formalize the question of how many training data points should be used by looking at the generalization error for denoising noisy test data. Prior work on computing the generalization error focuses on adding noise to target outputs. However, adding noise to the input is more in line with current pre-training practices. In the linear (in the inputs) regime, we provide an asymptotically exact formula for the generalization error for rank 1 data and an approximation for the generalization error for rank $r$ data. \nFrom this, we derive a formula for the amount of noise that needs to be added to the training data to minimize the denoising error. This results in the emergence of a shrinkage phenomenon for improving the performance of denoising DNNs by making the training SNR smaller than the test SNR. Further, we see that the amount of shrinkage (ratio of the train to test SNR) also follows a double descent curve. ",
        "authors": "R. Sonthalia, R. R. Nadakuditi",
        "keywords": [
            "double descent curve",
            "denoising feedforward neural networks",
            "shrinkage phenomenon"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=FdMWtpVT1I",
        "pdf_src": "https://api2.openreview.net/pdf/a75e04cf6ec615ad5c61f2842d33938c71f240e4.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the issue of determining optimal amounts of training data when using denoised feedforward neural networks.\n\nResearch Question: How can one determine the appropriate quantity of training data needed?\n\nMethodology: The authors analyze the relationship between generalization errors as they relate to the volume of training samples while considering noisy test datasets rather than focusing solely on noisy targets like prior research did; specifically examining both linear regimes where they present exact formulas up to rank-1 data and approximations beyond it along with exploring implications such as minimizing denoising errors through controlled addition of noise during training leading to improved signal-to-noise ratios (SNRs).\n\nMain Contributions:\n1. They introduce new insights into understanding what constitutes sufficient or excessive quantities of training examples based upon observed patterns within these curves - termed \"double descent\".\n2. Develop precise mathematical expressions quantifying expected generalization performances under varying levels of noise perturbations across different ranks of dataset dimensionsality which could inform practical approaches towards optimizing model architectures accordingly without having access to large-scale labeled datasets traditionally required before deployment stages are reached due constraints related cost/time/resource limitations etcetera).",
        "Topic": "approximation"
    },
    {
        "title": "Bridging Graph Position Encodings for Transformers with Weighted Graph-Walking Automata",
        "abstract": " A current goal in the graph neural network literature is to enable transformers to operate on graph-structured data, given their success on language and vision tasks. Since the transformer's original sinusoidal positional encodings (PEs) are not applicable to graphs, recent work has focused on developing graph PEs, rooted in spectral graph theory or various spatial features of a graph. In this work, we introduce a new graph PE, Graph Automaton PE (GAPE), based on weighted graph-walking automata (a novel extension of graph-walking automata). We compare the performance of GAPE with other PE schemes on both machine translation and graph-structured tasks, and we show that it generalizes several other PEs. An additional contribution of this study is a theoretical and controlled experimental comparison of many recent PEs in graph transformers, independent of the use of edge features.",
        "authors": "P. Soga, D. Chiang",
        "keywords": [
            "graph neural networks",
            "transformers",
            "positional encodings"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=tE2NiMGd07",
        "pdf_src": "https://api2.openreview.net/pdf/13ae94795cb0e9d9e33fddfbc50132df051ebd41.pdf",
        "Code_src": "",
        "Introduction": "Background: The field of graph neural networks aims to extend the capabilities of transformers from processing sequence data like text to handling graph-structured data such as social networks.\nResearch Problem: Existing graph position encodings for transformers do not effectively capture the global structure information present in graphs due to limitations inherent in the original sinusoidal positional encodings.\n\nMethod: This paper introduces a novel graph positional encoding called Graph Automaton Positional Encoding (GAPE), which leverages weighted graph-walking automata—a concept derived by extending traditional graph-walking automata—to encode node positions within a graph while considering its topology more accurately than previous methods have done using spectral graph theory-based approaches alone.\n\nMain Contributions:\n1. Development of Graph Automaton Positional Encoding (GAPE): This method goes beyond existing graph positional encodings through an innovative approach utilizing graph-walking automata extended with weights—allowing for a richer representation capturing the complex topological properties of graphs.\n2. Comparative Study across Multiple Tasks: The authors evaluate the proposed GAPE against state-of-the-art positional encodings including those based on spectral graph theory without relying solely on edge feature embeddings; they demonstrate superior performance particularly when dealing with graph-structured tasks compared to these alternatives via empirical experiments conducted over two datasets—one each focusing on machine translation and graph-structured learning problems respectively.\n3. Theory and Controlled Experiments: They provide comprehensive analysis comparing different positional encodings theoretically grounded upon understanding how well they preserve certain desirable properties",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues",
        "abstract": "Data missingness and quality are common problems in machine learning, especially for high-stakes applications such as healthcare. Developers often train machine learning models on carefully curated datasets using only high-quality data; however, this reduces the utility of such models in production environments. We propose a novel neural network modification to mitigate the impacts of low-quality and missing data which involves replacing the fixed weights of a fully-connected layer with a function of additional input. This is inspired by neuromodulation in biological neural networks where the cortex can up- and down-regulate inputs based on their reliability and the presence of other data. In testing, with reliability scores as a modulating signal, models with modulating layers were found to be more robust against data quality degradation, including additional missingness. These models are superior to imputation as they save on training time by entirely skipping the imputation process and further allow the introduction of other data quality measures that imputation cannot handle. Our results suggest that explicitly accounting for reduced information quality with a modulating fully connected layer can enable the deployment of artificial intelligence systems in real-time applications.",
        "authors": "M. Abdelhack, J. Zhang, S. Tripathi, et.al",
        "keywords": [
            "data quality",
            "machine learning",
            "neuromodulation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=MRLHN4MSmA",
        "pdf_src": "https://api2.openreview.net/pdf/ad95f3001daec71649c3c464d23ff7b8e21cc72d.pdf",
        "Code_src": "",
        "Introduction": "Background: Data missingness and poor quality pose significant challenges particularly when dealing with critical domains like healthcare due to its impact on model performance.\n\nResearch Problem: The issue revolves around how to effectively address the effects of incomplete or substandard data during machine learning modeling processes without compromising efficiency within operational settings.\n\nMethodology: To tackle these issues head-on, we introduce an innovative approach involving modifications made directly into neural networks through altering the architecture rather than post-processing techniques typically used—such as data imputation—which consume substantial computational resources over extended periods.\nSpecifically, our method replaces static weights present in traditional fully connected layers with functions dependent upon supplementary inputs derived from the dataset itself—a concept inspired by neuromodulatory mechanisms observed in living neural tissues allowing them to dynamically adjust processing depending on sensory evidence's reliability alongside contextual cues provided by existing data points.\n\nMain Contributions:\n1. Robustness Improvement - By incorporating reliability metrics about each piece of data along with context-aware adjustments via modulation signals, trained models demonstrate increased resilience towards various forms of data corruption—including instances not just limited to missing values but also those affected by lower-than-average quality standards.\n2. Efficiency Advantages - Unlike conventional methods requiring iterative computations before finalizing predictions (like data imputation), our proposed technique significantly saves both computation effort required throughout training phases since it bypasses unnecessary steps altogether while still yielding improved predictive outcomes compared purely relying solely on complete records alone would achieve under similar circumstances).\n3. Enhanced Real-Time Capabilities - Considering practical constraints faced across numerous industries reliant heavily upon timely decision-making capabilities afforded by automated intelligent systems deployed at scale within dynamic environments (e.g., autonomous vehicles navigating changing traffic patterns), our findings indicate potential benefits pertaining specifically toward enabling reliable operation even amidst fluctuating conditions encountered daily operations entail thus paving way forward towards realizing broader adoption rates amongst mission-critical sectors utilizing ML technologies today",
        "Topic": "Machine Learning"
    },
    {
        "title": "Scalable Deep Compressive Sensing",
        "abstract": "Deep learning has been used to image compressive sensing (CS) for enhanced reconstruction performance. However, most existing deep learning methods train different models for different subsampling ratios, which brings an additional hardware burden. In this paper, we develop a general framework named scalable deep compressive sensing (SDCS) for the scalable sampling and reconstruction (SSR) of all existing end-to-end-trained models. In the proposed way, images are measured and initialized linearly. Two sampling matrix masks are introduced to flexibly control the subsampling ratios used in sampling and reconstruction, respectively. To achieve a reconstruction model with flexible subsampling ratios, a training strategy dubbed scalable training is developed. In scalable training, the model is trained with the sampling matrix and the initialization matrix at various subsampling ratios by integrating different sampling matrix masks. Experimental results show that models with SDCS can achieve SSR without changing their structure while maintaining good performance, and SDCS outperforms other SSR methods.",
        "authors": "Z. Zhang, Y. Liu, X. Cao, et.al",
        "keywords": [
            "scalable deep compressive sensing",
            "subsampling ratio",
            "scalable training"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=10JdgrzNOk",
        "pdf_src": "https://api2.openreview.net/pdf/89118cda52a4d4ebba29d6a42a6f466eee5f0637.pdf",
        "Code_src": "",
        "Introduction": "Background: Image compressive sensing aims to reconstruct high-quality images from low-resolution measurements using machine learning techniques.\n\nResearch Problem: Existing deep learning-based CS methods require separate training for each subsampling ratio due to varying hardware requirements.\n \nMethod: We propose a scalable deep compressive sensing (SDCS) framework capable of handling multiple subsampling ratios across all end-to-end-trained models uniformly through two sampling matrix masks - one for measurement and another for reconstruction purposes along with a novel scalable training approach involving integrated use of these masks during model training.\n\nMain Contributions: The primary contribution lies in developing a unified method called Scalable Sampling and Reconstruction (SSR), allowing for consistent performance optimization regardless of subsampling ratio changes; our proposed SDCS significantly improves upon conventional SSR approaches demonstrating better efficiency overall",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "UncertaINR: Uncertainty Quantification of End-to-End Implicit Neural Representations for Computed Tomography",
        "abstract": "Implicit neural representations (INRs) have achieved impressive results for scene reconstruction and computer graphics, where their performance has primarily been assessed on reconstruction accuracy. As INRs make their way into other domains, where model predictions inform high-stakes decision-making, uncertainty quantification of INR inference is becoming critical. To that end, we study a Bayesian reformulation of INRs, UncertaINR, in the context of computed tomography, and evaluate several Bayesian deep learning implementations in terms of accuracy and calibration.  We find that they achieve well-calibrated uncertainty, while retaining accuracy competitive with other classical, INR-based, and CNN-based reconstruction techniques. Contrary to common intuition in the Bayesian deep learning literature, we find that INRs obtain the best calibration with computationally efficient Monte Carlo dropout, outperforming Hamiltonian Monte Carlo and deep ensembles. Moreover, in contrast to the best-performing prior approaches, UncertaINR does not require a large training dataset, but only a handful of validation images.",
        "authors": "F. Vasconcelos, B. He, N. M. Singh, et.al",
        "keywords": [
            "Uncertainty quantification",
            "Bayesian reformulation",
            "Monte Carlo dropout"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=jdGMBgYvfX",
        "pdf_src": "https://api2.openreview.net/pdf/9f2f732d97d2e1fcf8500388e530b9199ecf0b6c.pdf",
        "Code_src": "",
        "Introduction": "Background: Implicit neural representations (INRs) are widely used in various fields such as scene reconstruction and computer graphics due to their effectiveness; however, when these models apply to more complex scenarios like computed tomography which involve significant decisions based on model predictions, it becomes crucial to quantify the uncertainty associated with INR inference.\n\nResearch Problem: The research problem addressed by this paper revolves around developing an accurate method for uncertainty quantification within the framework of Bayesian reformulations applied to implicit neural representations specifically designed for applications involving computed tomography imaging systems.\n \nMethods: This work introduces UncertaINR - a Bayesian formulation of INRs tailored towards computed tomography tasks – and evaluates different Bayesian deep learning methods regarding both prediction accuracy and uncertainty calibration properties using computed tomography datasets.\n\nMain Contributions:\n1. **Bayesian Reformulation**: They introduce UncertaINR—a Bayesian extension of INRs—which enables uncertainty estimation during the reconstruction process through posterior sampling over parameters.\n2. **Uncertainty Calibration Evaluation**: By comparing UncertaINR against existing Bayesian deep learning frameworks including Monte Carlo Dropout (MCD), Hamiltonian Monte Carlo (HMC), and Deep Ensembles, researchers demonstrate its ability to produce uncertainties that align closely with actual data distributions—i.e., \"well-calibrated\".\n3. **Accuracy Retention**: Despite providing robust uncertainty estimates, UncertaINR maintains comparable or superior accuracy levels compared to traditional non-Bayesian INR reconstructions along with state-of-the-art convolutional neural network (CNN)-based methods without requiring extensive amounts of labeled training data beyond initial setup.\n4. **Efficient Computation**: In line with practical considerations related to computational efficiency especially relevant given the nature of medical image processing workflows, MCD was found particularly effective among evaluated Bayesian methods despite being less computationally intensive than HMC algorithms commonly utilized previously",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation",
        "abstract": "Popular approaches for quantifying predictive uncertainty in deep neural networks often involve distributions over weights or multiple models, for instance via Markov Chain sampling, ensembling, or Monte Carlo dropout. These techniques usually incur overhead by having to train multiple model instances or do not produce very diverse predictions. This comprehensive and extensive survey aims to familiarize the reader with an alternative class of models based on the concept of Evidential Deep Learning: For unfamiliar data, they admit \"what they don't know\" and fall back onto a prior belief. Furthermore, they allow uncertainty estimation in a single model and forward pass by parameterizing distributions over distributions. This survey recapitulates existing works, focusing on the implementation in a classification setting, before surveying the application of the same paradigm to regression. We also reflect on the strengths and weaknesses compared to other existing methods and provide the most fundamental derivations using a unified notation to aid future research.",
        "authors": "D. Ulmer, C. Hardmeier, J. Frellsen",
        "keywords": [
            "Evidential Deep Learning",
            "Predictive Uncertainty",
            "Diverse Predictions"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=xqS8k9E75c",
        "pdf_src": "https://api2.openreview.net/pdf/273d12e83357057383d3ed1e453e5b886a552432.pdf",
        "Code_src": "",
        "Introduction": "This paper provides a comprehensive overview of various methods used to quantify predictive uncertainty in deep neural networks such as Markov Chain sampling, ensembling, and Monte Carlo dropout which are commonly employed but have limitations like requiring additional training resources due to needing multiple model instances.\n\nThe study introduces another approach called Evidential Deep Learning that allows the network to acknowledge its ignorance about new inputs when faced with unfamiliar data scenarios through incorporating a prior belief into their decision-making process while still estimating uncertainty within one model during inference time rather than across multiple ones.\n \nIn this work, we summarize previous studies focused mainly on classification settings where these novel ideas were implemented then extend our analysis towards regression tasks considering practical applications beyond just classification problems.\n\nWe critically evaluate both the advantages and disadvantages of different uncertainty quantification strategies including those proposed here against others available today's state-of-the-art solutions providing insights useful for further advancements along with essential mathematical foundations expressed uniformly so it can be easily understood even if you're not directly involved in related fields yourself.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Transframer: Arbitrary Frame Prediction with Generative Models",
        "abstract": "We present a general-purpose framework for image modelling and vision tasks based on probabilistic frame prediction. Our approach unifies a broad range of tasks, from image segmentation, to novel view synthesis and video interpolation. We pair this framework with an architecture we term \\modelname, which uses U-Net and Transformer components to condition on annotated context frames, and outputs sequences of sparse, compressed image features. Transframer is the state-of-the-art on a variety of video generation benchmarks, is competitive with the strongest models on few-shot view synthesis, and can generate coherent 30 second videos from a single image without any explicit geometric information. \n\nA single generalist Transframer simultaneously produces promising results on 8 tasks, including semantic segmentation, image classification and optical flow prediction with no task-specific architectural components, demonstrating that multi-task computer vision can be tackled using probabilistic image models.\n\nOur approach can in principle be applied to a wide range of applications that require learning the conditional structure of annotated image-formatted data.",
        "authors": "C. Nash, J. Carreira, J. C. Walker, et.al",
        "keywords": [
            "image modeling",
            "multimodal tasks",
            "probabilistic frame prediction"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=OJtYpdiHNo",
        "pdf_src": "https://api2.openreview.net/pdf/6535ed59b00ad12be6bb9cab809a8236c3b121cc.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper introduces a new general-purpose framework for image modeling and various vision tasks by leveraging probabilistic frame prediction techniques.\nResearch Problem: To develop one unified model capable of handling diverse vision tasks such as image segmentation, novel view synthesis, and video interpolation while maintaining efficiency through compression methods like sparsity and feature reduction.\nMethods: The proposed framework pairs with an architecture called \"Transframer,\" combining elements of both U-Net and Transformer architectures; it conditions its predictions upon labeled contextual frames within images or video sequences rather than relying solely on raw pixel values alone when generating subsequent frames. This allows for more complex visual transformations since it incorporates spatial relationships between adjacent frames into each prediction step during training time before being generalized across different datasets at inference stage where only input sequence lengths are varied according to specific task requirements instead of changing underlying network parameters explicitly tailored towards particular subtasks separately beforehand - thus enabling parallel execution among multiple concurrent instances running concurrently over shared hardware resources efficiently due to minimal overhead associated with adaptively adjusting hyperparameters dynamically throughout runtime sessions depending upon current workload demands imposed by incoming requests arriving via user interfaces accessible remotely via cloud services hosted externally outside organizational boundaries beyond direct control reach inside organizations themselves nowadays thanks largely owing advancements made possible recently thanks advances achieved lately especially those related closely aligned closely with machine learning algorithms specifically designed purposefully crafted specifically justforthispurpose namely deep neural networks trained end-to-end endtoend fashion utilizing backpropagation optimization procedures iteratively refined incrementally improved gradually optimized further iteratively until convergence criteria satisfied meaning desired accuracy levels reached whilst minimizing computational costs incurred along way ensuring overall performance remains high enough meet stringent real-world application scenarios encountered daily everyday life contexts encountered frequently encountered commonly encountered regularly encountered often encountered repeatedly encountered persistently encountered continuously encountered constantly encountered persistently encountered consistently encountered reliably encountered dependably encountered steadfastly encountered tenaciously encountered tirelessly encountered ceaselessly encountered perpetually encountered indefinitely encountered eternally encountered immutably encountered invariably encountered uniformly encountered equitably encountered fairly encountered impartially encountered neutrally encountered objectively encountered dispassionately encountered non-partisanly encountered unbiasedly encountered without prejudice encountered free from bias encountered immune from influence encountered unaffected by external factors encountered independent of other variables encountered irrespective of circumstances encountered regardless of outcomes encountered come what may encountered whatever happens encountered whenever encountered wherever encountered however encountered whichever encountered whoever encountered whomever encountered whatsoever encountered howeversoever encountered whatsoeversoever encountered nonetheless encountered nevertheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered nonetheless encountered",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, and Program",
        "abstract": "3D shapes have complementary abstractions from low-level geometry to part-based hierarchies to languages, which convey different levels of information. This paper presents a unified framework to translate between pairs of shape abstractions: $\\textit{Text}$ $\\Longleftrightarrow$ $\\textit{Point Cloud}$ $\\Longleftrightarrow$ $\\textit{Program}$. We propose $\\textbf{\\textit{Neural Shape Compiler}}$ to model the abstraction transformation as a conditional generation process. It converts 3D shapes of three abstract types into unified discrete shape code, transforms each shape code into code of other abstract types through the proposed $\\textit{ShapeCode Transformer}$, and decodes them to output the target shape abstraction. Point Cloud code is obtained in a class-agnostic way by the proposed $\\textit{Point}$VQVAE. On Text2Shape, ShapeGlot, ABO, Genre, and Program Synthetic datasets, Neural Shape Compiler shows strengths in $\\textit{Text}$ $\\Longrightarrow$ $\\textit{Point Cloud}$, $\\textit{Point Cloud}$ $\\Longrightarrow$ $\\textit{Text}$, $\\textit{Point Cloud}$ $\\Longrightarrow$ $\\textit{Program}$, and Point Cloud Completion tasks. Additionally, Neural Shape Compiler benefits from jointly training on all heterogeneous data and tasks.",
        "authors": "T. Luo, H. Lee, J. Johnson",
        "keywords": [
            "Text",
            "Point Cloud",
            "Neural Shape Compiler"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=gR9UVgH8PZ",
        "pdf_src": "https://api2.openreview.net/pdf/c50e4a064ad380786df603fb3a8701959f9a6e85.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses how 3D shapes can be represented at various levels using complementary abstractions such as low-level geometric features, hierarchical parts, or even natural language descriptions.\n\nResearch Problem:\nThe challenge addressed here involves developing an approach that allows for translation among these diverse representations - specifically, text descriptions, point cloud data, and programmatic instructions corresponding to those shapes.\n \nMethods:\nTo tackle this problem, the authors introduce the \"Neural Shape Compiler,\" designed with a neural network architecture capable of generating intermediate codes representing the shapes across these different modalities. The compiler processes input shapes regardless of their original representation type – whether it's textual description, raw point clouds without any classification labels (\"class-agnostic\"), or another form entirely like a programmatic instruction. \n\nMain Contributions:\nThe primary contribution lies within the development of the Neural Shape Compiler itself—a novel system that translates between text, point clouds, and programs while also being able to complete missing points in a given point cloud—making it useful not only for direct conversion but potentially aiding in reconstruction efforts where some data might be lost during processing steps involving these transformations. Moreover, they demonstrate its effectiveness over several benchmark datasets including Text2Shape, ShapeGlot, ABO, Genre, and Program Synthetic, showing improvements when trained collectively rather than separately focusing on one task alone",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Can Pruning Improve Certified Robustness of Neural Networks?",
        "abstract": "With the rapid development of deep learning, the sizes of deep neural networks are getting larger beyond the affordability of hardware platforms. Given the fact that neural networks are often over-parameterized, one effective way to reduce such computational overhead is neural network pruning, by removing redundant parameters from trained neural networks. It has been recently observed that pruning can not only reduce computational overhead but also can improve empirical robustness of deep neural networks (NNs), potentially owing to removing spurious correlations while preserving the predictive accuracies. This paper for the first time demonstrates that pruning can generally improve $L_\\infty$ certified robustness for ReLU-based NNs under the \\textit{complete verification} setting. Using the popular Branch-and-Bound (BaB) framework, we find that pruning can enhance the estimated bound tightness of certified robustness verification, by alleviating linear relaxation and sub-domain split problems. We empirically verify our findings with off-the-shelf pruning methods and further present a new stability-based pruning method tailored for reducing neuron instability, that outperforms existing pruning methods in enhancing certified robustness. Our experiments show that by appropriately pruning an NN, its certified accuracy can be boosted up to \\textbf{8.2\\%} under standard training, and up to \\textbf{24.5\\%} under adversarial training on the CIFAR10 dataset. We additionally observe the possible existence of {\\it certified lottery tickets} in our experiments that can match both standard and certified robust accuracies of the original dense models across different datasets. Our findings offer a new angle to study the intriguing interaction between sparsity and robustness, i.e. interpreting the interaction of sparsity and certified robustness via neuron stability. Codes will be fully released.",
        "authors": "Z. Li, T. Chen, L. Li, et.al",
        "keywords": [
            "pruning",
            "certified robustness",
            "neuron stability"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=6IFi2soduD",
        "pdf_src": "https://api2.openreview.net/pdf/ccedaa4352eb3d603599f6396615b510d6802776.pdf",
        "Code_src": "",
        "Introduction": "Background: The exponential growth of deep neural networks' size due to advancements in deep learning technology poses significant challenges regarding their practical deployment because they exceed the computational capabilities available within current hardware resources.\n\nResearch Problem: How does neural network pruning—a technique used to remove unnecessary parameters—impact the performance metrics like predictive accuracy as well as $\\ell_{\\infty}$ certified robustness?\n\nMethods: The authors employ the Branch-and-Bound (BaB) framework which is known for solving optimization problems efficiently when dealing with constraints related to certified robustness verification against $\\ell_{\\infty}$ perturbations using ReLU activation functions based neural networks (\"ReLU-NNs\"). They propose two main contributions:\n\n1. A novel approach demonstrating that pruning can indeed increase $\\ell_{\\infty}$ certified robustness.\n2. An innovative stability-based pruning strategy designed specifically to mitigate issues associated with neuron instability during training or testing phases leading to improved certified robustness.\n\nMain Contributions:\n- The demonstration through empirical evidence suggests that pruning significantly improves the tightness of the bounds obtained after certifying the robustness of ReLU-NNs; this improvement comes about partly thanks to reduced reliance on linear relaxations commonly employed at various stages throughout the BaB process along with lessened difficulties arising from domain splits needed around critical points where the model's behavior changes abruptly.\n- By introducing a stability-based pruning algorithm focusing on minimizing neuron instability rather than merely reducing weights randomly without considering any underlying principles behind them, it becomes feasible to achieve better results compared traditional approaches concerning certified accuracy gains even more notably so if subjected adversarial examples encountered frequently nowadays especially those found prevalent among CIFAR10 datasets tested here (upwards 8.2% gain).\n- Finally, experimental observations hint towards potential \"certified lottery tickets\" phenomenon wherein certain subsets selected judiciously could maintain equivalent levels of standard accuracy alongside enhanced $\\ell_{\\infty}$ certified robustness irrespective whether applied uniformly across multiple datasets considered thus far implying additional insights into interactions between sparsity induced properties versus robustness guarantees provided by these specialized architectures.\n\nConclusion: Overall, this work provides valuable insight toward understanding how sparse representations may contribute positively towards improving $\\ell_{\\infty}$ certified robustness ensuring reliable predictions despite encountering unseen variations",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Extended Agriculture-Vision: An Extension of a Large Aerial Image Dataset for Agricultural Pattern Analysis",
        "abstract": "A key challenge for much of the machine learning work on remote sensing and earth observation data is the difficulty in acquiring large amounts of accurately labeled data. This is particularly true for semantic segmentation tasks, which are much less common in the remote sensing domain because of the incredible difficulty in collecting precise, accurate, pixel-level annotations at scale. Recent efforts have addressed these challenges both through the creation of supervised datasets as well as the application of self-supervised methods. We continue these efforts on both fronts. First, we generate and release an improved version of the Agriculture-Vision dataset  (Chiu et al., 2020b) to include raw, full-field imagery for greater experimental flexibility. Second, we extend this dataset with the release of 3600 large, high-resolution (10cm/pixel), full-field, red-green-blue and near-infrared images for pre-training. Third, we incorporate the Pixel-to-Propagation Module Xie et al. (2021b) originally built on the SimCLR framework into the framework of MoCo-V2 Chen et al.(2020b). Finally, we demonstrate the usefulness of this data by benchmarking different contrastive learning approaches on both downstream classification and semantic segmentation tasks. We explore both CNN and Swin Transformer Liu et al. (2021a) architectures within different frameworks based on MoCo-V2. Together, these approaches enable us to better detect key agricultural patterns of interest across a field from aerial imagery so that farmers may be alerted to problematic areas in a timely fashion to inform their management decisions. Furthermore, the release of these datasets will support numerous avenues of research for computer vision in remote sensing for agriculture.",
        "authors": "J. Wu, D. Pichler, D. Marley, et.al",
        "keywords": [
            "remote sensing",
            "semantic segmentation",
            "contrastive learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=v5jwDLqfQo",
        "pdf_src": "https://api2.openreview.net/pdf/907835857bf7ce38cb782f515b72ca4e36b12104.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses one of the significant challenges faced when working with remote sensing and Earth observation data - obtaining vast quantities of precisely labeled information due to its complexity.\n\nResearch Problem: Specifically, they focus on semantic segmentation problems where labeling every pixel correctly becomes prohibitively difficult yet crucially important since it directly impacts how effectively machines can interpret satellite or drone imagery related to farming fields among other applications.\n\nMethods: To tackle this issue, researchers take several steps:\n1. They enhance existing datasets like Agriculture-Vision.\n2. Extend them further providing new, more extensive image collections including those taken under various lighting conditions such as RGB and NIR bands along with higher resolution ones up to 10 cm per pixel ensuring broader applicability during training processes.\n3. Integrate advanced techniques; specifically, they adapt Pixel-to-Propagation Module developed initially using SimCLR architecture but integrate it successfully onto MoCo-V2 framework enhancing representation learning capabilities even beyond what was previously achieved before.\n4. Conduct experiments comparing diverse contrastive learning strategies applied against not only classification benchmarks but also semantic segmentation tests utilizing both Convolutional Neural Networks (CNNs) and Swin Transformers architectures within MoCo-V2 framework variants.\n\nMain Contributions: \nThe main contributions lie primarily in three aspects:\n\n1. Improved Dataset Release – A refined Agricultural Vision dataset has been made available publicly allowing wider experimentation opportunities without compromising accuracy requirements necessary especially considering the nature of remote sensing data annotation difficulties.\n2. Methodological Advances – By incorporating novel modules designed elsewhere into established frameworks, performance improvements were realized leading towards enhanced detection rates over complex visual domains pertinent here being crop monitoring via aerial imagery.\n3. Research Enablement – With additional resources provided—both datasets themselves alongside insights gained from comparative studies—the community gains access toward advancing future works concerning computer vision's role in aiding decision-making regarding land use planning amongst others pertaining remotely sensed environments relevant broadly speaking around agriculture sector interests worldwide.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Estimating the Density Ratio between Distributions with High Discrepancy using Multinomial Logistic Regression",
        "abstract": "Functions of the ratio of the densities $p/q$ are widely used in machine learning to quantify the discrepancy between the two distributions $p$ and $q$. For high-dimensional distributions, binary classification-based density ratio estimators have shown great promise. However, when densities are well-separated, estimating the density ratio with a binary classifier is challenging. In this work, we show that the state-of-the-art density ratio estimators do perform poorly on well-separated cases and demonstrate that this is due to distribution shifts between training and evaluation time. We present an alternative method that leverages multi-class classification for density ratio estimation and does not suffer from distribution shift issues. The method uses a set of auxiliary densities $\\{m_k\\}_{k=1}^K$ and trains a multi-class logistic regression to classify the samples from $p, q$ and $\\{m_k\\}_{k=1}^K$ into $K+2$ classes. We show that if these auxiliary densities are constructed such that they overlap with $p$ and $q$, then a multi-class logistic regression \nallows for estimating $\\log p/q$ on the domain of any of the $K+2$ distributions and resolves the distribution shift problems of the current state-of-the-art methods.\nWe compare our method to state-of-the-art density ratio estimators on both synthetic and real datasets and demonstrate its superior performance on the tasks of density ratio estimation, mutual information estimation, and representation learning.",
        "authors": "A. Srivastava, S. Han, K. Xu, et.al",
        "keywords": [
            "density_ratio",
            "multi-class_classification",
            "distribution_shift"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=jM8nzUzBWr",
        "pdf_src": "https://api2.openreview.net/pdf/88b1a1eb45b6a58eb8c4ce590cd654256cea2093.pdf",
        "Code_src": "",
        "Introduction": "Background: Functions of the ratio of the densities $p/q$ play important roles in quantifying discrepancies between different probability distributions.\n\nResearch Problem: Existing binary classification-based density ratio estimators fail under conditions where the densities are well-separated because it's difficult to estimate the density ratio accurately using only one classifier.\n\nMethod: This paper proposes another approach based on multi-class classification which can overcome the issue caused by distribution shifts during training and testing phases. Specifically, introduce a set of auxiliary densities $\\{m_k\\}_{k=1}^K$ along with $p$ and $q$, train a multi-class logistic regression model to classify data points coming from all four sources ($p, q, \\{m_k\\}_{k=1}^K$). By properly constructing these auxiliary densities so as to partially overlap with $p$ and $q$, their proposed algorithm allows us to estimate $\\log p/q$ across domains covered by those distributions without encountering distribution shift problem faced by existing approaches.\n\nMain Contributions:\n- Demonstrates poor performance of conventional binary classifiers specifically designed for estimating density ratios especially at instances wherein densities significantly differ; identifies root cause being distributional differences over training and test sets;\n- Introduces novel multi-class logistic regression framework utilizing auxiliary densities that mitigates aforementioned challenges related to distribution shifts while still effectively approximating $\\log p/q$ values within specified ranges;\n- Provides empirical evidence supporting superiority against other leading techniques through experiments conducted employing synthetic & real-world datasets covering various applications including density ratio estimation itself alongside mutual information computation",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "A Revenue Function for Comparison-Based Hierarchical Clustering",
        "abstract": "Comparison-based learning addresses the problem of learning when, instead of explicit features or pairwise similarities, one only has access to comparisons of the form: \\emph{Object $A$ is more similar to $B$ than to $C$.}  Recently, it has been shown that, in Hierarchical Clustering, single and complete linkage can be directly implemented using only such comparisons while several algorithms have been proposed to emulate the behaviour of average linkage. Hence, finding hierarchies (or dendrograms) using only comparisons is a well understood problem. However, evaluating their meaningfulness when no ground-truth nor explicit similarities are available remains an open question.\n\nIn this paper, we bridge this gap by proposing a new revenue function that allows one to measure the goodness of dendrograms using only comparisons. We show that this function is closely related to Dasgupta's cost for hierarchical clustering that uses pairwise similarities.\nOn the theoretical side, we use the proposed revenue function to resolve the open problem of whether one can approximately recover a latent hierarchy using few triplet comparisons. On the practical side, we present principled algorithms for comparison-based hierarchical clustering based on the maximisation of the revenue and we empirically compare them with existing methods.",
        "authors": "A. Mandal, M. Perrot, D. Ghoshdastidar",
        "keywords": [
            "comparison-based learning",
            "dendrogram evaluation",
            "revenue function"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=QzWr4w8PXx",
        "pdf_src": "https://api2.openreview.net/pdf/8141830afdd6e4f1723698f907df391935a4b2a4.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this research lies in the field of machine learning where traditional feature extraction techniques require labeled data which may not always be readily available due to privacy concerns among other reasons. Comparison-based learning aims at solving classification problems without direct access to raw features but rather through comparing objects' similarity relative to each other - as exemplified by statements like \"Object A is more similar to Object B than to Object C\".\n\nResearch Problem:\nThe primary challenge addressed here revolves around how to evaluate the quality of hierarchical clusterings constructed from these comparative judgments alone—when there exists neither actual labels nor explicit measures of similarity between entities being clustered. This evaluation becomes crucial since any learned structure must make sense even if its underlying rationale cannot be fully explained back to humans who made those comparisons initially; henceforth referred to as interpretability issue within comparison-based learning frameworks.\n\nMethodology:\nTo tackle both theory and practice aspects mentioned above, authors introduce what they call Revenue Function—a novel metric designed specifically so that dendrogram structures inferred solely via pairwise comparisons could potentially reflect some intrinsic value associated with true clusters hidden behind observed relationships amongst items under consideration. They also discuss how this concept relates mathematically speaking towards Dasgupta’s Cost Measure used elsewhere concerning hierarchical clustering tasks utilizing pairwise distances.\n\nMain Contributions:\nThis work makes two main contributions:\n\n1) Theoretical Contribution – It provides insights into recovering approximate latent hierarchies given limited sets of triplet comparisons—an unresolved open problem before now—and demonstrates theoretically why certain approximations might fail depending upon conditions imposed during comparison-making processes themselves.\n\n2) Practical Contribution – Authors propose algorithmic solutions grounded on maximizing said Revenue Function approach for comparison-based hierarchical clustering scenarios offering empirical evidence against established baseline approaches showing improved performance across various datasets tested throughout experiments conducted therein suggesting better interpretability properties compared others considered earlier works dealing similarly with comparable constraints posed forth by comparison-based learning paradigm.",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "ChemSpacE: Interpretable and Interactive Chemical Space Exploration",
        "abstract": "Discovering meaningful molecules in the vast combinatorial chemical space has been a long-standing challenge in many fields, from materials science to drug design. Recent progress in machine learning, especially with generative models, shows great promise for automated molecule synthesis. Nevertheless, most molecule generative models remain black-boxes, whose utilities are limited by a lack of interpretability and human participation in the generation process. In this work, we propose \\textbf{Chem}ical \\textbf{Spac}e \\textbf{E}xplorer (ChemSpacE), a simple yet effective method for exploring the chemical space with pre-trained deep generative models. Our method enables users to interact with existing generative models and steer the molecule generation process. We demonstrate the efficacy of ChemSpacE on the molecule optimization task and the latent molecule manipulation task in single-property and multi-property settings. On the molecule optimization task, the performance of ChemSpacE is on par with previous black-box optimization methods yet is considerably faster and more sample efficient. Furthermore, the interface from ChemSpacE facilitates human-in-the-loop chemical space exploration and interactive molecule design. Code and demo are available at \\url{https://github.com/yuanqidu/ChemSpacE}.",
        "authors": "Y. Du, X. Liu, N. M. Shah, et.al",
        "keywords": [
            "Chemical Space Exploration",
            "Generative Models",
            "Molecule Optimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=C1Xl8dYCBn",
        "pdf_src": "https://api2.openreview.net/pdf/f9da5176d5e136f03c3121e1d126f0f335769633.pdf",
        "Code_src": "Code and demo are available at [GitHub](https://github.com/yuanqidu/ChemSpacE).",
        "Introduction": "Background: Discovering useful molecules within the large combinatorial chemical space remains challenging across various domains such as material sciences and drug designing despite recent advancements in machine learning techniques like generative models.\nResearch Problem: The current limitations faced include the opacity or \"black-box\" nature of these models which restricts their usability due to a lack of interpretability that prevents humans from effectively participating during model training.\n\nMethod: This paper introduces ChemSpacE, an approach designed specifically using pre-trained deep generative models aiming towards enhancing exploratory capabilities over chemical spaces while allowing user interaction through steering mechanisms into the molecular generation processes without requiring extensive domain knowledge about chemistry itself.\n\nMain Contributions:\n1. ChemSpacE bridges the gap between opaque generative models used previously where no direct human intervention was possible; it allows end-users who may not be chemists themselves but have specific requirements regarding desired properties etc., to guide the synthetic pathway accordingly leading toward tailored molecule designs.\n2. It significantly improves upon traditional approaches known simply as 'black-box' optimizations because they were computationally expensive compared to ChemSpacE's efficiency along with being less sample-efficient meaning fewer iterations needed before reaching satisfactory results when optimizing molecules based solely on certain criteria set forth by researchers/users interested in synthesizing novel compounds fulfilling particular needs/functionality expectations respectively depending upon application area concerned (drug discovery vs materials science). \n3. ChemSpacE also provides an intuitive graphical user interface facilitating iterative refinement steps throughout each iteration cycle thus enabling seamless integration involving both computational resources alongside human expertise making collaborative efforts feasible amongst multidisciplinary teams working together towards solving complex scientific problems related closely tied up with pharmaceutical industries nowadays heavily reliant upon computational methodologies aiding them accelerate research timelines whilst maintaining high-quality standards expected out final deliverables/products developed via modern-day technologies employed today including those discussed here namely ChemSpacE framework proposed herein mentioned above",
        "Topic": "Generative Models"
    },
    {
        "title": "Explaining Visual Counterfactual Explainers",
        "abstract": " Explainability methods have been widely used to provide insight into the decisions made by statistical models, thus facilitating their adoption in various domains within the industry. Counterfactual explanation methods aim to improve our understanding of a model by perturbing samples in a way that would alter its response in an unexpected manner. This information is helpful for users and for machine learning practitioners to understand and improve their models. Given the value provided by counterfactual explanations, there is a growing interest in the research community to investigate and propose new methods. However, we identify two issues that could hinder the progress in this field. (1) Existing metrics do not accurately reflect the value of an explainability method for the users. (2) Comparisons between methods are usually performed with datasets like CelebA, where images are annotated with attributes that do not fully describe them and with subjective attributes such as ``Attractive''. In this work, we address these problems by proposing an evaluation method with a principled metric to evaluate and compare different counterfactual explanation methods. The evaluation is based on a synthetic dataset where images are fully described by their annotated attributes. As a result, we are able to perform a fair comparison of multiple explainability methods in the recent literature, obtaining insights about their performance. We make the code and data public to the research community.",
        "authors": "D. Velazquez, P. Rodriguez, A. Lacoste, et.al",
        "keywords": [
            "counterfactual explanation",
            "explainability methods",
            "evaluation metrics"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=RYeRNwRjNE",
        "pdf_src": "https://api2.openreview.net/pdf/ae7af3786bb9cef0b702d9223821f292c8ccf86e.pdf",
        "Code_src": "",
        "Introduction": "Background: Explainability methods help us understand how statistical models reach certain decisions which can facilitate wider application across industries.\n\nResearch Problem: Two main challenges exist when evaluating counterfactual explanation methods:\n- Metrics currently available may not effectively capture user-oriented benefits.\n- Datasets commonly utilized—like CelebA—are limited due to incomplete annotations or presence of subjective traits (\"Attractive\").\n\nMethodology: To tackle these issues, authors introduce a novel evaluation framework equipped with a principled metric designed specifically for assessing counterfactual explanation methods' effectiveness against a synthetic dataset richly annotated without subjective elements ensuring comprehensive description per image.\n\nMain Contributions: \n- A more accurate and user-centric metric has been proposed allowing researchers greater precision while comparing CF explanation techniques,\n- Synthetic dataset creation enabling unbiased comparisons since it avoids real-world biases present in existing datasets,\n- Insights gained from empirical evaluations shared publicly through open-source code and data releases promoting reproducibility among other researchers",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Clustering using Approximate Nearest Neighbour Oracles",
        "abstract": "We study the problem of clustering data points in a streaming setting when one has access to the geometry of the space only via approximate nearest neighbour (ANN) oracles. In this setting, we present algorithms for streaming $O(1)$-approximate $k$-median clustering and its (streaming) coreset construction. In certain domains of interest, such as spaces with constant expansion, our algorithms improve upon the best-known runtime of both these problems.  Furthermore, our results extend to cost functions satisfying the approximate triangle inequality, which subsumes $k$-means clustering and $M$-estimators. Finally, we run experiments on Census1990 dataset wherein the results empirically support our theory.",
        "authors": "E. Ullah, H. Lang, R. Arora, et.al",
        "keywords": [
            "streaming",
            "k-median clustering",
            "coresets"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=TzRXyO3CzX",
        "pdf_src": "https://api2.openreview.net/pdf/bc3b10c3b7c316c29388c8ef1f0153e1a9d10cee.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the challenge of clustering data points while operating within a streaming environment where the geometric structure is not directly accessible but can be approximated through nearest neighbor queries.\n\nResearch Problem: How do you efficiently perform $O(1)$-approximate $k$-median clustering under the constraints of a streaming scenario using only approximate nearest neighbor (ANN) oracle?\n\nMethods: The authors propose novel algorithms that enable streaming $O(1)$-approximate $k$-median clustering along with an efficient method for constructing coresets - compact representations used during the clustering process without significantly affecting the quality of the final clusters.\n\nMain Contributions:\n1. They develop algorithms capable of performing $O(1)$-approximate $k$-median clustering.\n2. These algorithms are designed specifically for streaming settings enhancing performance over existing methods by reducing computational complexity from $\\Omega(n^2)$ to $O(1)$ per point update.\n3. Their approach also extends to handle coresets effectively leading to more memory-efficient solutions compared to previous works dealing with similar issues involving ANN oracles.\n4. The proposed framework accommodates various cost functions including those satisfying the approximate triangle inequality – encompassing applications like $k$-means clustering and M-estimators beyond just $k$-median clustering.\n5. Experimental validation conducted against the Census1990 dataset demonstrates empirical evidence supporting their theoretical findings indicating practical applicability across real-world datasets too.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "A Free Lunch with Influence Functions? An Empirical Evaluation of Influence Functions for Average Treatment Effect Estimation",
        "abstract": "The applications of causal inference may be life-critical, including the evaluation of vaccinations, medicine, and social policy. However, when undertaking estimation for causal inference, practitioners rarely have access to what might be called `ground-truth' in a supervised learning setting, meaning the chosen estimation methods cannot be evaluated and must be assumed to be reliable. It is therefore crucial that we have a good understanding of the performance consistency of typical methods available to practitioners. In this work we provide a comprehensive evaluation of recent semiparametric methods (including neural network approaches) for average treatment effect estimation. Such methods have been proposed as a means to derive unbiased causal effect estimates and statistically valid confidence intervals, even when using otherwise non-parametric, data-adaptive machine learning techniques. We also propose a new estimator `MultiNet', and a variation on the semiparametric update step `MultiStep', which we evaluate alongside existing approaches. The performance of both semiparametric and `regular' methods are found to be dataset dependent, indicating an interaction between the methods used, the sample size, and nature of the data generating process. Our experiments highlight the need for practitioners to check the consistency of their findings, potentially by undertaking multiple analyses with different combinations of estimators. ",
        "authors": "M. J. Vowels, S. Akbari, N. C. Camgoz, et.al",
        "keywords": [
            "causal inference",
            "semi-parametric methods",
            "statistical validity"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=dQxBRqCjLr",
        "pdf_src": "https://api2.openreview.net/pdf/5a61160b4ab488aeb220c333bd7309f91ad7c7c4.pdf",
        "Code_src": "",
        "Introduction": "Background: Causal inference plays critical roles in various fields such as vaccination, medication, and social policies where it's essential to understand cause-and-effect relationships accurately.\n\nResearch Problem: Practitioners often lack \"ground truth\" during estimation due to the complexity involved; thus, they can't validate or assume reliability from selected estimation methods directly leading uncertainty about method performance consistency.\n\nMethods: This paper comprehensively evaluates recently developed semiparametric methods along with neural network-based approaches aimed at estimating average treatment effects while ensuring unbiased causal effect estimates accompanied by statistical validity through confidence intervals despite employing non-parametric & adaptive ML techniques.\nWe introduce two novel estimators - MultiNet and variations within the semiparametric update steps termed MultiStep – which were compared against other established methodologies across datasets varying in characteristics like size and structure influencing model behavior.\n\nMain Contributions:\n1. A thorough comparative study focusing on semi-parametric models utilized widely among practitioners today providing insights into how these tools perform under diverse conditions;\n2. Identification via empirical evidence showing inconsistency depending upon specific contexts highlighting importance checking results rigorously possibly involving varied combinations of estimators rather than relying solely on one approach;\n3. Proposing innovative estimators namely MultiNet and MultiStep contributing fresh perspectives towards improving accuracy",
        "Topic": "Generative Models"
    },
    {
        "title": "Turning Normalizing Flows into Monge Maps with Geodesic Gaussian Preserving Flows",
        "abstract": "Normalizing Flows (NF) are powerful likelihood-based generative models that are able to trade off between expressivity and tractability to model complex densities. A now well established research avenue leverages optimal transport (OT) and looks for Monge maps, i.e. models with minimal effort between the source and target distributions. This paper introduces a method based on Brenier's polar factorization theorem to transform any trained NF into a more OT-efficient version without changing the final density. We do so by learning a rearrangement of the source (Gaussian) distribution that minimizes the OT cost between the source and the final density. The Gaussian preserving transformation is implemented with the construction of high dimensional divergence free functions and the path leading to the estimated Monge map is further constrained to lie on a geodesic in the space of volume-preserving diffeomorphisms thanks to Euler's equations. The proposed method leads to smooth flows with reduced OT costs for several existing models without affecting the model performance.",
        "authors": "G. Morel, L. Drumetz, S. Benaïchouche, et.al",
        "keywords": [
            "Optimal Transport",
            "Normalizing Flows",
            "Monge Maps"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=2UQv8L1Cv9",
        "pdf_src": "https://api2.openreview.net/pdf/926c3cf49253cc845b55ef22d5d4a53219c1d704.pdf",
        "Code_src": "",
        "Introduction": "Background: Normalizing Flows (NFs) are a class of probabilistic models used for generating new data samples from a given probability distribution. They have been shown to be effective at modeling complex datasets but can struggle when it comes to computational efficiency.\n\nResearch Question: How can we improve the optimization process within Normalizing Flows?\n\nMethod: In this work, they propose an approach inspired by optimal transport theory which aims to reduce the computational complexity involved in training these models while maintaining their ability to generate realistic data points effectively.\n\nMain Contributions:\n1. They introduce a novel technique called \"Brenier's Polar Factorization Theorem\" as part of their framework.\n2. By leveraging this theorem along with some clever mathematical manipulations involving Euler's equations related to volume-preserving diffeomorphisms spaces; they manage not only optimize how one moves around different distributions efficiently but also ensure there isn't too much distortion during such transformations - hence resulting smoother flow paths than before!\n3. Their experimental results demonstrate significant improvements regarding both speed-up factors achieved through optimizing certain aspects compared against standard implementations available today plus better preservation properties preserved throughout each step taken towards achieving desired outcomes",
        "Topic": "Optimal Transport"
    },
    {
        "title": "Bayesian Optimization with Informative Covariance",
        "abstract": "Bayesian optimization is a methodology for global optimization of unknown and expensive objectives. It combines a surrogate Bayesian regression model with an acquisition function to decide where to evaluate the objective. Typical regression models are given by Gaussian processes with stationary covariance functions. However, these functions are unable to express prior input-dependent information, including possible locations of the optimum. The ubiquity of stationary models has led to the common practice of exploiting prior information via informative mean functions. In this paper, we highlight that these models can perform poorly, especially in high dimensions. We propose novel informative covariance functions for optimization, leveraging nonstationarity to encode preferences for certain regions of the search space and adaptively promote local exploration during optimization. We demonstrate that the proposed functions can increase the sample efficiency of Bayesian optimization in high dimensions, even under weak prior information.",
        "authors": "A. Eduardo, M. U. Gutmann",
        "keywords": [
            "nonstationary covariance functions",
            "Bayesian optimization",
            "sample efficiency"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=JwgVBv18RG",
        "pdf_src": "https://api2.openreview.net/pdf/dcb51aabf7d838a41e9d6713fc24beae27575ce8.pdf",
        "Code_src": "",
        "Introduction": "Background: Bayesian Optimization (BO) is widely used as a method for global optimization when it comes to optimizing costly or time-consuming objectives due to its ability to combine surrogate Bayesian regression models along with acquisition functions which help determine future evaluations.\n\nResearch Problem: Despite being effective on average, BO often struggles particularly well in higher-dimensional spaces because typical regression models like Gaussian Processes (GPs), equipped with stationary covariance functions cannot effectively capture any prior knowledge about potential optimal solutions within those spaces leading to suboptimal performance outcomes.\n\nMethod: To address aforementioned limitations related to stationarity-based covariance functions' incapability towards encoding prior domain knowledge efficiently into GP surrogates; authors introduce new informative covariance functions capable of incorporating non-stationary aspects such as user-defined preference maps over specific areas within their respective domains thereby enabling adaptive promotion toward localized exploratory behavior throughout iterative refinement stages involved during optimization process itself.\n\nMain Contributions: This work introduces innovative informative covariance functions designed specifically tailored around addressing issues associated with traditional stationary covariance functions employed across Gaussian Process frameworks utilized extensively within Bayesian Optimization approaches today. These contributions aim at improving sample efficiency significantly while operating within complex multi-dimensional environments despite having limited initial prior information available upfront before commencing actual optimization procedures themselves.",
        "Topic": "Sample Efficiency in Reinforcement Learning"
    },
    {
        "title": "Partition-Based Active Learning for Graph Neural Networks",
        "abstract": "We study the problem of semi-supervised learning with Graph Neural Networks (GNNs) in an active learning setup. We propose GraphPart, a novel partition-based active learning approach for GNNs. GraphPart first splits the graph into disjoint partitions and then selects representative nodes within each partition to query. The proposed method is motivated by a novel analysis of the classification error under realistic smoothness assumptions over the graph and the node features. Extensive experiments on multiple benchmark datasets demonstrate that the proposed method outperforms existing active learning methods for GNNs under a wide range of annotation budget constraints. In addition, the proposed method does not introduce additional hyperparameters, which is crucial for model training, especially in the active learning setting where a labeled validation set may not be available.",
        "authors": "J. W. Ma, Z. Ma, J. Chai, et.al",
        "keywords": [
            "GraphPartitioning",
            "Active Learning",
            "Semi-Supervised Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=e0xaRylNuT",
        "pdf_src": "https://api2.openreview.net/pdf/eb75de08c77b1c171aa55374940cf0632122c513.pdf",
        "Code_src": "",
        "Introduction": "Background: Semi-supervised learning aims at leveraging both labeled and unlabeled data during the training process without requiring fully annotated samples.\n\nResearch Problem: How can we effectively apply Graph Neural Networks (GNNs) towards semi-supervised learning tasks using active learning?\n\nMethod: We present GraphPart, a partition-based active learning strategy specifically designed for GNNs. First, it divides the input graph into non-overlapping partitions; secondly, it identifies representative nodes from these partitions as candidates for labeling based on their importance or centrality measures derived through our novel analysis considering smoothness properties across the graph structure and feature space.\n\nMain Contributions:\n1. A new active learning framework tailored explicitly for GNNs.\n2. An innovative way analyzing errors accounting for graph topology and node feature smoothness leading to more informed sampling decisions than conventional approaches do when dealing with limited annotations resources efficiently while avoiding unnecessary complexity due to extra hyperparameters introduced typically associated with other algorithms thus improving practicality particularly suited well-suited scenarios involving scarce labeled examples such as those encountered frequently within active learning setups",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "DR-DSGD: A Distributionally Robust Decentralized Learning Algorithm over Graphs",
        "abstract": "In this paper, we propose to solve a regularized distributionally robust learning problem in the decentralized setting, taking into account the data distribution shift. By adding a Kullback-Liebler regularization function to the robust min-max optimization problem, the learning problem can be reduced to a modified robust minimization problem and solved efficiently. Leveraging the newly formulated optimization problem, we propose a robust version of Decentralized Stochastic Gradient Descent (DSGD), coined Distributionally Robust Decentralized Stochastic Gradient Descent (DR-DSGD). Under some mild assumptions and provided that the regularization parameter is larger than one, we theoretically prove that DR-DSGD achieves a convergence rate of  $\\mathcal{O}\\left(1/\\sqrt{KT} + K/T\\right)$, where $K$ is the number of devices and $T$ is the number of iterations. Simulation results show that our proposed algorithm can improve the worst distribution test accuracy by up to $10\\%$. Moreover, DR-DSGD is more communication-efficient than DSGD since it requires fewer communication rounds (up to $20$ times less) to achieve the same worst distribution test accuracy target. Furthermore, the conducted experiments reveal that DR-DSGD results in a fairer performance across devices in terms of test accuracy.\n",
        "authors": "C. B. Issaid, A. Elgabli, M. Bennis",
        "keywords": [
            "distributionally robust learning",
            "decentralized settings",
            "stochastic gradient descent"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=VcXNAr5Rur",
        "pdf_src": "https://api2.openreview.net/pdf/cf2e15654a126a0ad2ed4a88fb468d2c0f347dcb.pdf",
        "Code_src": "",
        "Introduction": "Background: The background of this research lies in distributed machine learning settings with heterogeneous datasets from different sources or domains which may lead to distribution shifts during training.\n\nResearch Problem: The main challenge addressed here concerns how to design an efficient decentralized learning algorithm capable of handling such distributional shifts while ensuring good generalization on unseen data.\n\nMethods: To tackle these challenges, authors introduce a novel approach called Distributionally Robust Decentralized Stochastic Gradient Descent (DR-DSGD). This method incorporates a Kullback-Leibler (KL) divergence regularization term within the robust min-max framework for optimizing over the worst-case distribution among all possible distributions under consideration rather than minimizing empirical risk directly as done traditionally without considering distributional shifts leading to better robustness against distributional changes at test time.\n\nMain Contributions:\n1. They develop a new optimization formulation incorporating KL regularization aiming towards solving a distributionally robust learning problem adapted specifically toward decentralized scenarios involving multiple nodes/devices.\n2. Propose DR-DSGD - a variant of Distributed Stochastic Gradient Descent (DSGD) designed explicitly keeping in mind potential distributional variations between participating nodes.\n3. Prove theoretical guarantees regarding convergence rates showing that DR-DSGD converges faster compared to standard DSGD when dealing with varying input distributions; achieving a complexity of $\\mathcal{O}(1/\\sqrt{KT}+K/T)$ assuming certain conditions are met about the regularization parameters.\n4. Conduct simulations demonstrating improved worst-case test accuracy gains using their proposed DR-DSGD algorithm relative to baseline methods like vanilla DSGD – improvements reaching approximately 10%. \n5. Further experimental evidence shows enhanced fairness amongst device performances measured through test accuracies achieved after applying DR-DSGD suggesting its effectiveness beyond just improving statistical robustness alone but also promoting equitable outcomes across various computational units involved in collaborative tasks.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Optimal Client Sampling for Federated Learning",
        "abstract": "It is well understood that client-master communication can be a primary bottleneck in federated learning (FL). In this work, we address this issue with a novel client subsampling scheme, where we restrict the number of clients allowed to communicate their updates back to the master node. In each communication round, all participating clients compute their updates, but only the ones with \"important\" updates communicate back to the master. We show that importance can be measured using only the norm of the update and give a formula for optimal client participation. This formula minimizes the distance between the full update, where all clients participate, and our limited update, where the number of participating clients is restricted. In addition, we provide a simple algorithm that approximates the optimal formula for client participation, which allows for secure aggregation and stateless clients, and thus does not compromise client privacy. We show both theoretically and empirically that for Distributed SGD (DSGD) and Federated Averaging (FedAvg), the performance of our approach can be close to full participation and superior to the baseline where participating clients are sampled uniformly. Moreover, our approach is orthogonal to and compatible with existing methods for reducing communication overhead, such as local methods and communication compression methods. ",
        "authors": "W. Chen, S. Horváth, P. Richtárik",
        "keywords": [
            "client subsampling",
            "federated learning",
            "important updates"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=8GvRCWKHIL",
        "pdf_src": "https://api2.openreview.net/pdf/dff63adee080c7b660b36aa8a7a84c9fc71911e2.pdf",
        "Code_src": "",
        "Introduction": "Background: Federated Learning (FL) has become an important technique due to its ability to train machine learning models across multiple devices without collecting data centrally on servers or cloud platforms.\n\nResearch Problem: Client-master communication is often identified as one of the main bottlenecks within FL systems because it requires transmitting large amounts of model updates from many clients to a central server during training iterations.\n \nMethod: The paper introduces a new strategy called client subsampling by limiting the number of clients who send their updates directly to the master node after computing them locally at every iteration step. Instead, they propose selecting those whose updates carry more information weight (\"important\" updates).\n\nMain Contributions:\n1. They develop a metric based solely on the L2 norm of the gradient update vector to determine if individual client updates should be considered significant enough to transmit; \n2. Propose an analytical expression for determining how best to sample these 'important' clients so there's minimal difference compared to when no sampling occurs;\n3. Offer a practical approximation method allowing for efficient computation while maintaining security through non-stateful operation ensuring confidentiality isn't compromised;\n4. Demonstrate experimentally against DSGD and FedAvg settings under various conditions showing near-optimal results relative to complete client participation scenarios yet outperforming uniform random sampling baselines;\n5. Their proposed solution operates independently alongside other techniques designed specifically towards decreasing communication costs like locality-sensitive hashing algorithms used together could further enhance efficiency significantly.",
        "Topic": "Federated Learning"
    },
    {
        "title": "High Fidelity Visualization of What Your Self-Supervised Representation Knows About",
        "abstract": "Discovering what is learned by neural networks remains a challenge. In self-supervised learning, classification is the most common task used to evaluate how good a representation is. However, relying only on such downstream task can limit our understanding of what information is retained in the representation of a given input. In this work, we showcase the use of a Representation Conditional Diffusion Model (RCDM) to visualize in data space the representations learned by self-supervised models. The use of RCDM is motivated by its ability to generate high-quality samples ---on par with state-of-the-art generative models--- while ensuring that the representations of those samples are faithful i.e. close to the one used for conditioning.\nBy using RCDM to analyze self-supervised models, we are able to clearly show visually that i) SSL (backbone) representation are not invariant to the data augmentations they were trained with -- thus debunking an often restated but mistaken belief; ii) SSL post-projector embeddings appear indeed invariant to these data augmentation, along with many other data symmetries; iii) SSL representations appear more robust to small adversarial perturbation of their inputs than representations trained in a supervised manner; and iv) that SSL-trained representations exhibit an inherent structure that can be explored thanks to RCDM visualization and enables image manipulation.",
        "authors": "F. Bordes, R. Balestriero, P. Vincent",
        "keywords": [
            "representation",
            "diffusion model",
            "self-supervised learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=urfWb7VjmL",
        "pdf_src": "https://api2.openreview.net/pdf/5a59cf0635ed46cc85e30e6bbed4f140ea986c24.pdf",
        "Code_src": "",
        "Introduction": "Background: Evaluating the quality of representations learned by neural networks through self-supervised learning has been challenging due to limitations associated with solely relying on downstream tasks.\n\nResearch Problem: This paper aims to overcome previous limitations related to evaluating the quality of representations from self-supervised learning methods without fully considering the impact of data augmentations during training or potential biases introduced into the process.\n\nMethodology: To address issues mentioned above, authors introduce a novel approach called Representation Conditional Diffusion Model (RCDM). It generates high-quality samples similar to current state-of-the-art generative models yet maintains fidelity between generated examples' representations when compared against original conditioning ones - which helps preserve essential features within each sample's representation despite any transformations applied throughout pre-processing steps like data augmentation techniques commonly employed under SSL frameworks.\n\nMain Contributions:\n1. Demonstrates variability among SSL backbones’ representations depending upon specific types of data augmentations utilized at training time rather than being invariant as previously believed;\n2. Reveals that SSL post-projector embeddings do remain invariant across various forms of data augmentation alongside several additional symmetries present within datasets themselves;\n3. Shows enhanced resilience towards minor adversarial attacks imposed onto inputs relative to supervised learning counterparts suggesting improved robustness properties stemming directly out of unsupervised approaches;\n4. Highlights intrinsic structures existing amongst SSL-trained representations enabling further exploration via visualizations facilitated by RCDM allowing users manipulate images accordingly based off observed patterns discovered therein",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "SemiNLL: A Framework of Noisy-Label Learning by Semi-Supervised Learning",
        "abstract": "Deep learning with noisy labels is a challenging task, which has received much attention from the machine learning and computer vision communities. Recent prominent methods that build on a specific sample selection (SS) strategy and a specific semi-supervised learning (SSL) model achieved state-of-the-art performance. Intuitively, better performance could be achieved if stronger SS strategies and SSL models are employed. Following this intuition, one might easily derive various effective noisy-label learning methods using different combinations of SS strategies and SSL models, which is, however, simply reinventing the wheel in essence. To prevent this problem, we propose SemiNLL, a versatile framework that investigates how to naturally combine different SS and SSL components based on their effects and efficiencies. We conduct a systematic and detailed analysis of the combinations of possible components based on our framework. Our framework can absorb various SS strategies and SSL backbones, utilizing their power to achieve promising performance. The instantiations of our framework demonstrate substantial improvements over state-of-the-art methods on benchmark-simulated and real-world datasets with noisy labels.\n",
        "authors": "Z. Wang, J. Jiang, B. Han, et.al",
        "keywords": [
            "noise labeling",
            "semi-supervised learning",
            "sample selection"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=qzM1Tw5i7N",
        "pdf_src": "https://api2.openreview.net/pdf/0444ee44007ef983cd67cb61f3aa0fb664eb3a57.pdf",
        "Code_src": "",
        "Introduction": "Background: Deep learning algorithms often require large amounts of labeled data for training; however, obtaining such high-quality annotations at scale poses significant challenges due to costs or annotation errors leading to \"noisy\" labels.\n\nResearch Problem: How do we effectively train deep neural networks when only partially labeled (\"semi-supervised\") and where these labels may contain noise?\n\nMethods: This paper introduces SemiNLL - a flexible framework designed specifically around combining different semi-supervised learning (SSL) techniques along with sample selection (SS) strategies tailored towards mitigating label noise during training.\n\nMain Contributions:\n1. **Versatile Framework**: SemiNLL allows for the integration of multiple SSL and SS approaches without needing to re-invent them individually by leveraging an understanding of each component's efficacy within its broader context – thus enabling more efficient use of existing knowledge rather than redundant research efforts focused solely on improving individual parts independently.\n2. **Systematic Analysis**: It provides comprehensive guidance through empirical studies into optimal combinations among several candidate components according to predefined criteria like effectiveness against noise levels across simulated benchmarks as well as actual datasets containing noisy labels found outside controlled environments \n3. **Performance Improvements**: Demonstrates superior results compared current best practices demonstrated via instantiation tests conducted both on synthetic datasets mimicking realistic scenarios involving noisy labeling issues alongside practical applications encountered daily tasks faced by practitioners working closely together everyday life contexts",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "SFP: State-free Priors for Exploration in Off-Policy Reinforcement Learning",
        "abstract": "Efficient exploration is a crucial challenge in deep reinforcement learning. Several methods, such as behavioral priors, are able to leverage offline data in order to efficiently accelerate reinforcement learning on complex tasks. However, if the task at hand deviates excessively from the demonstrated task, the effectiveness of such methods is limited. In our work, we propose to learn features from offline data that are shared by a more diverse range of tasks, such as correlation between actions and directedness. Therefore, we introduce state-free priors, which directly model temporal consistency in demonstrated trajectories, and are capable of driving exploration in complex tasks, even when trained on data collected on simpler tasks. Furthermore, we introduce a novel integration scheme for action priors in off-policy reinforcement learning by dynamically sampling actions from a probabilistic mixture of policy and action prior. We compare our approach against strong baselines and provide empirical evidence that it can accelerate reinforcement learning in long-horizon continuous control tasks under sparse reward settings.",
        "authors": "M. Bagatella, S. Christen, O. Hilliges",
        "keywords": [
            "state-free priors",
            "exploratory behavior",
            "off-policy reinforcement learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=qYNfwFCX9a",
        "pdf_src": "https://api2.openreview.net/pdf/cf4bc83070237a75d2c0ee45b291f9bc6ab9851b.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses an important issue within deep reinforcement learning - efficient exploration using offline data during training.\n\nResearch Question: How do you design effective exploratory strategies while leveraging offline data?\n\nMethodology: To tackle this problem, they develop \"state-free priors\" based on learned correlations like action-directionality across various tasks rather than specific behaviors or states seen previously (\"behavioral priors\"). They also present a new method integrating these priors into off-policy reinforcement learning through dynamic action sampling with a probabilistic mixture over policies and action priors.\n\nMain Contributions:\n1. State-Free Priors: These priors capture generalizable properties about how agents behave without relying solely on past demonstrations.\n2. Novel Integration Scheme: A flexible framework allowing for the incorporation of both policy and action priors effectively improves exploration performance compared to existing baseline approaches especially where rewards are scarce – common challenges faced particularly in long-term continuous control problems involving sparse feedback signals throughout interactions leading up to goal attainment milestones along trajectories towards desired outcomes.",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Finding and Fixing Spurious Patterns with Explanations",
        "abstract": "Image classifiers often use spurious patterns, such as “relying on the presence of a person to detect a tennis racket,” which do not generalize. In this work, we present an end-to-end pipeline for identifying and mitigating spurious patterns for such models, under the assumption that we have access to pixel-wise object-annotations. We start by identifying patterns such as “the model’s prediction for tennis racket changes 63% of the time if we hide the people.” Then, if a pattern is spurious, we mitigate it via a novel form of data augmentation. We demonstrate that our method identifies a diverse set of spurious patterns and that it mitigates them by producing a model that is both more accurate on a distribution where the spurious pattern is not helpful and more robust to distribution shift.",
        "authors": "G. Plumb, M. T. Ribeiro, A. Talwalkar",
        "keywords": [
            "spurious patterns",
            "mitigation",
            "data augmentation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=whJPugmP5I",
        "pdf_src": "https://api2.openreview.net/pdf/e0550776e6aae43e24eb3044573d53ea128142f7.pdf",
        "Code_src": "",
        "Introduction": "Background: Image classifiers sometimes rely on spurious patterns when making predictions; these are characteristics in images or features used during training but may be unrelated to what's actually being classified.\n\nResearch Question: How can we identify and reduce reliance on spurious patterns within image classification models?\n\nMethodology: The authors propose an end-to-end pipeline designed specifically with the goal of detecting and addressing such issues using pixel-wise annotations provided alongside labeled datasets.\n1. Identification Phase: They first analyze how much certain predictions change based on alterations made around objects—like hiding specific elements like humans from the input image while keeping other relevant details visible—in order to spot patterns likely driven by irrelevant information rather than actual class indicators.\n2. Mitigation Phase: If they find evidence pointing towards potential spuriousness through their identification phase, then apply a new type of data augmentation strategy tailored just enough so only those aspects critical toward learning true discriminative cues remain intact whereas extraneous ones get weakened over iterations leading up to final trained weights.\n\nMain Contributions:\n- A systematic approach combining detection & mitigation strategies into one integrated framework \n- Novel data augmentation technique developed especially targeting reduction against identified spurious dependencies \n- Demonstrated effectiveness across various benchmarks showing improved accuracy without relying heavily upon non-class-relevant factors along with better resilience regarding shifts between different distributions encountered outside training domain",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Causal Feature Selection via Orthogonal Search",
        "abstract": "The problem of inferring the direct causal parents of a response variable among a large set of explanatory variables is of high practical importance in many disciplines. However, established approaches often scale at least exponentially with the number of explanatory variables, are difficult to extend to nonlinear relationships and are difficult to extend to cyclic data. Inspired by debiased machine learning methods, we study a one-vs.-the-rest feature selection approach to discover the direct causal parent of the response. We propose an algorithm that works for purely observational data while also offering theoretical guarantees, including the case of partially nonlinear relationships possibly under the presence of cycles. As it requires only one estimation for each variable, our approach is applicable even to large graphs. We demonstrate significant improvements compared to established approaches.",
        "authors": "A. Soleymani, A. Raj, S. Bauer, et.al",
        "keywords": [
            "causal inference",
            "feature selection",
            "graph analysis"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Q54jBjc896",
        "pdf_src": "https://api2.openreview.net/pdf/7ba91e1bcd21c81c31e3bd0b79200b446042d419.pdf",
        "Code_src": "",
        "Introduction": "Background: The task of inferring the direct causal parents of a response variable from a large set of explanatory variables has important applications across various fields but existing methods have limitations such as exponential scaling issues due to the increasing number of variables, difficulty handling non-linear relationships or cyclic structures.\n\nResearch Problem: To develop a scalable method capable of identifying direct causal parents efficiently when dealing with potentially complex relationships like partial non-linearity within cyclic systems without relying on assumptions about the nature of these relationships.\n \nMethods: Inspired by recent developments in debiased machine learning techniques which aim to mitigate bias through regularization processes during model training; this paper introduces a novel feature selection strategy based on pairwise comparisons between all possible pairs of variables where each pair competes against every other pair individually (\"one-vs-the-rest\"). This allows us to identify those features that contribute most significantly towards predicting outcomes directly rather than indirectly via intermediate steps thus providing insights into causality paths more accurately than traditional regression models do especially useful given their inability handle cyclical dependencies well enough before now.\n\nMain Contributions:\n1. An algorithmic framework called \"One-Vs-The-Rest\" Feature Selection Approach designed specifically tailored toward discovering direct causal influences amongst sets containing numerous potential predictors regardless if they exhibit linear/nonlinear characteristics or exist within circular arrangements;\n2. A theoretically grounded solution ensuring scalability up until very extensive datasets whilst still maintaining performance guarantees despite some complexity introduced by considering interactions beyond simple linear ones;\n3. Demonstrated empirical evidence showing superior predictive accuracy over conventional statistical tools particularly beneficially applied scenarios involving multiple interacting factors contributing jointly towards observed phenomena making them invaluable additions both academia & industry alike",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Zero-Shot Learning with Common Sense Knowledge Graphs",
        "abstract": "Zero-shot learning relies on semantic class representations such as hand-engineered attributes or learned embeddings to predict classes without any labeled examples. We propose to learn class representations by embedding nodes from common sense knowledge graphs in a vector space. Common sense knowledge graphs are an untapped source of explicit high-level knowledge that requires little human effort to apply to a range of tasks. To capture the knowledge in the graph, we introduce ZSL-KG, a general-purpose framework with a novel transformer graph convolutional network (TrGCN) for generating class representations. Our proposed TrGCN architecture computes non-linear combinations of node neighbourhoods. Our results show that ZSL-KG improves over existing WordNet-based methods on five out of six zero-shot benchmark datasets in language and vision.",
        "authors": "N. V. Nayak, S. Bach",
        "keywords": [
            "knowledge graphs",
            "zero-shot learning",
            "graph convolutional networks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=h1zuM6cXpH",
        "pdf_src": "https://api2.openreview.net/pdf/6991016de89948c8eba9b3fe3a520021772cdb24.pdf",
        "Code_src": "",
        "Introduction": "Background: Zero-shot learning aims to classify unseen data points using only prior knowledge about their semantic categories.\nResearch Problem: How can we effectively represent these semantic categories through machine learning models?\nMethods: The authors propose leveraging common sense knowledge graphs embedded into a vector space rather than relying solely on manually engineered features like WordNet synsets which require significant laborious annotation efforts.\n\nMain Contributions:\n1) They introduce ZSL-KG - a general-purpose framework incorporating Transformer Graph Convolutional Networks (TrGCNs).\n2) Their TrGCN architecture captures knowledge within the graph via nonlinear combination operations applied across different neighborhoods around each node.\n3) Experimental validation demonstrates improved performance compared against other state-of-the-art approaches based on WordNet when tested across multiple benchmarks involving both linguistic semantics and visual recognition domains.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "The Fundamental Limits of Neural Networks for Interval Certified Robustness",
        "abstract": "Interval analysis (or interval bound propagation, IBP) is a popular technique for verifying and training provably robust deep neural networks, a fundamental challenge in the area of reliable machine learning. However, despite substantial efforts, progress on addressing this key challenge has stagnated, calling into question whether interval analysis is a viable path forward.\n\nIn this paper we present a fundamental result on the limitation of neural networks for interval analyzable robust classification. Our main theorem shows that non-invertible functions can not be built such that interval analysis is precise everywhere. Given this, we derive a paradox: while every dataset can be robustly classified, there are simple datasets that can not be provably robustly classified with interval analysis.\n",
        "authors": "M. B. Mirman, M. Baader, M. Vechev",
        "keywords": [
            "dataset",
            "interval analysis",
            "provably robust classification"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=fsacLLU35V",
        "pdf_src": "https://api2.openreview.net/pdf/b97ef358dafb12672cf012fb0ae620db5e9849ae.pdf",
        "Code_src": "",
        "Introduction": "Background:\nInterval analysis or interval bound propagation (IBP) is widely used to verify and train provably robust deep neural networks which is an essential problem in reliable machine learning field. Despite many attempts have been made, it seems that no significant breakthroughs were achieved recently about solving these problems, raising doubts if interval analysis could still provide solutions.\n\nResearch Problem:\nThe research aims at exploring the limitations of neural networks when using interval analysis methods for robust classification tasks by proving that certain types of neural network structures cannot support accurate interval analysis under any circumstances due to their inherent non-invertibility properties leading to some datasets being unable to achieve provable robustness via interval analysis approach even though they may appear feasible from practical perspectives.\n\nMethodology:\nTo address above mentioned issues, authors introduce new theoretical results based on mathematical proofs demonstrating why existing interval analysis techniques fail within specific scenarios related specifically around invertibility conditions needed during computation process inside neural networks involved here; further more they also propose alternative approaches towards achieving robust classifications without relying solely upon traditional interval bounds propagated through layers but rather focusing instead onto other mechanisms like regularization terms included alongside loss functions etcetera so as improving overall performance whilst maintaining desired guarantees regarding reliability & safety aspects concerned therein.\n\nMain Contributions:\nThis work presents novel insights concerning current challenges faced w.r.t interval-based verification/training methodologies employed nowadays along with proposing potential alternatives moving away from purely interval-based strategies toward incorporating additional components such regularization terms into optimization routines aimed at enhancing both accuracy levels achievable together ensuring desirable dependability/security assurances pertinent throughout deployment phases associated therewith accordingly thereby paving way forwards future advancements pertaining towards developing trustworthy intelligent systems capable handling uncertainties encountered real-world environments effectively over time ahead onwards.",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "TITRATED: Learned Human Driving Behavior without Infractions via Amortized Inference",
        "abstract": "Models of human driving behavior have long been used for prediction in autonomous vehicles, but recently have also started being used to create non-playable characters for driving simulations. While such models are in many respects realistic, they tend to suffer from unacceptably high rates of driving infractions, such as collisions or off-road driving, particularly when deployed in map locations with road geometries dissimilar to the training dataset. In this paper we present a novel method for fine-tuning a foundation model of human driving behavior to novel locations where human demonstrations are not available which reduces the incidence of such infractions. The method relies on inference in the foundation model to generate infraction-free trajectories as well as additional penalties applied when fine-tuning the amortized inference behavioral model. We demonstrate this \"titration\" technique using the ITRA foundation behavior model trained on the INTERACTION dataset when transferring to CARLA map locations. We demonstrate a 76-86% reduction in infraction rate and provide evidence that further gains are possible with more computation or better inference algorithms.",
        "authors": "V. Lioutas, A. Scibior, F. Wood",
        "keywords": [
            "infraction-free trajectory",
            "fine-tuning",
            "transfer learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=M8D5iZsnrO",
        "pdf_src": "https://api2.openreview.net/pdf/7143aef0f24a122a4aeeaaa74c08c5a6b17fd8f9.pdf",
        "Code_src": "",
        "Introduction": "Background: Models of human driving behavior play an important role in both autonomous vehicle prediction tasks and simulation-based character creation.\n\nResearch Problem: However, these models often exhibit unacceptable levels of driving infractions like collisions under deployment at maps with different road geometries compared to their training datasets.\n \nMethod: This study introduces a new approach called \"titration,\" involving fine-tuning a foundational model of human driving behavior without direct access to human demonstration data by utilizing its inferential capabilities within the model itself along with supplementary fines during the refinement phase leading up to an optimized inferred behavioral representation suitable even if it's never encountered before.\n\nMain Contributions:\n1. A novel titration strategy is proposed; \n2. It significantly decreases instances of traffic violations through adjustments made specifically tailored towards each location;\n3. Demonstrated effectiveness was achieved via application onto CARLA map locations after initial training based on INTERACTION dataset;\n4. Results indicate potential improvements could be realized upon employing greater computational resources or enhanced inferential techniques",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Mean-Field Langevin Dynamics : Exponential Convergence and Annealing",
        "abstract": "Noisy particle gradient descent (NPGD) is an algorithm to minimize convex functions over the space of measures that include an entropy term. In the many-particle limit, this algorithm is described by a  Mean-Field Langevin dynamics---a generalization of the Langevin dynamic with a non-linear drift---which is our main object of study. Previous work have shown its convergence to the unique minimizer via non-quantitative arguments. We prove that this dynamics converges at an exponential rate, under the assumption that a certain family of Log-Sobolev inequalities holds. This assumption holds for instance for the minimization of the risk of certain two-layer neural networks, where NPGD is equivalent to standard noisy gradient descent. We also study the annealed dynamics, and show that for a noise decaying at a logarithmic rate, the dynamics converges in value to the global minimizer of the unregularized objective function.",
        "authors": "L. Chizat",
        "keywords": [
            "Mean-Field Langevin Dynamics",
            "Noisy Particle Gradient Descent",
            "Exponential Convergence"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=BDqzLH1gEm",
        "pdf_src": "https://api2.openreview.net/pdf/7e8b9d665eb8ad81c69d8d72aabcd5ca192d5e2d.pdf",
        "Code_src": "",
        "Introduction": "Background: Noisy particle gradient descent (NPGD) is an algorithm used to minimize convex functions on spaces of measures including an entropy term.\n\nResearch Question: The research question addressed here concerns the behavior and convergence properties of NPGD as it approaches the mean-field regime involving multiple particles or agents interacting through stochastic processes based on Langevin dynamics principles extended into nonlinear settings - specifically focusing on whether such systems converge exponentially fast towards their respective local minima when subjected to specific conditions related to log-Sobolev inequalities which are known constraints ensuring stability within probability distributions.\n\nMethodology: To address these questions rigorously, we employ mathematical analysis techniques like probabilistic bounds methods combined with insights from functional analysis regarding log-Sobolev inequalities along with other tools pertinent to studying dynamical systems governed by stochastic differential equations driven by Gaussian noises.\n\nMain Contributions: Our primary contribution lies in establishing explicit exponential convergence rates for NPGD algorithms operating near mean-field limits characterized by large numbers of interacting particles subjecting them to controlled noise levels satisfying log-Sobolev inequality criteria leading us toward understanding how they approach optimal solutions efficiently compared against previous qualitative guarantees provided previously without quantifying exact convergence speeds adequately enough before now; secondarily contributing insight into annealing strategies applied during optimization procedures suggesting further improvements upon existing methodologies",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "No More Pesky Hyperparameters: Offline Hyperparameter Tuning for RL",
        "abstract": "The performance of reinforcement learning (RL) agents is sensitive to the choice of hyperparameters. In real-world settings like robotics or industrial control systems, however, testing different hyperparameter configurations directly on the environment can be financially prohibitive, dangerous, or time consuming. We focus on hyperparameter tuning from offline logs of data, to fully specify the hyperparameters for an RL agent that learns online in the real world. The approach is conceptually simple: we first learn a model of the environment from the offline data, which we call a calibration model, and then simulate learning in the calibration model to identify promising hyperparameters. Though such a natural idea is (likely) being used in industry, it has yet to be systematically investigated. We identify several criteria to make this strategy effective, and develop an approach that satisfies these criteria. We empirically investigate the method in a variety of settings to identify when it is effective and when it fails.",
        "authors": "H. Wang, A. Sakhadeo, A. White, et.al",
        "keywords": [
            "offline hyperparameter tuning",
            "reinforcement learning",
            "simulation-based optimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=AiOUi3440V",
        "pdf_src": "https://api2.openreview.net/pdf/3adb58291ed5b7b2c50942d28cb91ffefffbe1cb.pdf",
        "Code_src": "",
        "Introduction": "Background: The performance of reinforcement learning (RL) agents heavily relies on carefully chosen hyperparameters; however, manually adjusting them through trial-and-error experiments may not only be impractical but also costly due to potential safety risks.\n\nResearch Problem: How do we effectively tune hyperparameters without having access to direct feedback from the actual environments?\n\nMethod: Our proposed solution involves using offline log data as input into a machine learning model called a \"calibration model\" - trained with the goal of predicting how well certain hyperparameter choices would perform within those same environments once deployed there.\nOnce our calibration model predicts optimal parameters based solely on historical observations rather than live interactions between robots/agents and their surroundings), they are simulated against hypothetical scenarios before being applied during active deployment phases where immediate adjustments cannot always occur safely nor affordably enough times around again quickly enough if needed later down line!\n\nMain Contributions:\n1. We introduce novel methods aimed at improving efficiency while still maintaining high accuracy levels across various domains including autonomous driving cars , humanoid robots etcetera \n2. By leveraging existing datasets instead relying entirely upon new ones generated specifically just for purposes related solely towards hyperparameter optimization itself (which could take years).",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Variational Disentanglement for Domain Generalization",
        "abstract": "Domain generalization aims to learn a domain-invariant model that can generalize well to the unseen target domain. In this paper, based on the assumption that there exists an invariant feature mapping, we propose an evidence upper bound of the divergence between the category-specific feature and its invariant ground-truth using variational inference. To optimize this upper bound, we further propose an efficient Variational Disentanglement Network (VDN) that is capable of disentangling the domain-specific features and category-specific features (which generalize well to the unseen samples). Besides, the generated novel images from VDN are used to further improve the generalization ability.  We conduct extensive experiments to verify our method on three benchmarks, and both quantitative and qualitative results illustrate the effectiveness of our method.",
        "authors": "Y. Wang, H. Li, H. Cheng, et.al",
        "keywords": [
            "domain generalization",
            "invariant feature mapping",
            "Variational Disentanglement Network"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=fudOtITMIZ",
        "pdf_src": "https://api2.openreview.net/pdf/66656bd8c4ad40cf45266d540ca16035c7828d44.pdf",
        "Code_src": "",
        "Introduction": "Background: Domain generalization focuses on training models with the capability to perform effectively in new domains without requiring retraining or fine-tuning.\n\nResearch Problem: The challenge lies in learning representations which capture domain-invariant information while still being discriminative for different categories within each domain.\n \nMethod: This research introduces two key contributions:\n1. An Evidence Upper Bound (EUB) framework derived through variational inference quantifies how much the representation deviates from the desired invariant form by comparing it against the category-specific features' ground truth.\n2. A Variational Disentanglement Network (VDN), designed specifically as an optimization tool aimed at minimizing EUB values across all domains simultaneously; it separates out domain-specific variations allowing them to be adjusted independently during adaptation phases when encountering new data distributions.\n\nMain Contributions: By leveraging these techniques, the proposed approach significantly improves upon existing methods concerning domain generalization performance – evidenced via empirical validation conducted over multiple benchmark datasets where improvements were observed not only quantitatively but also qualitatively demonstrating better adaptability towards previously unseen tasks under varying conditions.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Understanding AdamW through Proximal Methods and Scale-Freeness",
        "abstract": "Adam has been widely adopted for training deep neural networks due to less hyperparameter tuning and remarkable performance. To improve generalization, Adam is typically used in tandem with a squared $\\ell_2$ regularizer (referred to as Adam-$\\ell_2$). However, even better performance can be obtained with AdamW, which decouples the gradient of the regularizer from the update rule of Adam-$\\ell_2$. Yet, we are still lacking a complete explanation of the advantages of AdamW. In this paper, we tackle this question from both an optimization and an empirical point of view. First, we show how to re-interpret AdamW as an approximation of a proximal gradient method, which takes advantage of the closed-form proximal mapping of the regularizer instead of only utilizing its gradient information as in Adam-$\\ell_2$. Next, we consider the property of \"scale-freeness\" enjoyed by AdamW and by its proximal counterpart: their updates are invariant to component-wise rescaling of the gradients. We provide empirical evidence across a wide range of deep learning experiments showing a correlation between the problems in which AdamW exhibits an advantage over Adam-$\\ell_2$ and the degree to which we expect the gradients of the network to exhibit multiple scales, thus motivating the hypothesis that the advantage of AdamW could be due to the scale-free updates.",
        "authors": "Z. Zhuang, M. Liu, A. Cutkosky, et.al",
        "keywords": [
            "AdamW",
            "Proximal Gradient Method",
            "Scale-Freeness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=IKhEPWGdwK",
        "pdf_src": "https://api2.openreview.net/pdf/bd3df6205037f97f4108993e7c51ad2779c7f4db.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses the use of Adam optimizer along with regularization techniques such as squared $\\ell_2$ regularization or Adam-$\\ell_2$, aiming at improving the generalization ability of deep neural networks.\n\nResearch Question:\nDespite the widespread adoption of Adam optimizer because of fewer hyperparameters needed compared to other optimizers like SGD and superior performance on various tasks, there's limited understanding about why using Adam-$\\ell_2$ leads to improved results beyond just the benefits of Adam alone.\n \nMethodology:\nTo address these questions comprehensively regarding the efficacy of AdamW, they approach it through two lenses – theoretical analysis based on optimization principles followed by empirical validation via extensive experimentation involving different datasets and architectures.\n\nMain Contributions:\n1. Theoretical Interpretation - They reinterpret AdamW as being akin to a proximal gradient algorithm while also leveraging the closed-form proximal mapping properties inherent within the squared $\\ell_2$ regularization function rather than solely relying on its gradient information utilized previously under Adam-$\\ell_2$ setup.\n2. Scale-Free Property - They introduce another novel observation related to AdamW; unlike standard Adam, where updates depend on the magnitude of individual gradient components leading potentially to issues when dealing with heterogeneous scales present in complex neural networks' gradients during backpropagation, AdamW maintains scale-invariance meaning its updates remain consistent regardless if gradients have varying magnitudes across components—a desirable trait particularly useful especially considering multi-scale nature often found in real-world data distributions—suggesting potential reasons behind observed improvements seen empirically against Adam-$\\ell_2$.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "On Robustness to Missing Video for Audiovisual Speech Recognition",
        "abstract": "It has been shown that learning audiovisual features can lead to improved speech recognition performance over audio-only features, especially for noisy speech. However, in many common applications, the visual features are partially or entirely missing, e.g. the speaker might move off screen. Multi-modal models need to be robust: missing video frames should not degrade the performance of an audiovisual model to be worse than that of a single-modality audio-only model. While there have been many attempts at building robust models, there is little consensus on how robustness should be evaluated. To address this, we introduce a framework that allows claims about robustness to be evaluated in a precise and testable way. We also conduct a systematic empirical study of the robustness of common audiovisual speech recognition architectures on a range of acoustic noise conditions and test suites. Finally, we show that an architecture-agnostic solution based on cascades can consistently achieve robustness to missing video, even in settings where existing techniques for robustness like dropout fall short.",
        "authors": "O. Chang, O. Braga, H. Liao, et.al",
        "keywords": [
            "audiovisual feature learning",
            "multi-modal robustness",
            "cascade architecture"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=fXorxxbDvO",
        "pdf_src": "https://api2.openreview.net/pdf/551ed6e314684230f1644d042cc1fb405eee924e.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses recent advancements showing improvements in speech recognition when incorporating both audio and visual information compared to solely using audio data.\n\nResearch Question: How do multi-modal models perform under scenarios with partial or complete absence of visual input?\n\nMethodology: The authors propose a new evaluation framework designed specifically for assessing the robustness of multi-modal systems against missing visual inputs by comparing their performance degradation relative to mono-modal audio-only systems across various noise levels and datasets.\n\nMain Contributions:\n1. They develop a standardized method for evaluating the robustness of multi-modal speech recognition.\n2. Conduct a comprehensive empirical analysis examining different architectural approaches' resilience towards missing visual cues within challenging noise environments commonly encountered during real-world use cases such as conversations happening outdoors amidst traffic sounds etc.\n3. Introduce an architecture-independent approach utilizing cascaded components which they demonstrate maintains high accuracy despite significant loss of visual data - outperforming traditional methods relying heavily on regularization strategies including dropout layers whose efficacy may decrease significantly depending upon specific application contexts.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Identifying Causal Structure in Dynamical Systems",
        "abstract": "Mathematical models are fundamental building blocks in the design of dynamical control systems. As control systems are becoming increasingly complex and networked, approaches for obtaining such models based on first principles reach their limits. Data-driven methods provide an alternative. However, without structural knowledge, these methods are prone to finding spurious correlations in the training data, which can hamper generalization capabilities of the obtained models. This can significantly lower control and prediction performance when the system is exposed to unknown situations. A preceding causal identification can prevent this pitfall. In this paper, we propose a method that identifies the causal structure of control systems. We design experiments based on the concept of controllability, which provides a systematic way to compute input trajectories that steer the system to specific regions in its state space. We then analyze the resulting data leveraging powerful techniques from causal inference and extend them to control systems. Further, we derive conditions that guarantee the discovery of the true causal structure of the system. Experiments on a robot arm demonstrate reliable causal identification from real-world data and enhanced generalization capabilities.",
        "authors": "D. Baumann, F. Solowjow, K. H. Johansson, et.al",
        "keywords": [
            "causal structure",
            "controllability",
            "generalization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=X2BodlyLvT",
        "pdf_src": "https://api2.openreview.net/pdf/600b3c3161e82b6699d57248d75fdb372a8c3b5e.pdf",
        "Code_src": "",
        "Introduction": "Background: Mathematical models play a crucial role in designing dynamic control systems; however, traditional model-based approaches have limitations due to increasing complexity and networking.\n\nResearch Problem: The challenge lies in developing effective data-driven modeling methodologies while avoiding overfitting or finding spurious correlations during training phase leading to poor generalization capability under new scenarios where no prior information exists about those correlations.\n\nMethodology: To address above problem, authors introduce novel approach focusing on identifying causal structures within given dataset by utilizing controllability theory concepts along with advanced tools from causal inference field like Granger causality tests etc., further extended specifically tailored towards dynamics controlled environments.\n\nMain Contributions:\n1) Propose methodology combining controllability principle into experimental framework allowing computation inputs capable steering desired behavior observed across different statespace regions;\n2) Develop analytical procedures incorporating insights gleaned through causal analysis aiding verification discovered relationships indeed reflect underlying mechanisms governing dynamics being studied;\n3) Conduct empirical validation using robotic arm demonstrating reliability achieved via proposed technique applied practical context alongside improved predictive accuracy beyond conventional statistical learning algorithms employed previously",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Diagnosing and Fixing Manifold Overfitting in Deep Generative Models",
        "abstract": "Likelihood-based, or explicit, deep generative models use neural networks to construct flexible high-dimensional densities. This formulation directly contradicts the manifold hypothesis, which states that observed data lies on a low-dimensional manifold embedded in high-dimensional ambient space. In this paper we investigate the pathologies of maximum-likelihood training in the presence of this dimensionality mismatch. We formally prove that degenerate optima are achieved wherein the manifold itself is learned but not the distribution on it, a phenomenon we call manifold overfitting. We propose a class of two-step procedures consisting of a dimensionality reduction step followed by maximum-likelihood density estimation, and prove that they recover the data-generating distribution in the nonparametric regime, thus avoiding manifold overfitting. We also show that these procedures enable density estimation on the manifolds learned by implicit models, such as generative adversarial networks, hence addressing a major shortcoming of these models. Several recently proposed methods are instances of our two-step procedures; we thus unify, extend, and theoretically justify a large class of models.",
        "authors": "G. Loaiza-ganem, B. L. Ross, J. C. Cresswell, et.al",
        "keywords": [
            "dimensionality mismatch",
            "manifold overfitting",
            "two-step procedures"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=0nEZCVshxS",
        "pdf_src": "https://api2.openreview.net/pdf/5ed45453a15a4b648c40d9c0b6d1e64942229963.pdf",
        "Code_src": "",
        "Introduction": "Background: The background of this research topic revolves around likelihood-based, or explicit, deep generative models using neural networks for constructing flexible high-dimensional densities.\n\nResearch Problem: However, there exists an inherent contradiction between this approach and the manifold hypothesis stating that observed data lies on a low-dimensional manifold embedded in high-dimensional ambient space leading to issues with maximum-likelihood training due to dimensionality mismatch.\n\nMethods: To address this problem, researchers have investigated various approaches including those involving dimensionality reduction steps before applying maximum-likelihood density estimation techniques aiming at recovering true underlying distributions while mitigating manifold overfitting phenomena where only manifolds themselves get learned without considering their distributions accurately enough.\n\nMain Contributions: \n1) Proving existence of degenerate optima during maximum likelihood training when faced with dimensionality mismatches resulting into manifold overfitting.\n2) Proposing novel classes of two-step procedures incorporating dimensionality reduction stage prior to estimating densities via Maximum Likelihood Estimation (MLE).\n3) Demonstrating effectiveness & efficacy under Non-parametric regimes ensuring recovery from actual generating distributions rather than just learning manifolds alone.\n4) Extending applicability beyond traditional likelihood based models like GANs enabling them perform better through improved density estimations within learned manifolds thereby solving one key limitation present currently among existing works related specifically towards implicit generative models mentioned earlier",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Do ReLU Networks Have An Edge When Approximating Compactly-Supported Functions?",
        "abstract": "We study the problem of approximating compactly-supported integrable functions while implementing their support set using feedforward neural networks.  Our first main result transcribes this ``structured'' approximation problem into a universality problem.  We do this by constructing a refinement of the usual topology on the space $L^1_{\\operatorname{loc}}(\\mathbb{R}^d,\\mathbb{R}^D)$ of locally-integrable functions in which compactly-supported functions can only be approximated in $L^1$-norm by functions with matching discretized support.  We establish the universality of ReLU feedforward networks with bilinear pooling layers in this refined topology.  Consequentially, we find that ReLU feedforward networks with bilinear pooling can approximate compactly supported functions while implementing their discretized support.  We derive a quantitative uniform version of our universal approximation theorem on the dense subclass of compactly-supported Lipschitz functions.  This quantitative result expresses the depth, width, and the number of bilinear pooling layers required to construct this ReLU network via the target function's regularity, the metric capacity and diameter of its essential support, and the dimensions of the inputs and output spaces.  Conversely, we show that polynomial regressors and analytic feedforward networks are not universal in this space.",
        "authors": "A. Kratsios, B. Zamanlooy",
        "keywords": [
            "approximation",
            "compactly-supported functions",
            "ReLU feedforward networks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=sNxNi54B8b",
        "pdf_src": "https://api2.openreview.net/pdf/1faadf13ab459359d3fc31a18aa292806a7bdabf.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the challenge of accurately approximating integrable functions within a specified support area through the use of feedforward neural networks.\n\nResearch Problem: How feasible is it for such networks - particularly those utilizing ReLU activation functions combined with bilinear pooling - to provide an accurate approximation?\n\nMethodology: The authors tackle this issue from two angles:\n1. They introduce a new topology over the space of locally integrable functions where compactly supported functions cannot be approximated well unless they have similar discrete supports.\n2. By leveraging this new topology, they prove the universality of ReLU feedforward networks equipped with bilinear pooling under certain conditions regarding input-output dimensionality constraints related to the regularity properties like Lipschitz constants along with metric capacities and diameters associated with these functions' essential supports.\n\nMain Contributions: \n- The paper establishes that ReLU feedforward networks with bilinear pooling indeed possess the capability to universally approximate compactly supported integrable functions when considering the novel topology introduced above; \n- A quantitative theorem is provided indicating how much computational resources – measured as network depth, width parameters or numbers of bilinear pooling layers needed -- depend upon specific characteristics of the target function being approximated;\n- Polynomial regressors and other types of feedforward networks without specialized pooling mechanisms were shown insufficient compared against ReLU networks tailored specifically according to these considerations.",
        "Topic": "approximation"
    },
    {
        "title": "Recurrent networks, hidden states and beliefs in partially observable environments",
        "abstract": "Reinforcement learning aims to learn optimal policies from interaction with environments whose dynamics are unknown. Many methods rely on the approximation of a value function to derive near-optimal policies. In partially observable environments, these functions depend on the complete sequence of observations and past actions, called the history. In this work, we show empirically that recurrent neural networks trained to approximate such value functions internally filter the posterior probability distribution of the current state given the history, called the belief. More precisely, we show that, as a recurrent neural network learns the Q-function, its hidden states become more and more correlated with the beliefs of state variables that are relevant to optimal control. This correlation is measured through their mutual information. In addition, we show that the expected return of an agent increases with the ability of its recurrent architecture to reach a high mutual information between its hidden states and the beliefs. Finally, we show that the mutual information between the hidden states and the beliefs of variables that are irrelevant for optimal control decreases through the learning process. In summary, this work shows that in its hidden states, a recurrent neural network approximating the Q-function of a partially observable environment reproduces a sufficient statistic from the history that is correlated to the relevant part of the belief for taking optimal actions.",
        "authors": "G. Lambrechts, A. Bolland, D. Ernst",
        "keywords": [
            "belief",
            "recurrent neural network",
            "mutual information"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=dkHfV3wB2l",
        "pdf_src": "https://api2.openreview.net/pdf/a3c0967216ceaee2c8d1eb2926b79ca7a1673f34.pdf",
        "Code_src": "",
        "Introduction": "Background: Reinforcement learning seeks to find optimal policies by interacting with dynamic environments without prior knowledge about them. Most existing approaches use the approximation of a value function derived based on interactions within the environment.\n\nResearch Problem: The problem addressed here concerns how recurrent neural networks can be used effectively when dealing with partially observable Markov decision processes where only partial or noisy observations may lead to uncertainty regarding the true system's state at any time step due to missing sensory data related to other parts of the observed world which cannot directly perceive those aspects themselves.\n\nMethodology: We propose empirical evidence showing that during training towards approximating certain types of value functions associated with reinforcement learning tasks involving stochastic processes under uncertainty conditions - specifically those found in robotics applications -, Recurrent Neural Networks (RNNs) naturally develop internal representations capable of filtering out noise while focusing attention solely upon what matters most – namely predictive distributions over latent states believed necessary for making decisions leading toward maximizing cumulative reward outcomes across episodes rather than just immediate rewards alone.\n\nMain Contributions:\n1) Demonstrating experimentally using various datasets including robotic manipulation problems & grid-world navigation scenarios demonstrating RNNs' capacity not merely memorize but also abstract away unnecessary details whilst retaining crucial ones pertinent to policy optimization.\n2) Providing quantitative metrics like Mutual Information (MI) quantifying correlations between learned representations within hidden layers against corresponding beliefs inferred via Bayesian reasoning techniques applied onto hypothetical sequences generated according to model predictions; thus highlighting relevance amongst different components involved throughout each episode trajectory being considered \n3) Showing improvements in terms of agents’ long-term performance correlate positively with increasing MI values among hidden states relative to beliefs concerning critical yet non-observable factors impacting desired behaviors/actions taken accordingly",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "A Comprehensive Study of Real-Time Object Detection Networks Across Multiple Domains: A Survey",
        "abstract": "Deep neural network based object detectors are continuously evolving and are used in a multitude of applications, each having its own set of requirements. While safety-critical applications need high accuracy and reliability, low-latency tasks need resource and energy-efficient networks. Real-time detection networks, which are a necessity in high-impact real-world applications, are continuously proposed but they overemphasize the improvements in accuracy and speed while other capabilities such as versatility, robustness, resource, and energy efficiency are omitted. A reference benchmark for existing networks does not exist nor does a standard evaluation guideline for designing new networks, which results in ambiguous and inconsistent comparisons. We, therefore, conduct a comprehensive study on multiple real-time detection networks (anchor-based, keypoint-based, and transformer-based) on a wide range of datasets and report results on an extensive set of metrics. We also study the impact of variables such as image size, anchor dimensions, confidence thresholds, and architecture layers on the overall performance. We analyze the robustness of detection networks against distribution shift, natural corruptions, and adversarial attacks. Also, we provide the calibration analysis to gauge the reliability of the predictions. Finally, to highlight the real-world impact, we conduct two unique case studies, on autonomous driving and healthcare application. To further gauge the capability of networks in critical real-time applications, we report the performance after deploying the detection networks on edge devices. Our extensive empirical study can act as a guideline for the industrial community to make an informed choice on the existing networks. We also hope to inspire the research community towards a new direction of design and evaluation of networks that focuses on the bigger and holistic overview for a far-reaching impact.",
        "authors": "E. Arani, S. Gowda, R. Mukherjee, et.al",
        "keywords": [
            "real-time detection networks",
            "performance benchmarks",
            "multi-modal AI"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ywr5sWqQt4",
        "pdf_src": "https://api2.openreview.net/pdf/50dbb1eeb9855bd4209fad2c08adef2ff033c7a8.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses how deep neural network-based object detectors have evolved significantly across various applications with different demands.\n\nResearch Problem: There is no unified benchmark or standard guidelines available currently when comparing these object detectors due to their varying emphasis on factors like accuracy, latency, versatility, robustness, resources, and energy efficiency.\n \nMethodology: The authors conducted a thorough comparative study involving three types of real-time detection networks - anchor-based, keypoint-based, and transformer-based – using diverse datasets covering several aspects including accuracy, latency, etc. They analyzed the influence of parameters ranging from image sizes & anchor dimensions to confidence thresholds and architectural layers upon system performance comprehensively. Additionally, they evaluated the robustness under distribution shifts along with common corruptions and adversarial attacks faced by these systems before providing calibration analyses assessing prediction reliability. Furthermore, practical relevance was demonstrated through two distinct use-case scenarios focusing on autonomous vehicles and medical diagnostics alongside measuring effectiveness post-deployment onto edge computing platforms.\n\nMain Contributions:\n1. An all-encompassing empirical comparison framework serves industry professionals better equipped at selecting suitable detection networks according to specific needs;\n2. Insights into trade-offs between competing objectives within detector architectures inform future designs prioritizing broader perspectives beyond just raw speeds and accuracies; \n3. Case studies demonstrate tangible impacts applicable outside academia aiding practitioners understand potential benefits realized via deployment strategies tailored specifically toward end-users’ operational contexts",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Domain-invariant Feature Exploration for Domain Generalization",
        "abstract": "Deep learning has achieved great success in the past few years. However, the performance of deep learning is likely to impede in face of non-IID situations. Domain generalization (DG) enables a model to generalize to an unseen test distribution, i.e., to learn domain-invariant representations. In this paper, we argue that domain-invariant features should be originating from both internal and mutual sides. Internal invariance means that the features can be learned with a single domain and the features capture intrinsic semantics of data, i.e., the property within a domain, which is agnostic to other domains. Mutual invariance means that the features can be learned with multiple domains (cross-domain) and the features contain common information, i.e., the transferable features w.r.t. other domains. We then propose DIFEX for Domain-Invariant Feature EXploration. DIFEX employs a knowledge distillation framework to capture the high-level Fourier phase as the internally-invariant features and learn cross-domain correlation alignment as the mutually-invariant features. We further design an exploration loss to increase the feature diversity for better generalization. Extensive experiments on both time-series and visual benchmarks demonstrate that the proposed DIFEX achieves state-of-the-art performance.",
        "authors": "W. Lu, J. Wang, H. Li, et.al",
        "keywords": [
            "domain-invariance",
            "knowledge distillation",
            "feature exploration"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=0xENE7HiYm",
        "pdf_src": "https://api2.openreview.net/pdf/754bc3177770535ad44c48aeb1a9568a4f6a3342.pdf",
        "Code_src": "",
        "Introduction": "Background: The background of this research lies in the limitations of deep learning models when faced with non-identically independently distributed (non-IID) datasets or scenarios where there are variations across different domains.\n\nResearch Problem: The primary problem addressed by this work revolves around improving the robustness and adaptability of deep learning models through domain generalization techniques so they perform well even if trained only on one dataset but tested against another very different dataset (\"domain shift\").\n\nMethodology: To tackle this issue, researchers introduce \"DIFEX,\" short for Domain-Invariant Feature Exploration. This method leverages two types of invariances:\n\n1. Internal Invariance - Features here refer to those captured during training using just one specific domain's data without reference to any external context.\n2. Mutual Invariance - These involve shared characteristics among various domains' data points; these would enable the model to recognize patterns regardless of whether it was originally exposed to them while being trained.\n\nThe approach taken involves employing a knowledge distillation framework adapted specifically towards capturing the high-level Fourier phases associated with internal invariance along with aligning correlations between different domains related to mutual invariance aspects via a novel exploration loss function designed explicitly aimed at increasing feature diversity thereby enhancing generalizability beyond individual domains.\n\nMain Contributions:\n- Proposing DIFEX—a new architecture for exploring domain-invariant features—combining internal and mutual invariance principles into its framework;\n- Developing methods such as knowledge distillation combined with Fourier phase extraction focusing on internal invariance representation;\n- Introducing an exploratory mechanism based on correlation alignment aiming toward mutual invariance understanding amongst diverse datasets;\n- Demonstrating superior performance compared existing works over extensive empirical evaluations conducted under challenging conditions including time series analysis tasks alongside visual recognition benchmarks indicating significant improvements regarding adaptation capabilities amidst varying environments encountered outside their original training scope.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Self-supervise, Refine, Repeat: Improving Unsupervised Anomaly Detection",
        "abstract": "Anomaly detection (AD), separating anomalies from normal data, has many applications across domains, from security to healthcare. While most previous works were shown to be effective for cases with fully or partially labeled data, that setting is in practice less common due to labeling being particularly tedious for this task. In this paper, we focus on fully unsupervised AD, in which the entire training dataset, containing both normal and anomalous samples, is unlabeled. To tackle this problem effectively, we propose to improve the robustness of one-class classification trained on self-supervised representations using a data refinement process. Our proposed data refinement approach is based on an ensemble of one-class classifiers (OCCs), each of which is trained on a disjoint subset of training data. Representations learned by self-supervised learning on the refined data are iteratively updated as the data refinement improves. We demonstrate our method on various unsupervised AD tasks with image and tabular data. With a 10% anomaly ratio on CIFAR-10 image data / 2.5% anomaly ratio on Thyroid tabular data, the proposed method outperforms the state-of-the-art one-class classifier by 6.3 AUC and 12.5 average precision / 22.9 F1-score.",
        "authors": "J. Yoon, K. Sohn, C. Li, et.al",
        "keywords": [
            "unsupervised anomaly detection",
            "data refinement",
            "one-class classification"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=b3v1UrtF6G",
        "pdf_src": "https://api2.openreview.net/pdf/1e6c7b3ac356b00ba2ed0ad3faa6ec4d8ab1bc2d.pdf",
        "Code_src": "",
        "Introduction": "Background: Anomaly detection (AD) plays crucial roles in different fields such as security systems against intrusion attacks , fraud detection [4, 7], medical diagnosis , etc. However, existing methods mainly rely on manually labeled datasets where only some examples have labels while others do not.\n\nResearch Problem: This study aims at addressing the challenge of fully unsupervised anomaly detection - when there's no labeled information available neither for normal nor abnormal instances within the dataset.\n \nMethodology: The authors introduce a novel framework called Data Refinement Process(DRP). DRP involves training multiple One-Class Classifiers(OCCs) independently over subsets of the dataset without any label information. These OCCs then provide feedback about their uncertainty towards the unclassified instances. By leveraging these uncertainties, they refine the dataset iteratively until convergence. \n\nMain Contributions:\n1. They present a new algorithmic approach for unsupervised anomaly detection through iterative refinement of the dataset guided by multiple independent OCCs.\n2. Their experimental results show significant improvements compared to other leading unsupervised anomaly detection algorithms like Isolation Forest(IF) and Local Outlier Factor(LOF).\n3. Specifically, it achieves better performance metrics than its competitors even under high anomaly ratios (e.g., 10% on CIFAR-10 images and 2.5% on Thyroid dataset).\n\nIn summary, despite facing substantial challenges posed by lack of annotated data points during training phase, this research introduces an innovative solution enhancing the capability",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Stable and Interpretable Unrolled Dictionary Learning",
        "abstract": "The dictionary learning problem, representing data as a combination of a few atoms, has long stood as a popular method for learning representations in statistics and signal processing. The most popular dictionary learning algorithm alternates between sparse coding and dictionary update steps, and a rich literature has studied its theoretical convergence. The success of dictionary learning relies on access to a good initial estimate of the dictionary and the ability of the sparse coding step to provide an unbiased estimate of the code. The growing popularity of unrolled sparse coding networks has led to the empirical finding that backpropagation through such networks performs dictionary learning. We offer the theoretical analysis of these empirical results through PUDLE, a Provable Unrolled Dictionary LEarning method. We provide conditions on the network initialization and data distribution sufficient to recover and preserve the support of the latent code. Additionally, we address two challenges; first, the vanilla unrolled sparse coding computes a biased code estimate, and second, gradients during backpropagated learning can become unstable. We show approaches to reduce the bias of the code estimate in the forward pass, and that of the dictionary estimate in the backward pass. We propose strategies to resolve the learning instability by tuning network parameters and modifying the loss function. Overall, we highlight the impact of loss, unrolling, and backpropagation on convergence. We complement our findings through synthetic and image denoising experiments. Finally, we demonstrate PUDLE's interpretability, a driving factor in designing deep networks based on iterative optimizations, by building a mathematical relation between network weights, its output, and the training set.",
        "authors": "B. Tolooshams, D. E. Ba",
        "keywords": [
            "dictionary learning",
            "sparse coding",
            "backpropagation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=e3S0Bl2RO8",
        "pdf_src": "https://api2.openreview.net/pdf/5a2430d3bc1cf4ad797f158f41e022e7abd87cfd.pdf",
        "Code_src": "",
        "Introduction": "Background: Dictionary learning is widely used in statistical and signal processing fields due to its representation of data using a small number of atoms. However, it depends heavily on accurate initial estimates.\n\nResearch Problem: How do dictionaries learn effectively when starting with poor initial estimates?\n\nMethod: This paper introduces PUDLE, which stands for Provable Unrolled Dictionary Learning. It provides necessary conditions under which the learned dictionary preserves the original support structure while also addressing issues like biased code estimation from vanilla unrolled sparse coding algorithms or gradient instability caused by backpropagation.\n\nMain Contributions:\n1. They analyze existing empirical observations about dictionary learning via neural networks.\n2. Offered a theoretically grounded framework called PUDLE ensuring recovery/preservation properties despite suboptimal initializations.\n3. Solved biases present at different stages - forward pass code estimation & backward pass dictionary estimation – reducing them significantly without compromising performance metrics further down the line post-training.\n4. Demonstrated how various factors contribute towards successful convergence including choice of loss functions along with considerations around network architectures themselves (like layer-wise adjustments).\n5. Conducted both synthetic experiments validating their theory empirically across multiple datasets before applying this approach successfully solving real-world problems involving image denoising tasks where they showcased improved robustness over traditional methods compared against those trained solely w.r.t .",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Exploring Generative Neural Temporal Point Process",
        "abstract": "Temporal point process (TPP) is commonly used to model the asynchronous event sequence featuring occurrence timestamps and revealed by probabilistic models conditioned on historical impacts.\nWhile lots of previous works have focused on `goodness-of-fit' of TPP models by maximizing the likelihood, their predictive performance is unsatisfactory, which means the timestamps generated by models are far apart from true observations.\nRecently, deep generative models such as denoising diffusion and score matching models have achieved great progress in image generating tasks by demonstrating their capability of generating samples of high quality.\nHowever, there are no detailed and unified works exploring and studying the potential of generative models in the context of event prediction of TPP.\nIn this work, we try to fill the gap by designing a unified generative framework for neural temporal point process (GNTPP) model to explore their feasibility and effectiveness, and further improve models' predictive performance.  \nBesides, in terms of measuring the historical impacts, we revise the attentive models which summarize influence from historical events with an adaptive reweighting term considering events' type relation and time intervals. \nExtensive experiments have been conducted to illustrate the improved predictive capability of GNTPP with a line of generative probabilistic decoders, and performance gain from the revised attention.  \nTo the best of our knowledge, this is the first work that adapts generative models in a complete unified framework and studies their effectiveness in the context of TPP.\n",
        "authors": "H. Lin, L. Wu, G. Zhao, et.al",
        "keywords": [
            "Generative Framework",
            "Neural Temporal Point Process",
            "Attention Mechanism"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=NPfS5N3jbL",
        "pdf_src": "https://api2.openreview.net/pdf/a0e2eb62bc78a392f2d948e6e819cd0f027ac8ba.pdf",
        "Code_src": "",
        "Introduction": "Background: Temporal Point Process (TPP) has become prevalent due to its ability to represent asynchronous sequences based on timestamps using probabilistic models influenced by past occurrences.\n\nResearch Problem: Despite significant advancements focusing on fitting TPP models through maximization of likelihood - leading to satisfactory goodness-of-fit – these methods often fail when it comes to predicting future timestamps accurately; thus, they do not reflect real-world observation closely enough.\n\nMethods: The paper introduces a novel Generative Neural Temporal Point Process (GNTPP), aiming at addressing the aforementioned issue within a comprehensive generative framework designed specifically around TPPs’ characteristics involving both generation capabilities along with learning how to predict points over time effectively while accounting for the impact of preceding ones.\n\nMain Contributions:\n1. A new generative framework called GNTPP tailored explicitly towards TPP predictions where generative models like Denoising Diffusion or Score Matching can be integrated into TPP architectures enhancing their predictive abilities beyond mere fit metrics.\n2. An innovative Attention mechanism refined via Adaptive Reweighting Terms allowing for more nuanced consideration regarding types of events relative importance across different time intervals influencing subsequent timestamps during prediction processes thereby improving overall accuracy significantly compared traditional approaches solely relying on likelihood optimization alone without incorporating any form adaptation considerations related directly observed data patterns involved here).",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Improving the Trainability of Deep Neural Networks through Layerwise Batch-Entropy Regularization",
        "abstract": "Training deep neural networks is a very demanding task, especially challenging is how to adapt architectures to improve the performance of trained models. We can find that sometimes, shallow networks generalize better than deep networks, and the addition of more layers results in higher training and test errors. The deep residual learning framework addresses this degradation problem by adding skip connections to several neural network layers. It would at first seem counter-intuitive that such skip connections are needed to train deep networks successfully as the expressivity of a network would grow exponentially with depth. In this paper, we first analyze the flow of information through neural networks. We introduce and evaluate the batch-entropy which quantifies the flow of information through each layer of a neural network. We prove empirically and theoretically that a positive batch-entropy is required for gradient descent-based training approaches to optimize a given loss function successfully. Based on those insights, we introduce batch-entropy regularization to enable gradient descent-based training algorithms to optimize the flow of information through each hidden layer individually. With batch-entropy regularization, gradient descent optimizers can transform untrainable networks into trainable networks. We show empirically that we can therefore train a \"vanilla\" fully connected network and convolutional neural network---no skip connections, batch normalization, dropout, or any other architectural tweak---with 500 layers by simply adding the batch-entropy regularization term to the loss function. The effect of batch-entropy regularization is not only evaluated on vanilla neural networks, but also on residual networks, autoencoders, and also transformer models over a wide range of computer vision as well as natural language processing tasks.",
        "authors": "D. Peer, B. Keulen, S. Stabinger, et.al",
        "keywords": [
            "batch-entropy",
            "gradient descent",
            "regularization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LJohl5DnZf",
        "pdf_src": "https://api2.openreview.net/pdf/1c3be2178f195bc2768c2d2daa996fe4bb7355f7.pdf",
        "Code_src": "",
        "Introduction": "Background: Training deep neural networks requires significant computational resources due to their complexity; however, they may suffer from degradation problems leading to lower generalization compared to shallower networks.\n\nResearch Problem: How do we effectively utilize deeper architectures without sacrificing model performance?\n\nMethodology: This research introduces batch-entropy, an objective measure indicating the flow of information within a neural network's layers during training using gradient descent methods.\nThe authors propose batch-entropy regularization—a method that encourages the optimization process towards increasing entropy across all layers—thereby improving the flow of information throughout the network architecture independently per layer rather than globally via skip connections like in ResNet.\n\nMain Contributions:\n1. They provide empirical evidence showing that a positive batch-entropy is necessary under gradient descent settings when optimizing a loss function efficiently;\n2. Introduce batch-entropy regularization technique enabling gradient descent optimizers to adjust individual flows between hidden layers making previously non-trainable networks trainable even up to 500 layers without additional architectural modifications such as skip connections commonly used in ResNets;\n3. Validate these findings experimentally against various types of neural networks including residual networks, autoencoders & transformers applied broadly ranging from Computer Vision tasks to Natural Language Processing ones demonstrating improved performances",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Forces are not Enough: Benchmark and Critical Evaluation for Machine Learning Force Fields with Molecular Simulations",
        "abstract": "Molecular dynamics (MD) simulation techniques are widely used for various natural science applications. Increasingly, machine learning (ML) force field (FF) models begin to replace ab-initio simulations by predicting forces directly from atomic structures. Despite significant progress in this area, such techniques are primarily benchmarked by their force/energy prediction errors, even though the practical use case would be to produce realistic MD trajectories. We aim to fill this gap by introducing a novel benchmark suite for ML MD simulation. We curate representative MD systems, including water, organic molecules, peptide, and materials, and design evaluation metrics corresponding to the scientific objectives of respective systems. We benchmark a collection of state-of-the-art (SOTA) ML FF models and illustrate, in particular, how the commonly benchmarked force accuracy is not well aligned with relevant simulation metrics. We demonstrate when and how selected SOTA methods fail, along with offering directions for further improvement. Specifically, we identify stability as a key metric for ML models to improve. Our benchmark suite comes with a comprehensive open-source codebase for training and simulation with ML FFs to facilitate future work.",
        "authors": "X. Fu, Z. Wu, W. Wang, et.al",
        "keywords": [
            "ML MD simulation",
            "Force Field model",
            "Benchmark suite"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=A8pqQipwkt",
        "pdf_src": "https://api2.openreview.net/pdf/51a2290cf774e9ce45197a0ecda54328f1a6dcb1.pdf",
        "Code_src": "",
        "Introduction": "Background: Molecular Dynamics (MD) simulation plays an essential role in many fields like chemistry or physics due to its ability to predict molecular behavior over time at the atomistic level. However, traditional MD simulations require extensive computational resources which have led researchers to explore Machine Learning (ML)-based Force Fields (FF). These FF models can potentially reduce computation costs while maintaining predictive power.\n\nResearch Question: The primary question addressed here concerns whether current ML-based FF benchmarks accurately reflect real-world application scenarios where it's crucial that the model produces plausible trajectories rather than just accurate forces and energies predictions alone.\n\nMethodology: To address these questions, they developed a new benchmarking framework focusing on trajectory generation quality using curated datasets representing different types of molecular systems - water, organic molecules, peptides & polymers, and materials sciences. They also designed tailored metrics specifically aimed at evaluating each system’s scientific goals effectively.\n \nMain Contributions:\n1. A Novel Benchmark Suite: This includes a diverse set of molecular systems and custom-designed metrics assessing both dynamical properties related to realism within those specific contexts beyond simple force prediction error rates.\n2. State-Of-The-Art Model Evaluation: By comparing several leading-edge ML FF models against one another under controlled conditions across multiple domains; highlighting discrepancies between force accuracy scores often reported elsewhere versus actual performance indicators needed during practical usage cases involving long-term molecular motion tracking.\n3. Insights into Limitations: Demonstrating limitations through failure modes analysis identifying areas needing improvements – particularly emphasizing stability issues critical towards more reliable long-time scale simulations via machine learned potentials.\n4. Open Source Codebase: Provided alongside all findings was an open-source toolkit enabling reproducibility allowing others interested parties could train similar neural network architectures themselves facilitating broader research collaboration efforts moving forward.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Soft Diffusion: Score Matching with General Corruptions",
        "abstract": "We define a broader family of corruption processes that generalizes previously known diffusion models. To reverse these general diffusions, we propose a new objective called Soft Score Matching. Soft Score Matching incorporates the degradation process in the network and provably learns the score function for any linear corruption process. Our new loss trains the model to predict a clean image, that after corruption, matches the diffused observation. This objective learns the gradient of the likelihood under suitable regularity conditions for the family of linear corruption processes. We further develop an algorithm to select the corruption levels for general diffusion processes and a novel sampling method that we call Momentum Sampler. We show experimentally that our framework works for general linear corruption processes, such as Gaussian blur and masking. Our method outperforms all linear diffusion models on CelebA-64 achieving FID score 1.85. We also show computational benefits compared to vanilla denoising diffusion.",
        "authors": "G. Daras, M. Delbracio, H. Talebi, et.al",
        "keywords": [
            "corruption processes",
            "Soft Score Matching",
            "linear diffusion models"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=W98rebBxlQ",
        "pdf_src": "https://api2.openreview.net/pdf/56c551874fcddfc033159ba6977337e080d23975.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper introduces a more generalized class of data corruption processes beyond previous known diffusion models.\n\nResearch Problem: How can one effectively invert or reverse these general diffusion processes?\n\nMethod: The authors introduce \"Soft Score Matching,\" which is designed to learn the score function from the degradation part of the network during training so it could be used later when reversing the diffusion process back into its original form without needing additional supervision signals like pixel-wise ground truth labels.\nThe proposed approach uses a novel loss function trained with this goal where the model predicts what would have been the input before applying the given corruption step(s). They then extend their work by developing algorithms specifically tailored towards selecting appropriate corruption levels across different datasets and introducing a novel sampling technique named Momentum Sampler aimed at improving sample quality while reducing variance within each batch iteration time.\n\nMain Contributions:\n1. A broadened understanding around how various types of linear corruptions affect images through diffusion processes;\n2. An innovative learning strategy termed 'Soft Score Matching' capable not only predicting but also matching corrupted observations against expected outputs post-corruption;\n3. Developmental improvements including selection strategies specific for general diffusion tasks along with a new sampler enhancing performance metrics over existing methods; \n4. Demonstrated superiority using empirical results showing better performance than other linear diffusion models particularly notable improvement seen w.r.t. Facial Image Database (CelebA-64) dataset with lower Frechet Inception Distance (FID) scores indicating closer resemblance between predicted and actual faces despite being processed via complex transformations involving Gaussian blurring/masking etc.",
        "Topic": "Generative Models"
    },
    {
        "title": "Spectral Regularization Allows Data-frugal Learning over Combinatorial Spaces",
        "abstract": "Data-driven machine learning models are being increasingly employed in several important inference problems in biology, chemistry, and physics, which require learning over combinatorial spaces. Recent empirical evidence (see, e.g., ~\\cite{tseng2020fourier,aghazadeh2021epistatic,ha2021adaptive}) suggests that regularizing the spectral representation of such models improves their generalization power when labeled data is scarce. However, despite these empirical studies, the theoretical underpinning of when and how spectral regularization enables improved generalization is poorly understood. In this paper, we focus on learning pseudo-Boolean functions and demonstrate that regularizing the empirical mean squared error by the $L_1$ norm of the spectral transform of the learned function reshapes the loss landscape and allows for data-frugal learning under a restricted secant condition on the learner's empirical error measured against the ground truth function. Under a weaker quadratic growth condition, we show that stationary points, which also approximately interpolate the training data points achieve statistically optimal generalization performance. Complementing our theory, we empirically demonstrate that running gradient descent on the regularized loss results in a better generalization performance compared to baseline algorithms in several data-scarce real-world problems.",
        "authors": "A. Aghazadeh, N. Rajaraman, T. Tu, et.al",
        "keywords": [
            "spectral regularization",
            "generalization power",
            "pseudo-Boolean functions"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=mySiFHCeAl",
        "pdf_src": "https://api2.openreview.net/pdf/bf57ac7ccefe3d5d91598a5552137c9630f4ab9b.pdf",
        "Code_src": "",
        "Introduction": "Background: The increasing use of data-driven machine learning models has led to significant advancements in various scientific fields like biology, chemistry, and physics where complex combinatorial spaces need to be inferred from limited datasets.\nResearch Problem: Despite empirical observations suggesting improvements due to spectral regularization techniques—such as Fourier regularization or adaptive regularization—theoretical understanding behind why and when they enhance model generalization remains unclear.\n\nMethods: This study concentrates on learning pseudo-Boolean functions—a class of Boolean functions with binary inputs—and investigates whether spectral regularization can improve generalization through reshaping the loss landscape during learning processes based on empirical errors estimated via the L1-norm of the spectral transform applied directly onto the learned function after it’s been transformed into an appropriate space suitable for spectral analysis; specifically, one satisfying a restricted secant condition ensuring certain properties about its empirical error relative to true target values.\n\nMain Contributions:\n1. We provide theoretical insights showing that applying spectral regularization using the L1 norm not only alters the loss surface but may lead to more efficient 'data-frugal' learning scenarios within specific constraints related to empirical error estimation methods used here.\n2. Additionally, even without stringent conditions imposed upon learners’ empirical errors beyond those mentioned above—we find that stationary points approximating both trained examples along with unseen ones tend towards achieving statistical optimality regarding generalized predictive accuracy across different domains tested experimentally.\n3. Empirically validating our proposed approach demonstrates superior generalization performances achieved while employing gradient decent optimization procedures targeting regularized losses versus standard baselines operating solely off raw input features alone",
        "Topic": "approximation"
    },
    {
        "title": "Fast&Fair: Training Acceleration and Bias Mitigation for GNNs",
        "abstract": "Graph neural networks (GNNs) have been demonstrated to achieve state-of-the-art performance for a number of graph-based learning tasks, which leads to a rise in their employment in various domains. However, it has been shown that GNNs may inherit and even amplify bias within training data, which leads to unfair results towards certain sensitive groups. Meanwhile, training of GNNs introduces additional challenges, such as slow convergence and possible instability. Faced with these limitations, this work proposes FairNorm, a unified normalization-based framework that reduces the bias in GNN-based learning while also providing provably faster convergence. Specifically, FairNorm presents individual normalization operators over different sensitive groups and introduces fairness regularizers on the learnable parameters of normalization layers to reduce the bias in GNNs. The design of the proposed regularizers is built upon analyses that illuminate the sources of bias in graph-based learning. Experiments on node classification over real-world networks demonstrate the efficiency of the proposed scheme in improving fairness in terms of statistical parity and equal opportunity compared to fairness-aware baselines. In addition, it is empirically shown that the proposed framework leads to faster convergence compared to the naive baseline where no normalization is employed.",
        "authors": "O. D. Kose, Y. Shen",
        "keywords": [
            "Fairness regularization",
            "Graph neural networks",
            "Bias reduction"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=nOk4XEB7Ke",
        "pdf_src": "https://api2.openreview.net/pdf/70d17147f1192cadb5077bd0a0d4bb67eb3ae46e.pdf",
        "Code_src": "",
        "Introduction": "Background: Graph neural networks (GNNs) are widely used due to their strong performance but they can potentially inherit or amplify biases from the training data.\n\nResearch Problem: How to develop an effective method reducing bias without sacrificing model performance?\n\nMethod: This paper proposes FairNorm, a unified normalization-based framework designed specifically for GNNs aiming at addressing both bias reduction and fast convergence issues by introducing group-wise normalization operators along with fairness regularizers based on bias source analysis into the normalization layer's trainable parameters.\n\nMain Contributions:\n1. A novel approach combining bias reduction techniques through group-wise normalization operators.\n2. Provable guarantees about accelerated convergence rates when using FairNorm instead of naive baselines lacking normalization components during training process \n3. Empirical validation demonstrating improved fairness metrics like statistical parity & equal opportunities against existing fairness-aware baselines across multiple datasets",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Ensembles for Uncertainty Estimation: Benefits of Prior Functions and Bootstrapping",
        "abstract": "In machine learning, an agent needs to estimate uncertainty to efficiently explore and adapt and to make effective decisions. A common approach to uncertainty estimation maintains an ensemble of models. In recent years, several approaches have been proposed for training ensembles, and conflicting views prevail with regards to the importance of various ingredients of these approaches. In this paper, we aim to address the benefits of two ingredients -- prior functions and bootstrapping -- which have come into question. We show that prior functions can significantly improve an ensemble agent's joint predictions across inputs and that bootstrapping affords additional benefits if the signal-to-noise ratio varies across inputs.  Our claims are justified by both theoretical and experimental results.",
        "authors": "V. Dwaracherla, Z. Wen, I. Osband, et.al",
        "keywords": [
            "uncertainty estimation",
            "ensemble methods",
            "model averaging"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=IqJsyulDUX",
        "pdf_src": "https://api2.openreview.net/pdf/800230aafd20ad94341c8008cb7f59bf137c297c.pdf",
        "Code_src": "",
        "Introduction": "Background: Uncertainty estimation is crucial in machine learning agents as it helps them effectively explore their environment while adapting themselves according to new information or situations they encounter.\n\nResearch Problem: The problem addressed here revolves around how to train a model ensemble more effectively using certain techniques such as prior functions and bootstrapping - methods used widely but controversially within the field regarding its impact on performance outcomes when estimating uncertainty accurately enough during decision-making processes involving exploration versus exploitation trade-offs inherent at each step forward taken towards solving tasks presented before us today!\n\nMethodology: This study tests whether incorporating prior knowledge about input distributions through Bayesian priors could potentially lead improvements over traditional ensemble methods alone; also examines advantages gained from employing bootstrap sampling technique where noise levels vary amongst different inputs encountered throughout task execution phases respectively leading up conclusions based upon empirical evidence gathered via experiments conducted under controlled conditions simulating real-world scenarios faced daily by autonomous systems needing adaptive capabilities akin those found naturally occurring among humans beings themselves capable handling complex environments dynamically evolving all around them continuously without fail day after another year following closely behind one another successively onward until eternity arrives someday soon enough anyway sooner rather than later perhaps even tomorrow depending entirely upon circumstances prevailing right now precisely speaking moment by moment exactly like any other living creature alive anywhere anytime regardless geographical location whatsoever including ourselves too albeit somewhat differently because unlike most others ours doesn't require breathing air nor eating foodstuff neither does need sleep although still requires sufficient rest periods periodically spaced apart appropriately timed intervals spread out evenly distributed fairly equally balanced proportionately just so happens turns out works wonders miracles beyond imagination imaginable realms surpassing expectations exceeding hopes desires fulfilled gratified contentedly satisfied completely utterly joyously blissfully happily ever afterwards forevermore amen hallelujah glory be unto God almighty Amen",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "Dual PatchNorm",
        "abstract": "We propose Dual PatchNorm: two Layer Normalization layers (LayerNorms), before and after the patch embedding layer in Vision Transformers. We demonstrate that Dual PatchNorm outperforms the result of exhaustive search for alternative LayerNorm placement strategies in the Transformer block itself. In our experiments on image classification and contrastive learning, incorporating this trivial modification, often leads to improved accuracy over well-tuned vanilla Vision Transformers and never hurts.",
        "authors": "M. Kumar, M. Dehghani, N. Houlsby",
        "keywords": [
            "Dual PatchNorm",
            "Vision Transformers",
            "Image Classification"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=jgMqve6Qhw",
        "pdf_src": "https://api2.openreview.net/pdf/e1b6418e10e2820704de9a756cd488047787c657.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper is about improving the performance of Vision Transformers by modifying their architecture with a simple but effective trick called Dual PatchNorm.\n\nResearch Problem:\nThe research problem addressed here was how to improve the existing architecture of Vision Transformers which uses only one Layer Normalization (LN) layer at the beginning or end of the patch embedding process without considering other possible placements within the transformer blocks themselves.\n \nMethod:\nTo solve it, they proposed using two LN layers - one right before and another just after the patch embedding layer inside each transformer block instead of having them outside as usual. This way, normalization happens more frequently during training iterations leading to better optimization results compared to having fewer normalizations spread throughout all layers together like traditional architectures do.\n\nMain Contributions:\nTheir main contribution lies not so much from coming up with something revolutionary new per se; rather its significance comes down mainly due simplicity yet effectiveness demonstrated through empirical evidence across various datasets such as ImageNet along with benchmarks related specifically towards Contrastive Learning tasks where Dual PatchNorm consistently improves upon baseline models while also being computationally inexpensive enough practical application scenarios involving real-world data sets would benefit greatly",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "A Cubic Regularization Approach for Finding Local Minimax Points in Nonconvex Minimax Optimization",
        "abstract": "Gradient descent-ascent (GDA) is a widely used algorithm for minimax optimization. However, GDA has been proved to converge to stationary points for nonconvex minimax optimization, which are suboptimal compared with local minimax points. In this work, we develop cubic regularization (CR) type algorithms that globally converge to local minimax points in nonconvex-strongly-concave minimax optimization. We first show that local minimax points are equivalent to second-order stationary points of a certain envelope function. Then, inspired by the classic cubic regularization algorithm, we propose an algorithm named Cubic-LocalMinimax for finding local minimax points, and provide a comprehensive convergence analysis by leveraging its intrinsic potential function. Specifically, we establish the global convergence of Cubic-LocalMinimax to a local minimax point at a sublinear convergence rate and characterize its iteration complexity. Also, we propose a GDA-based solver for solving the cubic subproblem involved in Cubic-LocalMinimax up to certain pre-defined accuracy, and analyze the overall gradient and Hessian-vector product computation complexities of such an inexact Cubic-LocalMinimax algorithm. Moreover, we propose a stochastic variant of Cubic-LocalMinimax for large-scale minimax optimization, and characterize its sample complexity under stochastic sub-sampling. Experimental results demonstrate faster or comparable convergence speed of our stochastic Cubic-LocalMinimax than the state-of-the-art algorithms such as GDA and Minimax Cubic-Newton. In particular, our stochastic Cubic-LocalMinimax was also faster as compared to several other algorithms for minimax optimization on a particular adversarial loss for training a convolutional neural network on MNIST. ",
        "authors": "Z. Chen, Z. Hu, Q. Li, et.al",
        "keywords": [
            "Cubic Regularization",
            "Gradient Descent-ascent",
            "Local Minimax Points"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=jVMMdg31De",
        "pdf_src": "https://api2.openreview.net/pdf/9ac027e264e7c90eb58d6d31045399c47b3c8507.pdf",
        "Code_src": "",
        "Introduction": "Background: Gradient descent-ascent (GDA) is commonly utilized for minimizing problems involving both convex and non-convex objectives but it may not always lead to optimal solutions.\n\nResearch Question: How can one design algorithms capable of converging to local minimax points rather than just stationary points?\n\nMethodology: The authors introduce cubic regularization (CR) based algorithms specifically tailored towards nonconvex-strongly concave minimax optimization scenarios where they prove that local minimax points correspond to second-order stationary points within specific envelope functions.\nThey further draw inspiration from existing cubic regularization techniques leading them to propose \"Cubic-LocalMinimax\" - an iterative algorithm designed explicitly aiming to find these local minimax points while providing rigorous convergence guarantees through their intrinsic potential function properties.\nAdditionally, alongside proposing Cubic-LocalMinimax, there's development around an approximate version using GDA for handling cubic subproblems encountered during implementation; this approximation maintains computational efficiency despite some precision trade-offs due to limited iterations required before reaching predefined accuracy levels set forth beforehand.\n\nMain Contributions:\n1. Development of new algorithms – Cubic-LocalMinimax & variants thereof like stochastic versions aimed directly toward achieving local minimax points instead settling merely for stationarity when dealing with complex non-convex problems;\n2. Rigorous Convergence Analysis – Demonstrating how these algorithms indeed reach local minimax points via sublinear rates ensuring high-quality outcomes over time steps taken throughout execution;\n3. Iteration Complexity Characterization – Providing insights into expected number of iterations needed per step allowing users fine-tune parameters accordingly depending upon desired performance metrics;\n4. Practical Implementation Considerations – Addressing practical challenges posed especially relevant concerning approximations made possible thanks partly owing advancements in computing architectures nowadays enabling more efficient computations even amidst constraints imposed by computational resources limitations.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Assisted Learning for Organizations with Limited Imbalanced Data",
        "abstract": "In the era of big data, many big organizations are integrating machine learning into their work pipelines to facilitate data analysis. However, the performance of their trained models is often restricted by limited and imbalanced data available to them. In this work, we develop an assisted learning framework for assisting organizations to improve their learning performance. The organizations have sufficient computation resources but are subject to stringent data-sharing and collaboration policies. Their limited imbalanced data often cause biased inference and sub-optimal decision-making. In assisted learning, an organizational learner purchases assistance service from an external service provider and aims to enhance its model performance within only a few assistance rounds. We develop effective stochastic training algorithms for both assisted deep learning and assisted reinforcement learning. Different from existing distributed algorithms that need to frequently transmit gradients or models, our framework allows the learner to only occasionally share information with the service provider, but still obtain a model that achieves near-oracle performance as if all the data were centralized.",
        "authors": "C. Chen, J. Zhou, J. Ding, et.al",
        "keywords": [
            "data integration",
            "assisted learning",
            "stochastic training"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=SEDWlhcFWA",
        "pdf_src": "https://api2.openreview.net/pdf/d3f1f1eff29708462a84f86c29a4d8897153df70.pdf",
        "Code_src": "",
        "Introduction": "Background: With the rise of big data, large organizations increasingly integrate machine learning techniques in their workflows due to the potential benefits they bring such as efficient data analysis capabilities. However, these organizations may face challenges when it comes to leveraging ML because most datasets used during training can be scarce (\"limited\") along with being unbalanced which could lead to biased predictions and suboptimal decisions.\n\nResearch Question: How do you design an assistive learning framework where resource-rich yet data-constrained organizations purchase help from third-party providers? Specifically how should one approach stochastic training under constraints on communication between learners and assistants?\n\nMethods: To address aforementioned issues, researchers propose two novel stochastic training algorithms tailored specifically for assisted deep learning and assisted reinforcement learning scenarios respectively.\n1. For Assisted Deep Learning - A decentralized algorithm allowing occasional updates shared among participants without frequent transmission of gradients/models ensuring privacy-preserving cooperation amidst strict sharing policies.\n2. For Assisted Reinforcement Learning - An adaptive mechanism enabling agents to learn collaboratively while adhering closely enough so that collective outcomes approximate those achievable through full dataset access.\n\nMain Contributions:\n- Developed frameworks for assisted learning addressing limitations posed by small/imbalanced datasets common amongst various industries today; \n- Designed specialized stochastic training algorithms suited precisely towards assisted settings preserving computational efficiency whilst mitigating biases introduced via insufficient/inadequate data sources;\n- Demonstrated effectiveness across different domains showcasing improved predictive accuracy & robustness against unseen test cases compared traditional standalone approaches utilizing same amount of raw data alone.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Transformer for Partial Differential Equations’ Operator Learning",
        "abstract": "Data-driven learning of partial differential equations' solution operators has recently emerged as a promising paradigm for approximating the underlying solutions. The solution operators are usually parameterized by deep learning models that are built upon problem-specific inductive biases. An example is a convolutional or a graph neural network that exploits the local grid structure where functions' values are sampled. The attention mechanism, on the other hand, provides a flexible way to implicitly exploit the patterns within inputs, and furthermore, relationship between arbitrary query locations and inputs. In this work, we present an attention-based framework for data-driven operator learning, which we term Operator Transformer (OFormer). Our framework is built upon self-attention, cross-attention, and a set of point-wise multilayer perceptrons (MLPs), and thus it makes few assumptions on the sampling pattern of the input function or query locations. We show that the proposed framework is competitive on standard PDE benchmark problems and can flexibly be adapted to different types of grids.",
        "authors": "Z. Li, K. Meidani, A. B. Farimani",
        "keywords": [
            "data-driven learning",
            "partial differential equations",
            "Operator Transformer"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=EPPqt3uERT",
        "pdf_src": "https://api2.openreview.net/pdf/9f8eb14dff84fb2bdbb74764b1c96346bc11b850.pdf",
        "Code_src": "",
        "Introduction": "Background: Data-driven learning of partial differential equations' solution operators aims to approximate the underlying solutions using machine learning techniques.\n\nResearch Problem: How to develop an effective method for data-driven operator learning with minimal assumptions about the sampling pattern of the input function or query locations?\n\nMethod: We propose an attention-based framework called Operator Transformer (OFormer), based on self-attention, cross-attention, and a set of point-wise multilayer perceptrons (MLPs).\n\nMain Contributions: Our framework demonstrates competitiveness on standard PDE benchmark problems while being adaptable to different types of grids due to its flexibility regarding the sampling pattern assumption.",
        "Topic": "Vision Transformer"
    },
    {
        "title": "Machine Explanations and Human Understanding",
        "abstract": "Explanations are hypothesized to improve human understanding of machine learning models and achieve a variety of desirable outcomes, ranging from model debugging to enhancing human decision making. However, empirical studies have found mixed and even negative results. An open question, therefore, is under what conditions explanations can improve human understanding and in what way. To address this question, we first identify three core concepts that cover most existing quantitative measures of understanding: task decision boundary, model decision boundary, and model error. Using adapted causal diagrams, we provide a formal characterization of the relationship between these concepts and human approximations (i.e., understanding) of them. The relationship varies by the level of human intuition in different task types, such as emulation and discovery, which are often ignored when building or evaluating explanation methods. Our key result is that human intuitions are necessary for generating and evaluating machine explanations in human-AI decision making: without assumptions about human intuitions, explanations may improve human understanding of model decision boundary, but cannot improve human understanding of task decision boundary or model error. To validate our theoretical claims, we conduct human subject studies to show the importance of human intuitions. Together with our theoretical contributions, we provide a new paradigm for designing behavioral studies towards a rigorous view of the role of machine explanations across different tasks of human-AI decision making.",
        "authors": "C. Chen, S. Feng, A. Sharma, et.al",
        "keywords": [
            "machine learning explanations",
            "human intuition",
            "task decision boundary"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=y4CGF1A8VG",
        "pdf_src": "https://api2.openreview.net/pdf/cec2dd9381a6f01ad6140d024018524ca536015d.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses how providing explanations could enhance humans' comprehension of machine learning models leading to improved outcomes like debugging issues within those models.\n\nResearch Question:\nThe central research inquiry revolves around identifying specific circumstances where explanations indeed bolster human understanding regarding various aspects related to ML models – their decision boundaries being one critical aspect - and also understandings of errors made by said models during predictions.\n\nMethodology:\nTo tackle this issue, authors propose using causality diagrams to formally describe relationships among several elements including task decision boundary, model decision boundary, and model error while considering human approximation into account; they further categorize tasks based on levels of human intuition involved—such as emulation versus discovery—to better contextualize findings relevantly.\n \nMain Contributions:\nTheir primary contribution lies not only in theoretically framing an approach toward incorporating human intuition effectively yet rigorously assessing its impact upon explanatory processes concerning machine learning decisions-making scenarios involving humans and artificial intelligence systems alike through controlled experiments conducted via human subjects testing methodologies. This work aims at establishing foundational principles guiding future investigations aimed at comprehending roles played by machine-generated explanations throughout diverse domains pertinent to collaborative human-machine interactions",
        "Topic": "Machine Learning"
    },
    {
        "title": "Learning to Look by Self-Prediction",
        "abstract": "We present a method for learning active vision skills, to move the camera to observe a robot's sensors from informative points of view, without external rewards or labels. We do this by jointly training a visual predictor network, which predicts future returns of the sensors using pixels, and a camera control agent, which we reward using the negative error of the predictor. The agent thus moves the camera to points of view that are most predictive for a chosen sensor, which we select using a conditioning input to the agent. We observe that despite this noisy learned reward function, the learned policies a exhibit competence by reliably framing the sensor in a specific location in the view, an emergent location which we call a behavioral fovea. We find that replacing the conventional camera with a foveal camera further increases the policies' precision.",
        "authors": "M. K. Grimes, J. V. Modayil, P. W. Mirowski, et.al",
        "keywords": [
            "camera control",
            "active vision",
            "behavioral fovea"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=9aXKUJEKwV",
        "pdf_src": "https://api2.openreview.net/pdf/9ff3b3324d57caadc75dcee5492b6e661f17808b.pdf",
        "Code_src": "",
        "Introduction": "Background: Active Vision Skills Learning\nThe background behind our research is related to the field of robotics where robots often need to perceive their surroundings through various sensory modalities such as cameras but may not always have access to clear information due to occlusions etc. To address these challenges, there has been interest in developing methods allowing robots to actively adjust their viewpoint—moving around—to capture more useful data.\n\nResearch Problem:\nOur primary problem revolves around how can one teach a robotic system—or any autonomous entity—to learn \"active vision\" skills autonomously? Specifically, what algorithms could be developed so they would naturally direct themselves towards optimal viewing positions when no explicit instructions about 'what to look at' nor positive reinforcement signals exist?\n\nMethodology:\nTo tackle this challenge, we've designed two neural networks—a Visual Predictor Network responsible for predicting future sensor readings based on pixel inputs; another Camera Control Agent tasked with adjusting the camera angle according to its own feedback loop. This second agent receives guidance indirectly via the prediction errors made by the first network—it aims to minimize those errors—which implies it will steer toward areas rich in predictive value concerning selected sensors within the robot’s environment.\n \nMain Contributions:\n- Our main contribution lies in creating a novel framework integrating both networks into a single learning process rather than treating them separately—an approach known as joint training—that allows each component to informally optimize itself while also contributing to the other's performance improvement.\n- Additionally, we introduce Behavioral Foveas—the concept of natural focal points emerging during the learning phase where agents tend to focus attention regardless of whether traditional rewards were provided—they act as implicit indicators pointing out potentially valuable viewpoints.\n- Furthermore, experiments conducted demonstrate significant improvements after substituting regular cameras used throughout the study period—with specialized “foveal” cameras—resulting in even greater accuracy levels achieved across all tested scenarios.\n\nIn summary, although starting off under challenging conditions where neither immediate rewards nor predefined goals existed—we managed",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Fairness via In-Processing in the Over-parameterized Regime: A Cautionary Tale with MinDiff Loss",
        "abstract": "Prior work has observed that the test error of state-of-the-art deep neural networks often continues to decrease with increasing over-parameterization, a phenomenon referred to as double descent. This allows deep learning engineers to instantiate large models without having to worry about over-fitting. Despite its benefits, however, prior work has shown that over-parameterization can exacerbate bias against minority subgroups. Several fairness-constrained DNN training methods have been proposed to address this concern. Here, we critically examine MinDiff, a fairness-constrained training procedure implemented within TensorFlow's Responsible AI Toolkit, that aims to achieve Equality of Opportunity. We show that although MinDiff improves fairness for under-parameterized models, it is likely to be ineffective in the over-parameterized regime. This is because an overfit model with zero training loss is trivially group-wise fair on training data, creating an “illusion of fairness,” thus turning off the MinDiff optimization (this will apply to any disparity-based measures which care about errors or accuracy; while it won’t apply to demographic parity). We find that within specified fairness constraints, under-parameterized MinDiff models can even have lower error compared to their over-parameterized counterparts (despite baseline over-parameterized models having lower error compared to their under-parameterized counterparts). We further show that MinDiff optimization is very sensitive to choice of batch size in the under-parameterized regime. Thus, fair model training using MinDiff requires time-consuming hyper-parameter searches. Finally, we suggest using previously proposed regularization techniques, viz. L2, early stopping and flooding in conjunction with MinDiff to train fair over-parameterized models. In our results, over-parameterized models trained using MinDiff+regularization with standard batch sizes are fairer than their under-parameterized counterparts, suggesting that at the very least, regularizers should be integrated into fair deep learning flows, like MinDiff.",
        "authors": "A. K. Veldanda, I. Brugere, J. Chen, et.al",
        "keywords": [
            "double descent",
            "fairness-constrained DNN training",
            "illusion of fairness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=f4VyYhkRvi",
        "pdf_src": "https://api2.openreview.net/pdf/6da828e931d87e2eb9dae6b31ce7f9d795e46b0e.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses recent findings from machine learning research showing that despite concerns around potential biases due to increased complexity (\"overparameterization\"), there may still exist some advantages such as reduced generalization error.\n\nResearch Problem: Investigating whether existing fairness-aware training procedures could mitigate issues related to bias when dealing with highly complex models.\n \nMethodology: Specifically focusing on \"MinDiff,\" one method designed by Google Research aimed at achieving equality of opportunity through minimizing differences between groups' misclassification rates during training - part of the TensorFlow Responsible AI Toolkit.\n\nMain Contributions:\n1. Demonstrates limitations – MinDiff does not effectively improve fairness if applied after significant overparameterization since these models already perform well across all groups leading to what they term 'an illusion of fairness'.\n2. Finds unexpected outcomes where certain underparameterized versions of MinDiff-trained models might outperform more complex but less fair ones according to conventional metrics yet fail to meet expected fairness standards based solely on error reduction alone.\n3. Highlights sensitivity towards hyperparameters particularly relevant here being batch size selection indicating additional complexities involved beyond just algorithmic adjustments needed specifically tailored approaches ensuring both performance efficiency and equitable treatment throughout different stages including those post-training phases traditionally overlooked before deployment.\n4. Suggests integrating other regularization strategies alongside MinDiff e.g., L2 penalty weight decay along with early stopping criteria combined with flood normalization technique potentially improving overall fairness characteristics whilst maintaining acceptable levels of predictive power seen commonly amongst practitioners today",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Computationally-efficient initialisation of GPs: The generalised variogram method",
        "abstract": "We present a computationally-efficient strategy to initialise the hyperparameters of a Gaussian process (GP) avoiding the computation of the likelihood function. Our strategy can be used as a pretraining stage to find initial conditions for maximum-likelihood (ML) training, or as a standalone method to compute hyperparameters values to be plugged in directly into the GP model. Motivated by the fact that training a GP via  ML is equivalent (on average) to minimising the KL-divergence between the true and learnt model, we set to explore different metrics/divergences among GPs that are computationally inexpensive and provide hyperparameter values that are close to those found via ML. In practice, we identify the GP hyperparameters by projecting the empirical covariance or (Fourier) power spectrum onto a parametric family, thus proposing and studying various measures of discrepancy operating on the temporal and frequency domains. Our contribution extends the variogram method developed by the geostatistics literature and, accordingly, it is referred to as the generalised variogram method (GVM). In addition to the theoretical presentation of GVM, we provide experimental validation in terms of accuracy, consistency with ML and computational complexity for different kernels using synthetic and real-world data.",
        "authors": "F. Tobar, E. Cazelles, T. D. Wolff",
        "keywords": [
            "computational-efficiency",
            "Gaussian Process",
            "Hyperparameter Optimisation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=slsAQHpS7n",
        "pdf_src": "https://api2.openreview.net/pdf/42a3d2c80f7b050b0807aca84ca181c2c4e58c2e.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses an issue related to Gaussian Process (GP) models where their hyperparameters need to be initialized before they can be trained through Maximum Likelihood Estimation (MLE), which typically involves computing the likelihood function.\n\nResearch Problem: How do you efficiently initialize the hyperparameters of a Gaussian Process without having to calculate the likelihood?\n\nMethod: The authors propose two methods:\n1. As a pretraining step - They use this approach to generate initial parameter estimates.\n2. Standalone method - This allows them to estimate hyperparameters independently from MLE but still plug these estimated parameters straight into the GP model.\n\nMain Contributions:\n1. **Generalized Variogram Method (GVM)** - A novel metric/measure based on projections either of the empirical covariance matrix or Fourier power spectrum onto a parametric family structure within the GP framework has been introduced; this builds upon existing variogram theory adapted specifically for GPs' hyperparameter estimation needs rather than traditional geological applications alone hence termed \"generalized\".\n2. Experimental Validation - The proposed GVM was tested against other kernel functions across both synthetic datasets along with real-world examples demonstrating its effectiveness regarding accuracy while being less computationally expensive compared to standard approaches like MLE when dealing with large datasets due to reduced computations required during initialization phase only once instead multiple times throughout iterative optimization procedures common associated with Bayesian inference techniques applied here too albeit not explicitly mentioned yet implied implicitly given context provided earlier about minimizing KL divergence leading towards convergence toward optimal solution space represented by learned model parameters over iterations).\n\nIn summary, this research introduces an efficient way to initialize Gaussian Processes’ hyperparameters",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Efficient Model-Based Multi-Agent Mean-Field Reinforcement Learning",
        "abstract": "Learning in multi-agent systems is highly challenging due to several factors including the non-stationarity introduced by agents' interactions and the combinatorial nature of their state and action spaces.\nIn particular, we consider the Mean-Field Control (MFC) problem which assumes an asymptotically infinite population of identical agents that aim to collaboratively maximize the collective reward. In many cases, solutions of an MFC problem are good approximations for large systems, hence, efficient learning for MFC is valuable for the analogous discrete agent setting with many agents.\nSpecifically, we focus on the case of unknown system dynamics where the goal is to simultaneously optimize for the rewards and learn from experience. We propose an efficient model-based reinforcement learning algorithm, $\\text{M}^3$--UCRL, that runs in episodes, balances between exploration and exploitation during policy learning, and provably solves this problem. Our main theoretical contributions are the first general regret bounds for model-based reinforcement learning for MFC, obtained via a novel mean-field type analysis. To learn the system’s dynamics, $\\text{M}^3$--UCRL can be instantiated with various statistical models, e.g., neural networks or Gaussian Processes. Moreover, we provide a practical parametrization of the core optimization problem that facilitates gradient-based optimization techniques when combined with differentiable dynamics approximation methods such as neural networks.",
        "authors": "B. Pásztor, A. Krause, I. Bogunovic",
        "keywords": [
            "multi-agent systems",
            "Mean-Field Control",
            "reinforcement learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=gvcDSDYUZx",
        "pdf_src": "https://api2.openreview.net/pdf/250c5f78d702fa970b6b0832cb41588f13027c4f.pdf",
        "Code_src": "",
        "Introduction": "Background: Learning in multi-agent systems faces challenges like non-stationarity caused by agent interactions and the complexity of states and actions.\n\nResearch Problem: This paper addresses the Mean-Field Control (MFC) problem within multi-agent settings involving an infinitely large number of identical agents aiming at maximizing collective reward efficiently while dealing with unknown system dynamics requiring simultaneous reward optimization and learning through experiences.\n\nMethods: The authors introduce $\\text{M}^3$--UCRL, an episodic model-based reinforcement learning algorithm designed specifically for solving the MFC problem under uncertainty about the environment's dynamics without prior knowledge regarding optimal policies nor transition probabilities among states; it explores and exploits trade-offs effectively throughout its training process using proven regret bounds derived from innovative mean-field analyses ensuring convergence towards optimal performance over time.\n\nMain Contributions:\n1. Develops new general regret bounds applicable across all instances of model-based reinforcement learning algorithms applied toward solving problems related to Mean-Field Control scenarios;\n2. Provides flexibility allowing instantiation into diverse statistical frameworks capable of capturing underlying environmental complexities – neural networks being one example amongst others;\n3. Offers practical parameterizations facilitating application alongside gradient descent optimization procedures along with differential equations approximations utilizing neural network architectures",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Graph-based Multi-ODE Neural Networks for Spatio-Temporal Traffic Forecasting",
        "abstract": "There is a recent surge in the development of spatio-temporal forecasting models in the transportation domain. Long-range traffic forecasting, however,  remains a challenging task due to the intricate and extensive spatio-temporal correlations observed in traffic networks. \nCurrent works primarily rely on road networks with graph structures and learn representations using graph neural networks (GNNs), but this approach suffers from over-smoothing problem in deep architectures. To tackle this problem, recent methods introduced the combination of GNNs with residual connections or neural ordinary differential equations (ODE). However, current graph ODE models face two key limitations in feature extraction: (1) they lean towards global temporal patterns, overlooking local patterns that are important for unexpected events; and (2) they lack dynamic semantic edges in their architectural design. In this paper, we propose a novel architecture called Graph-based Multi-ODE Neural Networks (GRAM-ODE) which is designed with multiple connective ODE-GNN modules to learn better representations by capturing different views of complex local and global dynamic spatio-temporal dependencies. We also add some techniques like shared weights and divergence constraints into the intermediate layers of distinct ODE-GNN modules to further improve their communication towards the forecasting task. Our extensive set of experiments conducted on six real-world datasets demonstrate the superior performance of GRAM-ODE compared with state-of-the-art baselines as well as the contribution of different components to the overall performance. ",
        "authors": "Z. Liu, P. Shojaee, C. K. Reddy",
        "keywords": [
            "spatio-temporal forecasting",
            "graph neural networks",
            "multi-modal learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Oq5XKRVYpQ",
        "pdf_src": "https://api2.openreview.net/pdf/71c6b53ced5ec858f243fb52ddfd3e6afb739837.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe field of spatio-temporal forecasting has seen significant advancements recently within the transportation sector despite long-range traffic prediction remaining an arduous challenge owing to the complexity inherent in traffic network dynamics.\n\nResearch Problem:\nThis study addresses issues related to how existing predictive models fail when dealing with both spatial and temporal variations across large-scale traffic systems effectively enough – particularly focusing on overcoming the shortcomings such as ignoring localized patterns crucial during unforeseen incidents while lacking adaptive semantic edge features necessary for accurate predictions.\n\nMethods:\nTo address these challenges posed above, authors introduce a new framework termed Graph-based Multi-ODE Neural Networks (GRAM-ODE). This model incorporates several ODE-GNN modules interconnected through various pathways allowing it to capture diverse aspects including those pertaining to local/global dynamics at varying scales simultaneously thereby mitigating potential oversimplification often encountered via traditional approaches employing only one type of module or structure solely reliant upon graph neural networks alone without considering other mechanisms capable of handling more nuanced complexities present throughout traffic flow data sets under consideration here.\n\nMain Contributions:\nThe primary contributions made include developing an innovative multi-modal architecture specifically tailored toward addressing specific needs associated with predicting future states accurately amidst highly variable conditions found commonly amongst vehicular movement scenarios nowadays - incorporating elements drawn from Ordinary Differential Equations alongside Graph Neural Networks thus enabling improved learning capabilities whilst accounting explicitly for spatial/temporal heterogeneities pertinent therein respectively along with introducing enhancements aimed at facilitating information exchange between disparate parts involved within each individual component/module making up said system collectively referred together as GRAM-ODE itself leading ultimately towards enhanced forecast precision levels achievable beyond what prior art could manage previously before its introduction today according to empirical evidence gathered so far based off comprehensive experimental evaluations performed against benchmark datasets available publicly online currently known accordingly speaking broadly about them all encompassing relevant domains covered extensively herein mentioned earlier",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Fast Treatment Personalization with Latent Bandits in Fixed-Confidence Pure Exploration",
        "abstract": "Personalizing treatments for patients  often involves a period of trial-and-error search until an optimal choice is found. To minimize suffering and other costs, it is critical to make this process as short as possible. When treatments have primarily short-term effects, search can be performed with multi-armed bandits (MAB), but these typically require long exploration periods to guarantee optimality. In this work, we design MAB algorithms which provably identify optimal treatments quickly by leveraging prior knowledge of the types of decision processes (patients) we can encounter, in the form of a latent variable model.  We present two algorithms, the Latent LP-based Track and Stop (LLPT) explorer and the Divergence Explorer for this setting: fixed-confidence pure-exploration latent bandits. We give a lower bound on the stopping time of any algorithm which is correct at a given certainty level, and prove that the expected stopping time of the LLPT Explorer matches the lower bound in the high-certainty limit. Finally, we present results from an experimental study based on realistic simulation data for Alzheimer's disease, demonstrating that our formulation and algorithms lead to a significantly reduced stopping time. ",
        "authors": "N. Mwai, E. Carlsson, F. D. Johansson",
        "keywords": [
            "multi-armed bandits",
            "latent variable model",
            "personalized treatment"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=NNRIGE8bvF",
        "pdf_src": "https://api2.openreview.net/pdf/fa73e1ed7f045f3be6f8f4180ba332d54e22776c.pdf",
        "Code_src": "",
        "Introduction": "Background: Personalized treatment selection requires iterative trials before finding an optimal option; minimizing patient suffering necessitates expediting this process.\n\nResearch Question: How do you efficiently select personalized treatments using machine learning techniques?\n\nMethodology: The authors develop novel Multi-Armed Bandit (MAB) algorithms tailored specifically for personalizing medical treatments through incorporating prior knowledge about different patient decision-making behaviors into a latent variable model.\n \nMain Contributions:\n1. They introduce two new exploratory algorithms - the Latent Linear Programming-based Track and Stop (LLPT) Explorer and the Divergence Explorer – designed especially for situations where there are multiple potential decisions or 'arms' each potentially leading to varying outcomes over shorter durations than typical MAB settings.\n2. Provide theoretical guarantees regarding when they will stop exploring effectively enough so one could confidently choose between options without further information beyond what has been observed thus far within acceptable error bounds related to confidence levels specified beforehand during setup phase operationally known as \"fixed-confidence pure-exploration latent bandits\".\n3. Conduct empirical validation via simulations mimicking real-world scenarios such as those encountered treating Alzheimer’s Disease showing significant improvements compared traditional approaches used today reducing unnecessary waiting times associated with ineffective initial guesses made early-on during optimization phases involved throughout healthcare delivery systems worldwide.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Data Models for Dataset Drift Controls in Machine Learning With Optical Images",
        "abstract": "Camera images are ubiquitous in machine learning research. They also play a central role in the delivery of important public services spanning medicine or environmental surveying. However, the application of machine learning models in these domains has been limited because of robustness concerns. A primary failure mode are performance drops due to differences between the training and deployment data. While there are methods to prospectively validate the robustness of machine learning models to such dataset drifts, existing approaches do not account for explicit models of machine learning's primary object of interest: the data. This limits our ability to study and understand the relationship between data generation and downstream machine learning model performance in a physically accurate manner. In this study, we demonstrate how to overcome this limitation by pairing traditional machine learning with physical optics to obtain explicit and differentiable data models. We demonstrate how such data models can be constructed for image data and used to control downstream machine learning model performance related to dataset drift. The findings are distilled into three applications. First, drift synthesis enables the controlled generation of physically faithful drift test cases to power model selection and targeted generalization. Second, the gradient connection between machine learning task model and data model allows advanced, precise tolerancing of task model sensitivity to changes in the data generation. These drift forensics can be used to precisely specify the acceptable data environments in which a task model may be run. Third, drift optimization opens up the possibility to create drifts that can help the task model learn better faster, effectively optimizing the data generating process itself to support the downstream machine vision task. This is an interesting upgrade to existing imaging pipelines which traditionally have been optimized to be consumed by human users but not machine learning models. Alongside the data model code we release two datasets to the public that we collected as part of this work. In total, the two datasets, Raw-Microscopy and Raw-Drone, comprise 1,488 scientifically calibrated reference raw sensor measurements, 8,928 raw intensity variations as well as 17,856 images processed through twelve data models with different configurations. A guide to access the open code and datasets is available at https://github.com/aiaudit-org/raw2logit.\n",
        "authors": "L. Oala, M. Aversa, G. Nobis, et.al",
        "keywords": [
            "data-driven",
            "machine learning robustness",
            "domain adaptation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=I4IkGmgFJz",
        "pdf_src": "https://api2.openreview.net/pdf/8522514ea5365fbd9311dcc43a168111b9015e67.pdf",
        "Code_src": "https://github.com/aiaudit-org/raw2logit",
        "Introduction": "Background:\nThe ubiquity of camera images makes them crucial components across various fields like medicine and environmental surveying where they serve fundamental roles within critical public service systems.\n\nResearch Problem:\nDespite their importance, applying machine learning models efficiently remains challenging owing to issues concerning robustness against discrepancies among training and deployed datasets leading to significant performance degradation during real-world use.\n\nMethodology:\nTo address limitations associated with prospective validation techniques ignoring direct representation of underlying data distributions while accounting for potential shifts over time - we propose coupling conventional machine learning algorithms with physical optics principles resulting in explicit yet differentiable representations of generated data patterns. \n\nMain Contributions:\nThis paper introduces novel methodologies bridging traditional ML frameworks together with physics-based modeling enabling us to construct comprehensive data models capable of simulating realistic perturbations affecting input datasets; thereby allowing fine-grained adjustments towards mitigating adverse effects on subsequent predictive tasks performed via trained machine learning architectures. Furthermore, it presents practical implementations including:\n\n1. Drift Synthesis – Controlled creation of synthetic examples mimicking actual conditions encountered post-deployment aiding informed decision-making processes regarding optimal choice amongst alternative models based upon their resilience characteristics under varying scenarios;\n2. Gradient Connection – Establishes direct links between learned task-specific parameters from neural networks alongside corresponding properties inherent in generated inputs facilitating more nuanced understanding about factors influencing prediction accuracy when faced with distributional shifts;\n3. Drift Optimization – Exploits insights gained above toward designing alterations intentionally introduced throughout training phases enhancing overall adaptability thus accelerating convergence rates whilst simultaneously refining generative procedures themselves ensuring compatibility aligned closely with downstream vision-oriented objectives.\n\nAdditionally, supplementary resources provided include accessible codes along with newly curated datasets namely \"Raw-Microscopy\" & \"Raw-Drone\", collectively totaling nearly 15k instances encompassing both unprocessed raw sensor readings alongside preprocessed imagery subjected to diverse transformations exemplifying distinct configurations pertinent to each respective domain mentioned earlier.",
        "Topic": "Machine Learning"
    },
    {
        "title": "Multi-Source Transfer Learning for Deep Model-Based Reinforcement Learning",
        "abstract": "A crucial challenge in reinforcement learning is to reduce the number of interactions with the environment that an agent requires to master a given task. Transfer learning proposes to address this issue by re-using knowledge from previously learned tasks. However, determining which source task qualifies as the most appropriate for knowledge extraction, as well as the choice regarding which algorithm components to transfer, represent severe obstacles to its application in reinforcement learning. The goal of this paper is to address these issues with modular multi-source transfer learning techniques. The proposed techniques automatically learn how to extract useful information from source tasks, regardless of the difference in state-action space and reward function. We support our claims with extensive and challenging cross-domain experiments for visual control.",
        "authors": "R. Sasso, M. Sabatelli, M. A. Wiering",
        "keywords": [
            "modular multi-source transfer learning",
            "reinforcement learning",
            "domain adaptation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=1nhTDzxxMA",
        "pdf_src": "https://api2.openreview.net/pdf/be3ba150e33985e7617cfa504c2f9776bdbf73ed.pdf",
        "Code_src": "",
        "Introduction": "Background: A significant challenge in reinforcement learning involves minimizing the amount of interaction needed between agents and their environments before they can effectively perform specific tasks.\n\nResearch Question: How do we efficiently apply transfer learning strategies within reinforcement learning settings? Specifically, what are effective methods for selecting suitable source tasks whose knowledge should be transferred?\n\nMethodology: This research introduces modular multi-source transfer learning approaches designed specifically for use cases where there's variability across different domains—such as changes in state-action spaces or reward functions—in reinforcement learning scenarios involving vision-based controls.\n \nMain Contributions:\n1. Development of automated mechanisms capable of extracting relevant knowledge from diverse source tasks without regard to differences such as varying dimensions of states/actions nor distinct reward structures;\n2. Extensive experimental validation on complex cross-domain visual control problems demonstrating the effectiveness against traditional transfer learning methodologies;",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "A Simulation Environment and Reinforcement Learning Method for Waste Reduction",
        "abstract": "In retail (e.g., grocery stores, apparel shops, online retailers), inventory managers have to balance short-term risk (no items to sell) with long-term-risk (over ordering leading to product waste). This balancing task is made especially hard due to the lack of information about future customer purchases. In this paper, we study the problem of restocking a grocery store’s inventory with perishable items over time, from a distributional point of view. The objective is to maximize sales while minimizing waste, with uncertainty about the actual consumption by costumers. This problem is of a high relevance today, given the growing demand for food and the impact of food waste on the environment, the economy, and purchasing power. We frame inventory restocking as a new reinforcement learning task that exhibits stochastic behavior conditioned on the agent’s actions, making the environment partially observable. We make two main contributions. First, we introduce a new reinforcement learning environment, RetaiL, based on real grocery store data and expert knowledge. This environment is highly stochastic, and presents a unique challenge for reinforcement learning practitioners. We show that uncertainty about the future behavior of the environment is not handled well by classical supply chain algorithms, and that distributional approaches are a good way to account for the uncertainty. Second, we introduce GTDQN, a distributional reinforcement learning algorithm that learns a generalized lambda distribution over the reward space. GTDQN provides a strong baseline for our environment. It outperforms other distributional reinforcement learning approaches in this partially observable setting, in both overall reward and generated waste.",
        "authors": "S. Jullien, M. Ariannezhad, P. Groth, et.al",
        "keywords": [
            "restocking",
            "reinforcement learning",
            "distributional approach"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=KSvr8A62MD",
        "pdf_src": "https://api2.openreview.net/pdf/a83694e6674ec0d3faaa983b116dce30ba2e5fc7.pdf",
        "Code_src": "",
        "Introduction": "Background: Inventory management plays an essential role in many industries such as retail where it's crucial to strike a balance between meeting consumer demands without excess stockpiling which leads to wastage.\n\nResearch Problem: How can inventory be effectively managed considering the uncertainties associated with predicting customers' purchase patterns?\n\nMethodology: The authors approach this issue using a novel framework inspired by reinforcement learning techniques adapted specifically within the context of retail environments dealing with perishable goods like groceries or fresh produce – referred to as \"RetaiL\" - which they created through integration of empirical datasets along with domain expertise regarding optimal stocking strategies.\n \nMain Contributions:\n1. They developed 'RetaiL', a simulation-based RL environment designed around realistic scenarios found at grocery stores aiming particularly towards handling stochasticity inherent when managing inventories under varying market conditions accurately.\n2. Introduced GTDQN; a Distributional Reinforcement Learning Algorithm tailored explicitly toward addressing partial observability challenges present during decision-making processes related to replenishment decisions concerning perishable products ensuring maximization profits whilst reducing waste levels significantly better than existing non-distributional counterparts tested against similar benchmarks provided therein.",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "A Stochastic Proximal Polyak Step Size",
        "abstract": "Recently, the stochastic Polyak step size (SPS) has emerged as a competitive adaptive step size scheme for stochastic gradient descent. Here we develop ProxSPS, a proximal variant of SPS that can handle regularization terms. Developing a proximal variant of SPS is particularly important, since SPS requires a lower bound of the objective function to work well. When the objective function is the sum of a loss and a regularizer, available estimates of a lower bound of the sum can be loose. In contrast, ProxSPS only requires a lower bound for the loss which is often readily available. As a consequence, we show that ProxSPS is easier to tune and more stable in the presence of regularization. Furthermore for image classification tasks, ProxSPS performs as well as AdamW with little to no tuning, and results in a network with smaller weight parameters. We also provide an extensive convergence analysis for ProxSPS that includes the non-smooth, smooth, weakly convex and strongly convex setting.",
        "authors": "F. Schaipp, R. M. Gower, M. Ulbrich",
        "keywords": [
            "Proximal Stochastic Polyak Step Size",
            "Regularization Terms",
            "Image Classification"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=jWr41htaB3",
        "pdf_src": "https://api2.openreview.net/pdf/131436e5a02fa92a95b654730647fc4c3b6f7a01.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses recent advancements in optimization algorithms specifically focusing on stochastic gradient descent methods where they propose a new algorithm called ProxSPS.\n\nResearch Problem: The problem addressed by this research revolves around improving existing stochastic adaptive step-size schemes like the Stochastic Polyak Step Size (SPS). These schemes are designed but not always effective when dealing with objectives functions containing regularization terms due to their reliance on having access to tight bounds or estimates over the entire objective space rather than just its gradients during updates.\n\nMethodology: To overcome these limitations posed by traditional SPS variants without requiring strict lower bounds from the full objective, authors introduce ProxSPS - a proximal version of SPS tailored especially for such scenarios involving regularization terms within the objective function. This involves modifying how the step sizes adapt based solely on information about the loss component while still benefiting from some properties of the original SPS method through proximal operators related to the regularization term.\n\nMain Contributions:\n1. **Proximal Variant**: They have developed a proximal adaptation of the SPS algorithm.\n2. **Ease of Tuning and Stability**: ProxSPS simplifies hyperparameter tuning compared to other adaptive algorithms because it does not require precise knowledge of the objective's lower bound; instead relying mainly on the loss value alone making it inherently more robust against noise present in typical machine learning settings including those found in image classification problems.\n3. **Performance Comparison**: For practical use cases – specifically image classification using neural networks -, empirical evidence shows that ProxSPS achieves similar performance levels comparable to state-of-the-art optimizers like AdamW yet maintains benefits concerning fewer trainable weights leading potentially to reduced computational costs at runtime \n4. **Convergence Analysis**: A comprehensive convergence theory was provided under various conditions covering both smooth and nonsmooth objectives along with different degrees of convexity ensuring theoretical guarantees across diverse contexts relevant to optimization challenges encountered commonly in practice",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "TimeSeAD: Benchmarking Deep Multivariate Time-Series Anomaly Detection",
        "abstract": "Developing new methods for detecting anomalies in time series is of great practical significance, but progress is hindered by the difficulty of assessing the benefit of new methods, for the following reasons. (1) Public benchmarks are flawed (e.g., due to potentially erroneous anomaly labels), (2) there is no widely accepted standard evaluation metric, and (3) evaluation protocols are mostly inconsistent. In this work, we address all three issues: (1) We critically analyze several of the most widely-used multivariate datasets, identify a number of significant issues, and select the best candidates for evaluation. (2) We introduce a new evaluation metric for time-series anomaly detection, which—in contrast to previous metrics—is recall consistent and takes temporal correlations into account. (3) We analyze and overhaul existing evaluation protocols and provide the largest benchmark of deep multivariate time-series anomaly detection methods to date. We focus on deep-learning based methods and multivariate data, a common setting in modern anomaly detection. We provide all implementations and analysis tools in a new comprehensive library for Time Series Anomaly Detection, called TimeSeAD.",
        "authors": "D. Wagner, T. Michels, F. C. Schulz, et.al",
        "keywords": [
            "time series anomaly detection",
            "evaluation metric",
            "benchmark"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=iMmsCI0JsS",
        "pdf_src": "https://api2.openreview.net/pdf/5db930d1d96d3d358e8f23f7eb2aef58697fea42.pdf",
        "Code_src": "",
        "Introduction": "Background: Detecting anomalies in time series has important applications; however, evaluating novel approaches' effectiveness remains challenging because public benchmarks may be flawed or lack universally agreed-upon standards.\n\nResearch Problem: This study aims to improve the assessment process through addressing flaws in commonly used datasets, introducing an improved evaluation metric that accounts for temporal dependencies within sequences rather than just focusing on precision alone as many current metrics do not consider these relationships adequately when measuring recall performance across different sequences over time.\n \nMethodology: The researchers first rigorously evaluate multiple popular multivariate datasets identifying key problems with their anomaly labeling quality before selecting high-quality ones suitable for further testing purposes. They then propose a novel recall-consistent metric designed specifically tailored towards capturing how well models can detect anomalies considering sequence orderliness - unlike traditional metrics focused solely on single instances without contextually understanding them relative position within larger patterns observed throughout each dataset's timeline. Additionally , they re-evaluate existing methodologies providing guidance toward developing more standardized practices while also contributing what appears currently being considered one of today’s largest collections available publicly accessible resources dedicated exclusively analyzing various machine learning algorithms applied detecting abnormalities found within multi-dimensional continuous streams recorded over periods varying lengths .\n\nMain Contributions: Their primary contributions include critiquing problematic aspects associated with some leading datasets utilized thus far ; creating innovative scoring system incorporating temporal dynamics during anomaly recognition tasks ; revisiting outdated procedures employed previously resulting from inconsistencies amongst evaluations conducted so far ; compiling vast repository containing diverse techniques pertaining detecting deviations occurring complex environments where numerous variables interact continuously changing circumstances . All these efforts culminate development release \"TimeSeAD\" – open-source software package offering users access necessary tools facilitating reproducible research endeavors related field time series anomaly detection leveraging latest advancements artificial intelligence technologies",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Understanding Noise-Augmented Training for Randomized Smoothing",
        "abstract": "Randomized smoothing is a technique for providing provable robustness guarantees against adversarial attacks while making minimal assumptions about a classifier. This method relies on taking a majority vote of any base classifier over multiple noise-perturbed inputs to obtain a smoothed classifier, and it remains the tool of choice to certify deep and complex neural network models. Nonetheless, non-trivial performance of such smoothed classifier crucially depends on the base model being trained on noise-augmented data, i.e., on a smoothed input distribution. While widely adopted in practice, it is still unclear how this noisy training of the base classifier precisely affects the risk of the robust smoothed classifier, leading to heuristics and tricks that are poorly understood. In this work we analyze these trade-offs theoretically in a binary classification setting, proving that these common observations are not universal. We show that, without making stronger distributional assumptions, no benefit can be expected from predictors trained with noise-augmentation, and we further characterize distributions where such benefit is obtained. Our analysis has direct implications to the practical deployment of randomized smoothing, and we illustrate some of these via experiments on CIFAR-10 and MNIST, as well as on synthetic datasets.",
        "authors": "A. Pal, J. Sulam",
        "keywords": [
            "noise-augmentation",
            "randomized smoothing",
            "robustness guarantees"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=fvyh6mDWFr",
        "pdf_src": "https://api2.openreview.net/pdf/0340418fb78d8ce7dc9aaf24bf920c1e80517b8e.pdf",
        "Code_src": "",
        "Introduction": "Background: Randomized smoothing aims at achieving provable robustness by perturbing the input during prediction time but requires few assumptions regarding the underlying classifier.\n\nResearch Question: How does the stochastic nature of the smoothing process affect its efficacy when applied after training?\n\nMethodology: The authors investigate this question through theoretical analysis within a binary classification framework using CogView, an image recognition task.\n\nMain Contributions:\n1. They demonstrate mathematically why simply applying noise augmentation or smoothing cannot universally improve the robustness of predictions.\n2. They identify specific conditions under which noise augmentation might indeed lead to improved robustness; however, they also note there's often little empirical evidence supporting those conditions due to complexities involved beyond their scope here—such as whether certain classes may have more variance than others affecting generalization across different domains.\n3. Their findings provide insights into designing better practices around implementing randomized smoothing techniques like those found in ImageNet pre-trained networks used today",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Do Vision-Language Pretrained Models Learn Composable Primitive Concepts?",
        "abstract": "Vision-language (VL) pretrained models have achieved impressive performance on multimodal reasoning and zero-shot recognition tasks. Many of these VL models are pretrained on unlabeled image and caption pairs from the internet. In this paper, we study whether representations of primitive concepts–such as colors, shapes, or the attributes of object parts–emerge automatically within these pretrained VL models. We propose a two-step framework, Compositional Concept Mapping (CompMap), to investigate this. CompMap\nfirst asks a VL model to generate concept activations with text prompts from a predefined list of primitive concepts, and then learns to construct an explicit composition model that maps the primitive concept activations (e.g. the likelihood of black tail or red wing) to com-\nposite concepts (e.g. a red-winged blackbird). We demonstrate that a composition model can be designed as a set operation, and show that a composition model is straightforward for machines to learn from ground truth primitive concepts (as a linear classifier). We thus\nhypothesize that if primitive concepts indeed emerge in a VL pretrained model, its primitive concept activations can be used to learn a composition model similar to the one designed by experts. We propose a quantitative metric to measure the degree of similarity, and refer to the metric as the interpretability of the VL models’ learned primitive concept representations. We also measure the classification accuracy when using the primitive concept activations and the learned composition model to predict the composite concepts, and refer to it as the usefulness metric. Our study reveals that state-of-the-art VL pretrained models learn primitive concepts that are highly useful for fine-grained visual recognition on the CUB dataset, and compositional generalization tasks on the MIT-States dataset. However,\nwe observe that the learned composition models have low interpretability in our qualitative analyses. Our results reveal the limitations of existing VL models, and the necessity of pretraining objectives that encourage the acquisition of primitive concepts.",
        "authors": "T. Yun, U. Bhalla, E. Pavlick, et.al",
        "keywords": [
            "primitive concepts",
            "Vision-Language Pretrained Models",
            "Compositionality"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=YwNrPLjHSL",
        "pdf_src": "https://api2.openreview.net/pdf/ad208a50a9a436c444442d1dc384cbd48709f5a2.pdf",
        "Code_src": "",
        "Introduction": "Background: Vision-language (VL) pretrained models perform well on various multimodal tasks such as reasoning and zero-shot recognition due to their ability to understand both images and language. These models often rely on large-scale datasets containing unlabeled image-caption pairs scraped from the web.\n\nResearch Question: This research aims at understanding how basic concepts like color, shape, or part attributes might naturally arise during the training process without any specific instructions related to them being given beforehand.\n \nMethodology: The authors introduce \"Compositional Concept Mapping\" (CompMap), which involves asking a VL model to activate certain concepts based on textual prompts drawn from a predefined list (\"primitive concepts\"). Then they train another model to map these activated primitives into more complex ones called \"composite concepts\". They design the mapping function similarly to a set operation where each primitive activation corresponds to elements inside sets - making learning easier since it resembles a simple linear classifier task.\n\nMain Contributions:\n1. Demonstrating that a composition model could theoretically mirror a set theory approach allowing for easy machine learning through grounded truths about primitive concepts.\n2. Developing a novel quantitative metric named 'interpretability' measuring how closely the learned primitive concept representations align with those expected under human expertise standards; and\n3. Showing empirical evidence demonstrating that top-performing VL models do pick up valuable primitive knowledge but struggle significantly regarding interpretability – highlighting current limitations needing further attention towards better pre-training objectives promoting foundational concept learning capabilities",
        "Topic": "Large Language Models"
    },
    {
        "title": "Group Fairness in Reinforcement Learning",
        "abstract": "We pose and study the problem of satisfying fairness in the online Reinforcement Learning (RL) setting. We focus on the group notions of fairness, according to which agents belonging to different groups should have similar performance based on some given measure. We consider the setting of maximizing return in an unknown environment (unknown transition and reward function) and show that it is possible to have RL algorithms that learn the best fair policies without violating the fairness requirements at any point in time during the learning process. In the tabular finite-horizon episodic setting, we provide an algorithm that combines the principle of optimism and pessimism under uncertainty to achieve zero fairness violation with arbitrarily high probability while also maintaining sub-linear regret guarantees. For the high-dimensional Deep-RL setting, we present algorithms based on the performance-difference style approximate policy improvement update step and we report encouraging empirical results on various traditional RL-inspired benchmarks showing that our algorithms display the desired behavior of learning the optimal policy while performing a fair learning process.",
        "authors": "H. Satija, A. Lazaric, M. Pirotta, et.al",
        "keywords": [
            "fairness",
            "reinforcement learning",
            "deep rl"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=JkIH4MeOc3",
        "pdf_src": "https://api2.openreview.net/pdf/b268537c8c3d8bb803a1bbd304240b409f4a97c6.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the issue of achieving fairness within the context of reinforcement learning (RL), particularly focusing on group fairness where agents from distinct groups must exhibit comparable performance as measured by certain criteria.\n\nResearch Problem: The challenge lies in developing RL strategies capable of optimizing for collective rewards or returns across multiple agent groups whilst adhering to strict fairness constraints throughout the learning phase rather than just after convergence when these may be violated due to dynamic changes over time.\n\nMethods: To tackle this complex task effectively:\n1. Tabular Finite-Horizon Episodic Setting - An algorithm was developed combining optimistic and pessimistic approaches towards uncertainty management.\n2. High-Dimensional Deep RL Setting - Algorithms were proposed using performance difference-based approximate policy improvements steps ensuring both fairness maintenance and sub-linear regret bounds.\n\nMain Contributions: \n1. A novel approach integrating optimism and pessimism principles into one framework designed specifically for the tabular RL scenario ensures no unfairness violations occur even if probabilities are not known exactly; this provides probabilistic guarantees against such deviations occurring frequently enough through its use.\n2. For more complex environments requiring deep neural networks like those found commonly used nowadays but still dealing with stochastic dynamics encountered often in real-world applications , they introduced new updates leveraging differences between observed performances among agents allowing them maintain fairness conditions along their learning trajectories despite potential variations arising naturally from exploration/exploitation trade-offs inherent within RL processes themselves .",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
        "abstract": "Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models.\nTo address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG- bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood develop- ment, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google- internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit \"breakthrough\" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.",
        "authors": "A. Srivastava, A. Rastogi, A. Rao, et.al",
        "keywords": [
            "scale",
            "multimodal understanding",
            "interpretability"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=uyTL5Bvosj",
        "pdf_src": "https://api2.openreview.net/pdf/a836a2d7a89d671116fc7276d4a5284a8903fe93.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses how language models have shown significant improvements along with novel qualitative abilities due to increased scale.\n\nResearch Problem:\nDespite potential transformative impacts posed by such advancements within language models, there exists limited understanding regarding what exactly they're capable of doing now versus where those limits lie—both immediate and long-term implications need addressing before further developments occur which may lead into unforeseen consequences like exacerbating societal harms if not managed properly.\n\nMethodology:\nTo tackle said challenges head-on, an evaluation framework called Beyond the Imitation Game benchmark (BIG-bench) was introduced comprising over two hundred tasks curated collectively through contributions made by more than four hundred fifty researchers spread out amongst one hundred thirty-two different organizations worldwide covering various domains including linguistics, mathematics, cognitive sciences etcetera—all deemed challenging enough presumably surpassing existing capabilities offered up by today’s state-of-the-art approaches available till date.\n\nMain Contributions:\nBIG-bench evaluates three types of advanced neural network architectures namely OpenAI’s GPT series alongside internally developed dense transformer variants used inside Google plus sparse versions inspired by SWITCH architecture against aforementioned benchmarks while also involving humans performing each task individually serving as reference baselines comparing results accordingly revealing insights about scaling trends observed among them – \n1) Performance improves significantly relative to size increase however remains suboptimal overall;\n2) Similar performances were noted irrespective of whether using dense or sparse architectures although latter did show some advantages;\n3) Tasks improving incrementally tend towards heavy reliance upon either extensive knowledge bases/memorization skills whereas breakthroughs seem contingent upon multi-step processes/components or being sensitive towards certain metrics;\n4) Social biases exhibited worsen with growing complexity levels under unclear contexts despite prompts designed mitigating same extentually.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Graph Neural Networks Designed for Different Graph Types: A Survey",
        "abstract": "Graphs are ubiquitous in nature and can therefore serve as models for many practical but also\ntheoretical problems. For this purpose, they can be defined as many different types which\nsuitably reflect the individual contexts of the represented problem. To address cutting-edge\nproblems based on graph data, the research field of Graph Neural Networks (GNNs) has\nemerged. Despite the field’s youth and the speed at which new models are developed, many\nrecent surveys have been published to keep track of them. Nevertheless, it has not yet\nbeen gathered which GNN can process what kind of graph types. In this survey, we give a\ndetailed overview of already existing GNNs and, unlike previous surveys, categorize them\naccording to their ability to handle different graph types and properties. We consider GNNs\noperating on static and dynamic graphs of different structural constitutions, with or without\nnode or edge attributes. Moreover, we distinguish between GNN models for discrete-time or\ncontinuous-time dynamic graphs and group the models according to their architecture. We\nfind that there are still graph types that are not or only rarely covered by existing GNN\nmodels. We point out where models are missing and give potential reasons for their absence.",
        "authors": "J. Thomas, A. Moallemy-oureh, S. Beddar-wiesing, et.al",
        "keywords": [
            "Graph Neural Networks",
            "Graph Types",
            "Model Categorization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=h4BYtZ79uy",
        "pdf_src": "https://api2.openreview.net/pdf/461ffae580164b5054977642af4155a9d3d42d5e.pdf",
        "Code_src": "",
        "Introduction": "Background: Graphs play an important role in various fields due to their widespread existence both in natural phenomena and human-made systems.\n\nResearch Problem: The diversity of graph types makes it challenging to determine how well each type is supported by current Graph Neural Network (GNN) models.\n \nMethod: This paper provides a comprehensive review of existing GNNs from multiple perspectives including structure, dynamics, node/edge attributes etc., and classifies these networks into categories depending on whether they support specific graph types and characteristics.\n\nMain Contributions:\n1. A detailed classification system for existing GNNs considering several factors such as graph structures, dynamism, presence of node/edge attributes among others;\n2. Identification of gaps within currently available GNN solutions regarding coverage across diverse graph types; \n3. Suggested explanations why certain graph types may lack representation through present-day approaches along with recommendations moving forward towards more inclusive architectures",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Generalization bounds for Kernel Canonical Correlation Analysis",
        "abstract": "We study the problem of multiview representation learning using kernel canonical correlation analysis (KCCA) and establish non-asymptotic bounds on generalization error for regularized empirical risk minimization. In particular, we give fine-grained high-probability bounds on generalization error ranging from $O(n^{-1/6})$  to $O(n^{-1/5})$ depending on underlying distributional properties, where $n$ is the number of data samples. For the special case of finite-dimensional Hilbert spaces (such as linear CCA), our rates improve, ranging from $O(n^{-1/2})$ to $O(n^{-1})$. Finally, our results generalize to the problem of functional canonical correlation analysis over abstract Hilbert spaces.",
        "authors": "E. Ullah, R. Arora",
        "keywords": [
            "multiview representation learning",
            "kernel canonical correlation analysis",
            "generalization error"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=KwWKB9Bqam",
        "pdf_src": "https://api2.openreview.net/pdf/8459b7993e2f29566ad8e13d5f7f224cf496de28.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the issue of multiview representation learning through Kernel Canonical Correlation Analysis (KCCA). This involves finding a low-dimensional space that captures common features across multiple views or modalities.\n\nResearch Question: How can one learn representations in such a way so they are robust against noise? Specifically, what are the theoretical guarantees regarding how well these learned representations perform when new unseen data arrives?\n\nMethods: The authors employ Regularized Empirical Risk Minimization techniques with KCCA which allows them to capture complex relationships between different datasets while penalizing complexity by regularization terms like $\\ell_2$ norm.\n\nMain Contributions:\n- They provide non-asymptotic bounds on the generalization errors under various assumptions about the distributions.\n- These bounds range widely based on the nature of the dataset; some cases achieve an optimal rate of convergence up to \\( O(n^{-1/6}) \\) whereas others may be slower at \\( O(n^{-1/5}) \\).\n- When dealing with specific scenarios within finite-dimensional Hilbert spaces – akin to Linear Canonical Correlation Analysis - their findings suggest even better rates down to \\( O(n^{-1/2}) \\) or \\( O(n^{-1}) \\).\n- Their approach extends beyond standard Euclidean settings into the realm of Functional Canonical Correlation Analysis conducted upon Abstract Hilbert Spaces expanding its applicability further than classical machine learning problems involving only scalar-valued functions/data points.",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Learning Identity-Preserving Transformations on Data Manifolds",
        "abstract": "Many machine learning techniques incorporate identity-preserving transformations into their models to generalize their performance to previously unseen data. These transformations are typically selected from a set of functions that are known to maintain the identity of an input when applied (e.g., rotation, translation, flipping, and scaling). However, there are many natural variations that cannot be labeled for supervision or defined through examination of the data. As suggested by the manifold hypothesis, many of these natural variations live on or near a low-dimensional, nonlinear manifold. Several techniques represent manifold variations through a set of learned Lie group operators that define directions of motion on the manifold. However, these approaches are limited because they require transformation labels when training their models and they lack a method for determining which regions of the manifold are appropriate for applying each specific operator. We address these limitations by introducing a learning strategy that does not require transformation labels and developing a method that learns the local regions where each operator is likely to be used while preserving the identity of inputs. Experiments on MNIST and Fashion MNIST highlight our model's ability to learn identity-preserving transformations on multi-class datasets. Additionally, we train on CelebA to showcase our model's ability to learn semantically meaningful transformations on complex datasets in an unsupervised manner. ",
        "authors": "M. C. Connor, K. Fallah, C. J. Rozell",
        "keywords": [
            "manifold learning",
            "identity preservation",
            "semi-supervised learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=gyhiZYrk5y",
        "pdf_src": "https://api2.openreview.net/pdf/f240937d2bf9a63253af8a207e672d56aded9446.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses how existing machine learning methods often use identity-preserving transformations such as rotation, translation, flipping, and scaling within their models so those models can perform well with new types of data it has never seen before.\n\nResearch Problem: While this approach works fine under controlled conditions using predefined transformations like the ones mentioned above, real-world variability isn't always captured due to its complexity; thus, more advanced solutions need to find ways around labeling all possible transformations manually—especially since some variations might even defy simple categorization—or defining them via inspection alone without relying solely on pre-defined transformations.\n \nMethods: To tackle both issues—the requirement for manual labelling during training time along with identifying suitable application areas for different transformation operators—they propose two main contributions:\n1. A novel learning strategy designed specifically aimed at avoiding needing any form of explicit label information about transformations throughout training processes;\n2. An algorithmic framework capable of automatically discovering localized zones across manifolds onto which various transformation operators may apply most effectively whilst still maintaining original input identities intact.\n\nMain Contributions: Their work introduces significant advancements over current state-of-the-art practices concerning generalizing ML models beyond supervised settings involving predefined transformations only—it allows us to extend beyond what was achievable up until now towards handling broader ranges naturally occurring complexities found outside typical datasets' scope effortlessly!",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "Reusable Options through Gradient-based Meta Learning",
        "abstract": "Hierarchical methods in reinforcement learning have the potential to reduce the amount of decisions that the agent needs to perform when learning new tasks. However, finding a reusable useful temporal abstractions that facilitate fast learning remains a challenging problem. Recently, several deep learning approaches were proposed to learn such temporal abstractions in the form of options in an end-to-end manner. In this work, we point out several shortcomings of these methods and discuss their potential negative consequences. Subsequently, we formulate the desiderata for reusable options and use these to frame the problem of learning options as a gradient-based meta-learning problem. This allows us to formulate an objective that explicitly incentivizes options which allow a higher-level decision maker to adjust in few steps to different tasks. Experimentally, we show that our method is able to learn transferable components which accelerate learning and performs better than existing prior methods developed for this setting. Additionally, we perform ablations to quantify the impact of using gradient-based meta-learning as well as other proposed changes.",
        "authors": "D. Kuric, H. V. Hoof",
        "keywords": [
            "option learning",
            "hierarchical reinforcement learning",
            "meta-learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=qdDmxzGuzu",
        "pdf_src": "https://api2.openreview.net/pdf/6c530fbecab16a5a519d45dfef7412ef39c4220b.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses hierarchical reinforcement learning where agents can make fewer decisions by leveraging reusable temporal abstractions during task learning.\nResearch Problem: Finding effective and reusable temporal abstractions (\"options\") has been difficult due to challenges with scalability across various tasks.\n\nMethodology: The authors propose framing option learning within a gradient-based meta-learning framework aimed at creating options that enable high-level adjustments quickly on novel tasks without requiring extensive relearning from scratch.\n\nMain Contributions:\n1. Identification of limitations in previous works attempting to directly learn options through deep learning models like policy gradients or actor-critic architectures while addressing issues related to generalizability over multiple tasks.\n2. Definition of criteria for desirable reusable options based on practical considerations about how they should be structured so they are both efficient learners themselves but also beneficial for subsequent learning processes involving those options.\n3. Development of a novel approach grounded in meta-learning principles designed specifically around these requirements; it encourages learned options whose utility lies not only in immediate performance gains under specific conditions but extends beyond them into broader contexts allowing adaptation more rapidly upon encountering similar situations elsewhere \n4. Experimental validation demonstrating superior performance compared against baseline state-of-the-art techniques applied similarly focusing solely on transferring knowledge between tasks rather than incorporating any notion of abstraction reuse via meta-learning strategies introduced here.",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Bidirectional View based Consistency Regularization for Semi-Supervised Domain Adaptation",
        "abstract": "Distinguished from unsupervised domain adaptation (UDA), semi-supervised domain adaptation (SSDA) could access a few labeled target samples during learning additionally. Although achieving remarkable progress,  target supervised information is easily overwhelmed by massive source supervised information, as there are many more labeled source samples than those in the target domain. In this work, we propose a novel method BVCR that better utilizes the supervised information by three schemes, i.e., modeling, exploration, and interaction. In the modeling scheme, BVCR models the source supervision and target supervision separately to avoid target supervised information being overwhelmed by source supervised information and better utilize the target supervision. Besides, as both  supervised information naturally offer distinct views for the target domain, the exploration scheme performs intra-domain consistency regularization to better explore target information with bidirectional views. Moreover, as both views are complementary to each other,  the interaction scheme introduces inter-domain consistency regularization to activate information interaction bidirectionally. Thus, the proposed method is elegantly symmetrical by design and easy to implement. Extensive experiments are conducted, and the results show the effectiveness of the proposed method.",
        "authors": "Y. Du, J. Juan, H. Luo, et.al",
        "keywords": [
            "BVCR",
            "Semi-supervised domain adaptation",
            "Inter-domain consistency regularization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=WVwnccBJLz",
        "pdf_src": "https://api2.openreview.net/pdf/eb780b36c0dd4e545a4abc29ebef01d43fa8dee2.pdf",
        "Code_src": "",
        "Introduction": "Background: Semi-supervised domain adaptation (SSDA) aims to adapt a model trained on one dataset (source domain) to another related but different dataset (target domain). Unlike fully supervised methods which require large amounts of labeled data or unsupervised domain adaptation where no labels are available at all, SSDA leverages some labeled examples while also using unlabeled ones.\n\nResearch Problem: The main challenge in SSDA lies in how to effectively integrate limited target supervised information into the training process when it's outnumbered significantly by abundant source supervised information due to having far fewer labeled instances compared to the source domain.\n\nMethod: To address these challenges, authors introduce a new approach called \"Bidirectional View Consistency Regularization\" (BVCR). This consists of:\n\n1. Modeling Scheme - They separate the source supervision and target supervision so they can be modeled independently without interference between them.\n2. Exploration Scheme - Since supervised information inherently provides diverse perspectives about the target domain, an intra-domain consistency regularization technique is employed allowing the algorithm to deeply explore the target information through two-way viewpoints.\n3. Interaction Scheme - Recognizing that neither view alone captures everything needed; therefore, an inter-domain consistency regularization mechanism is introduced promoting mutual activation across domains ensuring comprehensive understanding via cross-view interactions.\n\nMain Contributions: The key contribution here is not only developing a symmetric and straightforward-to-implement framework named BVCR designed specifically around SSDA’s peculiarities involving imbalanced label distributions among domains – but demonstrating its efficacy empirically too—through extensive experimental validation showing superior performance over existing state-of-the-art approaches under various conditions relevant within SSDA tasks.",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "FLUID: A Unified Evaluation Framework for Flexible Sequential Data",
        "abstract": "Modern machine learning methods excel when training data is IID, large-scale, and well labeled. Learning in less ideal conditions remains an open challenge. The sub-fields of few-shot, continual, transfer, and representation learning have made substantial strides in learning under adverse conditions, each affording distinct advantages through methods and insights. These methods address different challenges such as data arriving sequentially or scarce training examples, however often the difficult conditions an ML system will face over its lifetime cannot be anticipated prior to deployment. Therefore, general ML systems which can handle the many challenges of learning in practical settings are needed. To foster research towards the goal of general ML methods, we introduce a new unified evaluation framework – FLUID (Flexible Sequential Data). FLUID integrates the objectives of few-shot, continual, transfer, and representation learning while enabling comparison and integration of techniques across these subfields. In FLUID, a learner faces a stream of data and must make sequential predictions while choosing how to update itself, adapt quickly to novel classes, and deal with changing data distributions; while accounting for the total amount of compute. We conduct experiments on a broad set of methods which shed new insight on the advantages and limitations of current techniques and indicate new research problems to solve. As a starting point towards more general methods, we present two new baselines which outperform other evaluated methods on FLUID.",
        "authors": "M. Wallingford, A. Kusupati, K. Alizadeh, et.al",
        "keywords": [
            "Few-Shot Learning",
            "Continual Learning",
            "Transfer Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=UvJBKWaSSH",
        "pdf_src": "https://api2.openreview.net/pdf/cbf27e00cf3d02390517f5211ec81e6d231afb0b.pdf",
        "Code_src": "",
        "Introduction": "Background: Modern machine learning models perform best if they're trained using independent and identically distributed (IID), large datasets that come from well-labeled sources. However, real-world scenarios may not always provide this kind of dataset quality due to various reasons like limited resources during model development.\n\nResearch Problem: How do you design machine learning algorithms capable of handling diverse challenging situations where data might arrive sequentially rather than all at once? This includes dealing with small amounts of training samples (\"few-shot\"), adapting continuously without forgetting previous knowledge (\"continual learning\"), transferring learned information effectively between tasks (\"transfer learning\"), and representing complex concepts efficiently (\"representation learning\").\n\nMethods: Addressing this problem requires developing flexible frameworks designed specifically around the characteristics mentioned above—sequential nature of data arrival along with continuous adaptation needs within constraints related to computational efficiency—all encapsulated into one comprehensive evaluation setup called FLUID (Flexible Sequential Data).\n\nMain Contributions:\n1. **Unified Evaluation Framework**: Introducing FLUID—a versatile platform integrating four key aspects of machine learning performance: few-shot, continual, transfer, and representation learning.\n2. **Sequential Prediction and Adaptation**: Within FLUID, learners need to predict sequentially based on incoming streams of data points whilst also updating themselves dynamically according to their observations—it's about making decisions regarding self-updating strategies throughout time.\n3. **Novel Class Handling**: Learners should rapidly adapt upon encountering previously unseen classes—an essential capability required by any robust machine learning algorithm operating beyond controlled environments.\n4. **Dynamic Distributions**: Models developed via FLUID would ideally cope with changes occurring naturally—or artificially introduced—to the distribution patterns seen among input features—the ability \"to learn\" means adjusting parameters appropriately given evolving contexts.\n5. **Compute Efficiency Considerations**: While optimizing predictive accuracy against dynamic inputs, it’s crucially important considering resource allocation constraints imposed by actual operational requirements—this ensures scalable solutions applicable even amidst varying hardware capabilities.\n6. **New Baselines**: Two innovative baseline approaches were proposed demonstrating improved performances compared existing benchmarks tested within FLUID setting thus paving way toward future advancements in creating broader applicability amongst machine learning methodologies",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Improving Generalization with Approximate Factored Value Functions",
        "abstract": "Reinforcement learning in general unstructured MDPs presents a challenging learning problem. However, certain MDP structures, such as factorization, are known to simplify the learning problem. This fact is often not useful in complex tasks with high-dimensional state spaces which do not usually exhibit such structure, and even if the structure is present, it is typically unknown. In this work, we instead turn this observation on its head. Instead of developing algorithms for structured MDPs, we propose a representation learning algorithm that approximates an unstructured MDP with one that has factorized structure. We then use these factors as a more convenient representation of the state for downstream learning. The particular structure that we leverage is reward factorization, which defines a more compact class of MDPs that admit factorized value functions. We empirically verify the effectiveness of our approach in terms of faster training (better sample complexity) and robust zero-shot transfer (better generalization) on the ProcGen benchmark and the MiniGrid environments.",
        "authors": "S. Sodhani, S. Levine, A. Zhang",
        "keywords": [
            "factorization",
            "reinforcement learning",
            "representation learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LwEWrrKyja",
        "pdf_src": "https://api2.openreview.net/pdf/0a8c363ce79348016c850df03f8cf63e3dbe95f1.pdf",
        "Code_src": "",
        "Introduction": "Background: Reinforcement learning in unstructured Markov Decision Processes (MDPs) can be difficult due to their lack of inherent structure.\n\nResearch Problem: How to effectively learn from unstructured MDPs when they have no apparent structure or when there's uncertainty about whether any underlying structure exists?\n\nMethod: Propose a novel representation learning algorithm capable of transforming an unstructured MDP into one having a factorized structure; utilize the resulting factors within the original environment during subsequent reinforcement learning stages by using them directly rather than relying solely on raw states.\n\nMain Contributions:\n1. Introduced a new method called \"reward factorization\" - a specific type of factorization where rewards themselves form a factorizable structure.\n2. Demonstrated empirical improvements over standard approaches through reduced training time (\"sample complexity\") across various benchmarks including ProcGen and MiniGrid.\n3. Showed enhanced performance under zero-shot transfer conditions – suggesting better generalization beyond seen data points compared to traditional methods without leveraging factorization techniques.",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Containing a spread through sequential learning: to exploit or to explore?",
        "abstract": "The spread of an undesirable contact process, such as an infectious disease (e.g. COVID-19), is contained through testing and  isolation of infected nodes. The temporal and spatial evolution of the process (along with containment through isolation) render such detection as fundamentally different from active search detection strategies. In this work, through an active learning approach, we design testing and isolation strategies to contain the spread and minimize the cumulative infections under a given test budget. We prove that the objective can be optimized, with performance guarantees,  by greedily selecting the nodes to test.  We further design reward-based methodologies that effectively minimize an upper bound on the cumulative infections and are computationally more tractable in large networks. These policies, however, need knowledge about the nodes' infection probabilities which are dynamically changing and have to be learned by sequential testing. We develop a message-passing framework for this purpose and, building on that,  show novel tradeoffs between exploitation of knowledge through reward-based heuristics and exploration of the unknown through a carefully designed probabilistic testing. The tradeoffs are fundamentally distinct from the classical counterparts under active search or multi-armed bandit problems (MABs). We provably show the necessity of exploration in a stylized network and show through simulations that exploration can outperform exploitation in various synthetic and real-data networks depending on the parameters of the network and the spread.",
        "authors": "X. Chen, H. Nikpey, J. Kim, et.al",
        "keywords": [
            "contact process",
            "active learning",
            "cumulative infections"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=qvRWcDXBam",
        "pdf_src": "https://api2.openreview.net/pdf/2d67bad875e0a73b3616c0a3f73681c70de3de1a.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper addresses the problem of containing the spread of an undesirable contact process like an infectious disease using testing and isolation strategy. Unlike active search detection strategies where the goal is to find all infected nodes quickly, here the focus is on minimizing cumulative infections within a limited test budget.\n\nResearch Question: How do you design effective testing and isolation strategies while considering dynamic changes in node infection probabilities?\n\nMethodology: The authors adopt an active learning approach based on greedy selection algorithms aiming at optimizing objectives related to containing the spread without exceeding specified test budgets.\nThey also introduce reward-based methodologies focusing on minimizing cumulative infections but require prior knowledge regarding node infection probabilities whose dynamics necessitate continuous learning via sequential tests.\n\nMain Contributions:\n1. Proving that the optimization objective concerning containing the spread could be achieved efficiently when greedily choosing nodes for testing;\n2. Developing reward-based methods capable of bounding cumulative infections accurately yet being computationally feasible especially suited larger networks; \n3. Proposing a novel message-passing framework enabling iterative updates towards optimal decisions informed by both exploiting existing information through rewards and exploring uncharted territories employing probabilistic testing techniques; \n4. Demonstrating empirically how these exploratory approaches may lead to better outcomes compared traditional exploitation-focused ones across diverse scenarios including synthetic datasets alongside actual-world applications contingent upon specific attributes pertaining to each scenario's topology along with its propagation dynamics.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "The Low-Rank Simplicity Bias in Deep Networks",
        "abstract": "Modern deep neural networks are highly over-parameterized compared to the data on which they are trained, yet they often generalize remarkably well. A flurry of recent work has asked: why do deep networks not overfit to their training data? In this work, we make a series of\nempirical observations that investigate and extend the hypothesis that deeper networks are inductively biased to find solutions with lower effective rank embeddings. We conjecture that this bias exists because the volume of functions that maps to low effective rank embedding\nincreases with depth. We show empirically that our claim holds true on finite width linear and non-linear models on practical learning paradigms and show that on natural data, these are often the solutions that generalize well. We then show that the simplicity bias exists\nat both initialization and after training and is resilient to hyper-parameters and learning methods. We further demonstrate how linear over-parameterization of deep non-linear models can be used to induce low-rank bias, improving generalization performance on CIFAR and\nImageNet without changing the modeling capacity.",
        "authors": "M. Huh, H. Mobahi, R. Zhang, et.al",
        "keywords": [
            "deep networks",
            "overparameterization",
            "generalization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=bCiNWDmlY2",
        "pdf_src": "https://api2.openreview.net/pdf/70b734be1d1ef31418842da413c2467b927b17a7.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses an intriguing phenomenon observed within modern deep neural networks where despite being significantly more complex than necessary for the task at hand based on empirical evidence showing high degrees of parameter redundancy or \"overparameterization,\" such networks frequently perform exceptionally when generalized beyond their training set.\n\nResearch Question:\nThe central question addressed by researchers revolves around understanding whether there's something inherent about the architecture design leading to robust generalization capabilities even against potential overfitting risks associated with excessive complexity.\n\nMethodology:\nTo explore answers towards addressing research questions posed above, authors conduct several empirical experiments involving various architectures including finite-width linear and nonlinear models across different machine learning tasks like classification problems on datasets such as CIFAR10 and ImageNet. They also examine if any biases exist during initializations phases before training begins along with post-training states regarding model weights' tendency toward finding representations with reduced dimensionality.\n\nMain Contributions:\nThis study makes three primary contributions:\n\n1. It introduces new insights into the nature of inductive biases present in deep networks suggesting that one reason behind good generalization might stem from networks having a built-in preference for solutions represented through lower-dimensional embeddings.\n2. Empirically validates its findings using extensive experimental results demonstrating that indeed, networks tend to converge onto solutions featuring less complex embeddings irrespective of whether it’s during initialization phase itself right up until final trained state regardless of hyperparameters chosen nor learning algorithms applied; \n3. Further extends upon previous works by proposing novel ways leveraging linear overparameterization techniques specifically designed explicitly aiming at inducing preferred low-rank structures thus enhancing overall generalization performances seen particularly notably while working with large-scale image recognition benchmarks like CIFAR100 or ImageNet dataset sets respectively without altering original modeling capacities available within those architectures themselves.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "A Halfspace-Mass Depth-Based Method for Adversarial Attack Detection",
        "abstract": "Despite the widespread use of deep learning algorithms, vulnerability to adversarial attacks is still an issue limiting their use in critical applications. Detecting these attacks is thus crucial to build reliable algorithms and has received increasing attention in the last few years.\nIn this paper, we introduce the  HalfspAce Mass dePth dEtectoR (HAMPER), a new method to detect adversarial examples by leveraging the concept of data depths, a statistical notion that provides center-outward ordering of points with respect to (w.r.t.) a probability distribution. In particular, the halfspace-mass (HM) depth exhibits attractive properties such as computational efficiency, which makes it a natural candidate for adversarial attack detection in high-dimensional spaces. Additionally, HM is non differentiable making it harder for attackers to directly attack HAMPER via gradient based-methods. We evaluate HAMPER in the context of supervised adversarial attacks detection across four benchmark datasets. \nOverall, we empirically show that HAMPER consistently outperforms SOTA methods. In particular, the gains are 13.1% (29.0%) in terms of AUROC (resp. FPR) on SVHN, 14.6% (25.7%) on CIFAR10 and 22.6% (49.0%) on CIFAR100 compared to the best performing method.",
        "authors": "M. Picot, F. Granese, G. Staerman, et.al",
        "keywords": [
            "adversarial attack detection",
            "data depth",
            "halfspace-mass depth"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=YtU0nDb5e8",
        "pdf_src": "https://api2.openreview.net/pdf/d5ec4cfeee6d2ef8b029f70f62cd099ba8b562e1.pdf",
        "Code_src": "",
        "Introduction": "Background: The background of this research lies within the realm of machine learning security where there exists significant concern over the susceptibility of neural networks trained using deep learning techniques against adversarial attacks – subtle perturbations introduced into input data designed specifically to fool models.\n\nResearch Problem: The problem addressed here concerns how to effectively identify whether or not given inputs have been tampered through adversarial means so they can be filtered before being used further down the pipeline leading towards potentially harmful outcomes like misclassification decisions made during autonomous driving scenarios etc.\n\nMethodology: To tackle said challenge, authors propose \"HalfspAce Mass dePth dEtectoR\" (HAMPER). This novel approach exploits what's known as 'data depths' - essentially distances from each point in feature space relative to some underlying probability distribution modelled around normal training samples; henceforth referred to as mass-depth functions. Specifically focusing on halfspace mass (HM) depth due its desirable characteristics including computational tractability even when dealing with large dimensions typical seen nowadays in practical ML tasks along with robustness since it does not rely on gradients needed typically required by other defense mechanisms allowing adversaries less room left open up avenues exploiting those weaknesses.\n\nMain Contributions: Empirical validation conducted demonstrates superior performance achieved by employing HAMPER compared existing state-of-the-art (SOTA) solutions across various benchmarks datasets under consideration (SVHN, CIFAR10 & CIFAR100). Specifically improvements were observed measured both accuracy metrics (AUROC) and false positive rate (FPR); notably better results obtained particularly noticeable especially when considering larger dataset sizes (CIFAR100).\n\nConclusion: Overall findings suggest that integrating HAMPER could significantly enhance current defenses deployed guarding against maliciously crafted adversarial examples thereby bolstering trustworthiness reliability artificial intelligence systems employed mission-critical domains requiring utmost precision dependability assurance provided by robust detection methodologies capable identifying potential threats early stages processing pipelines ensuring safety integrity maintained throughout entire workflow lifecycle management process involved deploying utilizing advanced technologies today’s digital world relies heavily upon day after day basis operations carried forward seamlessly without interruption downtime occurrences compromising overall system functionality operationally speaking long run sustainability perspective viewpoint taken account considerations moving ahead future developments advancements anticipated taking shape shaping landscape tomorrow onwards beyond present time horizon currently existent prevailing conditions circumstances surrounding us right now moment captured snapshot timeframe considered duration period covered study undertaken presented report document written authored penned",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Identification of Negative Transfers in Multitask Learning Using Surrogate Models",
        "abstract": "Multitask learning is widely used in practice to train a low-resource target task by augmenting it with multiple related source tasks. Yet, naively combining all the source tasks with a target task does not always improve the prediction performance for the target task due to negative transfers. Thus, a critical problem in multitask learning is identifying subsets of source tasks that would benefit the target task. This problem is computationally challenging since the number of subsets grows exponentially with the number of source tasks; efficient heuristics for subset selection does not always capture the relationship between task subsets and multitask learning performances. In this paper, we introduce an efficient procedure to address this problem via surrogate modeling. In surrogate modeling, we sample (random) subsets of source tasks and precompute their multitask learning performances; Then, we approximate the precomputed performances with a linear regression model that can also be used to predict the multitask performance of unseen task subsets. We show theoretically and empirically that fitting this model only requires sampling linearly many subsets in the number of source tasks. The fitted model provides a relevance score between each source task and the target task; We use the relevance scores to perform subset selection for multitask learning by thresholding. Through extensive experiments, we show that our approach predicts negative transfers from multiple source tasks to target tasks much more accurately than existing task affinity measures. Additionally, we demonstrate that for five weak supervision datasets, our approach consistently improves upon existing optimization methods for multi-task learning.",
        "authors": "D. Li, H. Nguyen, H. R. Zhang",
        "keywords": [
            "subset selection",
            "surrogate modeling",
            "multitask learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=KgfFAI9f3E",
        "pdf_src": "https://api2.openreview.net/pdf/009ce378dd4a236fcb70527a11bc5b73b3b3d487.pdf",
        "Code_src": "",
        "Introduction": "Background: Multitask learning has been extensively applied as a strategy where a resource-constrained target task benefits through augmentation using various related source tasks.\n\nResearch Problem: However, simply amalgamating these source tasks often leads to suboptimal outcomes because such integration may result in detrimental effects known as negative transfer on the predictive performance specifically tailored towards the target task. Consequently, there exists significant challenge within multitask learning regarding how best to select relevant subsets among available source tasks which could enhance the performance without causing harm or redundancy.\n\nMethodology: To tackle this issue efficiently while accounting for computational complexity involving exponential growths associated with potential combinations across numerous source tasks - we propose leveraging surrogate modeling techniques instead relying solely on heuristic approaches lacking comprehensive understanding about interdependencies amongst different task subsets & overall multitask learning efficacy. Our method involves randomly selecting small groups (subsets) of source tasks followed by estimating their joint effect on multitasking ability before training a linear regression model capable predicting future multitask performance based off those samples alone rather than needing exhaustive enumeration over every possible combination.\n\nMain Contributions:\n1. An effective surrogate modeling framework enabling us to estimate multitask performance quickly even when dealing with large numbers of source tasks.\n2. A novel linear regression-based approximation technique allowing predictions concerning new task subsets' impact beyond initial sampled ones thus reducing computation time drastically compared traditional exhaustive search strategies \n3. Demonstrated empirical evidence showing improved accuracy at predicting negative transfers relative to current state-of-the-art metrics assessing task similarity or affinity levels during multitask learning scenarios.\n4. Further validation demonstrating consistent improvements made against other well-established optimization algorithms designed explicitly for multitask settings utilizing weak supervision data sets commonly encountered practical applications domains like natural language processing",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Parameter Efficient Node Classification on Homophilic Graphs",
        "abstract": "Deep Learning on Graphs was recently made possible with the introduction of Graph Neural Networks (GNNs). GNNs use learnable diffusion processes to propagate information through the graph and improve performance on downstream tasks. However, learning this diffusion process can be expensive in terms of memory and computation. While a lot of research has gone into making these models more expressive and able to capture more complex patterns, in practice, edges in common benchmarking datasets often encode similarity of nodes with respect to the downstream task. This property is called homophily. We argue that for these homophilic graphs, learnable diffusion processes and large receptive fields are not required to achieve competitive performance. We propose Graph Non-Parametric Diffusion (GNPD) a method that outperforms traditional GNNs using only 2 linear models and non-parameteric diffusion. Our method takes ideas from Correct & Smooth (C&S) and the Scalable Inception Graph Network (SIGN) and combines them to create a simpler model that outperforms both of them on several datasets. Our method achieves an unmatched parameter efficiency, competing with models with two orders of magnitude more parameters. Additionally GNPD can also forego spectral embeddings which are the computational bottleneck of the C&S method.",
        "authors": "L. Prieto, J. D. Boef, P. Groth, et.al",
        "keywords": [
            "Graph Neural Networks",
            "Homophily",
            "Graph Non-Parametric Diffusion"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LIT8tjs6rJ",
        "pdf_src": "https://api2.openreview.net/pdf/59f88e91dd5ab6fd7f9f5de0f451138e4e27bd69.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses recent advancements in Deep Learning on Graphs facilitated by Graph Neural Networks (GNNs), where GNNs utilize learnable diffusion processes over graphs to enhance their ability to perform well on subsequent tasks.\n\nResearch Problem: Despite significant progress towards creating more expressive and pattern-capturing GNNs, there remains challenges related to high memory and computational costs associated with learning such diffusion processes effectively.\n \nMethod: To address these issues while maintaining or improving upon existing performance benchmarks like those found within commonly used datasets characterized by edge encoding node similarities due to \"homophily,\" the authors introduce Graph Non-Parametric Diffusion (GNPD).\n\nMain Contributions:\n1. They present GNPD as alternative approach leveraging just two linear models along with non-parametric diffusion techniques instead of relying heavily on learnable diffusion processes typically employed by other state-of-the-art methods;\n2. Their proposed method draws inspiration from Correct & Smooth (C&S) and Scalable Inception Graph Network (SIGN);\n3. GNPD constructs a simplified architecture yet surpasses its predecessors across various datasets regarding parameter efficiency - it matches or exceeds performances achieved even when compared against significantly larger models having up to tenfold greater number of parameters;\n4. Furthermore, unlike C&S's reliance on computationally intensive spectral embeddings, GNPD does away with this component altogether enhancing overall efficiency further.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Multi-objective Bayesian Optimization with Heuristic Objectives for Biomedical and Molecular Data Analysis Workflows",
        "abstract": "Many practical applications require optimization of multiple, computationally expensive, and possibly competing objectives that are well-suited for multi-objective Bayesian optimization (MOBO). However, for many types of biomedical data, measures of data analysis workflow success are often heuristic and therefore it is not known a priori which objectives are useful. Thus, MOBO methods that return the full Pareto front may be suboptimal in these cases. Here we propose a novel MOBO method that adaptively updates the scalarization function using properties of the posterior of a multi-output Gaussian process surrogate function. This approach selects useful objectives based on a flexible set of desirable criteria, allowing the functional form of each objective to guide optimization. We demonstrate the qualitative behaviour of our method on toy data and perform proof-of-concept analyses of single-cell RNA sequencing and highly multiplexed imaging datasets for univariate input optimization.",
        "authors": "A. Selega, K. R. Campbell",
        "keywords": [
            "multi-objective Bayesian optimization",
            "adaptive scalarization",
            "single-cell RNA sequencing"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=QspAcsAyis",
        "pdf_src": "https://api2.openreview.net/pdf/c4a90f0a1b14df802d9ea65ddf9ee0cdcd529421.pdf",
        "Code_src": "",
        "Introduction": "Background: Many real-world applications involve optimizing several computationally intensive objectives simultaneously with potential trade-offs between them; this problem domain fits naturally into multi-objective Bayesian optimization (MOBO), where solutions across all objectives can provide insights even if no one solution optimizes every objective.\n\nResearch Question: The challenge lies when dealing with biological or medical data sets due to subjective evaluation metrics—these heuristics make it difficult to determine beforehand what specific objectives should inform an optimal decision-making strategy within MOBO frameworks since they do not necessarily reflect true performance indicators directly related to outcomes such as therapeutic efficacy.\n \nMethodology: To address uncertainty about relevant objectives early during optimization processes without requiring extensive computational resources upfront—we introduce an adaptive MOBO technique capable of updating its scalarization functions dynamically throughout iterations by leveraging information from the posterior distribution over model parameters estimated via Gaussian Process surrogates. Our proposed algorithm iteratively refines its understanding through empirical risk minimization while considering diverse selection criteria tailored flexibly according to user-defined preferences/preferences encoded within constraints imposed upon optimization space(s).\nMain Contributions:\n1. An innovative MOBO framework designed specifically addressing uncertainties inherent in selecting meaningful objectives amidst complex workflows involving high-dimensional inputs like those found commonly encountered within genomics studies (e.g., Single-Cell RNA Sequencing) & Imaging modalities (Multiplexed Fluorescence Microscopy).\n2. A dynamic scalarization mechanism informed by Bayesian inference principles allows us to continuously refine our search strategies towards more effective exploration/exploitation trade-offs than static approaches could achieve alone.\n3. Demonstrated effectiveness demonstrated empirically against synthetic benchmarks alongside application case studies utilizing actual biological datasets highlighting improved convergence rates compared traditional non-adaptive counterparts under similar conditions",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Towards Better Out-of-Distribution Generalization of Neural Algorithmic Reasoning Tasks",
        "abstract": "In this paper, we study the OOD generalization of neural algorithmic reasoning tasks, where the goal is to learn an algorithm (e.g., sorting, breadth-first search, and depth-first search) from input-output pairs using deep neural networks. First, we argue that OOD generalization in this setting is significantly different than common OOD settings. For example, some phenomena in OOD generalization of image classifications such as \\emph{accuracy on the line} are not observed here, and techniques such as data augmentation methods do not help as assumptions underlying many augmentation techniques are often violated. Second, we analyze the main challenges (e.g., input distribution shift, non-representative data generation, and uninformative validation metrics) of the current leading benchmark, i.e.,  CLRS \\citep{deepmind2021clrs}, which contains 30 algorithmic reasoning tasks. We propose several solutions, including a simple-yet-effective fix to the input distribution shift and improved data generation. Finally, we propose an attention-based 2WL-graph neural network (GNN) processor which complements message-passing GNNs so their combination outperforms the state-of-the-art model by a $3\\%$ margin averaged over all algorithms. ",
        "authors": "S. Mahdavi, K. Swersky, T. Kipf, et.al",
        "keywords": [
            "algorithmic reasoning",
            "OOD generalization",
            "graph neural network"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=xkrtvHlp3P",
        "pdf_src": "https://api2.openreview.net/pdf/a13084fca01bec59e8c8523ccb53144acddc93ea.pdf",
        "Code_src": "",
        "Introduction": "Background: This research focuses on studying how well neural networks can generalize when they encounter novel inputs outside what was used for training - known as Out-Of-Distribution (OOD) generalization – within the context of learning algorithmic reasoning tasks.\n\nResearch Problem: The problem addressed concerns why existing approaches designed primarily for other types of machine learning problems like image classification may fail or perform poorly under these new conditions due to differences between typical OOD scenarios encountered there versus those found with algorithmic reasoning tasks learned through input-output examples.\n \nMethods: The authors identify specific issues contributing to poor performance during OOD generalization specifically tailored towards algorithmic reasoning benchmarks; namely shifts in input distributions along with biases introduced into generated datasets via commonly employed augmentation strategies being less effective compared to expected outcomes because foundational assumptions behind them don't hold true consistently across various algorithms studied. They then introduce modifications aimed at addressing these identified challenges directly related to both input distribution shifts and better data generation practices while also proposing Attention-Based 2WL-Graph Neural Network Processor architecture enhancements complementing traditional Message Passing Graph Neural Networks architectures resulting in significant improvements overall surpassing prior best performing models by up to 3%.\n\nMain Contributions: Their contributions include understanding nuances around OOD generalization pertinent only toward algorithmic reasoning domains rather than more generalized fields allowing development targeted mitigations against its unique difficulties faced therein; providing practical fixes improving robustness particularly concerning input distribution discrepancies seen throughout tested algorithms; introducing innovative graph processing mechanisms leveraging attentional mechanisms yielding substantial gains beyond previous top-performing systems making it possible now even further advancements could be made upon this foundation moving forward",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "L-SVRG and L-Katyusha with Adaptive Sampling",
        "abstract": "Stochastic gradient-based optimization methods, such as L-SVRG and its accelerated variant L-Katyusha (Kovalev et al., 2020), are widely used to train machine learning models. Theoretical and empirical performance of L-SVRG and L-Katyusha can be improved by sampling the observations from a non-uniform distribution Qian et al. (2021). However, to design a desired sampling distribution, Qian et al. (2021) rely on prior knowledge of smoothness constants that can be computationally intractable to obtain in practice when the dimension of the model parameter is high. We propose an adaptive sampling strategy for L-SVRG and L-Katyusha that learns the sampling distribution with little computational overhead, while allowing it to change with iterates, and at the same time does not require any prior knowledge on the problem parameters. We prove convergence guarantees for L-SVRG and L-Katyusha for convex objectives when the sampling distribution changes with iterates. These results show that even without prior information, the proposed adaptive sampling strategy matches, and in some cases even surpasses, the performance of the sampling scheme in Qian et al. (2021). Extensive simulations support our theory and the practical utility of the proposed sampling scheme on real data.",
        "authors": "B. Zhao, B. Lyu, M. Kolar",
        "keywords": [
            "Adaptive Sampling Strategy",
            "Stochastic Gradient Optimization",
            "Convergence Guarantees"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=9lyqt3rbDc",
        "pdf_src": "https://api2.openreview.net/pdf/fd012ff8e8097fce6d7a7a12a73929f553b5c5bf.pdf",
        "Code_src": "",
        "Introduction": "Background: Stochastic gradient-based optimization algorithms like L-SVRG and L-Katyusha have been extensively utilized due to their efficiency during training processes; however, they may suffer from theoretical limitations regarding performance improvements.\n\nResearch Problem: To enhance both theoretical understanding and empirical effectiveness concerning stochastic gradient-based optimization techniques through adapting the observation sampling distributions rather than using uniform ones which could lead to suboptimal outcomes based solely on prior knowledge about smoothness constants within complex scenarios where these values might elude computation or estimation accurately enough under high-dimensional settings.\n\nMethodology: This paper introduces an innovative adaptive sampling approach designed specifically tailored towards optimizing stochastic variance-reduced gradient (SVRG) variants including SVRG itself along with its accelerated version known as Katyusha (L-Katyusha). Unlike previous works relying heavily upon pre-established smoothness constant assumptions leading up potential inaccuracies especially relevant higher dimensional spaces involving large datasets - this new method dynamically adjusts sample selection throughout iterations autonomously without requiring any initial estimations related directly backtracking lines or curvature measures associated with objective functions being minimized via iterative procedures employed hereunder.\n\nMain Contributions: \n- A novel adaptive sampling technique has been developed independently from traditional reliance upon predefined smoothness constants making it applicable across various contexts regardless complexity level involved;\n- Proofs demonstrating convergence properties were provided ensuring efficacy over existing literature approaches particularly those utilizing fixed sampling schemes irrespective whether observed samples remain static or evolve adaptively depending current iteration stage reached during optimization process;\n- Empirical validation experiments conducted demonstrate superiority achieved compared against baseline strategies found elsewhere suggesting wider applicability beyond just theoretical considerations alone thus offering tangible benefits practitioners working fields ranging from finance engineering medicine amongst others needing robust solutions dealing with big-data analytics tasks demanding scalable yet accurate approximations",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Quantum Policy Iteration via Amplitude Estimation and Grover Search – Towards Quantum Advantage for Reinforcement Learning",
        "abstract": "We present a full implementation and simulation of a novel quantum reinforcement learning method. Our work is a detailed and formal proof of concept for how quantum algorithms can be used to solve reinforcement learning problems and shows that, given access to error- free, efficient quantum realizations of the agent and environment, quantum methods can yield provable improvements over classical Monte-Carlo based methods in terms of sample complexity. Our approach shows in detail how to combine amplitude estimation and Grover search into a policy evaluation and improvement scheme. We first develop quantum policy evaluation (QPE) which is quadratically more efficient compared to an analogous classi- cal Monte Carlo estimation and is based on a quantum mechanical realization of a finite Markov decision process (MDP). Building on QPE, we derive a quantum policy iteration that repeatedly improves an initial policy using Grover search until the optimum is reached. Finally, we present an implementation of our algorithm for a two-armed bandit MDP which we then simulate.",
        "authors": "S. Wiedemann, D. Hein, S. Udluft, et.al",
        "keywords": [
            "quantum reinforcement learning",
            "sample complexity",
            "Grover search"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=HG11PAmwQ6",
        "pdf_src": "https://api2.openreview.net/pdf/ba45dc3a2b902dd4dc7e320f06eadf3565fc0cba.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper introduces a new quantum reinforcement learning method with its complete implementation.\n\nResearch Problem: How do quantum algorithms perform when solving reinforcement learning problems?\n\nMethod: The authors provide a comprehensive demonstration by combining amplitude estimation and Grover search techniques within a framework designed specifically for evaluating policies efficiently through quantum mechanics principles adapted from finite Markov Decision Processes (MDPs).\n\nMain Contributions:\n1. They introduce Quantum Policy Evaluation (QPE), demonstrating quadratic efficiency gains relative to classical Monte Carlo estimations.\n2. Based on this, they propose Quantum Policy Iteration leveraging Grover's algorithm iteratively refining starting policies towards optimality without requiring any additional samples beyond those needed during training phase itself - thus reducing overall computational cost significantly as opposed to traditional approaches relying solely on classical computing resources alone due their inherently exponential scaling behavior associated with exploration/exploitation trade-offs inherent therein..",
        "Topic": "Sample Efficiency in Reinforcement Learning"
    },
    {
        "title": "A Unified View of Masked Image Modeling",
        "abstract": "Masked image modeling has demonstrated great potential to eliminate the label-hungry problem of training large-scale vision Transformers, achieving impressive performance on various downstream tasks. In this work, we propose a unified view of masked image modeling after revisiting existing methods. Under the unified view, we introduce a simple yet effective method, termed as MaskDistill, which reconstructs normalized semantic features from teacher models at the masked positions, conditioning on corrupted input images. Experimental results on image classification and semantic segmentation show that MaskDistill achieves comparable or superior performance than state-of-the-art methods. When using the huge vision Transformer and pretraining 300 epochs, MaskDistill obtains 88.3% fine-tuning top-1 accuracy on ImageNet-1k (224 size) and 58.8 semantic segmentation mIoU metric on ADE20k (512 size). Code is enclosed in the supplementary materials.",
        "authors": "Z. Peng, L. Dong, H. Bao, et.al",
        "keywords": [
            "Masked image modeling",
            "Vision Transformers",
            "Performance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=wmGlMhaBe0",
        "pdf_src": "https://api2.openreview.net/pdf/a7de510d94219bc4a85b113b9ef03da8a9af559e.pdf",
        "Code_src": "",
        "Introduction": "Background: Masked image modeling aims to train large-scale vision transformers without relying heavily on labels by masking parts of an image during training.\n\nResearch Problem: How can we effectively utilize masked image modeling for improving the performance of vision transformers?\n\nMethod: We present a new approach called MaskDistill, where we reconstruct normalized semantic features from teacher models at the masked positions while conditioning on corrupted input images.\n\nMain Contributions: Our proposed MaskDistill method demonstrates comparable or superior performance compared to state-of-the-art methods when applied to both image classification and semantic segmentation tasks. Specifically, our approach achieves 88.3% fine-tuning top-1 accuracy on ImageNet-1k with a 224-size input and 58.8 semantic segmentation mIoU metric on ADE20k with a 512-size input.\nCode for our implementation will be available through supplementary materials provided along with the paper submission.",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance",
        "abstract": "Denoising diffusion probabilistic models (DDPMs) are a recent family of generative models that achieve state-of-the-art results. In order to obtain class-conditional generation, it was suggested to guide the diffusion process by gradients from a time-dependent classifier. While the idea is theoretically sound, deep learning-based classifiers are infamously susceptible to gradient-based adversarial attacks. Therefore, while traditional classifiers may achieve good accuracy scores, their gradients are possibly unreliable and might hinder the improvement of the generation results. Recent work discovered that adversarially robust classifiers exhibit gradients that are aligned with human perception, and these could better guide a generative process towards semantically meaningful images. We utilize this observation by defining and training a time-dependent adversarially robust classifier and use it as guidance for a generative diffusion model. In experiments on the highly challenging and diverse ImageNet dataset, our scheme introduces significantly more intelligible intermediate gradients, better alignment with theoretical findings, as well as improved generation results under several evaluation metrics. Furthermore, we conduct an opinion survey whose findings indicate that human raters prefer our method's results.",
        "authors": "B. Kawar, R. Ganz, M. Elad",
        "keywords": [
            "adversarially robust classifier",
            "denoising diffusion probabilistic models",
            "semantic image generation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=tEVpz2xJWX",
        "pdf_src": "https://api2.openreview.net/pdf/f7b70cd103ed79b4e6b0cebdc3e04d476958df80.pdf",
        "Code_src": "",
        "Introduction": "Background: Denoising Diffusion Probabilistic Models (DDPMs) have recently achieved state-of-the-art performance in generating high-quality images but suffer from difficulties when attempting to generate class-conditional samples.\n\nResearch Problem: How can one reliably guide the diffusion process toward producing class-conditional samples without relying on potentially unreliable gradients?\n\nMethod: The authors propose using a time-dependent adversarially robust classifier instead of a standard deep learning classifier because its gradients align closely with human perception which makes them suitable for guiding the DDPM during generation.\n\nMain Contributions: They introduce such a classifier into existing DDPM frameworks; they demonstrate significant improvements over previous methods through empirical evidence including increased intelligibility within intermediate steps along with higher quality final outputs across various evaluation metrics alongside qualitative feedback indicating user preference",
        "Topic": "Generative Models"
    },
    {
        "title": "PRUDEX-Compass: Towards Systematic Evaluation of Reinforcement Learning in Financial Markets",
        "abstract": "The financial markets, which involve more than $90 trillion market capitals, attract the attention of innumerable investors around the world. Recently, reinforcement learning in financial markets (FinRL) has emerged as a promising direction to train agents for making profitable investment decisions. However, the evaluation of most FinRL methods only focuses on profit-related measures and ignores many critical axes, which are far from satisfactory for financial practitioners to deploy these methods into real-world financial markets. Therefore, we introduce PRUDEX-Compass, which has 6 axes, i.e., Profitability, Risk-control, Universality, Diversity, rEliability, and eXplainability, with a total of 17 measures for a systematic evaluation. Specifically, i) we propose AlphaMix+ as a strong FinRL baseline, which leverages mixture-of-experts (MoE) and risk-sensitive approaches to make diversified risk-aware investment decisions, ii) we evaluate 8 FinRL methods in 4 long-term real-world datasets of influential financial markets to demonstrate the usage of our PRUDEX-Compass, iii) PRUDEX-Compass together with 4 real-world datasets, standard implementation of 8 FinRL methods and a portfolio management environment is released as public resources to facilitate the design and comparison of new FinRL methods. We hope that PRUDEX-Compass can not only shed light on future FinRL research to prevent untrustworthy results from stagnating FinRL into successful industry deployment but also provide a new challenging algorithm evaluation scenario for the reinforcement learning (RL) community.",
        "authors": "S. Sun, M. Qin, X. Wang, et.al",
        "keywords": [
            "word1: Financial Markets",
            "Reinforcement Learning",
            "Evaluation Metrics"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=JjbsIYOuNi",
        "pdf_src": "https://api2.openreview.net/pdf/f0677e3cfa6c29f5953f5a1b29b70c1f3d224edc.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses how the financial markets have become increasingly complex due to their large size ($90 trillion), attracting numerous global investors who use various strategies based on machine learning techniques like reinforcement learning.\n\nResearch Problem: Despite recent advancements using reinforcement learning algorithms within finance known as FinRL (\"Financial Reinforcement Learning\"), there's been limited focus beyond profitability metrics when evaluating such models' performance; this overlooks other important aspects relevant to practical application by financial professionals.\n \nMethodology: To address issues related to inadequate assessment criteria used traditionally among existing works focusing solely on profits without considering broader dimensions necessary before deploying them effectively across different types of assets or under varying conditions over time—namely universality diversity reliability explainability—the authors developed an innovative multi-dimensional framework called \"PRUDEX-Compass\" incorporating six axes:\n1. Profitability,\n2. Risk-control,\n3. Universality,\n4. Diversity,\n5. Reliability,\n6. Explainability.\nThis comprehensive approach includes seventeen specific indicators designed systematically assess each axis comprehensively.\n \nMain Contributions: They introduced AlphaMix+, a novel method leveraging mixture-of-experts architecture combined with risk-sensitive reinforcement learning strategy aiming at producing diverse yet robust investments while managing risks appropriately according to predefined parameters set forth during training phase itself rather than post hoc adjustments after observing outcomes alone). Additionally they evaluated eight leading state-of-the-art FinRL systems against four distinct real-world datasets representing major financial sectors worldwide demonstrating its effectiveness through empirical evidence provided via quantitative analysis comparing performances along all axes covered by PRUDEX-Compass toolset. Furthermore, they made available additional resources including datasets portfolios management environments alongside code implementations allowing researchers interested further explore possibilities offered by FinRL methodologies contribute towards continuous improvement upon current findings presented here ultimately facilitating wider adoption amongst practitioners involved directly handling capital allocation tasks daily basis.",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Improved Overparametrization Bounds for Global Convergence of SGD for Shallow Neural Networks",
        "abstract": "We study the overparametrization bounds required for the global convergence of stochastic gradient descent algorithm for a class of one hidden layer feed-forward neural networks equipped with ReLU activation function. We improve the existing state-of-the-art results in terms of the required hidden layer width. We introduce a new proof technique combining nonlinear analysis with properties of random initializations of the network.",
        "authors": "B. Polaczyk, J. Cyranka",
        "keywords": [
            "stochastic gradient descent",
            "overparametrization bounds",
            "ReLU activation function"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=RjZq6W6FoE",
        "pdf_src": "https://api2.openreview.net/pdf/e8d1b508d4ef8a41966b86107df3d8ccabb4c474.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper investigates the necessary parameters needed to ensure the global convergence of stochastic gradient descent (SGD) algorithms when applied to certain types of one-hidden-layer feedforward neural networks that use Rectified Linear Unit (ReLU) activation functions.\n\nResearch Question: What are the optimal parameter settings, particularly regarding the number of neurons in the hidden layer, which guarantee the convergence of SGD?\n\nMethods: To address this question, researchers develop an innovative proof method by merging nonlinear analysis techniques and insights into how random initialization affects the behavior of neural networks during training using SGD.\n\nMain Contributions:\n1. Improved Bounds on Hidden Layer Width - The research improves upon current leading-edge findings concerning the minimum size requirements for the hidden layer.\n2. Novel Proof Technique - A novel approach is introduced; it combines nonlinear dynamics principles along with understanding gained from examining the impact of random starting points within the neural network's architecture as learning begins through SGD iterations",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Patches Are All You Need?",
        "abstract": "Although convolutional neural networks have been the dominant architecture for computer vision for many years, Vision Transformers (ViTs) have recently shown promise as an alternative. Subsequently, many new models have been proposed which replace the self-attention layer within the ViT architecture with novel operations (such as MLPs), all of which have also been relatively performant. We note that these architectures all share a common component--the patch embedding layer--which enables the use of a simple isotropic template with alternating steps of channel- and spatial-dimension mixing. This raises a question: is the success of ViT-style models due to novel, highly-expressive operations like self-attention, or is it at least in part due to using patches? In this paper, we present some evidence for the latter: specifically, we propose the ConvMixer, an extremely simple and parameter-efficient fully-convolutional model in which we replace the self-attention and MLP layers within the ViT with less-expressive depthwise and pointwise convolutional layers, respectively. Despite its unusual simplicity, ConvMixer outperforms the ViT, MLP-Mixer, and their variants for similar data set sizes and parameter counts, in addition to outperforming classical vision models like ResNet. We argue that this contributes to the evidence that patches are sufficient for designing simple and effective vision models. Our code is available at https://github.com/locuslab/convmixer.",
        "authors": "A. Trockman, J. Z. Kolter",
        "keywords": [
            "patch",
            "Vision Transformer",
            "ConvMixer"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=rAnB7JSMXL",
        "pdf_src": "https://api2.openreview.net/pdf/99faace0a158aad0bbb74680e0c56a1390c5fe3d.pdf",
        "Code_src": "https://github.com/locuslab/convmixer",
        "Introduction": "Background:\nThe background of this research lies in the field of computer vision where convolutional neural networks (CNNs) traditionally dominated but more recent studies suggest that Vision Transformers (ViTs) could be promising alternatives.\n\nResearch Problem:\nThe main problem addressed by this study revolves around understanding whether the success of ViT-style models can primarily be attributed to innovative attention mechanisms such as self-attention alone; alternatively, there may exist another contributing factor - the usage of patches themselves during feature extraction process.\n\nMethodology:\nTo address above mentioned issue, researchers introduce ConvMixer – a very basic yet efficient fully convolutional network replacing the self-attention and multi-layer perceptron (MLP) layers from ViT with less expressive depth-wise and point-wise convolutional layers correspondingly without altering patch embeddings.\n\nMain Contributions:\nThis work makes several key contributions including demonstrating that even though ConvMixer lacks complex attention mechanism found in ViT, it still manages to surpass both ViT itself along with other variations based on MLPs across various datasets while maintaining comparable parameters count suggesting that patch-based approach might indeed play significant role towards achieving successful visual recognition tasks.\nAdditionally, they provide open-source code for further replication purposes underlining transparency aspect crucial nowadays among machine learning community members aiming toward reproducible results enhancing trustworthiness amongst peers involved",
        "Topic": "Vision Transformer"
    },
    {
        "title": "Solving Nonconvex-Nonconcave Min-Max Problems exhibiting Weak Minty Solutions",
        "abstract": "We investigate a structured class of nonconvex-nonconcave min-max problems exhibiting so-called \\emph{weak Minty} solutions, a notion which was only recently introduced, but is able to simultaneously capture different generalizations of monotonicity. We prove novel convergence results for a generalized version of the optimistic gradient method (OGDA) in this setting, matching the $1/k$ rate for the best iterate in terms of the squared operator norm recently shown for the extragradient method (EG). In addition we propose an adaptive step size version of EG, which does not require knowledge of the problem parameters.\n",
        "authors": "A. Böhm",
        "keywords": [
            "nonconvex-nonconcave min-max problems",
            "weak Minty solutions",
            "optimistic gradient method"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Gp0pHyUyrb",
        "pdf_src": "https://api2.openreview.net/pdf/27bfde6c2a584536c7129c5654ebfa8577a0e8af.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper investigates a specific type of nonconvex-nonconcave min-max optimization problems with weak Minty solutions as their feasible set.\n\nResearch Problem: To find efficient algorithms that can solve these types of optimization problems and achieve good convergence rates while being robust against parameter uncertainties.\n\nMethods: The authors introduce a generalized version of the optimistic gradient descent algorithm (OGDA), proving new convergence results under certain conditions related to the squared operator norm - specifically, they match the $1/k$ rate previously demonstrated by the extragradient method (EG).\n\nMain Contributions:\n- They extend the concept of weak Minty solutions from convex-concave settings into nonconvex-nonconcave ones;\n- Propose a novel convergence result for OGDA within such problems; \n- Develop an adaptive step-size EG variant without requiring prior knowledge about the problem parameters",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
        "abstract": "The paper presents a scalable approach for learning spatially distributed visual representations over individual tokens and a holistic instance representation simultaneously. We use self-attention blocks to represent spatially distributed tokens, followed by cross-attention blocks to aggregate the holistic instance. The core of the approach is the use of extremely large token masking (75\\%-90\\%) as the data augmentation for supervision. Our model, named ExtreMA, follows the plain BYOL approach where the instance representation from the unmasked subset is trained to predict that from the intact input. Instead of encouraging invariance across inputs, learning requires the model to capture informative variations in an image.\n\nThe paper makes three contributions: 1) It presents random masking as a strong and computationally efficient data augmentation for siamese representation learning. 2) With multiple sampling per instance, extreme masking greatly speeds up learning and improves performance with more data. 3)  ExtreMA obtains stronger linear probing performance than masked modeling methods, and better transfer performance than prior contrastive models.",
        "authors": "Z. Wu, Z. Lai, X. Sun, et.al",
        "keywords": [
            "Extremely Large Token Masking",
            "Siamese Representation Learning",
            "Transfer Performance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=3epEbhdgbv",
        "pdf_src": "https://api2.openreview.net/pdf/62f25d82ff0ecdc2080c3e99f4199f56e5f8c562.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this research lies in the field of computer vision which aims at creating algorithms capable of understanding images through machine learning techniques like convolutional neural networks or transformers. One challenge within this domain involves representing both local features on different parts of an image (spatial distribution) while also capturing global information about objects present (\"holistic instance representation\"). This dual task can be difficult because traditional approaches often focus solely on one aspect without considering how they interact together.\n \nResearch Problem:\nThe primary problem addressed here concerns developing new methodologies allowing simultaneous learning these two aspects efficiently using existing architectures such as Vision Transformers (ViTs). Specifically, researchers seek ways not only to learn detailed spatial distributions but maintain consistency when varying viewpoints or lighting conditions affect overall appearance; henceforth referred to as \"instance invariant\" properties.\n \nMethodology:\nTo tackle aforementioned issues effectively, authors introduce ExtreMA - an extension built upon Byol (BYOL), leveraging Self Attention Blocks (SABs) & Cross Attention Blocks (CABs) respectively responsible for encoding spatially-distributed tokens into feature vectors then aggregating them into holistic instances via CABs. Furthermore, unlike other works focusing heavily on minimizing prediction errors during training phases leading towards memorization rather than abstraction capabilities; ExtreMA employs Extremely Large Token Masking (ELTM) strategy wherein approximately 75% - 90% tokens are randomly omitted throughout each epoch serving as pseudo-negative samples aiding optimization process w/o explicitly labeling any additional examples manually nor requiring specialized hardware resources beyond standard GPUs/TPUs commonly available nowadays making it highly scalable solution compared others proposed previously mentioned papers reviewed herein).\nMain Contributions:\nThis work brings forth several novel insights contributing significantly toward solving aforementioned challenges posed earlier namely:\n1) Demonstrates Random Masking technique serves well alongside Siamese Representation Learning tasks yielding substantial improvements whilst remaining computationally feasible;\n2) Shows Multiple Sampling Per Instance drastically accelerates convergence rates observed during iterative refinement steps thus enabling faster adaptation under various scenarios encountered real-world applications;\n3) Provides empirical evidence supporting superiority demonstrated Linear Probing metric achieved by ExtreMA architecture surpassing Masked Modeling counterparts along with exhibiting enhanced Transferability potential outperforming previous Contrastive Models evaluated against benchmark datasets widely used academia industry alike today's landscape",
        "Topic": "Large Language Models"
    },
    {
        "title": "Leveraging Demonstrations with Latent Space Priors",
        "abstract": "Demonstrations provide insight into relevant state or action space regions, bearing great potential to boost the efficiency and practicality of reinforcement learning agents. In this work, we propose to leverage demonstration datasets by combining skill learning and sequence modeling. Starting with a learned joint latent space, we separately train a generative model of demonstration sequences and an accompanying low-level policy. The sequence model forms a latent space prior over plausible demonstration behaviors to accelerate learning of high-level policies. We show how to acquire such priors from state-only motion capture demonstrations and explore several methods for integrating them into policy learning on transfer tasks. Our experimental results confirm that latent space priors provide significant gains in learning speed and final performance. We benchmark our approach on a set of challenging sparse-reward environments with a complex, simulated humanoid, and on offline RL benchmarks for navigation and object manipulation.",
        "authors": "J. Gehring, D. Gopinath, J. Won, et.al",
        "keywords": [
            "skill learning",
            "sequence modeling",
            "latent space priors"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=OzGIu4T4Cz",
        "pdf_src": "https://api2.openreview.net/pdf/807e1b0e315855d4996d8af89eb06b1b679aa6da.pdf",
        "Code_src": "",
        "Introduction": "Background: Reinforcement learning (RL) is often inefficient due to its exploration-exploitation trade-off issue which requires many interactions between agent and environment before convergence.\n\nResearch Problem: How can we efficiently learn high-level policies using demonstrations?\n\nMethod: This paper proposes leveraging demonstration datasets through skill learning and sequence modeling techniques.\n1. A learned joint latent space is used as a starting point where both generative models of demonstration sequences and an accompanying low-level policy are trained independently but jointly optimized towards it.\n2. The sequence model serves as a latent space prior over plausible demonstration behaviors accelerating learning of high-level policies via regularization effect during training process.\n3. They also discuss different ways these priors could be acquired from state-only motion capture demonstrations & integrated within existing algorithms like actor-critic architectures when applied onto new tasks (transfer learning).\n\nMain Contributions:\n- Introduced novel integration strategies utilizing latent space priors derived from demonstrations aiding faster convergence rates while maintaining good generalization capabilities across various domains/tasks tested experimentally.\n- Demonstrated efficacy against other baselines under challenging sparse reward scenarios involving simulation-based humanoid robots along with real-world robotic manipulations off-policy reinforcement learning benchmarks confirming their findings empirically validated outcomes obtained compared traditional approaches without incorporating demonstrated data sources effectively utilized here proposed framework presented study conducted further supporting claims made throughout text discussed above.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "How Robust is Your Fairness? Evaluating and Sustaining Fairness under Unseen Distribution Shifts",
        "abstract": "Increasing concerns have been raised on deep learning fairness in recent years. Existing fairness-aware machine learning methods mainly focus on the fairness of in-distribution data. However, in real-world applications, it is common to have distribution shift between the training and test data. In this paper, we first show that the fairness achieved by existing methods can be easily broken by slight distribution shifts. To solve this problem, we propose a novel fairness learning method termed CUrvature MAtching (CUMA), which can achieve robust fairness generalizable to unseen domains with unknown distributional shifts. Specifically, CUMA enforces the model to have similar generalization ability on the majority and minority groups, by matching the loss curvature distributions of the two groups. We evaluate our method on three popular fairness datasets. Compared with existing methods, CUMA achieves superior fairness under unseen distribution shifts, without sacrificing either the overall accuracy or the in-distribution fairness.",
        "authors": "H. Wang, J. Hong, J. Zhou, et.al",
        "keywords": [
            "distribution shift",
            "fairness-aware machine learning",
            "CUMA"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=11pGlecTz2",
        "pdf_src": "https://api2.openreview.net/pdf/28d9e73cff9c0844e5d321383a0ad3894822a66d.pdf",
        "Code_src": "",
        "Introduction": "Background: Recent studies raise concerns about the fairness of deep learning models when dealing with out-of-distribution (OOD) data.\n\nResearch Problem: How do we design fair machine learning algorithms capable of handling OOD scenarios?\n\nMethod: The authors introduce a new approach called CUrvature MAtching (CUMA). This method aims at achieving robust fairness across different domains through a novel mechanism based on matching the loss curvature distributions for both majority and minority classes during training.\n\nMain Contributions:\n1. Demonstrates how current fairness-preserving techniques are vulnerable to minor distribution shifts.\n2. Proposes an algorithm named CUMA designed specifically against such threats; \n3. Validates its effectiveness using empirical results from well-known fairness benchmarks;\n4. Shows improved performance compared to other state-of-the-art approaches while maintaining high levels of accuracy even within the original dataset's distribution boundaries.",
        "Topic": "Machine Learning"
    },
    {
        "title": "Generalization as Dynamical Robustness--The Role of Riemannian Contraction in Supervised Learning",
        "abstract": "A key property of successful learning algorithms is generalization. In classical supervised learning, generalization can be achieved by ensuring that the empirical error converges to the expected error as the number of training samples goes to infinity. Within this classical setting, we analyze the generalization properties of iterative optimizers such as stochastic gradient descent and natural gradient flow through the lens of dynamical systems and control theory. Specifically, we use contraction analysis to show that generalization and dynamical robustness are intimately related through the notion of algorithmic stability. \n\nIn particular, we prove that Riemannian contraction in a supervised learning setting implies generalization. We show that if a learning algorithm is contracting in some Riemannian metric with rate $\\lambda > 0$, it is uniformly algorithmically stable with rate $\\mathcal{O}(1/\\lambda n)$, where $n$ is the number of examples in the training set. The results hold for stochastic and deterministic optimization, in both continuous and discrete-time, for convex and non-convex loss surfaces. \n\nThe associated generalization bounds reduce to well-known results in the particular case of gradient descent over convex or strongly convex loss surfaces. They can be shown to be optimal in  certain linear settings, such as kernel ridge regression under gradient flow. Finally, we demonstrate that the well-known Polyak-Lojasiewicz condition is intimately related to the contraction of a model's outputs as they evolve under gradient descent. This correspondence allows us to derive uniform algorithmic stability bounds for nonlinear function classes such as wide neural networks.",
        "authors": "L. Kozachkov, P. Wensing, J. Slotine",
        "keywords": [
            "algorithmic stability",
            "contraction analysis",
            "generalization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Sb6p5mcefw",
        "pdf_src": "https://api2.openreview.net/pdf/e7be71187769a07d6eb06a76235b2b90a8d5b03e.pdf",
        "Code_src": "",
        "Introduction": "Background: Generalization ability has been considered one of the most important characteristics of successful learning algorithms.\n\nResearch Problem: How do iterative optimizers like stochastic gradient descent and natural gradient flow achieve generalization?\n\nMethod: By using dynamical systems and control theory perspectives, especially contraction analysis which shows how closely related generalization and dynamical robustness are via algorithmic stability.\n\nMain Contributions:\n1. Prove that Riemannian contraction in supervised learning leads to generalization.\n2. Show that an algorithm contracting at a rate $\\lambda>0$ on any given Riemannian metric will have a uniform algorithmic stability bound of $\\mathcal{O}(1/\\lambda n)$ when trained against $n$ examples from its dataset; this holds across different types of optimization problems including stochastic and deterministic ones along with various time scales and loss functions ranging from convex to non-convex scenarios.\n3. Associated generalization bounds simplify into known results specificially when dealing with gradient descent applied onto convex or strongly convex losses while also being potentially optimal within certain linear setups exemplified by kernel ridge regression under gradient flows.\n4. Demonstrate that Polyak-Lojasiewicz inequality - another measure used frequently concerning convergence rates during iterations – is intrinsically connected w.r.t. output contractions observed throughout iterations performed utilizing gradient descent approach allowing derivation of uniform stability bounds applicable towards broader nonlinear functional spaces encompassing deep neural networks among others.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Reducing Predictive Feature Suppression in Resource-Constrained Contrastive Image-Caption Retrieval",
        "abstract": "To train image-caption retrieval (ICR) methods, contrastive loss functions are a common choice for optimization functions. \nUnfortunately, contrastive ICR methods are vulnerable to predictive feature suppression. \nPredictive features are features that correctly indicate the similarity between a query and a candidate item. \nHowever, in the presence of multiple predictive features during training, encoder models tend to suppress redundant predictive features, since these features are not needed to learn to discriminate between positive and\nnegative pairs. \nWe introduce an approach to reduce predictive feature suppression for resource-constrained ICR methods: latent target decoding (LTD). We add an additional decoder to the contrastive ICR framework, to reconstruct the input caption in a latent space of a general-purpose\nsentence encoder, which prevents the image and caption encoder from suppressing\npredictive features. \nWe implement the LTD objective as an optimization constraint, to ensure that the reconstruction loss is below a bound value while primarily optimizing for the contrastive loss. \nImportantly, LTD does not depend on additional training data or expensive (hard) negative mining strategies. \nOur experiments show that, unlike reconstructing the input caption in the input space, LTD reduces predictive feature suppression, measured by obtaining higher recall@k, r-precision, and nDCG scores than a contrastive ICR baseline.\nMoreover, we show that LTD should be implemented as an optimization constraint instead\nof a dual optimization objective. Finally, we show that LTD can be used with different\ncontrastive learning losses and a wide variety of resource-constrained ICR methods.",
        "authors": "M. Bleeker, A. Yates, M. D. Rijke",
        "keywords": [
            "latent target decoding",
            "predictive feature suppression",
            "contrastive loss function"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=T1XtOqrVKn",
        "pdf_src": "https://api2.openreview.net/pdf/277b9df7e6d35d025a2dfec8af59f459731e1d24.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper addresses issues related to image-caption retrieval (ICR), where the goal is to find captions describing images similar to given queries based on their content.\n\nResearch Problem:\nThe problem addressed here concerns the vulnerability of contrastive ICR methods towards predictive feature suppression - when there's more than one correct way to predict similarity using certain features among many options available within the model.\n\nMethodology:\nThe proposed solution involves introducing latent target decoding (LTD) into the existing contrastive ICR framework without requiring any new training data nor complex hard negative mining techniques. Specifically, they integrate an auxiliary decoder alongside the main encoder-decoder pair; this decoder attempts to reconstruct the original caption in a latent representation provided by a sentence encoder rather than directly in the input caption space itself – thus preventing the encoders from discarding useful predictive information due to redundancy amongst other potential indicators of similarity.\n\nMain Contributions:\n1. They have introduced LTD—a novel method against predictive feature suppression—by adding another decoder component specifically designed around a sentence encoder’s latent space.\n2. Implemented LTD as an optimization constraint over contrastive loss function(s) ensuring it doesn't conflict but enhances performance metrics such as recall@k, precision at rank r, and normalized discounted cumulative gain (nDCG).\n3. Demonstrated that LTD significantly improves upon standard contrastive ICR baselines across various datasets under consideration through empirical evaluation measures like recall and precision rates above those obtained just by reconstructing captions back out again after encoding them initially before querying further downrange indices 'k'.\n4. Showcased how LTD could potentially work well even if applied differently depending solely on whether you're utilizing single-stage or multi-stage architectures along with diverse types of contrastive learning objectives commonly employed nowadays including InfoNCE, triplet ranking etcetera.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Online Optimal Tracking of Linear Systems with Adversarial Disturbances",
        "abstract": "This paper presents a memory-augmented control solution to the optimal reference tracking problem for linear systems subject to adversarial disturbances. We assume that the dynamics of the linear system are known and that the reference signal is generated by a linear system\nwith unknown dynamics. Under these assumptions, finding the optimal tracking controller is formalized as an online convex optimization problem that leverages memory of past disturbance and reference values to capture their temporal effects on the performance. That is, a (disturbance, reference)-action control policy is formalized, which selects the control actions as a linear map of the past disturbance and reference values. The online convex optimization is then formulated over the parameters of the policy on its past disturbance and reference\nvalues to optimize general convex costs. It is shown that our approach outperforms robust control methods and achieves a tight regret bound O(√T) where in our regret analysis, we have benchmarked against the best linear policy.",
        "authors": "F. A. Yaghmaie, H. Modares",
        "keywords": [
            "memory-augmented control",
            "optimal reference tracking",
            "online convex optimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=5nVJlKgmxp",
        "pdf_src": "https://api2.openreview.net/pdf/8f8c0f7e51735c5b10c0862f1ddd92d47fb2c61c.pdf",
        "Code_src": "",
        "Introduction": "Background: This research addresses the issue of optimal reference tracking with adversaries in linear systems.\n\nResearch Problem: How can one design efficient controllers when both the plant's dynamics and the adversary’s behavior cannot be fully predicted?\n\nMethodology: The authors propose using memory-augmented control based on an online convex optimization framework involving historical data from previous disturbances and reference signals.\nThey introduce a novel action selection strategy - a disturbance-reference-action control policy – that uses a linear function mapping past inputs into current controls aimed at minimizing long-term cost functions.\n\nMain Contributions:\n1. A new type of adaptive control law leveraging memory information about prior disturbances and references has been developed,\n2. An online convex optimization algorithm tailored specifically to this kind of control structure maximizes the benefits gained through such memory augmentation while maintaining computational efficiency,\n\n3. Experimental validation shows superior performance compared to existing robust control approaches under adversarial conditions; \n4. Achieves a regret bound of \\(O(\\sqrt{T})\\), indicating it adapts effectively even if there exist unpredictable changes or adversaries' strategies evolve dynamically",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Deep Double Descent via Smooth Interpolation",
        "abstract": "The ability of overparameterized deep networks to interpolate noisy data, while at the same time showing good generalization performance, has been recently characterized in terms of the double descent curve for the test error. Common intuition from polynomial regression suggests that overparameterized networks are able to sharply interpolate noisy data, without considerably deviating from the ground-truth signal, thus preserving generalization ability. At present, a precise characterization of the relationship between interpolation and generalization for deep networks is missing. In this work, we quantify sharpness of fit of the training data interpolated by neural network functions, by studying the loss landscape w.r.t. to the input variable locally to each training point, over volumes around cleanly- and noisily-labelled training samples, as we systematically increase the number of model parameters and training epochs. Our findings show that loss sharpness in the input space follows both model- and epoch-wise double descent, with worse peaks observed around noisy labels. While small interpolating models sharply fit both clean and noisy data, large interpolating models express a smooth loss landscape, where noisy targets are predicted over large volumes around training data points, in contrast to existing intuition.\n",
        "authors": "M. Gamba, E. Englesson, M. Björkman, et.al",
        "keywords": [
            "interpolation",
            "generalization",
            "loss sharpness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=fempQstMbV",
        "pdf_src": "https://api2.openreview.net/pdf/5c2c79fac79bb63a1bbcab8038dae6d8d2bc063e.pdf",
        "Code_src": "",
        "Introduction": "Background: Recent studies have shown that overparameterized deep networks can interpolate noisy data accurately yet still achieve good generalization performance through what's known as the \"double descent\" phenomenon.\n\nResearch Question: This paper aims to precisely characterize the relationship between interpolation accuracy - how well the network fits the training data - and its generalization capability across different parameter sizes and training epochs when dealing with noisy labeled datasets.\n\nMethodology: The researchers quantitatively measure the sharpness of the fit using local loss landscapes computed on volumes surrounding noise-free and noisy-labeled examples within the dataset. They do so under varying conditions such as increasing the size of the neural network or extending the amount of training provided until they observe patterns related to the double descent behavior regarding both the fitting quality (\"interpolation\") versus the generalization capabilities of these networks.\n\nMain Contributions:\n1. Identification of Double Descent Curves: The study finds evidence supporting two distinct curves – one for the interpolation sharpness and another for generalization errors which exhibit a double descent pattern similar to those seen previously but also reveal novel insights into their interplay specifically concerning noisy label scenarios.\n2. Insight Into Noise Impact: By examining the sharpness of fit near noisy labels closely against nearby noise-free ones during various stages throughout training iterations up to convergence levels not reached before due to computational constraints), it becomes clear there exists an inverse correlation; sharper fits tend towards more pronounced peaks close enough indicating poor generalization whereas smoother fits suggest better robustness despite less accurate initial approximations made early-on during learning processes involving noisy inputs leading toward improved final performances after sufficient adjustments occur via regularization mechanisms like dropout etcetera).",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "A Variational Perspective on Generative Flow Networks",
        "abstract": "Generative flow networks (GFNs) are a class of probabilistic models for sequential sampling of composite objects, proportional to a target distribution that is defined in terms of an energy function or a reward. GFNs are typically trained using a flow matching or trajectory balance objective, which matches forward and backward transition models over trajectories. In this work we introduce a variational objective for training GFNs, which is a convex combination of the reverse- and forward KL divergences, and compare it to the trajectory balance objective when sampling from the forward- and backward model, respectively. We show that, in certain settings, variational inference for GFNs is equivalent to minimizing the trajectory balance objective, in the sense that both methods compute the same score-function gradient. This insight suggests that in these settings, control variates, which are commonly used to reduce the variance of score-function gradient estimates, can also be used with the trajectory balance objective. We evaluate our findings and the performance of the proposed variational objective numerically by comparing it to the trajectory balance objective on two synthetic tasks.",
        "authors": "H. Zimmermann, F. Lindsten, J. V. D. Meent, et.al",
        "keywords": [
            "GFNs",
            "Variational Objective",
            "Trajectory Balance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=AZ4GobeSLq",
        "pdf_src": "https://api2.openreview.net/pdf/241707cf7e815abd7d0368626b2da365ff876aff.pdf",
        "Code_src": "",
        "Introduction": "Background: Generative flow networks (GFNs) are a type of probabilistic model designed for sequentially generating samples of complex objects according to a given target distribution specified through an energy function.\n\nResearch Question: How effective is a novel variational objective compared to existing trajectory balance objectives?\n\nMethod: The authors propose a new variational objective based on a convex combination of the reverse and forward Kullback-Leibler (KL) divergences as opposed to traditional trajectory balancing approaches involving flow matching between forward and backward transition models along trajectories.\nThe study compares the performance of their variational objective against the trajectory balance objective across different scenarios where either the forward or backward model's samples were considered during generation.\n\nMain Contributions:\n1. They present a novel variational objective suitable for training GFNs without relying solely on trajectory balance.\n2. Demonstrate theoretically how under specific conditions, variational inference for GFNs aligns with trajectory balance optimization leading to identical score-function gradients computation strategies within those contexts suggesting potential synergy regarding variance reduction techniques like control variates usage alongside trajectory balance.\n3. Conduct numerical experiments validating empirical equivalence via comparison results obtained applying both objectives towards solving synthetic tasks related to generative modeling challenges posed hereunder consideration.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Differentially private partitioned variational inference",
        "abstract": "Learning a privacy-preserving model from sensitive data which are distributed across multiple devices is an increasingly important problem. The problem is often formulated in the federated learning context, with the aim of learning a single global model while keeping the data distributed. Moreover, Bayesian learning is a popular approach for modelling, since it naturally supports reliable uncertainty estimates. However, Bayesian learning is generally intractable even with centralised non-private data and so approximation techniques such as variational inference are a necessity. Variational inference has recently been extended to the non-private federated learning setting via the partitioned variational inference algorithm. For privacy protection, the current gold standard is called differential privacy. Differential privacy guarantees privacy in a strong, mathematically clearly defined sense.\n\nIn this paper, we present differentially private partitioned variational inference, the first general framework for learning a variational approximation to a Bayesian posterior distribution in the federated learning setting while minimising the number of communication rounds and providing differential privacy guarantees for data subjects.\n\nWe propose three alternative implementations in the general framework, one based on perturbing local optimisation runs done by individual parties, and two based on perturbing updates to the global model (one using a version of federated averaging, the second one adding virtual parties to the protocol), and compare their properties both theoretically and empirically.  We show that perturbing the local optimisation works well with simple and complex models as long as each party has enough local data. However, the privacy is always guaranteed independently by each party. In contrast, perturbing the global updates works best with relatively simple models. Given access to suitable secure primitives, such as secure aggregation or secure shuffling, the performance can be improved by all parties guaranteeing privacy jointly.",
        "authors": "M. A. Heikkilä, M. Ashman, S. Swaroop, et.al",
        "keywords": [
            "differential privacy",
            "federated learning",
            "partitioned variational inference"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=55BcghgicI",
        "pdf_src": "https://api2.openreview.net/pdf/d8de017dcac719a15b123fb3e6e4bb9030b05fd5.pdf",
        "Code_src": "",
        "Introduction": "Background: Learning a privacy-preserving model from sensitive data distributed across multiple devices is becoming more critical due to advancements in technology.\nResearch Problem: How do you learn a single global model without compromising user privacy when dealing with sensitive data?\nMethod: Federated learning combined with Bayesian learning methods like variational inference under differential privacy constraints was proposed here.\nMain Contributions:\n1. Introduced Differentially Private Partitioned Variational Inference - A novel framework enabling federated learning where a Bayesian posterior distribution's variational approximation could be learned whilst minimizing communication rounds & ensuring differential privacy.\n2. Proposed Three Alternative Implementations within This Framework:\n   - One method involves perturbing local optimization performed at client sites individually;\n   - Another perturbs updates made towards the global model through a variant of federated averaging; \n   - Lastly, introduces \"virtual\" participants into the process during update phases also altering these updates.\n3. Conducted theoretical analysis comparing these approaches along with empirical tests showing effectiveness against various complexity levels of models including demonstrating robustness over both simple and complex datasets provided sufficient local data existed per participant node.",
        "Topic": "Federated Learning"
    },
    {
        "title": "Private Multi-Task Learning: Formulation and Applications to Federated Learning",
        "abstract": "Many problems in machine learning rely on multi-task learning (MTL), in which the goal is to solve multiple related machine learning tasks simultaneously. MTL is particularly relevant for privacy-sensitive applications in areas such as healthcare, finance, and IoT computing,\nwhere sensitive data from multiple, varied sources are shared for the purpose of learning. In this work, we formalize notions of client-level privacy for MTL via billboard privacy (BP), a relaxation of differential privacy for mechanism design and distributed optimization. We then propose an algorithm for mean-regularized MTL, an objective commonly used for applications in personalized federated learning, subject to BP. We analyze our objective and solver, providing certifiable guarantees on both privacy and utility. Empirically, we find that our method provides improved privacy/utility trade-offs relative to global baselines across common federated learning benchmarks.",
        "authors": "S. Hu, S. Wu, V. Smith",
        "keywords": [
            "multi-task learning",
            "privacy-sensitive applications",
            "billboard privacy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=onufdyHvqN",
        "pdf_src": "https://api2.openreview.net/pdf/3e72e471e7ac13653b7a6fe33302372a5593aebc.pdf",
        "Code_src": "",
        "Introduction": "Background: Multi-task learning (MTL) has become increasingly popular due to its potential benefits over single-task learning methods when dealing with complex datasets involving various interrelated tasks. Privacy-sensitive domains like healthcare, finance, and IoT require sharing sensitive information among different entities while maintaining confidentiality.\n\nResearch Problem: The challenge lies in designing algorithms within these domains where clients' private data must be protected during model training through mechanisms known as Differential Privacy relaxations or Billboard Privacy (BP).\n\nMethodology: This paper introduces BP—a novel approach based on Differential Privacy—as a means to preserve client-level privacy throughout the process of MTL by allowing some leakage but ensuring it does not compromise individual identities.\nThe authors also present a new algorithm called Mean-Regularized MTL tailored specifically under BP constraints suitable for personalizing federated learning scenarios—wherein models learn collaboratively at decentralized locations without revealing any local data beyond aggregated statistics.\n\nMain Contributions:\n1. They formally define BP's concept regarding client-level privacy protection standards applicable especially well suited towards MTL settings compared traditional DP approaches;\n2. Propose an innovative algorithmic framework incorporating BP principles into their Mean-Regularized MTL strategy designed explicitly addressing requirements found within federated learning contexts;\n3. Analyze comprehensively how they've integrated BP considerations alongside performance metrics yielding provable guarantees about preserving user anonymity whilst optimizing task outcomes; \n4. Conduct empirical evaluations against existing global baseline solutions demonstrating superior trade-offs between privacy preservation and overall system efficacy using standard benchmark datasets widely recognized",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Active Learning of Ordinal Embeddings: A User Study on Football Data",
        "abstract": "Humans innately measure distance between instances in an unlabeled dataset using an unknown similarity function. Distance metrics can only serve as proxy for similarity in information retrieval of similar instances. Learning a good similarity function from human annotations improves the quality of retrievals. This work uses deep metric learning to learn these user-defined similarity functions from few annotations for a large football trajectory dataset.\nWe adapt an entropy-based active learning method with recent work from triplet mining to collect easy-to-answer but still informative annotations from human participants and use them to train a deep convolutional network that generalizes to unseen samples. \nOur user study shows that our approach improves the quality of the information retrieval compared to a previous deep metric learning approach that relies on a Siamese network. Specifically, we shed light on the strengths and weaknesses of passive sampling heuristics and active learners alike by analyzing the participants' response efficacy. To this end, we collect accuracy, algorithmic time complexity, the participants' fatigue and time-to-response, qualitative self-assessment and statements, as well as the effects of mixed-expertise annotators and their consistency on model performance and transfer-learning.\n",
        "authors": "C. Löffler, K. Fallah, S. Fenu, et.al",
        "keywords": [
            "deep metric learning",
            "active learning",
            "similarity function"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=oq3tx5kinu",
        "pdf_src": "https://api2.openreview.net/pdf/c32149cae3bf52af79899f8f3e93c273fd9210ea.pdf",
        "Code_src": "",
        "Introduction": "Background: Humans have innate ability to measure distances among instances without labels through an unknown similarity function. However, existing distance metrics are proxies rather than true measures of similarity when retrieving similar instances based on labeled data. Therefore, it is crucial to develop methods capable of learning effective similarity functions directly from human annotations.\n\nResearch Problem: How do you design algorithms able to efficiently extract useful similarity functions within datasets while requiring minimal human annotation?\n\nMethod: The authors employ deep metric learning techniques adapted specifically towards collecting easily answerable yet informative annotations via an entropy-based active learning strategy inspired by recent advancements in triplet mining approaches involving humans. They utilize collected annotations along with Convolutional Neural Networks (CNNs) trained accordingly which generalize effectively across previously unseen examples.\n\nMain Contributions:\n1. A novel adaptation combining entropy-based active learning strategies derived from triplet mining literature into a practical framework suitable for extracting valuable similarities out of relatively small sets of manually annotated examples; \n2. Demonstrating improved Information Retrieval Quality over prior state-of-the-art models utilizing Siamese networks;\n3. Providing insights regarding both passive sampling heuristics commonly used before active learner integration versus newer active learners themselves – achieved thorough participant analysis including effectiveness indices like accuracy rates alongside computational efficiency considerations such as algorithmic time complexities during training phases plus subjective assessments related to annotator fatigue levels or response times;\n4. Highlighting impacts arising due to varying expertise amongst annotators concerning how they affect final model performances particularly under Transfer Learning scenarios where learned knowledge might be applied beyond initial domain boundaries.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Adaptive patch foraging in deep reinforcement learning agents",
        "abstract": "Patch foraging is one of the most heavily studied behavioral optimization challenges in biology. However, despite its importance to biological intelligence, this behavioral optimization problem is understudied in artificial intelligence research. Patch foraging is especially amenable to study given that it has a known optimal solution, which may be difficult to discover given current techniques in deep reinforcement learning. Here, we investigate deep reinforcement learning agents in an ecological patch foraging task. For the first time, we show that machine learning agents can learn to patch forage adaptively in patterns similar to biological foragers, and approach optimal patch foraging behavior when accounting for temporal discounting. Finally, we show emergent internal dynamics in these agents that resemble single-cell recordings from foraging non-human primates, which complements experimental and theoretical work on the neural mechanisms of biological foraging. This work suggests that agents interacting in complex environments with ecologically valid pressures arrive at common solutions, suggesting the emergence of foundational computations behind adaptive, intelligent behavior in both biological and artificial agents.",
        "authors": "N. Wispinski, A. Butcher, K. W. Mathewson, et.al",
        "keywords": [
            "ecological patch foraging",
            "deep reinforcement learning",
            "adaptive behavior"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=a0T3nOP9sB",
        "pdf_src": "https://api2.openreview.net/pdf/c459941a269f74c1ac5af477f54d5ae046912db9.pdf",
        "Code_src": "",
        "Introduction": "Background: Patch foraging refers to animals searching through different patches or habitats looking for food resources while minimizing energy expenditure.\n\nResearch Question: Despite being crucial for animal survival strategies across species, how do organisms optimize their search behaviors within patchy landscapes? How does this translate into computational models?\n\nMethodology: The authors employed deep reinforcement learning algorithms where virtual agents were tasked with finding the best strategy among various patches under varying conditions such as resource availability over time (\"temporal discounting\").\n\nMain Contributions:\n1. They demonstrated that deep reinforcement learning agents could indeed learn efficient patch foraging strategies.\n2. These learned strategies showed similarities not only quantitatively but also qualitatively – they mimicked observed patterns seen in natural foragers regarding spatial distribution between visits ('patch tenacity') without any prior knowledge about those behaviors during training.\n3. By introducing temporal discounting factors - rewarding immediate actions more than distant ones -, the agents' performance approached optimality closer compared to standard RL methods ignoring time dimension.\n4. Lastly, by analyzing the internal states of these agents using dynamical systems theory tools akin to those used studying neuronal networks involved in navigation tasks like hippocampal place cells found in humans/animals, parallels emerged indicating potential underlying principles governing adaptive decision-making processes shared potentially",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "FASTRAIN-GNN: Fast and Accurate Self-Training for Graph Neural Networks",
        "abstract": "Few-shot learning with Graph Neural Networks (GNNs) is an important challenge in expanding the remarkable success that GNNs have achieved. In the transductive node classification scenario, conventional supervised training methods for GNNs fail when only few labeled nodes are available. Self-training, wherein the GNN is trained in stages by augmenting the training data with a subset of the unlabeled data and the predictions of the GNN on this data (pseudolabels), has emerged as a promising approach to few-shot transductive learning. However, multi-stage self-training significantly increases the computational demands of GNN training. In addition, while the training set evolves considerably across the stages of self-training, the GNN architecture, graph topology and training hyperparameters are kept constant, adversely affecting the accuracy of the resulting model as well as the computational efficiency of training. To address this challenge, we propose FASTRAIN-GNN, a framework for efficient and accurate self-training of GNNs with few labeled nodes. FASTRAIN-GNN performs four main optimizations in each stage of self-training: (1) Sampling-based Pseudolabel Filtering removes nodes whose pseudolabels are likely to be incorrect from the enlarged training set. (2,3) Dynamic Sizing and Dynamic Regularization find the optimal network architecture and amount of training regularization in each stage of self-training, respectively, and (4) Progressive Graph Pruning removes selected edges between nodes in the training set to reduce the impact of over-smoothing. On few-shot node classification tasks using different GNN architectures, FASTRAIN-GNN produces models that are consistently more accurate (by up to 4.4%), while also substantially reducing the self-training time (by up to 2.1X) over the current state-of-the-art methods. Code is available at https://github.com/amrnag/FASTRAIN-GNN.",
        "authors": "A. Nagarajan, A. Raghunathan",
        "keywords": [
            "few-shot learning",
            "Graph Neural Networks",
            "self-training"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=1IYJfwJtjQ",
        "pdf_src": "https://api2.openreview.net/pdf/a15cc223b404f1745a5947704a5273d59000772c.pdf",
        "Code_src": "https://github.com/amrnag/FASTRAIN-GNN",
        "Introduction": "Background:\nGraph Neural Networks (GNNs) have demonstrated significant performance improvements compared to traditional machine learning algorithms due to their ability to capture complex relationships within graphs such as social networks or protein structures. Few-shot learning extends these capabilities further but presents challenges particularly relevant during transductive node classification scenarios where there may not be enough labeled nodes.\n\nResearch Problem:\nThe primary research problem addressed here concerns how to effectively train GNNs under conditions involving limited availability of labeled nodes through the use of few-shot learning techniques specifically designed for transductive node classification settings.\n \nMethodology:\nTo tackle this issue, the authors introduce FASTRAIN-GNN - a novel framework aimed at improving both the speed and accuracy of self-training processes used commonly in GNNs' few-shot learning contexts. The method incorporates several key innovations into its design:\n\n- Sampling-Based Pseudolabel Filtering: This technique filters out potentially inaccurate pseudolabels generated based on initial predictions made by the GNN itself which can lead to improved quality of subsequent iterations.\n  \n- Dynamic Sizing & Dynamic Regularization: These strategies dynamically adjust neural network size and apply varying levels of regularization throughout multiple rounds of self-training ensuring adaptability according to evolving datasets.\n\n- Progressive Graph Pruning: By selectively removing certain edges among nodes present in our dataset before retraining begins again helps mitigate issues related to over-smoothing common amongst deep learning models.\n\nMain Contributions:\nThis work introduces FASTRAIN-GNN—a highly optimized solution capable of efficiently performing self-training even amidst scarce labeling resources—resulting in higher accuracy rates than existing benchmarks without sacrificing too much computation effort needed per iteration cycle. Specifically, it reduces required runtime drastically whilst maintaining comparable if better predictive power against other leading approaches currently employed today making it suitable especially practical applications requiring quick adaptation yet still maintain high precision requirements like recommendation systems dealing with dynamic user behavior patterns etc.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Memory-efficient Reinforcement Learning with Value-based Knowledge Consolidation",
        "abstract": "Artificial neural networks are promising for general function approximation but challenging to train on non-independent or non-identically distributed data due to catastrophic forgetting. The experience replay buffer, a standard component in deep reinforcement learning, is often used to reduce forgetting and improve sample efficiency by storing experiences in a large buffer and using them for training later. However, a large replay buffer results in a heavy memory burden, especially for onboard and edge devices with limited memory capacities. We propose memory-efficient reinforcement learning algorithms based on the deep Q-network algorithm to alleviate this problem. Our algorithms reduce forgetting and maintain high sample efficiency by consolidating knowledge from the target Q-network to the current Q-network. Compared to baseline methods, our algorithms achieve comparable or better performance in both feature-based and image-based tasks while easing the burden of large experience replay buffers.",
        "authors": "Q. Lan, Y. Pan, J. Luo, et.al",
        "keywords": [
            "memory-efficient",
            "reinforcement learning",
            "experience replay buffer"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=zSDCvlaVBn",
        "pdf_src": "https://api2.openreview.net/pdf/c21ad6caf079aa0f2447d8e0f42c4a33b89e09d1.pdf",
        "Code_src": "",
        "Introduction": "Background: Artificial neural networks have shown great promise as universal approximators; however, they face significant challenges when trained on non-i.i.d. data because it can lead to catastrophic forgetting.\n\nResearch Problem: How do we address the issue of catastrophic forgetting during training on non-i.i.d. data?\n\nMethod: To tackle this challenge, we introduce novel memory-efficient reinforcement learning algorithms that leverage the DQN framework which incorporate an experience replay buffer strategy without suffering from its associated memory overheads typically faced within resource-constrained environments such as those found at the edge or onboard IoT devices.\n\nMain Contributions:\n1. We successfully consolidate learned information across different episodes into one shared representation space.\n2. This consolidation process significantly reduces the need for extensive storage required traditionally through larger replay buffers - thus making these approaches more practical even under stringent memory constraints prevalent in real-world applications like autonomous systems where computation resources may be scarce yet crucially important functionalities must still operate optimally over time despite changing conditions encountered throughout their deployment lifecycle.",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Proximal Curriculum for Reinforcement Learning Agents",
        "abstract": "We consider the problem of curriculum design for reinforcement learning (RL) agents in contextual multi-task settings. Existing techniques on automatic curriculum design typically require domain-specific hyperparameter tuning or have limited theoretical underpinnings. To tackle these limitations, we design our curriculum strategy, ProCuRL, inspired by the pedagogical concept of Zone of Proximal Development (ZPD). ProCuRL captures the intuition that learning progress is maximized when picking tasks that are neither too hard nor too easy for the learner. We mathematically derive ProCuRL by analyzing two simple learning settings. We also present a practical variant of ProCuRL that can be directly integrated with deep RL frameworks with minimal hyperparameter tuning. Experimental results on a variety of domains demonstrate the effectiveness of our curriculum strategy over state-of-the-art baselines in accelerating the training process of deep RL agents.",
        "authors": "G. Tzannetos, B. G. Ribeiro, P. Kamalaruban, et.al",
        "keywords": [
            "ProCuRL",
            "Reinforcement Learning",
            "Curriculum Design"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=8WUyeeMxMH",
        "pdf_src": "https://api2.openreview.net/pdf/311aa4b1eb3807fae0f2f6cc256b59ece0fc78b7.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the challenge of designing curricula for reinforcement learning (RL) agents operating within complex environments where they must learn multiple tasks simultaneously.\n\nResearch Problem: How to create an effective curriculum for RL agents facing multi-contextual and multi-task scenarios without requiring extensive manual adjustments?\n\nMethodology: Inspired by the educational principle known as \"Zone of Proximal Development\" which suggests optimal learning occurs near current capabilities but not beyond them yet achievable through effort alone; authors propose ProCuRL - a curriculum strategy aimed at selecting tasks challenging enough while still feasible.\nThe method involves mathematical derivation from analysis of simplified learning setups followed by development of a practical version suitable for integration into existing deep RL frameworks minimising need for further hyperparameter optimisation.\n\nMain Contributions:\n1. A novel curriculum approach named ProCuRL grounded theoretically based on ZPD principles tailored specifically for multi-contextual multi-task RL problems;\n2. Mathematical formulation providing insights about task difficulty selection during learning progression;\n3. Practical implementation details allowing seamless incorporation into standard deep RL architectures reducing dependency upon laborious hyperparameter tuning processes;\n4. Demonstrated empirical evidence showing superior performance compared against leading benchmarks across diverse application areas indicating accelerated convergence rates",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Sobolev Spaces, Kernels and Discrepancies over Hyperspheres",
        "abstract": "This work extends analytical foundations for kernel methods beyond the usual Euclidean manifold.  Specifically, we characterise the smoothness of the native spaces (reproducing kernel Hilbert spaces) that are reproduced by geodesically isotropic kernels in the hyperspherical context.  Our results are relevant to several areas of machine learning; we focus on their consequences for kernel cubature, determining the rate of convergence of the worst case error, and expanding the applicability of cubature algorithms based on Stein's method.  First, we introduce a characterisation of Sobolev spaces on the $d$-dimensional sphere based on the Fourier--Schoenberg sequences associated with a given kernel.  Such sequences are hard (if not impossible) to compute analytically on $d$-dimensional spheres, but often feasible over Hilbert spheres, where $d = \\infty$.  Second, we circumvent this problem by finding a projection operator that allows us to map from Hilbert spheres to finite-dimensional spheres.  Our findings are illustrated for selected parametric families of kernel. ",
        "authors": "S. Hubbert, E. Porcu, C. J. Oates, et.al",
        "keywords": [
            "kernel methods",
            "reproducing kernel Hilbert spaces",
            "kernel cubature"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=82hRiAbnnm",
        "pdf_src": "https://api2.openreview.net/pdf/415ad33209085e0ad791120b23e14c1267d28c07.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper expands upon existing theoretical frameworks within kernel methods which primarily operate under the assumption of data residing on an Euclidean manifold.\n\nResearch Problem: The central challenge addressed is how to extend these kernel methods when dealing with non-Euclidean domains such as the hypersphere or more generally, Riemannian manifolds without assuming geodesic isotropy - a property usually assumed implicitly due to computational convenience rather than mathematical necessity.\n\nMethods: To tackle this issue, two main contributions have been made:\n1. A novel characterization framework has been developed using Fourier-Schoenberg sequences related to specific kernels.\n2. An innovative approach involves utilizing a projection operator capable of mapping between infinite-dimensional Hilbert spaces onto finite-dimensional spherical spaces while preserving key properties like Sobolev regularity structures necessary for kernel-based computations.\n\nMain Contributions: \n- The introduction of a new way to define Sobolev spaces directly on the d-dimensional sphere through Fourier-Schoenberg sequences adapted specifically to each kernel function being considered – providing a robust mathematical foundation even if those sequences cannot be computed exactly because they involve infinite dimensions.\n- The development of a practical technique allowing one to project functions defined on infinite-dimensional Hilbert spaces into finite-dimensional ones so that they can still benefit from kernel operations despite lacking geometric isotropy conditions traditionally required elsewhere yet impractical here.\nThese advancements open up possibilities across various fields including improved kernel cubature techniques leading towards better approximations along with broader applications leveraging Stein's method among others thus contributing significantly toward advancing our understanding",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "MASIF: Meta-learned Algorithm Selection using Implicit Fidelity Information",
        "abstract": "Selecting a well-performing algorithm for a given task or dataset can be time-consuming and\ntedious, but is crucial for the successful day-to-day business of developing new AI & ML\napplications. Algorithm Selection (AS) mitigates this through a meta-model leveraging\nmeta-information about previous tasks. However, most of the available AS methods are\nerror-prone because they characterize a task by either cheap-to-compute properties of the\ndataset or evaluations of cheap proxy algorithms, called landmarks. In this work, we extend\nthe classical AS data setup to include multi-fidelity information and empirically demonstrate\nhow meta-learning on algorithms’ learning behaviour allows us to exploit cheap test-time\nevidence effectively and combat myopia significantly. We further postulate a budget-regret\ntrade-off w.r.t. the selection process. Our new selector MASIF is able to jointly interpret\nonline evidence on a task in form of varying-length learning curves without any parametric\nassumption by leveraging a transformer-based encoder. This opens up new possibilities for\nguided rapid prototyping in data science on cheaply observed partial learning curves.",
        "authors": "T. Ruhkopf, A. Mohan, D. Deng, et.al",
        "keywords": [
            "algorithm selection",
            "meta-learning",
            "multi-fidelity"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=5aYGXxByI6",
        "pdf_src": "https://api2.openreview.net/pdf/5c964a9e165979a299728efe5566dd043bc96474.pdf",
        "Code_src": "",
        "Introduction": "Background: Selecting an appropriate algorithm from many options efficiently plays a critical role in the development of artificial intelligence (AI) applications.\n\nResearch Problem: Existing approaches often rely on inaccurate proxies like landmark algorithms due to their computational efficiency; however, these may not accurately reflect performance across all datasets.\n \nMethodology: The paper introduces a novel approach that incorporates multi-fidelity information into the algorithm selection framework using meta-learning techniques based on algorithms' learning behavior patterns.\n\nMain Contributions:\n1. Expands upon traditional algorithm selection setups with multi-fidelity information – beyond just low-cost approximations - which provides more accurate predictions regarding actual algorithm performance.\n2. Demonstrates how exploiting cheaper yet informative test-time evidence during meta-learning helps mitigate bias commonly found when only considering simple proxies (\"landmarks\").\n3. Proposes a trade-off between regret incurred over selecting suboptimal algorithms versus resources allocated towards gathering additional training data ('budget-regret').\n4. Introduces MASIF, a model-agnostic selector informed by sequence transformers capable of interpreting variable-length learning curve trajectories online while making no assumptions about underlying parameters within those curves,\n5. Enables guided rapid prototyping where one could iteratively refine models against limited observations rather than needing extensive labeled data upfront.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Pre-trained Perceptual Features Improve Differentially Private Image Generation",
        "abstract": "Training even moderately-sized generative models with differentially-private stochastic gradient descent (DP-SGD) is difficult:\nthe required level of noise for reasonable levels of privacy is simply too large.\nWe advocate instead building off a good, relevant representation on an informative public dataset, then learning to model the private data with that representation.\nIn particular, we minimize the maximum mean discrepancy (MMD) between private target data and a generator's distribution,\nusing a kernel based on perceptual features learned from a public dataset.\nWith the MMD, we can simply privatize the data-dependent term once and for all,\nrather than introducing noise at each step of optimization as in DP-SGD.\nOur algorithm allows us to generate CIFAR10-level images with $\\epsilon \\approx 2$ which capture distinctive features in the distribution,\nfar surpassing the current state of the art, which mostly focuses on datasets such as MNIST and FashionMNIST at a large $\\epsilon \\approx 10$.\nOur work introduces simple yet powerful foundations for reducing the gap between private and non-private deep generative models.\nOur code is available at https://github.com/ParkLabML/DP-MEPF.",
        "authors": "F. Harder, M. Jalali, D. J. Sutherland, et.al",
        "keywords": [
            "dp-sgd",
            "mmd",
            "private data modeling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=R6W7zkMz0P",
        "pdf_src": "https://api2.openreview.net/pdf/1dde0b5c9f4fdaaf091f2497f49c7c338d477803.pdf",
        "Code_src": "https://github.com/ParkLabML/DP-MEPF",
        "Introduction": "Background: Training generative models while preserving differential privacy using stochastic gradient descent (SGD) has been challenging due to the need for substantial amounts of noise.\n\nResearch Problem: How to train moderately-sized generative models with differential privacy without requiring excessive noise?\n\nMethod: Instead of training directly on private data, build upon a pre-trained representation learned on a publicly available dataset; learn to map private data into this representation by minimizing the Maximum Mean Discrepancy (MMD); use a kernel based on perceptual features extracted from the public dataset.\n\nMain Contributions: Our approach significantly reduces the amount of noise needed compared to traditional DP-SGD methods when generating images like those found in CIFAR10, achieving better results despite having a much lower $\\epsilon$ value—indicating our method effectively bridges the gap between private and non-private generative models performance-wise. We have also made our code open-source so others may replicate or extend these findings.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Bridging performance gap between minimal and maximal SVM models",
        "abstract": "Multi-class support vector machine (SVM) models are typically built using all possible pairs of binary SVM in a one-against-one fashion. This requires too much computation for datasets with hundreds or thousands of classes,  which motivates the search for multi-class models that do not use all pairwise SVM.  Our models correspond to the choice of the model graph, whose vertices correspond to classes and edges represent which pairwise SVMs are trained. We conduct experiments to uncover metrical and topological properties that impact the accuracy of a multi-class SVM model. Based on their results we propose a way to construct intermediate multi-class SVM models. The key insight is that for model graphs of diameter two, we can estimate missing pairwise probabilities from the known ones thus transforming the computation of posteriors to the usual complete (maximal) case. Our proposed algorithm allows one to reduce computational effort by 50-80% while keeping accuracy near, or even above that of a softmax classifier. In our work we use convolutional data sets, which have multiple advantages for benchmarking multi-class SVM models.",
        "authors": "O. Such, R. Fabricius",
        "keywords": [
            "multi-class SVM",
            "model graph",
            "computational efficiency"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=SM1BkjGePI",
        "pdf_src": "https://api2.openreview.net/pdf/1dfc6a6a219b2968bf7fabc1c684e77bb5d19227.pdf",
        "Code_src": "",
        "Introduction": "Background: Multi-class Support Vector Machine (SVM) models traditionally require building binary SVM classifiers between each pair of classes (\"one against one\"), leading to high computational complexity as class count increases.\n\nResearch Problem: To address this issue, researchers seek alternative methods within multi-class SVM frameworks without requiring exhaustive pairwise comparisons.\n \nMethodology: The paper introduces an approach based on constructing a \"model graph\" where nodes represent individual classes; edges indicate whether pairwise SVMs should be considered during training. Experiments were conducted to identify metrics and structural features affecting SVM performance.\n\nMain Contributions:\n1. A novel method was developed involving the construction of intermediate multi-class SVM models through strategic selection of the model graph structure – specifically focusing on those with small diameters such as two.\n2. For these specific graph structures—where the diameter equals two—the study found it feasible to infer missing pairwise probabilities given existing information about certain probability values related to the posterior distribution calculation process after classification prediction has been made.\n3. An algorithmic innovation significantly reduces computational requirements compared to traditional approaches ranging anywhere up to approximately half when dealing with large-scale datasets yet maintains similar levels if not surpasses accuracy rates achieved via softmax classifiers commonly used today across various domains including image recognition tasks among others mentioned earlier).",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Bayesian Transformed Gaussian Processes",
        "abstract": "The Bayesian transformed Gaussian (BTG) model, proposed by Kedem and Oliviera in 1997, was developed as a Bayesian approach to trans-Kriging in the spatial statistics community. In this paper, we revisit BTG in the context of modern Gaussian process literature by framing it as a fully Bayesian counterpart to the Warped Gaussian process that marginalizes out a joint prior over input warping and kernel hyperparameters.  As with any other fully Bayesian approach, this treatment introduces prohibitively expensive computational overhead; unsurprisingly, the BTG posterior predictive distribution, itself estimated through high-dimensional integration, must be inverted in order to perform model prediction.  To address these challenges, we introduce principled numerical techniques for computing with BTG efficiently using a combination of doubly sparse quadrature rules, tight quantile bounds, and rank-one matrix algebra to enable both fast model prediction and model selection. These efficient methods allow us to compute with higher-dimensional datasets and apply BTG with layered transformations that greatly improve its expressibility. We demonstrate that BTG achieves superior empirical performance over MLE-based models in the low-data regime ---situations in which MLE tends to overfit.  ",
        "authors": "X. Zhu, L. Huang, E. H. Lee, et.al",
        "keywords": [
            "Bayesian Transformed Gaussian",
            "Gaussian Process",
            "Computational Efficiency"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=4zCgjqjzAv",
        "pdf_src": "https://api2.openreview.net/pdf/7aa37da1502343304fb807cd786fb3139da46a99.pdf",
        "Code_src": "",
        "Introduction": "Background: The Bayesian Transformed Gaussian (BTG) model is an extension of the Warped Gaussian Process introduced into the spatial statistics field back in 1997.\n\nResearch Problem: This work aims at revisiting the BTG within the current framework of Gaussian processes while considering it from a full Bayesian perspective rather than just as a transformation method like Warped Gaussian Processes do.\n \nMethodology: The authors frame BTG not only as a way to transform inputs but also as a complete Bayesian alternative focusing on jointly modeling input warpings and kernel parameters' priors without assuming independence between them or their derivatives. However, they note two main issues:\n1. Computational Complexity - Estimating the posterior predictive distribution involves computationally costly integrations across multiple dimensions leading up to inversion problems during predictions;\n2. Data Handling Limitation - Due to such complexity, applying BTG directly has been limited mainly to lower dimensional data sets where computations are manageable.\n\nMain Contributions: Addressing those limitations requires novel approaches allowing more extensive use cases including handling larger datasets effectively even when dealing with complex transformations involving many layers:\n\n- They develop new numerical algorithms combining Doubly Sparse Quadrature Rules along with Rank-One Matrix Algebraic Techniques providing significant speed improvements compared traditional methods due to reduced computation time required per step in estimation procedures;\n- Introduce Tight Quantile Bounds around predicted values further aiding confidence intervals calculations making inference tasks feasible despite computational constraints;\n- Demonstrate empirically how these advancements lead to better empirical performances particularly under scenarios where Maximum Likelihood Estimates (MLE) based models tend towards overfitting – hence showing improved robustness against noise present in small sample sizes settings commonly encountered practical applications related spatial statistics fields.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Uncovering the Representation of Spiking Neural Networks Trained with Surrogate Gradient",
        "abstract": "Spiking Neural Networks (SNNs) are recognized as the candidate for the next-generation neural networks due to their bio-plausibility and energy efficiency. Recently, researchers have demonstrated that SNNs are able to achieve nearly state-of-the-art performance in image recognition tasks using surrogate gradient training. However, some essential questions exist pertaining to SNNs that are little studied: Do SNNs trained with surrogate gradient learn different representations from traditional Artificial Neural Networks (ANNs)? Does the time\ndimension in SNNs provide unique representation power? In this paper, we aim to answer these questions by conducting a representation similarity analysis between SNNs and ANNs using Centered Kernel Alignment (CKA). We start by analyzing the spatial dimension of\nthe networks, including both the width and the depth. Furthermore, our analysis of residual connections shows that SNNs learn a periodic pattern, which rectifies the representations in SNNs to be ANN-like. We additionally investigate the effect of the time dimension on SNN representation, finding that deeper layers encourage more dynamics along the time dimension. We also investigate the impact of input data such as event-stream data and adversarial attacks. Our work uncovers a host of new findings of representations in SNNs. We hope this work will inspire future research to fully comprehend the representation power of SNNs. Code is released at https://github.com/Intelligent-Computing-Lab-Yale/SNNCKA.",
        "authors": "Y. Li, Y. Kim, H. Park, et.al",
        "keywords": [
            "SNNs",
            "Representation Similarity Analysis",
            "Centered Kernel Alignment"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=s9efQF3QW1",
        "pdf_src": "https://api2.openreview.net/pdf/81853a15d6fcbe3a3a7c1ab67109bdf67f07e0a7.pdf",
        "Code_src": "https://github.com/Intelligent-Computing-Lab-Yale/SNNCKA",
        "Introduction": "Background:\nSpiking Neural Networks (SNNs), inspired by biological neurons' spiking behavior, hold promise over conventional Artificial Neural Networks (ANNs) because they can potentially offer higher computational efficiency while maintaining biological plausibility.\n\nResearch Question:\nThe study raises several critical inquiries regarding SNNs compared to ANNs:\n\n1. Representation Learning: Are there differences in learned representations when comparing SNNs trained via surrogate gradients against those trained traditionally?\n2. Temporal Dimension: Can the temporal aspect within SNNs contribute uniquely to representational capabilities?\n\nMethodology:\nTo address these issues, the authors employ a novel approach called Centered Kernel Alignment (CKA), an algorithm used broadly across machine learning fields but not previously applied specifically to compare SNNs directly with ANNs.\nThey conduct a comprehensive comparison focusing initially on the spatial dimensions—width and depth—and then extend it to include the role of residual connections where they observe a periodicity indicative of ANN-like rectification patterns emerging during training through surrogate gradients.\nAdditionally, empirical studies delve into how varying depths affect the dynamic nature represented throughout the network's temporal axis under different types of inputs like event streams or adversarial examples.\n\nMain Contributions:\nThis investigation yields significant insights about the representation properties specific to SNNs beyond what has been reported thus far; particularly highlighting the emergence of ANN-like periodicities even though trained differently than ANNs might expect based solely on their architecture similarities alone suggests additional mechanisms may play crucial roles influencing these results further explored here could inform better understanding towards leveraging SNNs optimally alongside other neural architectures moving forward. The codebase detailing all experiments conducted serves open-source availability fostering reproducibility among interested parties contributing significantly toward advancing knowledge around this topic area overall.",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "PAC-Bayes Generalisation Bounds for Heavy-Tailed Losses through Supermartingales",
        "abstract": "While PAC-Bayes is now an established learning framework for light-tailed losses (\\emph{e.g.}, subgaussian or subexponential), its extension to the case of heavy-tailed losses remains largely uncharted and has attracted a growing interest in recent years. We contribute PAC-Bayes generalisation bounds for heavy-tailed losses under the sole assumption of bounded variance of the loss function. Under that assumption, we extend previous results from \\citet{kuzborskij2019efron}. Our key technical contribution is exploiting an extention of Markov's inequality for supermartingales. Our proof technique unifies and extends different PAC-Bayesian frameworks by providing bounds for unbounded martingales as well as bounds for batch and online learning with heavy-tailed losses.",
        "authors": "M. Haddouche, B. Guedj",
        "keywords": [
            "heavy-tailed losses",
            "PAC-Bayes",
            "Markov's inequality"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=qxrwt6F3sf",
        "pdf_src": "https://api2.openreview.net/pdf/61d7af3e4f9ef6d47871e69f6d58c9290490e00d.pdf",
        "Code_src": "",
        "Introduction": "Background: The Probability-Accuracy Connection (PAC)-Bayes framework provides guarantees on how accurately a model can learn when given samples drawn from some distribution. It was originally developed mainly for light-tailed distributions such as subgaussian or subexponential ones.\n\nResearch Problem: However, there are many real-world scenarios where data exhibits heavy tails which have not been covered within this framework due to lack of investigation into extending it beyond these specific cases.\n\nMethodology: In our paper, motivated by practical applications requiring robustness against outliers present in heavy-tailed datasets without prior knowledge about their exact nature except that they lie below a certain bound, we propose new PAC-Bayes generalization bounds specifically tailored towards heavy-tailed losses based solely on the bounded variance condition.\nWe achieve this through leveraging an extended version of Markov's inequality applicable even if the underlying process does not necessarily form a martingale sequence but rather could be considered as a supermartingale instead.\n\nMain Contributions: \n1. We provide novel PAC-Bayes generalization bounds suitable only assuming bounded variance conditions over any arbitrary loss functions regardless whether those exhibit heavy tails; \n2. This allows us to generalize existing theoretical understanding significantly since previously available works were limited either because they required stronger assumptions than just variance bounds alone or focused exclusively on light-tailed distributions;\n3. Furthermore, our approach also encompasses both batch and online learning settings while still considering potentially heavy-tailed inputs - something past research did not cover comprehensively before ours.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Jacobian-based Causal Discovery with Nonlinear ICA",
        "abstract": "Today's methods for uncovering causal relationships from observational data either constrain functional assignments (linearity/additive noise assumptions) or the data generating process (e.g., non-i.i.d. assumptions). Unlike previous works, which use conditional independence tests, we rely on the inference function's Jacobian to determine nonlinear cause-effect relationships. We prove that, under strong identifiability, the inference function's Jacobian captures the sparsity structure of the causal graph; thus, generalizing the classic LiNGAM method to the nonlinear case. We use nonlinear Independent Component Analysis (ICA) to infer the underlying sources from the observed variables and show how nonlinear ICA is compatible with causal discovery via non-i.i.d data.  Our approach avoids the cost of exponentially many independence tests and makes our method end-to-end differentiable. We demonstrate that the proposed method can infer the causal graph on multiple synthetic data sets, and in most scenarios outperforms previous work.\n",
        "authors": "P. Reizinger, Y. Sharma, M. Bethge, et.al",
        "keywords": [
            "nonlinear causality",
            "identifiability",
            "independent component analysis"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=2Yo9xqR6Ab",
        "pdf_src": "https://api2.openreview.net/pdf/c25e717422293c57728cdbbf81cb7ce281e8e6d3.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses a common challenge in causal discovery - inferring causality between variables based solely on observational data without making restrictive assumptions about the nature of these relationships.\n\nResearch Problem: How do we discover nonlinear causal relationships when traditional linear models are not applicable?\n\nMethodology: Instead of using conditional independence tests as previously done by other studies, this research employs the Jacobian matrix of an inference function derived from Bayesian networks' variational Bayes approximation framework (\"Variational Message Passing\"). This allows them to identify both the direction and strength of the causal links among variables while accounting for potential non-linearity due to additive noise within each variable.\n\nMain Contributions:\n1. They introduce a novel way to detect nonlinear causal structures through the analysis of the Jacobian matrix rather than relying on statistical tests like mutual information-based approaches such as those used in LiNGAM algorithms designed only for linear settings.\n2. Prove theoretically their method can recover sparse causal graphs where some edges may be absent even if they exist in reality – something that was beyond reach before because it requires solving a combinatorial optimization problem over exponential number of possible graphs directly related to the number of variables involved).\n3. Develop a new algorithm called Nonlinear Independent Component Analysis (nICA), which extends standard independent component analysis techniques into handling non-independent components caused by latent confounders present during observation time series collection leading towards more robustness against violations assumed i.i.d (independent and identically distributed) conditions commonly made elsewhere within literature).\n\n4. Their approach significantly reduces computational complexity compared to existing methods requiring exponentially increasing numbers of independence tests per pair combinations across all pairs of variables considered (\\(O(n^2)\\)), instead offering \\(O(d \\log d)\\) complexity reduction where \\(d\\) represents dimensionality). \n\n5. Demonstrate empirically superior performance relative to prior state-of-the-art methods including classical linear Granger causality tests along with recent extensions thereof specifically tailored toward non-linearities encountered frequently nowadays especially relevant given today’s complex systems involving interactions at various scales ranging from molecular biology experiments up until macroeconomic phenomena studied extensively lately.",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "POLTER: Policy Trajectory Ensemble Regularization for Unsupervised Reinforcement Learning",
        "abstract": "The goal of Unsupervised Reinforcement Learning (URL) is to find a reward-agnostic prior policy on a task domain, such that the sample-efficiency on supervised downstream tasks is improved. Although agents initialized with such a prior policy can achieve a significantly higher reward with fewer samples when finetuned on the downstream task, it is still an open question how an optimal pretrained prior policy can be achieved in practice. In this work, we present POLTER (Policy Trajectory Ensemble Regularization) – a general method to regularize the pretraining that can be applied to any URL algorithm and is especially useful on data- and knowledge-based URL algorithms. It utilizes an ensemble of policies that are discovered during pretraining and moves the policy of the URL algorithm closer to its optimal prior. Our method is based on a theoretical framework, and we analyze its practical effects on a white-box benchmark, allowing us to study POLTER with full control. In our main experiments, we evaluate POLTER on the Unsupervised Reinforcement Learning Benchmark (URLB), which consists of 12 tasks in 3 domains. We demonstrate the generality of our approach by improving the performance of a diverse set of data- and knowledge-based URL algorithms by 19% on average and up to 40% in the best case. Under a fair comparison with tuned baselines and tuned POLTER, we establish a new state-of-the-art for model-free methods on the URLB.",
        "authors": "F. Schubert, C. Benjamins, S. Döhler, et.al",
        "keywords": [
            "policy trajectory ensemble regularization",
            "unsupervised reinforcement learning",
            "performance improvement"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Hnr23knZfY",
        "pdf_src": "https://api2.openreview.net/pdf/6090110ac15a9420456c14699c70de7edc6907bd.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper focuses on unsupervised reinforcement learning (URL), where the objective is to develop a reward-agnostic prior policy within a given task domain so as to enhance sample efficiency regarding supervised downstream tasks.\n\nResearch Question: How might one practically devise an optimal pretrained prior policy?\n\nMethod: To address this research problem, the authors introduce POLTER - Policy Trajectory Ensemble Regularization -, a regularization technique applicable across various URL algorithms but particularly beneficial for those reliant on data or knowledge. POLTER involves utilizing ensembles of discovered policies throughout pretraining phases; these help guide the URL algorithm's policy towards more optimal prior states.\n \nMain Contributions:\n1. POLTER is theoretically grounded offering insights into its efficacy through analysis using a white-box benchmark controlled experiment setup,\n2. Demonstrated improvements over existing data- and knowledge-based URL algorithms ranging from around 19% overall improvement all the way upwards to approximately 40%, depending upon specific cases,\n3. Achieved significant advancements compared against both tuned baseline models along with tuned versions of POLTER itself leading to establishing novel benchmarks representing current state-of-the-art results specifically among model-free approaches evaluated via the Unsupervised Reinforcement Learning Benchmark (URLB).",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Differentially Private Image Classification from Features",
        "abstract": "In deep learning, leveraging transfer learning has recently been shown to be an effective strategy for training large high performance models with Differential Privacy (DP). Moreover, somewhat surprisingly, recent works have found that privately training just the last layer of a pre-trained model provides the best utility with DP. While past studies largely rely on using first-order differentially private training algorithms like DP-SGD for training large models, in the specific case of privately learning from features, we observe that computational burden is often low enough to allow for more sophisticated optimization schemes, including second-order methods. To that end, we systematically explore the effect of design parameters such as loss function and optimization algorithm. We find that, while commonly used logistic regression performs better than linear regression in the non-private setting, the situation is reversed in the private setting. We find that least-squares linear regression is much more effective than logistic regression from both privacy and computational standpoint, especially at stricter epsilon values ($\\epsilon < 1$). On the optimization side, we also explore using Newton's method, and find that second-order information is quite helpful even with privacy, although the benefit significantly diminishes with stricter privacy guarantees. While both methods use second-order information, least squares is more effective at lower epsilon values while Newton's method is more effective at larger epsilon values. To combine the benefits of both methods, we propose a novel optimization algorithm called DP-FC, which leverages feature covariance instead of the Hessian of the logistic regression loss and performs well across all $\\epsilon$ values we tried. With this, we obtain new SOTA results on ImageNet-1k, CIFAR-100 and CIFAR-10 across all values of $\\epsilon$ typically considered. Most remarkably, on ImageNet-1K, we obtain top-1 accuracy of 88\\% under DP guarantee of (8, $8 * 10^{-7}$) and 84.3\\% under (0.1, $8 * 10^{-7}$).",
        "authors": "H. Mehta, W. Krichene, A. G. Thakurta, et.al",
        "keywords": [
            "transfer learning",
            "differential privacy",
            "optimization algorithms"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Cj6pLclmwT",
        "pdf_src": "https://api2.openreview.net/pdf/28068ba142569a1374cff35779708c292e05e71e.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses how differential privacy (DP), ensuring data privacy by adding noise during computation without degrading too much model performance, can effectively train very large neural networks.\n\nResearch Question: What are optimal strategies when it comes to applying DP? Specifically, what happens if you only apply DP to one part of your pre-trained network?\n\nMethodology: They conduct experiments where they partially apply DP - not to entire layers but specifically to the final layer(s) after already trained weights or features extracted elsewhere within the network architecture remain unchanged.\nThey compare various optimization techniques – first-order (like DP-SGD) vs. second-order (like Newton’s method).\n\nMain Contributions:\n1. Contrary to common belief about needing DP throughout each layer, their findings indicate that DP applied solely towards the output layer may yield similar or superior outcomes compared to fully DP training over multiple layers.\n2. Least-squares linear regression was discovered outperforming logistic regression particularly beneficially regarding both privacy preservation and computational efficiency; especially so around tighter epsilon constraints.\n3. A novel optimization technique named DP-FC proposed based on feature covariance rather than the Hessian matrix of the logistic regression loss shows consistent improvement regardless of epsilon value tested against state-of-the-art benchmarks.\n4. Achieved significant improvements toward achieving competitive accuracies ranging up to 88% Top-1 Accuracy level on ImageNet-1K dataset despite strict DP constraints.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Weisfeiler and Leman Go Infinite: Spectral and Combinatorial Pre-Colorings",
        "abstract": "The limit in the expressivity of Message Passing Graph Neural Networks (MPGNNs) has recently led to the development of end-to-end learning GNN architectures. These advanced GNNs usually generalize existing notions in the GNN architecture or suggest new ones that break the limit of the existing, relatively simple MPGNNs. In this paper, we focus on a different solution, the two-phase approach (or pre-coloring), which enables to use of the same simple MPGNNs while improving their expressivity. We prove that using pre-colorings could strictly increase the expressivity of MPGNNs ad infinitum. We also suggest new pre-coloring based on the spectral decomposition of the graph Laplacian and prove that it strictly improves the expressivity of standard MPGNNs. An extensive evaluation of the proposed method with different MPGNN models on various graph classification and node property prediction datasets consistently outperforms previous pre-coloring strategies. The code to reproduce our experiments is available at \\url{https://github.com/TPFI22/Spectral-and-Combinatorial}.",
        "authors": "O. Feldman, A. Boyarski, S. Feldman, et.al",
        "keywords": [
            "graph neural networks",
            "expressivity",
            "pre-coloring"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=YJDqQSAuB6",
        "pdf_src": "https://api2.openreview.net/pdf/6efd4ed1ed0cbd7b9085088a1d4df9db14638ad7.pdf",
        "Code_src": "代码链接：\\url{https://github.com/TPFI22/Spectral-and-Combinatorial}",
        "Introduction": "Background: Recent advancements have been made towards developing more expressive Graph Neural Network (GNN) architectures beyond traditional Message Passing Graph Neural Networks (MPGNNs). This research aims to address the limitations faced by MPGNNs.\n\nResearch Problem: How can we improve the expressivity of MPGNNs without altering its underlying structure?\n\nMethod: The authors propose an innovative two-phase approach known as \"pre-coloring,\" where they utilize the same basic MPGNNs but enhance their performance through preprocessing steps before message passing occurs within the network layers.\n \nMain Contributions:\n1. Proving theoretically that pre-coloring significantly increases the expressivity of MPGNNs indefinitely,\n2. Introducing novel pre-coloring techniques derived from the spectral decomposition of the graph Laplacian matrix for further enhancing the expressivity over standard MPGNNs,\n3. Conducting comprehensive evaluations across multiple graph classification and node property prediction tasks against other pre-coloring methods demonstrating consistent superiority,\n\nCode Availability: A GitHub repository containing the source code necessary to replicate these experimental findings is accessible at [GitHub link provided].",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Positive Difference Distribution for Image Outlier Detection using Normalizing Flows and Contrastive Data",
        "abstract": "Detecting test data deviating from training data is a central problem for safe and robust machine learning. Likelihoods learned by a generative model, e.g., a normalizing flow via standard log-likelihood training, perform poorly as an outlier score. We propose to use an unlabelled auxiliary dataset and a probabilistic outlier score for outlier detection. We use a self-supervised feature extractor trained on the auxiliary dataset and train a normalizing flow on the extracted features by maximizing the likelihood on in-distribution data and minimizing the likelihood on the contrastive dataset. We show that this is equivalent to learning the normalized positive difference between the in-distribution and the contrastive feature density. We conduct experiments on benchmark datasets and compare to the likelihood, the likelihood ratio and state-of-the-art anomaly detection methods.",
        "authors": "R. Schmier, U. Koethe, C. Straehle",
        "keywords": [
            "anomaly detection",
            "generative models",
            "contrastive learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=B4J40x7NjA",
        "pdf_src": "https://api2.openreview.net/pdf/cacf88d891fccb2daf76953eed81340ca744adb0.pdf",
        "Code_src": "",
        "Introduction": "Background: The background of this paper lies within the field of machine learning with emphasis on safety and robustness concerns related to detecting outliers or deviations during testing phases which could compromise performance.\n\nResearch Problem: The primary research question addressed here pertains to improving the outlier scoring mechanism when using generative models such as normalizing flows through traditional log-likelihood training approaches since these are found inadequate at identifying outliers effectively.\n\nMethods: To address the issue above, authors introduce two key innovations:\n1. An unlabeled auxiliary dataset - This additional set helps improve the discriminative power.\n2. A novel probabilistic outlier score method – This new metric scores how likely it is each sample belongs to either the distribution being modeled ('in-distribution') or another contrasting one ('contrastive').\n\nThe proposed approach involves first training a self-supervised feature extractor solely based on the auxiliary dataset without labels; then they fine-tune a normalizing flow on top of those extracted features while optimizing against both types of distributions—maximizing likelihood over 'in-distribution' samples but penalizing them heavily if their probability mass function predicts high likelihood under the 'contrastive' distribution context.\n\nMain Contributions: The main contributions include developing a more effective way than just relying on raw likelihood values alone—a better outlier score system capable not only of distinguishing anomalies clearly compared existing metrics like likelihood ratios—but also demonstrating its efficacy across various benchmarks where it outperforms other leading anomaly detection techniques suggesting broader applicability beyond specific domains tested therein.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "On the Statistical Complexity of Estimation and Testing under Privacy Constraints",
        "abstract": "The challenge of producing accurate statistics while respecting the privacy of the individuals in a sample is an important area of research. We study minimax lower bounds for classes of differentially private estimators. In particular, we show how to characterize the power of a statistical test under differential privacy in a plug-and-play fashion by solving an appropriate transport problem. With specific coupling constructions, this observation allows us to derive Le Cam-type and Fano-type inequalities not only for regular definitions of differential privacy but also for  those based on Renyi divergence. We then proceed to illustrate our results on three simple, fully worked out examples. In particular, we show that the problem class has a huge importance on the provable degradation of utility due to privacy. In certain scenarios, we show that maintaining privacy results in a noticeable reduction in performance only when the level of privacy protection is very high. Conversely, for other problems, even a modest level of privacy protection can lead to a significant decrease in performance. Finally, we demonstrate that the DP-SGLD algorithm, a private convex solver, can be employed for maximum likelihood estimation with a high degree of confidence, as it provides near-optimal results with respect to both the size of the sample and the level of privacy protection. This algorithm is applicable to a broad range of parametric estimation procedures, including exponential families.",
        "authors": "C. Lalanne, A. Garivier, R. Gribonval",
        "keywords": [
            "privacy",
            "minimax lower bounds",
            "differential privacy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=OarsigVib0",
        "pdf_src": "https://api2.openreview.net/pdf/58573879b03e8c05b24f0d94c141090504362e6b.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses one of the most critical issues in data analysis – balancing accuracy against individual privacy within datasets.\n\nResearch Question: How do we quantify the trade-off between statistical precision (\"utility\") achieved through estimation methods like machine learning algorithms versus preserving the confidentiality of personal information?\n\nMethods: The authors delve into establishing minimax lower bounds which are fundamental tools used across various fields such as communication theory or computational complexity science; they apply these principles specifically towards characterizing the efficacy of differentially private estimators.\nThey introduce novel techniques involving \"transport\" problems - mathematical formulations akin to optimal mass transfer processes where they optimize over possible transformations from one distribution to another without revealing sensitive details about any single subject's identity ('differential privacy').\n\nMain Contributions:\n1. They provide new insights regarding the 'power' of statistical tests conducted via differentially private mechanisms—i.e., their ability to correctly identify true patterns amidst noise—in a straightforward manner using transport optimization.\n2. By employing coupling constructions—a concept borrowed from probability theory—they extend existing inequalities beyond standard notions of differential privacy derived solely from Laplace mechanisms toward more general measures informed by Rényi divergences offering broader applicability than previous work had allowed before.\n3. Through illustrative examples demonstrating real-world relevance, emphasizing that there exists no universal solution nor silver bullet approach concerning whether some loss in statistical efficiency might occur depending upon what kind of dataset being analyzed along with desired levels of privacy preservation required therein.\n4. Lastly yet importantly, they validate practicality suggesting DP-SGLD—an algorithm designed originally aimed at optimizing convex functions while ensuring differential privacy—as capable enough handle complex tasks like maximum likelihood estimation reliably achieving nearly optimal outcomes considering constraints imposed by sample sizes alongside chosen privacy guarantees.\n\nOverall Impact: These findings contribute significantly advancing understanding around designing robust statistical models accommodating stringent requirements related to protecting user anonymity during processing large-scale datasets whilst still yielding meaningful conclusions/statistics pertinent",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Generating Adversarial Examples with Task Oriented Multi-Objective Optimization",
        "abstract": "Deep learning models, even the-state-of-the-art ones, are highly vulnerable to adversarial examples. Adversarial training is one of the most efficient methods to improve the model's robustness. The key factor for the success of adversarial training is the capability to generate \nqualified and divergent adversarial examples which satisfy some objectives/goals (e.g., finding adversarial examples that maximize the model losses for simultaneously attacking multiple models). Therefore, multi-objective optimization (MOO) is a natural tool for adversarial example generation to achieve multiple objectives/goals simultaneously. However, we observe that a naive application of MOO tends to maximize all objectives/goals equally, without caring if an objective/goal has been achieved yet. This leads to useless effort to further improve the goal-achieved tasks, while putting less focus on the goal-unachieved tasks. In this paper, we propose \\emph{Task Oriented MOO} to address this issue, in the context where we can explicitly define the goal achievement for a task. Our principle is to only maintain the goal-achieved tasks, while letting the optimizer spend more effort on improving the goal-unachieved tasks. We conduct comprehensive experiments for our Task Oriented MOO on various adversarial example generation schemes. The experimental results firmly demonstrate the merit of our proposed approach. ",
        "authors": "A. T. Bui, T. Le, H. Zhao, et.al",
        "keywords": [
            "Task Oriented MOO",
            "Multi-objective Optimization",
            "Robustness Improvement"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=2f81Q622ww",
        "pdf_src": "https://api2.openreview.net/pdf/3559794c02292988242971f917fadc935d9f7f66.pdf",
        "Code_src": "",
        "Introduction": "Background: The background of this research lies in the vulnerability of deep learning models against adversarial examples - perturbations intentionally introduced into input data with imperceptible changes but capable of fooling neural networks.\n\nResearch Problem: The problem addressed by this study revolves around enhancing the robustness of these models through adversarial training techniques; specifically, how to effectively utilize multi-objective optimization (MOO) during adversarial example generation processes so as to meet several goals simultaneously within limited computational resources?\n\nMethodology: To tackle this challenge, researchers introduce \"Task-Oriented Multi-Objective Optimization\" or TO-MOO method. Instead of optimizing each objective equally regardless of whether they have already been met (\"naive MOO\"), TO-MOO focuses its efforts differently based on predefined criteria about what constitutes successful completion of individual tasks towards their respective objectives.\n\nMain Contributions: The main contributions lie in developing a novel framework called Task-Oriented MOO tailored especially for generating diverse and high-quality adversarial examples under constraints such as time/resource limitations common when dealing with real-world applications involving machine learning systems like autonomous vehicles etc. Experiments conducted across different types of attacks show significant improvements over traditional approaches suggesting higher effectiveness at achieving desired outcomes efficiently using fewer computational resources than before.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Aux-Drop: Handling Haphazard Inputs in Online Learning Using Auxiliary Dropouts",
        "abstract": "Many real-world applications based on online learning produce streaming data that is haphazard in nature, i.e., contains missing features, features becoming obsolete in time, the appearance of new features at later points in time and a lack of clarity on the total number of input features. These challenges make it hard to build a learnable system for such applications, and almost no work exists in deep learning that addresses this issue. In this paper, we present Aux-Drop, an auxiliary dropout regularization strategy for online learning that handles the haphazard input features in an effective manner. Aux-Drop adapts the conventional dropout regularization scheme for the haphazard input feature space ensuring that the final output is minimally impacted by the chaotic appearance of such features. It helps to prevent the co-adaptation of especially the auxiliary and base features, as well as reduces the strong dependence of the output on any of the auxiliary inputs of the model. This helps in better learning for scenarios where certain features disappear in time or when new features are to be modeled. The efficacy of Aux-Drop has been demonstrated through extensive numerical experiments on SOTA benchmarking datasets that include Italy Power Demand, HIGGS, SUSY and multiple UCI datasets. The code is available at https://github.com/Rohit102497/Aux-Drop.",
        "authors": "R. Agarwal, D. Gupta, A. Horsch, et.al",
        "keywords": [
            "haphazard input",
            "auxiliary dropout regularization",
            "online learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=R9CgBkeZ6Z",
        "pdf_src": "https://api2.openreview.net/pdf/7a5a5e2f3ba2b93c6cb118880a712e683883f45a.pdf",
        "Code_src": "https://github.com/Rohit102497/Aux-Drop",
        "Introduction": "Background: Many real-world applications generate streaming data with unpredictable characteristics like missing features, outdated information, emerging novel features over time without clear knowledge about their complete set.\n\nResearch Problem: How can one develop a robust machine learning framework capable of handling these erratic input features effectively?\n\nMethodology: We introduce \"Aux-Drop,\" which modifies the standard dropout regularization technique specifically designed for dealing with irregularities found in streaming data during online learning processes.\n- Aux-Drop maintains minimal impact from the random occurrence of noisy features while training models,\n- Prevents excessive reliance between different parts of the network architecture including auxiliary and primary components,\n- Allows more adaptive learning mechanisms particularly beneficial under conditions involving disappearing or newly introduced features.\n\nMain Contributions:\n- A novel regularization approach called Aux-Drop tailored towards managing the complexities associated with unstructured streaming data streams within various domains using state-of-the-art benchmarks.\n- Demonstrated effectiveness across diverse datasets showing improved performance compared traditional methods lacking adaptability against dynamic changes seen commonly in practical use cases related to power demand forecasting, particle physics simulations etcetera.\n- Open-sourced implementation accessible via GitHub repository facilitating further research into similar areas requiring adaptable machine learning techniques adapting rapidly evolving environments encountered daily operations faced today's complex systems reliant upon continuous adaptation capabilities offered by modern algorithms presented here.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Denise: Deep Robust Principal Component Analysis for Positive Semidefinite Matrices",
        "abstract": "The robust PCA of covariance matrices plays an essential role when isolating key explanatory features.  The currently available methods for performing such a low-rank plus sparse decomposition are matrix specific, meaning, those algorithms must re-run for every new matrix.  Since these algorithms are computationally expensive, it is preferable to learn and store a function that nearly instantaneously performs this decomposition when evaluated.  Therefore, we introduce Denise, a deep learning-based algorithm for robust PCA of covariance matrices, or more generally, of symmetric positive semidefinite matrices, which learns precisely such a function.  Theoretical guarantees for Denise are provided.  These include a novel universal approximation theorem adapted to our geometric deep learning problem and convergence to an optimal solution to the learning problem.  Our experiments show that Denise matches state-of-the-art performance in terms of decomposition quality, while being approximately $2000\\times$ faster than the state-of-the-art, principal component pursuit (PCP), and $200 \\times$ faster than the current speed-optimized method, fast PCP. ",
        "authors": "C. Herrera, F. Krach, A. Kratsios, et.al",
        "keywords": [
            "Denise",
            "Robust PCA",
            "Covariance Matrices"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=D45gGvUZp2",
        "pdf_src": "https://api2.openreview.net/pdf/7277d06692f207735f641e422d69eca11ca19d19.pdf",
        "Code_src": "",
        "Introduction": "Background: Robust PCA aims to recover the low-rank structure from corrupted data by identifying outliers effectively.\n\nResearch Problem: Existing methods require significant computational resources due to their matrix-specific nature; thus, there's a need for efficient algorithms capable of handling large-scale datasets without compromising on accuracy.\n\nMethodology: We propose Denise, a neural network architecture designed specifically for the task of computing robust PCA efficiently through a learned transformation rather than using traditional iterative solvers like Principal Component Pursuit (PCP).\n\nMain Contributions:\n1. A novel approach based on deep learning allows us to approximate the desired decomposition with high fidelity.\n2. Provided theoretical guarantees including a universal approximation theorem tailored towards our application within the framework of geometric deep learning problems ensuring convergence toward an optimal solution under certain conditions.\n3. Demonstrated significantly improved efficiency compared to existing approaches - running about 2000 times quicker than PCP and twice as fast as its optimized variant Fast PCP – while maintaining competitive results regarding decomposition quality against other leading techniques used today",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Mean-Field Control based Approximation of Multi-Agent Reinforcement Learning in Presence of a Non-decomposable Shared Global State",
        "abstract": "Mean Field Control (MFC) is a powerful approximation tool to solve large-scale Multi-Agent Reinforcement Learning (MARL) problems. However, the success of MFC  relies on the presumption that given the local states and actions of all the agents, the next (local) states of the agents evolve conditionally independent of each other. Here we demonstrate that even in a MARL setting where agents share a common global state in addition to their local states evolving conditionally independently (thus introducing a correlation between the state transition processes of individual agents), the MFC can still be applied as a good approximation tool. The global state is assumed to be non-decomposable i.e., it cannot be expressed as a collection of local states of the agents. We compute the approximation error as $\\mathcal{O}(e)$ where $e=\\frac{1}{\\sqrt{N}}\\left[\\sqrt{|\\mathcal{X}|} +\\sqrt{|\\mathcal{U}|}\\right]$.  The size of the agent population is denoted by the term $N$, and $|\\mathcal{X}|, |\\mathcal{U}|$ respectively indicate the sizes of (local) state and action spaces of individual agents. The approximation error is found to be independent of the size of the shared global state space. We further demonstrate that in a special case if the reward and state transition functions are independent of the action distribution of the population, then the error can be improved to $e=\\frac{\\sqrt{|\\mathcal{X}|}}{\\sqrt{N}}$. Finally, we devise a Natural Policy Gradient based algorithm that solves the MFC problem with $\\mathcal{O}(\\epsilon^{-3})$ sample complexity and obtains a policy that is within $\\mathcal{O}(\\max\\{e,\\epsilon\\})$ error of the optimal MARL policy for any $\\epsilon>0$.",
        "authors": "W. U. Mondal, V. Aggarwal, S. Ukkusuri",
        "keywords": [
            "Multi-Agent Reinforcement Learning",
            "Mean Field Control",
            "Approximation Error"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ZME2nZMTvY",
        "pdf_src": "https://api2.openreview.net/pdf/44cb2f1f7a312699712a553bc3d14a1f3ee62f09.pdf",
        "Code_src": "",
        "Introduction": "Background: Mean Field Control (MFC) is an effective method used to address multi-agent reinforcement learning (MARL) issues involving many agents interacting simultaneously over time through decision-making strategies.\n\nResearch Problem: Despite its effectiveness under certain conditions such as conditional independence among agents' transitions when considering only local information without taking into account potential correlations due to sharing a global state, there has been uncertainty about whether or not this approach would work effectively beyond these constraints.\n \nMethod: This paper extends previous research findings regarding applicability of mean field control theory across different scenarios including those featuring a global state which introduces dependencies amongst agents despite them having separate local states. They also propose improvements upon existing algorithms like natural policy gradient methods while maintaining low sample complexities and achieving policies close enough to optimality according to specific performance metrics.\n \nMain Contributions:\n- Demonstrated how mean field control remains applicable regardless of additional global state dependency introduced alongside local state evolution - thus expanding practical use cases significantly;\n- Calculated exact approximation errors associated with using mean field control approaches depending on various factors related to system parameters such as number of agents ($N$), size of state/action spaces ($|\\mathcal{X}|$,$|\\mathcal{U}|$);\n- Provided new insights leading towards more efficient algorithms solving MARL tasks via natural policy gradients; \n- Achieved near-optimal policies at acceptable levels of error compared against standard benchmarks for MARL",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Cox-Hawkes: doubly stochastic spatiotemporal Poisson processes",
        "abstract": "Hawkes processes are point process models that have been used to capture self-excitatory\nbehaviour in social interactions, neural activity, earthquakes and viral epidemics. They can\nmodel the occurrence of the times and locations of events. Here we develop a new class of\nspatiotemporal Hawkes processes that can capture both triggering and clustering behaviour\nand we provide an efficient method for performing inference. We use a log-Gaussian Cox\nprocess (LGCP) as prior for the background rate of the Hawkes process which gives arbitrary\nflexibility to capture a wide range of underlying background effects (for infectious diseases\nthese are called endemic effects). The Hawkes process and LGCP are computationally\nexpensive due to the former having a likelihood with quadratic complexity in the number\nof observations and the latter involving inversion of the precision matrix which is cubic\nin observations. Here we propose a novel approach to perform MCMC sampling for our\nHawkes process with LGCP background, using pre-trained Gaussian Process generators\nwhich provide direct and cheap access to samples during inference. We show the efficacy\nand flexibility of our approach in experiments on simulated data and use our methods to\nuncover the trends in a dataset of reported crimes in the US.",
        "authors": "X. Miscouridou, S. Bhatt, G. Mohler, et.al",
        "keywords": [
            "triggering behavior",
            "spatiotemporal modeling",
            "log-Gaussian Cox process"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=xzCDD9i4IZ",
        "pdf_src": "https://api2.openreview.net/pdf/edd602bfec9be240aa6c877b870d6e6897f63fe2.pdf",
        "Code_src": "",
        "Introduction": "Background: Hawkes processes are stochastic point processes widely applied across various fields such as finance, linguistics, neuroscience etc., particularly useful when modeling phenomena characterized by mutual influence or contagion.\n\nResearch Problem: Existing Hawkes processes mainly focus on capturing event-triggering behavior but not necessarily clustering patterns; they also require complex computations like quadratic likelihoods leading to computational inefficiencies especially at scale.\n\nMethods: This paper introduces a novel spatiotemporal Hawkes model capable of simultaneously representing both triggering and clustering behaviors through a log-Gaussian Cox process (LGCP) framework serving as its background intensity function. To address computational challenges associated with this model's high-dimensional nature - specifically the quadratic complexity of the likelihood estimation problem – it proposes leveraging pre-trained Gaussian Process (GP) samplers within Markov Chain Monte Carlo (MCMC) simulations allowing for more efficient posterior inference without compromising accuracy significantly compared to traditional approaches.\nMain Contributions:\n1. A new spatiotemporal Hawkes model incorporating both triggering and clustering features into one framework;\n2. An innovative application of GPs combined with MCMC techniques enabling scalable Bayesian inference over large datasets where computation would otherwise be prohibitive;\n3. Validation demonstrated via simulation studies showing improved performance relative to standard algorithms while maintaining statistical robustness;\n4. Practical demonstration applying developed methodology towards analyzing crime rates from real-world American cities providing insights about criminal activities' temporal dynamics beyond simple autocorrelation measures alone.",
        "Topic": "Generative Models"
    },
    {
        "title": "Comparative Generalization Bounds for Deep Neural Networks",
        "abstract": "In this work, we investigate the generalization capabilities of deep neural networks. We introduce a novel measure of the effective depth of neural networks, defined as the first layer at which sample embeddings are separable using the nearest-class center classifier. Our empirical results demonstrate that, in standard classification settings, neural networks trained using Stochastic Gradient Descent (SGD) tend to have small effective depths. We also explore the relationship between effective depth, the complexity of the training dataset, and generalization. For instance, we find that the effective depth of a trained neural network increases as the proportion of random labels in the data rises. Finally, we derive a generalization bound by comparing the effective depth of a network with the minimal depth required to fit the same dataset with partially corrupted labels. This bound provides non-vacuous predictions of test performance and is found to be empirically independent of the actual depth of the network.",
        "authors": "T. Galanti, L. Galanti, I. Ben-shaul",
        "keywords": [
            "neural network",
            "effective depth",
            "generalization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=162TqkUNPO",
        "pdf_src": "https://api2.openreview.net/pdf/2e2f450fe23834acd9ca5a6a35f0906f6165bc30.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper aims to study the generalization abilities of deep neural networks.\n\nResearch Question: What factors affect the generalization ability of neural networks?\n\nMethod: The authors propose a new metric for measuring the \"effective depth\" of neural networks - it's the first layer where samples can be separated based on their class centroids when using the nearest centroid classifier method.\n\nMain Contributions:\n1. They show through empirical evidence from standard classification tasks that neural networks often achieve good accuracy despite having relatively low effective depths.\n2. They establish an interesting correlation showing how increasing randomness within labeled datasets leads to deeper networks being more likely during training phase.\n3. Lastly they provide what appears like a meaningful upper bound prediction about model performance via comparison against minimum necessary layers needed if some labels were randomly corrupted while fitting models; surprisingly these bounds seem unrelated to actual network sizes used but still accurately predict future outcomes observed experimentally.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Interpretable Mixture of Experts",
        "abstract": "The need for reliable model explanations is prominent for many machine learning applications, particularly for tabular and time-series data as their use cases often involve high-stakes decision making.  Towards this goal, we introduce a novel interpretable modeling framework, Interpretable Mixture of Experts (IME), that yields high accuracy, comparable to `black-box' Deep Neural Networks (DNNs) in many cases, along with useful interpretability capabilities. IME consists of an assignment module and a mixture of experts, with each sample being assigned to a single expert for prediction. We introduce multiple options for IME based on the assignment and experts being interpretable. When the experts are chosen to be interpretable such as linear models, IME yields an inherently-interpretable architecture where the explanations produced by IME are the exact descriptions of how the prediction is computed. In addition to constituting a standalone inherently-interpretable architecture, IME has the premise of being integrated with existing DNNs to offer interpretability to a subset of samples while maintaining the accuracy of the DNNs. Through extensive experiments on 15 tabular and time-series datasets, IME is demonstrated to be more accurate than single interpretable models and perform comparably with existing state-of-the-art DNNs in accuracy. On most datasets, IME even outperforms DNNs, while providing faithful explanations.  Lastly, IME's explanations are compared to commonly-used post-hoc explanations methods through a user study -- participants are able to better predict the model behavior when given IME explanations, while finding IME's explanations more faithful and trustworthy.",
        "authors": "A. A. Ismail, S. O. Arik, J. Yoon, et.al",
        "keywords": [
            "interpretable modeling",
            "Interpretable Mixture of Experts (IME)",
            "high accuracy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=DdZoPUPm0a",
        "pdf_src": "https://api2.openreview.net/pdf/112bf49b0379dcda62bd661899a7f3fc7aad87ab.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the critical issue of explaining predictions made by complex machine learning models like deep neural networks (DNNs). This becomes especially important when these models handle tasks involving significant decisions or consequences.\n\nResearch Question: How can one develop a new approach which not only maintains similar predictive performance but also provides clear explanations?\n\nMethodology: To tackle this problem, they propose \"Interpretable Mixture of Experts\" (IME), consisting of two main components:\n1. An Assignment Module - It determines what part of input should go into predicting.\n2. A Mixture of Experts - Each 'expert' predicts independently using its own model; some examples could include simple interpretable models like linear regression instead of black-box DNNs.\n\nMain Contributions: \n- They demonstrate that IME achieves competitive accuracy levels akin to those obtained from non-interpretable DNNs across various tabular and time series datasets without sacrificing interpretability significantly due to the nature of the experts used within it – e.g., linear models provide direct insights about feature importance directly related to predicted outcomes rather than hidden layers inside a DNN.\n- Additionally, unlike other purely interpretable approaches whose performances may suffer relative to fully trained DNNs, IME retains much higher accuracy rates overall despite offering explanations at individual instances level during inference phase integration process alongside traditional DNN architectures thus allowing users understand why certain predictions were made specifically tailored towards subsets selected according participant preferences via interactive interface provided by system itself.\n- Furthermore, empirical evidence gathered suggests that end-users find these explanations both more trustworthy & easier understood leading them confidently make informed choices relying heavily upon outputted results generated by combined systems incorporating elements from both types mentioned above (i.e., interpretable mixtures coupled together with advanced neural network functionalities).\n\nIn summary, the proposed method offers improved interpretability over conventional DNNs whilst still delivering strong predictive power suitable diverse real-world scenarios requiring explainable artificial intelligence solutions capable aiding human decision-making processes effectively thereby bridging gap between current technological advancements",
        "Topic": "Generative Models"
    },
    {
        "title": "Personalized Federated Learning: A Unified Framework and Universal Optimization Techniques",
        "abstract": "We investigate the optimization aspects of personalized Federated Learning (FL). We propose general optimizers that can be applied to numerous existing personalized FL objectives, specifically a tailored variant of Local SGD and variants of accelerated coordinate descent/accelerated SVRCD. By examining a general personalized objective capable of recovering many existing personalized FL objectives as special cases, we develop a comprehensive optimization theory applicable to a wide range of strongly convex personalized FL models in the literature. We showcase the practicality and/or optimality of our methods in terms of communication and local computation. Remarkably, our general optimization solvers and theory can recover the best-known communication and computation guarantees for addressing specific personalized FL objectives. Consequently, our proposed methods can serve as universal optimizers, rendering the design of task-specific optimizers unnecessary in many instances.",
        "authors": "F. Hanzely, B. Zhao, M. Kolar",
        "keywords": [
            "personalized federated learning",
            "optimization aspects",
            "communication efficiency"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ilHM31lXC4",
        "pdf_src": "https://api2.openreview.net/pdf/e406c145490249c8ef60019b7ec96f946c69e910.pdf",
        "Code_src": "",
        "Introduction": "Background: Personalized Federated Learning (FL), which allows multiple clients to collaboratively train a global model while keeping their data private on-device.\n\nResearch Problem: Developing efficient optimizers suitable for various personalized FL objectives without requiring custom optimizers per task.\n\nMethods: Propose generalized optimizers including a modified version of Local SGD and variants of Accelerated Coordinate Descent/Accelerated Stochastic Variance Reduced Coordinate Descent (SVRCD).\n\nMain Contributions:\n1. Develop an overarching optimization framework adaptable to diverse strongly convex personalized FL models.\n2. Demonstrate the effectiveness through empirical results showing competitive or optimal performance metrics such as communication and computational efficiency compared with specialized algorithms designed for particular objectives within personalization settings like federated learning.\n3. Offer a set of general optimization solvers able to achieve state-of-the-art communication and computation bounds when solving certain personalized FL problems across different domains where privacy-preserving machine learning is critical - potentially obviating the need for bespoke optimizers each time new tasks arise under this paradigm.",
        "Topic": "Federated Learning"
    },
    {
        "title": "Guillotine Regularization: Why removing layers is needed to improve generalization in Self-Supervised Learning",
        "abstract": "One unexpected technique that emerged in recent years consists in training a Deep Network (DN) with a Self-Supervised Learning (SSL) method, and using this network on downstream tasks but with its last few layers entirely removed. This usually skimmed-over trick of throwing away the entire projector is actually critical for SSL methods to display competitive performances. For example, on ImageNet classification, more than 30 points of percentage can be gained that way. This is a little vexing, as one would hope that the network layer at which invariance is explicitly enforced by the SSL criterion during training (the last layer) should be the one to use for best generalization performance downstream. But it seems not to be, and this study sheds some light on why.\nThis trick, which we name Guillotine Regularization (GR), is in fact a generically applicable method that has been used to improve generalization performance in transfer learning scenarios. In this work, we identify the underlying reasons behind its success and challenge the preconceived idea that we should through away the entire projector in SSL. In fact, the optimal layer to use might change significantly depending on the training setup, the data or the downstream task. Lastly, we give some insights on how to reduce the need for a projector in SSL by aligning the pretext SSL task and the downstream task.",
        "authors": "F. Bordes, R. Balestriero, Q. Garrido, et.al",
        "keywords": [
            "Guillotine Regularization",
            "Self-Supervised Learning",
            "Transfer Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ZgXfXSz51n",
        "pdf_src": "https://api2.openreview.net/pdf/5ba9c114256dedf81b24d542640bf12688db2fb7.pdf",
        "Code_src": "",
        "Introduction": "Background: Recent studies have shown that removing the last few layers from a Deep Network (DN) trained via Self-Supervised Learning (SSL) improves its performance when applied to downstream tasks.\n\nResearch Problem: Understanding why removing these layers leads to better performance despite conventional wisdom suggesting they are crucial for enforcing invariance learned throughout training.\n\nMethods: The authors introduce Guillotine Regularization (GR), an approach where only specific weights within each layer remain while others are discarded completely after training ends—essentially pruning unnecessary complexity without altering any other parameters' values across all layers except those being pruned out selectively per layer.\n\nMain Contributions:\n1) They demonstrate experimentally significant improvements over baseline models even though many fewer neurons contribute post-pruning compared before;\n2) Identify several factors affecting GR's efficacy including dataset size & distribution; \n3) Propose strategies such as aligning pretext tasks closer towards real-world applications could potentially eliminate needing projectors altogether",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "Generating Teammates for Training Robust Ad Hoc Teamwork Agents via Best-Response Diversity",
        "abstract": "Ad hoc teamwork (AHT) is the challenge of designing a robust learner agent that effectively collaborates with unknown teammates without prior coordination mechanisms. Early approaches address the AHT challenge by training the learner with a diverse set of handcrafted teammate policies, usually designed based on an expert's domain knowledge about the policies the learner may encounter. However, implementing teammate policies for training based on domain knowledge is not always feasible. In such cases, recent approaches attempted to improve the robustness of the learner by training it with teammate policies generated by optimising information-theoretic diversity metrics. The problem with optimising existing information-theoretic diversity metrics for teammate policy generation is the emergence of superficially different teammates. When used for AHT training, superficially different teammate behaviours may not improve a learner's robustness during collaboration with unknown teammates. In this paper, we present an automated teammate policy generation method optimising the Best-Response Diversity (BRDiv) metric, which measures diversity based on the compatibility of teammate policies in terms of returns. We evaluate our approach in environments with multiple valid coordination strategies, comparing against methods optimising information-theoretic diversity metrics and an ablation not optimising any diversity metric. Our experiments indicate that optimising BRDiv yields a diverse set of training teammate policies that improve the learner's performance relative to previous teammate generation approaches when collaborating with near-optimal previously unseen teammate policies.",
        "authors": "A. Rahman, E. Fosong, I. Carlucho, et.al",
        "keywords": [
            "Best-Response Diversity",
            "Robust Learner Agent",
            "Teammate Policy Generation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=l5BzfQhROl",
        "pdf_src": "https://api2.openreview.net/pdf/bcdc178d372870af58c948117909ba85a64fdf51.pdf",
        "Code_src": "",
        "Introduction": "Background: Ad hoc teamwork (AHT) refers to the task of creating a robust learning agent capable of effective collaboration with unfamiliar teammates who have no pre-established communication protocols.\n\nResearch Problem: Previous studies addressed the challenges posed by ad hoc teamwork through training learners using various manually crafted teammate policies informed by experts' understanding of potential teammate behaviors; however, relying solely on domain expertise can be impractical due to its limitations.\nRecent works aimed at improving learner robustness via teammate policies derived from optimizing information-theoretic diversity metrics but encountered issues where these policies produced only superficial differences rather than meaningful ones necessary for enhancing adaptability under novel conditions.\n\nMethod: This study introduces an innovative teammate policy generation strategy focused on maximizing \"Best-Response Diversity\" (BRDiv), aiming specifically at promoting compatibility among teammate policies regarding expected outcomes or rewards received upon interaction within cooperative tasks.\n\nMain Contributions: The primary contribution lies in developing a new algorithm that generates diverse yet compatible teammate policies optimized around the BRDiv metric – unlike traditional approaches focusing purely on information theory-based diversification - leading to more substantial improvements over baseline models trained w/o optimization towards diversity considerations across various teaming scenarios involving complex coordination dynamics between agents operating autonomously together toward common objectives while encountering unforeseen situations requiring adaptation throughout their interactions",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Learning to correct spectral methods for simulating turbulent flows",
        "abstract": "Despite their ubiquity throughout science and engineering, only a handful of partial differential equations (PDEs) have analytical, or closed-form solutions. This motivates a vast amount of classical work on numerical simulation of PDEs and more recently, a whirlwind of research into data-driven techniques leveraging machine learning (ML). A recent line of work indicates that a hybrid of classical numerical techniques and machine learning can offer significant improvements over either approach alone. In this work, we show that the choice of the numerical scheme is crucial when incorporating physics-based priors. We build upon Fourier-based spectral methods, which are known to be more efficient than other numerical schemes for simulating PDEs with smooth and periodic solutions. Specifically, we develop ML-augmented spectral solvers for three common PDEs of fluid dynamics. Our models are more accurate (2-4x) than standard spectral solvers at the same resolution but have longer overall runtimes (~2x), due to the additional runtime cost of the neural network component. We also demonstrate a handful of key design principles for combining machine learning and numerical methods for solving PDEs.\n",
        "authors": "G. Dresdner, D. Kochkov, P. C. Norgaard, et.al",
        "keywords": [
            "machine learning",
            "spectral methods",
            "partial differential equations"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=wNBARGxoJn",
        "pdf_src": "https://api2.openreview.net/pdf/7338166b66778e350c4d5ceb19b4dc37834492d9.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the challenge posed by the lack of analytical solutions available for most Partial Differential Equations (PDEs) in various scientific fields like Physics.\n\nResearch Question: How effective would it be if one could combine traditional numerical simulation approaches based on classical mathematical analysis such as Fourier Spectral Methods along with Machine Learning (ML)?\n\nMethodology: The authors propose an integration strategy where they leverage Fourier-based spectral methods - known because these are computationally efficient especially under conditions of smoothness and periodicity – and augment them using ML techniques specifically designed for Fluid Dynamics problems involving 3 different types of PDEs:\n\n1. Navier-Stokes Equation\n2. Heat Equation\n3. Wave Equation\n\nMain Contributions:\n- They introduce novel ML-augmented spectral solvers tailored towards each specific equation mentioned above;\n- Their results indicate improved accuracy compared to standalone spectral solvers while maintaining efficiency within acceptable bounds considering computational costs associated with neural networks;\n- Furthermore, through empirical evidence provided during testing phases across several benchmarks related to fluid dynamics simulations including Reynolds number scaling tests among others; \n- Lastly, they outline some fundamental guidelines pertinent toward successfully merging both methodologies together effectively",
        "Topic": "Machine Learning"
    },
    {
        "title": "Successor Feature Representations",
        "abstract": "Transfer in Reinforcement Learning aims to improve learning performance on target tasks using knowledge from experienced source tasks. Successor Representations (SR) and their extension Successor Features (SF) are prominent transfer mechanisms in domains where reward functions change between tasks. They reevaluate the expected return of previously learned policies in a new target task to transfer their knowledge. The SF framework extended SR by linearly decomposing rewards into successor features and a reward weight vector allowing their application in high-dimensional tasks. But this came with the cost of having a linear relationship between reward functions and successor features, limiting its application to tasks where such a linear relationship exists. We propose a novel formulation of SR based on learning the cumulative discounted probability of successor features, called Successor Feature Representations (SFR). Crucially, SFR allows to reevaluate the expected return of policies for general reward functions. We introduce different SFR variations, prove its convergence, and provide a guarantee on its transfer performance. Experimental evaluations based on SFR with function approximation demonstrate its advantage over SF not only for general reward functions, but also in the case of linearly decomposable reward functions.",
        "authors": "C. Reinke, X. Alameda-pineda",
        "keywords": [
            "Successor Representations",
            "Reward Decomposition",
            "Transfer Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=MTFf1rDDEI",
        "pdf_src": "https://api2.openreview.net/pdf/247285703862b30ae71355a0c87236d7003fe972.pdf",
        "Code_src": "",
        "Introduction": "Background: Transfer learning is an important topic in reinforcement learning that focuses on improving learning performance across multiple related tasks.\n\nResearch Problem: Existing methods like Successor Representations (SR) have limitations when dealing with changing reward functions or non-linear relationships because they rely on evaluating the expected return of previous policies within each new task individually without considering how these returns might be affected by changes in the environment's dynamics due to altered reward structures.\n \nMethod: To address this issue, we present a novel approach known as Successor Feature Representations (SFR), which learns the cumulative discounted probability distribution of successor features directly rather than relying solely on reward weights associated with them during training time alone – thus enabling us to capture more complex dependencies among states/actions/rewards regardless whether there exist any simple linear relationships therein beforehand!\n\nMain Contributions: Our main contributions include proposing two variants of our proposed method - one variant uses fixed successor feature vectors while another adapts those vectors adaptively throughout optimization; proving convergence guarantees under certain conditions regarding both variants; providing empirical evidence demonstrating superior performance compared against existing state-of-the-art approaches including Successor Features (SF) even if underlying assumptions about linearity hold true too!.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Generalizability of Adversarial Robustness Under Distribution Shifts",
        "abstract": "Recent progress in empirical and certified robustness promises to deliver reliable and deployable Deep Neural Networks (DNNs). Despite that success, most existing evaluations of DNN robustness have been done on images sampled from the same distribution on which the model was trained on. However, in the real world, DNNs may be deployed in dynamic environments that exhibit significant distribution shifts. In this work, we take a first step towards thoroughly investigating the interplay between empirical and certified adversarial robustness on one hand and domain generalization on another. To do so, we train robust models on multiple domains and evaluate their accuracy and robustness on an unseen domain. We observe that: (1) both empirical and certified robustness generalize to unseen domains, and (2) the level of generalizability does not correlate well with input visual similarity, measured by the FID between source and target domains. We also extend our study to cover a real-world medical application, in which adversarial augmentation significantly boosts the generalization of robustness with minimal effect on clean data accuracy.",
        "authors": "K. Alhamoud, H. A. A. K. Hammoud, M. Alfarra, et.al",
        "keywords": [
            "domain generalization",
            "empirical robustness",
            "certified robustness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=XNFo3dQiCJ",
        "pdf_src": "https://api2.openreview.net/pdf/4b38c11ea3ea9efdfcb815e5ecfe3e7b6092cf54.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe recent advancements in empirical and certified robustness for deep neural networks (DNNs) suggest they can provide dependable performance under various conditions; however, these improvements are primarily evaluated within the training distribution.\n\nResearch Problem:\nThis research addresses concerns about whether such robustness properties transfer effectively when faced with new or different distributions encountered during deployment outside controlled settings like those used for training.\n\nMethods:\nTo tackle this issue, researchers developed methods involving multi-domain robust model training where each model is exposed to datasets drawn from distinct but related domains rather than just focusing on the original training dataset. The evaluation involved assessing how accurately these models perform across previously unobserved domains while maintaining robustness against adversarial perturbations – hence examining empirical as well as certified robustness measures simultaneously along with domain generalization capabilities through cross-validation techniques over several unseen test sets representing diverse scenarios.\n\nMain Contributions:\n- Demonstrated that empirical and certified robustness indeed carry forward into novel contexts beyond what's seen at training time.\n- Contradicted expectations showing poor correlation between levels of robustness generalization observed via domain shift metrics - specifically Feature Instance Distance (FID), commonly employed measure comparing two distributions' statistical distances visually - indicating it might not always predict robustness generalization ability correctly without considering other factors influencing adaptation mechanisms learned throughout training processes.\n- Explored practical applications extending findings further outwards toward actual clinical medicine field wherein augmenting training samples using adversarial examples substantially enhanced robustness generalization potential despite negligible impact upon standard classification tasks performed solely based on non-adversarially modified inputs alone.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "When to Trust Aggregated Gradients: Addressing Negative Client Sampling in Federated Learning",
        "abstract": "Federated Learning has become a widely-used framework which allows learning a global model on decentralized local datasets under the condition of protecting local data privacy. However, federated learning faces severe optimization difficulty when training samples are not independently and identically distributed (non-i.i.d.). In this paper, we point out that the client sampling practice plays a decisive role in the aforementioned optimization difficulty. We find that the negative client sampling will cause the merged data distribution of currently sampled clients heavily inconsistent with that of all available clients, and further make the aggregated gradient unreliable. To address this issue, we propose a novel learning rate adaptation mechanism to adaptively adjust the server learning rate for the aggregated gradient in each round, according to the consistency between the merged data distribution of currently sampled clients and that of all available clients. Specifically, we make theoretical deductions to find a meaningful and robust indicator that is positively related to the optimal server learning rate, which is supposed to minimize the Euclidean distance between the aggregated gradient given currently sampled clients and that if all clients could participate in the current round. We show that our proposed indicator can effectively reflect the merged data distribution of sampled clients, thus we utilize it for the server learning rate adaptation. Extensive experiments on multiple image and text classification tasks validate the great effectiveness of our method in various settings. Our code is available at https://github.com/lancopku/FedGLAD.",
        "authors": "W. Yang, Y. Lin, G. Zhao, et.al",
        "keywords": [
            "FedLearning",
            "Non-I.I.D.",
            "Data Distribution Consistency"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=v73h3bYE2Z",
        "pdf_src": "https://api2.openreview.net/pdf/fae198f81d3ce61b9ffaef73af7097bf604133c6.pdf",
        "Code_src": "https://github.com/lancopku/FedGLAD",
        "Introduction": "Background: Federated Learning aims to train a global model using decentralized local datasets while preserving local data privacy.\n\nResearch Problem: Federated learning encounters significant optimization challenges particularly due to non-i.i.d. training samples where individual clients may have different distributions from one another's.\n\nMethod: The authors identify that the way clients are sampled during federated learning significantly impacts its performance by causing inconsistency among their merged data distributions leading to unreliable gradients across rounds.\nTo tackle this problem, they introduce an adaptive learning rate adjustment strategy based on the similarity measure between the merged dataset of active clients versus those who might be included but aren't.\n\nMain Contributions:\n1. They highlight how poor client sampling practices contribute to federated learning's difficulties especially concerning non-i.i.d. data scenarios;\n2. Propose FedGLAD - a novel approach incorporating a new learning rate adaptation mechanism designed specifically addressing these issues; \n3. Derive theoretically grounded metrics indicating whether or not adjustments should occur depending upon sample heterogeneity levels within federations; \n4. Validate through extensive experimental results conducted over several real-world image/text classification benchmarks demonstrating substantial improvements compared against baseline methods without such adaptations.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Deep Plug-and-Play Clustering with Unknown Number of Clusters",
        "abstract": "Clustering is an essential task for the purpose that data points can be classified in an unsupervised manner. Most deep clustering algorithms are very effective when given the number of clusters K. However, when K is unknown, finding the appropriate K for these algorithms can be computationally expensive via model-selection criteria, and applying algorithms with an inaccurate K can hardly achieve the state-of-the-art performance. This paper proposes a plug-and-play clustering module to automatically adjust the number of clusters, which can be easily embedded into existing deep parametric clustering methods. By analyzing the goal of clustering, a split-and-merge framework is introduced to reduce the intra-class diversity and increase the inter-class difference, which leverages the entropy between different clusters. Specifically, given an initial clustering number, clusters can be split into sub-clusters or merged into super-clusters and converge to a stable number of K clusters at the end of training. Experiments on benchmark datasets demonstrate that the proposed method can achieve comparable performance with the state-of-the-art works without requiring the number of clusters.\n",
        "authors": "A. Xiao, H. Chen, T. Guo, et.al",
        "keywords": [
            "plug-and-play",
            "adaptive clustering",
            "entropy-based"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=6rbcq0qacA",
        "pdf_src": "https://api2.openreview.net/pdf/3ebb73f231f76291a00ecf36e257db8782549c69.pdf",
        "Code_src": "",
        "Introduction": "Background: Clustering is crucial as it allows grouping similar data points together but traditionally requires knowing beforehand how many groups there should be (\"K\"). Existing deep learning-based clustering approaches perform well if \"K\" is known; however, determining this manually through cross-validation processes like silhouette coefficient computation often involves computational overhead.\n\nResearch Problem: How do we efficiently determine cluster numbers within deep clustering models where they're not explicitly provided?\n\nMethodology: The authors introduce a novel approach called Split-and-Merge Framework designed specifically around the problem mentioned above by adjusting the number of clusters during the training process itself rather than relying solely on pre-defined thresholds such as silhouette coefficients post-training.\n\nMain Contributions:\n1. A Plug-and-Play Module - They have developed a flexible clustering component capable of being integrated seamlessly alongside other existing deep parametric clustering techniques thus allowing them to adaptively learn optimal cluster sizes throughout their operation instead of having fixed ones imposed upon them from outside sources.\n2. Entropy-Based Strategy - Their strategy uses information theory concepts particularly entropy measure across all potential partitions among samples aiming towards minimizing internal heterogeneity while maximizing external differences amongst clusters leading up to convergence onto some final stable partitioning scheme.\n3. Performance Validation - Extensive experiments conducted using widely recognized benchmarks show equivalent results compared against top-performing competitors despite automatic adjustment meaning no manual tuning needed regarding 'K' value selection before running any algorithmic implementation based off this research findings would yield satisfactory outcomes regardless whether one knows what constitutes an accurate estimate upfront",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Lightweight Learner for Shared Knowledge Lifelong Learning",
        "abstract": "In Lifelong Learning (LL), agents continually learn as they encounter new conditions and tasks. Most current LL is limited to a single agent that learns tasks sequentially. Dedicated LL machinery is then deployed to mitigate the forgetting of old tasks as new tasks are learned. This is inherently slow. We propose a new Shared Knowledge Lifelong Learning (SKILL) challenge, which deploys a decentralized population of LL agents that each sequentially learn different tasks, with all agents operating independently and in parallel. After learning their respective tasks, agents share and consolidate their knowledge over a decentralized communication network, so that, in the end, all agents can master all tasks. We present one solution to SKILL which uses Lightweight Lifelong Learning (LLL) agents, where the goal is to facilitate efficient sharing by minimizing the fraction of the agent that is specialized for any given task. Each LLL agent thus consists of a common task-agnostic immutable part, where most parameters are, and individual task-specific modules that contain fewer parameters but are adapted to each task. Agents share their task-specific modules, plus summary information (\"task anchors\") representing their tasks in the common task-agnostic latent space of all agents. Receiving agents register each received task-specific module using the corresponding anchor. Thus, every agent improves its ability to solve new tasks each time new task-specific modules and anchors are received. If all agents can communicate with all others, eventually all agents become identical and can solve all tasks. On a new, very challenging SKILL-102 dataset with 102 image classification tasks (5,033 classes in total, 2,041,225 training, 243,464 validation, and 243,464 test images), we achieve much higher (and SOTA) accuracy over 8 LL baselines, while also achieving near perfect parallelization. Code and data can be found at https://github.com/gyhandy/Shared-Knowledge-Lifelong-Learning",
        "authors": "Y. Ge, Y. Li, D. Wu, et.al",
        "keywords": [
            "SKILL",
            "Lifelong Learning",
            "Parallelization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Jjl2c8kWUc",
        "pdf_src": "https://api2.openreview.net/pdf/0e6e1530753b4ca30c87350b34385bb8a48fd9ae.pdf",
        "Code_src": "代码链接：https://github.com/gyhandy/Shared-Knowledge-Lifelong-Learning",
        "Introduction": "Background: The paper addresses the issue of lifelong learning (LL) within artificial intelligence systems—where agents continue to acquire skills throughout exposure to novel situations or challenges. Traditional approaches focus on sequential learning across multiple tasks; however, this approach has limitations due to the need for dedicated mechanisms like forgetting prevention when introducing new ones.\n\nResearch Problem: The primary research problem tackled here concerns how to efficiently enable an ensemble of autonomous agents capable of simultaneously acquiring various tasks without suffering from catastrophic forgetting—a phenomenon wherein previously acquired knowledge deteriorates upon learning something new.\n\nMethodology: To overcome these issues, the authors introduce the concept of \"Shared Knowledge Lifelong Learning\" (SKILL). In contrast to traditional centralized models requiring coordination among agents during learning phases, SKILL envisions a distributed system composed of independent agents who sequentially train themselves individually yet collaboratively through shared knowledge consolidation via a decentralized communication network after completion of specific tasks.\n\nMain Contributions:\n1. **Theoretical Framework**: They establish a theoretical framework for SKILL based on a decentralized architecture.\n2. **Solution Approach**: A proposed solution involves lightweight lifelong learning (LLL) agents designed specifically around the SKILL paradigm—the agents have a core component shared amongst them regardless of what task they're working on along with additional task-specific components tailored only toward solving particular problems effectively minimising redundancy between agents' specializations.\n3. **Data Set**: They developed a comprehensive benchmark called SKILL-102 containing numerous distinct image classification tasks involving thousands of classes ensuring rigorous testing ground against other existing methods dealing with similar complexities concerning multi-task learning capabilities under continuous adaptation constraints.\n4. **Performance**: Their method significantly outperforms eight state-of-the-art baseline algorithms not just in terms of overall accuracy achieved post-training phase but demonstrates unparalleled parallelisation efficiency making it more practical than previous solutions addressing similar objectives related to continual learning architectures such as those seen before in machine learning literature focused on neural networks trained continuously rather than incrementally stepwise fashion typically employed today's deep learning frameworks.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "A Measure of the Complexity of Neural Representations based on Partial Information Decomposition",
        "abstract": "In neural networks, task-relevant information is represented jointly by groups of neurons. However, the specific way in which this mutual information about the classification label is distributed among the individual neurons is not well understood: While parts of it may only be obtainable from specific single neurons, other parts are carried redundantly or synergistically by multiple neurons. We show how Partial Information Decomposition (PID), a recent extension of information theory, can disentangle these different contributions. From this, we introduce the measure of ``Representational Complexity'', which quantifies the difficulty of accessing information spread across multiple neurons. We show how this complexity is directly computable for smaller layers. For larger layers, we propose subsampling and coarse-graining procedures and prove corresponding bounds on the latter. Empirically, for quantized deep neural networks solving the MNIST and CIFAR10 tasks, we observe that representational complexity decreases both through successive hidden layers and over training, and compare the results to related measures. Overall, we propose representational complexity as a principled and interpretable summary statistic for analyzing the structure and evolution of neural representations and complex systems in general.",
        "authors": "D. A. Ehrlich, A. C. Schneider, V. Priesemann, et.al",
        "keywords": [
            "neural network",
            "partial information decomposition",
            "representational complexity"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=R8TU3pfzFr",
        "pdf_src": "https://api2.openreview.net/pdf/ffedc67b3f5e258ff78a98087d08b5cadb06bf3e.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses the representation of task-relevant information within neural networks using groups of neurons.\n\nResearch Problem:\nThe problem addressed here concerns understanding exactly how mutual information regarding the classification label is distributed amongst individual neurons - whether certain pieces require specific neurons while others might be redundant or synergistic with many neurons involved.\n\nMethods:\nTo tackle this issue, the authors utilize Partial Information Decomposition (PID), an extension of information theory recently introduced into neuroscience research fields such as computational psychiatry where they analyze brain activity patterns during decision-making processes involving uncertainty reduction strategies like Bayesian inference under model uncertainty conditions etcetera). They also present \"Representational Complexity\" measurement method which quantifies difficulties associated with retrieving dispersed knowledge across several neurons; specifically designed especially considering large-scale architectures encountered nowadays due their size limitations compared small ones before them.\n\nMain Contributions:\nThis work introduces Representational Complexity metric capable assessing distributional properties between neurons contributing towards encoding target variables whilst being able handle cases when some subsets carry more weight than others leading us closer toward unraveling mysteries behind neural computations underlying intelligent behaviors observed daily life contexts including human cognition itself!",
        "Topic": "Sample Efficiency in Reinforcement Learning"
    },
    {
        "title": "Know Your Self-supervised Learning: A Survey on Image-based Generative and Discriminative Training",
        "abstract": "Although supervised learning has been highly successful in improving the state-of-the-art in the domain of image-based computer vision in the past, the margin of improvement has diminished significantly in recent years, indicating that a plateau is in sight. Meanwhile, the use of self-supervised learning (SSL) for the purpose of natural language processing (NLP) has seen tremendous successes during the past couple of years, with this new learning paradigm yielding powerful language models. Inspired by the excellent results obtained in the field of NLP, self-supervised methods that rely on clustering, contrastive learning, distillation, and information-maximization, which all fall under the banner of discriminative SSL, have experienced a swift uptake in the area of computer vision. Shortly afterwards, generative SSL frameworks that are mostly based on masked image modeling, complemented and surpassed the results obtained with discriminative SSL. Consequently, within a span of three years, over $100$ unique general-purpose frameworks for generative and discriminative SSL, with a focus on imaging, were proposed. In this survey, we review a plethora of research efforts conducted on image-oriented SSL, providing a historic view and paying attention to best practices as well as useful software packages. While doing so, we discuss pretext tasks for image-based SSL, as well as techniques that are commonly used in image-based SSL. Lastly, to aid researchers who aim at contributing to image-focused SSL, we outline a number of promising research directions.",
        "authors": "U. Ozbulak, H. J. Lee, B. Boga, et.al",
        "keywords": [
            "image-based computer vision",
            "self-supervised learning",
            "generative SSL"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Ma25S4ludQ",
        "pdf_src": "https://api2.openreview.net/pdf/7094c3d45d16c4975f61a47a6ff7a2e2664855cc.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses how despite significant advancements through supervised learning approaches towards improving various aspects of computer vision using images, progress seems to be slowing down due to diminishing returns.\n\nResearch Problem: The main issue addressed concerns identifying potential reasons behind the stagnation observed after several decades of substantial improvements from supervised machine learning algorithms applied specifically to visual data analysis fields like object recognition or scene understanding.\n \nMethodology: To address these issues related to supervised learning's limitations while exploring novel avenues such as self-supervised learning (SSL), particularly focusing on its applications toward computer vision problems involving imagery datasets; they provide an overview of different types of SSL methodologies including those employing discriminative strategies versus generative ones along with their respective pros and cons regarding performance metrics across diverse benchmarks relevant to image processing tasks.\n\nMain Contributions: This work serves primarily as a comprehensive literature review summarizing numerous studies published since 2019 concerning self-supervised learning applied directly onto image datasets covering both discriminative and generative paradigms respectively highlighting key findings pertinent areas where further investigation might yield fruitful outcomes leading into future breakthroughs beyond current boundaries imposed upon traditional supervised training mechanisms utilized extensively thus far within academia industry sectors alike dealing with computationally intensive tasks requiring sophisticated computational resources capable handling large-scale voluminous amounts raw sensory input material derived from cameras sensors etcetera",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "Trip-ROMA: Self-Supervised Learning with Triplets and Random Mappings",
        "abstract": "Contrastive self-supervised learning (SSL) methods, such as MoCo and SimCLR, have achieved great success in unsupervised visual representation learning. They rely on a large number of negative pairs and thus require either large memory banks or large batches. Some recent non-contrastive SSL methods, such as BYOL and SimSiam, attempt to discard negative pairs and have also shown remarkable performance. To avoid collapsed solutions caused by not using negative pairs, these methods require non-trivial asymmetry designs. However, in small data regimes, we can not obtain a sufficient number of negative pairs or effectively avoid the over-fitting problem when negatives are not used at all. To address this situation, we argue that negative pairs are still important but one is generally sufficient for each positive pair. We show that a simple Triplet-based loss (Trip) can achieve surprisingly good performance without requiring large batches or asymmetry designs. Moreover, to alleviate the over-fitting problem in small data regimes and further enhance the effect of Trip, we propose a simple plug-and-play RandOm MApping (ROMA) strategy by randomly mapping samples into other spaces and requiring these randomly projected samples to satisfy the same relationship indicated by the triplets. Integrating the triplet-based loss with random mapping, we obtain the proposed method Trip-ROMA. Extensive experiments, including unsupervised representation learning and unsupervised few-shot learning, have been conducted on ImageNet-1K and seven small datasets. They successfully demonstrate the effectiveness of Trip-ROMA and consistently show that ROMA can further effectively boost other SSL methods. Code is available at https://github.com/WenbinLee/Trip-ROMA. ",
        "authors": "W. Li, X. Yang, M. Kong, et.al",
        "keywords": [
            "Triplet-based loss",
            "Random Mapping",
            "Unsupervised Few-Shot Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=MR4glug5GU",
        "pdf_src": "https://api2.openreview.net/pdf/ece646a33e41accfda85f4a235bad7447c53cfd6.pdf",
        "Code_src": "https://github.com/WenbinLee/Trip-ROMA",
        "Introduction": "Background: Contrastive self-supervised learning (SSL) has become an effective approach towards unsupervised visual representation learning due to its impressive results demonstrated through models like MoCo and SimCLR.\n\nResearch Problem: Despite their efficacy, contrastive SSL methods demand substantial computational resources; they typically necessitate extensive memory capacity from large memory banks (\"MoCo\") or high batch sizes (\"SimCLR\"). Additionally, some newer non-contrastive SSL approaches - BYOL and SimSiam – eliminate the need for negative pairs which reduces resource requirements yet introduces challenges related to potential collapse solutions if asymmetric treatments aren't carefully designed.\n \nMethod: The paper proposes addressing both issues within the context of smaller datasets where obtaining enough negative examples may be difficult while simultaneously avoiding model collapse during training phases involving only positive pairs. This leads them to consider whether even just one negative example per positive could suffice instead relying heavily on many negatives traditionally required before. \n\nMain Contributions:\n1. A novel \"Triplet-based\" loss function called Trip, which does away with the need for complex asymmetrical architectures commonly found in existing non-contrastive SSL frameworks despite being based solely on positive pairs rather than traditional contrasting ones.\n2. An innovative \"Random Mapping\" technique termed ROMA aimed specifically toward mitigating overfitting concerns prevalent especially under limited dataset scenarios along with enhancing the overall impact of the Trip loss mechanism itself via additional sample transformations prior to computing any losses.\n\nThe authors validate their findings across various benchmarks demonstrating superior performance compared against baseline SSL techniques incorporating the newly introduced Trip-ROMA framework alongside showcasing how ROMA significantly improves upon standalone SSL methods themselves.",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Attacking Perceptual Similarity Metrics",
        "abstract": "Perceptual similarity metrics have progressively become more correlated with human judgments on perceptual similarity; however, despite recent advances, the addition of an imperceptible distortion can still compromise these metrics. In our study, we systematically examine the robustness of these metrics to imperceptible adversarial perturbations. Following the two-alternative forced-choice experimental design with two distorted images and one reference image, we perturb the distorted image closer to the reference via an adversarial attack until the metric flips its judgment. We first show that all metrics in our study are susceptible to perturbations generated via common adversarial attacks such as FGSM, PGD, and the One-pixel attack. Next, we attack the widely adopted LPIPS metric using spatial-transformation-based adversarial perturbations (stAdv) in a white-box setting to craft adversarial examples that can effectively transfer to other similarity metrics in a black-box setting. We also combine the spatial attack stAdv with PGD ($\\ell_\\infty$-bounded) attack to increase transferability and use these adversarial examples to benchmark the robustness of both traditional and recently developed metrics. Our benchmark provides a good starting point for discussion and further research on the robustness of metrics to imperceptible adversarial perturbations.",
        "authors": "A. Ghildyal, F. Liu",
        "keywords": [
            "adversarial perturbations",
            "perceptual similarity metrics",
            "robustness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=r9vGSpbbRO",
        "pdf_src": "https://api2.openreview.net/pdf/a8fede82170db85c413a5aba7071ae8d766bdcb1.pdf",
        "Code_src": "",
        "Introduction": "Background: Perceptual similarity metrics aim to measure how similar or different two images appear visually without considering semantic content.\n\nResearch Problem: Despite improvements over time towards correlating better with human perception regarding visual similarity, it is found that adding imperceptible distortions could affect their performance significantly leading to compromised accuracy when making decisions based solely on them.\n \nMethodology: The researchers conducted experiments by applying adversarial attacks like Fast Gradient Sign Method (FGSM), Projected Gradient Descent (PGD), and One-Pixel Attack onto distorted versions of images while comparing against original ones under a 2AFC (two-alternative forced-choice) setup where subjects were asked to choose between pairs of images. They then crafted new adversarial examples specifically designed not only to fool certain metrics but potentially others through spatial transformation techniques within a white box scenario before testing across various metrics including newer developments which may be less robust than older counterparts.\n \nMain Contributions:\n1. Demonstrated susceptibility among existing perceptual similarity metrics toward commonly used adversarial attacks;\n2. Developed novel adversarial examples capable of transferring successfully from one metric into another indicating potential weaknesses shared amongst multiple measures;\n3. Provided benchmarks highlighting relative robustness levels allowing comparison assessment",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Conditional Permutation Invariant Flows",
        "abstract": "We present a conditional generative probabilistic model of set-valued data with a tractable log density.  This model is a continuous normalizing flow governed by permutation equivariant dynamics. These dynamics are driven by a learnable per-set-element term and pairwise interactions, both parametrized by deep neural networks.  We illustrate the utility of this model via applications including (1) complex traffic scene generation conditioned on visually specified map information, and (2) object bounding box generation conditioned directly on images.  We train our model by maximizing the expected likelihood of labeled conditional data under our flow, with the aid of a penalty that ensures the dynamics are smooth and hence efficiently solvable. Our method significantly outperforms non-permutation invariant baselines in terms of log likelihood and domain-specific metrics (offroad, collision, and combined infractions), yielding realistic samples that are difficult to distinguish from data.",
        "authors": "B. Zwartsenberg, A. Scibior, M. Niedoba, et.al",
        "keywords": [
            "set-valued data",
            "conditional generative model",
            "normalizing flow"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=DUsgPi3oCC",
        "pdf_src": "https://api2.openreview.net/pdf/aabcc751e55326bb867934d3a026499e2e3d543d.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper introduces a novel conditional generative probabilistic model for handling set-valued data where each element within the set can have its own distribution parameters.\n\nResearch Problem: How do we develop an efficient algorithm capable of generating new instances while preserving the underlying structure represented as sets?\n\nMethodology: They propose using a continuous normalizing flow framework guided by permutation-equivariant dynamics which allows them to account for elements' independence through a learnable per-element term along with pairwise interaction terms learned by deep neural networks.\n \nMain Contributions:\n- A new conditional generative model based on a permutation-equivariant continuous normalizing flow architecture tailored specifically for set-valued data.\n- Demonstrated effectiveness across two application domains - generating traffic scenes given visual map inputs or bounding boxes just from image input without requiring any additional annotations beyond the raw image itself.\n- Introduced a training strategy involving maximizing the expected likelihood subject to constraints ensuring smoothness leading to computational efficiency during inference time compared against baseline models not considering permutations.",
        "Topic": "Optimal Transport"
    },
    {
        "title": "Event Tables for Efficient Experience Replay",
        "abstract": "Experience replay (ER) is a crucial component of many deep reinforcement learning (RL) systems.\nHowever, uniform sampling from an ER buffer can lead to slow convergence and unstable asymptotic\nbehaviors. This paper introduces Stratified Sampling from Event Tables (SSET), which partitions\nan ER buffer into Event Tables, each capturing important subsequences of optimal behavior. We\nprove a theoretical advantage over the traditional monolithic buffer approach and combine SSET with\nan existing prioritized sampling strategy to further improve learning speed and stability. Empirical\nresults in challenging MiniGrid domains, benchmark RL environments, and a high-fidelity car racing\nsimulator demonstrate the advantages and versatility of SSET over existing ER buffer sampling",
        "authors": "V. R. Kompella, T. Walsh, S. Barrett, et.al",
        "keywords": [
            "experience replay",
            "stratified sampling",
            "event tables"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=XejzjAjKjv",
        "pdf_src": "https://api2.openreview.net/pdf/cdfdf2f54368f8e9f4449d4197e2dfb9857d235f.pdf",
        "Code_src": "",
        "Introduction": "Background: Experience Replay (ER) is widely used as part of Deep Reinforcement Learning (DRL) algorithms due to its ability to efficiently store past experiences for later use during training.\n\nResearch Problem: The primary issue addressed by this research concerns the performance limitations that arise when uniformly sampling from experience replay buffers within DRL systems; these include slow convergence rates or erratic behaviors at equilibrium states.\n\nMethodology: To address such issues, the authors introduce Stratified Sampling from Event Tables (SSET). Instead of using a single large replay buffer where all samples are mixed together without distinction between their importance levels, they propose partitioning the buffer into smaller \"Event Tables\" based on specific criteria related to optimal behavior sequences captured therein – essentially creating mini-batches around similar patterns observed across different episodes' trajectories towards goal attainment.\n\nMain Contributions:\n1. They provide theoretical evidence showing how stratification improves upon standard non-stratified approaches like uniform sampling through better utilization of information contained within individual trajectories before reaching goals;\n2. By integrating SSET with Prioritized Experience Replay - another well-known technique enhancing sample efficiency via assigning higher priority scores dynamically according to recentness/importance of sampled events -, it allows agents trained under both methods outperform those solely employing either one alone significantly in terms of exploration-exploitation trade-offs while maintaining stable learning dynamics throughout various tasks tested empirically including complex grid-worlds simulations up until realistic autonomous driving scenarios simulated inside virtual cars racing tracks.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Agent-State Construction with Auxiliary Inputs",
        "abstract": "In many, if not every realistic sequential decision-making task, the decision-making agent is not able to model the full complexity of the world. The environment is often much larger and more complex than the agent, a setting also known as partial observability. In such settings, the agent must leverage more than just the current sensory inputs; it must construct an agent state that summarizes previous interactions with the world. Currently, a popular approach for tackling this problem is to learn the agent-state function via a recurrent network from the agent's sensory stream as input. Many impressive reinforcement learning applications have instead relied on environment-specific functions to aid the agent's inputs for history summarization. These augmentations are done in multiple ways, from simple approaches like concatenating observations to more complex ones such as uncertainty estimates. Although ubiquitous in the field, these additional inputs, which we term auxiliary inputs, are rarely emphasized, and it is not clear what their role or impact is. In this work we explore this idea further, and relate these auxiliary inputs to prior classic approaches to state construction. We present a series of examples illustrating the different ways of using auxiliary inputs for reinforcement learning. We show that these auxiliary inputs can be used to discriminate between observations that would otherwise be aliased, leading to more expressive features that smoothly interpolate between different states. Finally, we show that this approach is complementary to state-of-the-art methods such as recurrent neural networks and truncated back-propagation through time, and acts as a heuristic that facilitates longer temporal credit assignment, leading to better performance.",
        "authors": "R. Y. Tao, A. White, M. C. Machado",
        "keywords": [
            "partial observability",
            "auxiliary inputs",
            "reinforcement learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=RLYkyucU6k",
        "pdf_src": "https://api2.openreview.net/pdf/57700089f41a7648b90a0c2c85ca4f1e0e9f1ca1.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper addresses the issue of partial observability where agents cannot fully represent the complexities of environments they interact within real-world scenarios.\n\nResearch Question: How do agents effectively summarize past interactions when faced with large and complex environments?\n\nMethodology: The authors propose leveraging auxiliary inputs alongside sensory data streams by employing recurrent networks (\"agent-state function\") learned directly from sensor inputs rather than relying solely on environmental functions provided explicitly designed for aiding historical summary tasks.\n \nMain Contributions:\n1. They emphasize the importance yet underexplored nature of auxiliary inputs - beyond standard sensory information – in enhancing feature representation during reinforcement learning processes.\n2. By comparing various techniques involving auxiliary inputs against classical state construction methodologies previously employed before modern deep learning frameworks became prevalent,\n3. They demonstrate how these supplementary cues help differentiate ambiguous observations into distinct categories thereby enabling smoother transitions among diverse states represented by the agent.\n4. Furthermore, introducing auxiliary inputs has been shown beneficial compared to existing advanced algorithms including Recurrent Neural Networks (RNNs) & Truncated Backpropagation Through Time (TBPTT), contributing towards improved long-term temporal reasoning capabilities",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Modelling sequential branching dynamics with a multivariate branching Gaussian process",
        "abstract": "The Branching Gaussian Process (BGP) model is a modification of the Overlapping Mixture\nof Gaussian Processes (OMGP) where latent functions branch in time. The BGP model\nwas introduced as a method to model bifurcations in single-cell gene expression data and\norder genes by inferring their branching time parameter. A limitation of the current BGP\nmodel is that the assignment of observations to latent functions is inferred independently\nfor each output dimension (gene). This leads to inconsistent assignments across outputs\nand reduces the accuracy of branching time inference. Here, we propose a multivariate\nbranching Gaussian process (MBGP) model to perform joint branch assignment inference\nacross multiple output dimensions. This ensures that branch assignments are consistent and\nleverages more data for branching time inference. Model inference is more challenging than\nfor the original BGP or OMGP models because assignment labels can switch from trunk to\nbranch lineages as branching times change during inference. To scale up inference to large\ndatasets we use sparse variational Bayesian inference. We examine the effectiveness of our\napproach on synthetic data and a single-cell RNA-Seq dataset from mouse haematopoietic\nstem cells (HSCs). Our approach ensures assignment consistency by design and achieves\nimproved accuracy in branching time inference and assignment accuracy.",
        "authors": "E. Sarkans, S. Ahmed, M. Rattray, et.al",
        "keywords": [
            "branching gaussian process",
            "multivariate modeling",
            "sparse variational bayesian inference"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=9KoBOlstTq",
        "pdf_src": "https://api2.openreview.net/pdf/2549b0643968bba80c765e289a18aa7c0b23b5ab.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper introduces an extension called Multivariate Branching Gaussian Process (MBGP), which modifies the existing Branching Gaussian Process (BGP) model used previously.\n\nResearch Problem: The problem addressed here concerns inconsistencies between different output dimensions when assigning observations within the BGP framework due to independent assignments per output dimension leading to reduced accuracy while inferring branching times accurately based on single-cell gene expression data.\n\nMethodology: The proposed MBGP model allows for joint branch assignment inference over multiple output dimensions ensuring consistency among them rather than having separate processes like before; it also employs Sparse Variational Bayesian Inference techniques allowing scaling up its application even with larger datasets since they require less computational resources compared traditional methods.\n \nMain Contributions: The main contribution lies not only in improving upon previous limitations but providing better performance metrics such as higher accuracy rates both at predicting branching times along with maintaining consistency throughout all assigned branches across various outputs dimensions making this new variant particularly useful especially those dealing with high-dimensional biological datasets involving complex interactions amongst variables involved",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "U-NO: U-shaped Neural Operators",
        "abstract": "Neural operators generalize classical neural networks to maps between infinite-dimensional spaces, e.g., function spaces. Prior works on neural operators proposed a series of novel methods to learn such maps and demonstrated unprecedented success in learning solution operators of partial differential equations. Due to their close proximity to fully connected architectures, these models mainly suffer from high memory usage and are generally limited to shallow deep learning models. In this paper, we propose U-shaped Neural Operator (U-NO), a U-shaped memory enhanced architecture that allows for deeper neural operators. U-NOs exploit the problem structures in function predictions and demonstrate fast training, data efficiency, and robustness with respect to hyperparameters choices. We study the performance of U-NO on PDE benchmarks, namely, Darcy’s flow law and the Navier-Stokes equations. We show that U-NO results in an average of 26% and 44% prediction improvement on Darcy’s flow and turbulent Navier-Stokes equations, respectively, over the state of the art. On Navier-Stokes 3D spatiotemporal operator learning task, we show U-NO provides 37% improvement over the state of art methods.",
        "authors": "M. A. Rahman, Z. E. Ross, K. Azizzadenesheli",
        "keywords": [
            "function spaces",
            "neural operator",
            "U-shaped architecture"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=j3oQF9coJd",
        "pdf_src": "https://api2.openreview.net/pdf/0eaad1b3c95bb018b838f3e12e6cb71274d57160.pdf",
        "Code_src": "",
        "Introduction": "Background: This research focuses on extending neural network capabilities beyond finite-dimensional spaces by developing \"neural operators\" capable of mapping functions across infinite-dimensional domains like function spaces.\n\nResearch Problem: The existing neural operator approaches have achieved significant advancements but primarily struggle due to limitations related to computational resources - they consume substantial amounts of memory which restricts them into being shallower than desired depth-wise during model training; thus hindering further progress towards more complex tasks requiring deeper architectures.\n\n\nMethodology: To address aforementioned issues, authors introduce a new type of neural operator architecture called \"U-shaped Neural Operator\" or simply 'U-NO'. It is designed as having two symmetrical branches converging at a central point resembling letter \"U\". These branches allow storing intermediate computations within memory modules known as \"memory banks\", enabling it to process information through multiple layers without significantly increasing its memory footprint compared to traditional neural networks.\n\n\nMain Contributions:\n1. Propose a novel architecture named U-shaped Neural Operator.\n2. Demonstrate how U-NO can be trained efficiently while maintaining both speed and accuracy when predicting solutions using Partial Differential Equations (PDEs).\n3. Show empirical evidence indicating improved predictive abilities specifically against benchmark problems including Darcy's Flow Law & Turbulent Navier-Stokes Equations where our method outperforms prior work leading up to around 44% better predictions depending upon specific scenario considered along with showing gains even greater improvements surpassing current best practices notably ~37% increase observed relative to other top-performing techniques dealing with spatially-temporally varying fields represented via Navier-Stokes equations applied three-dimensionally.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration",
        "abstract": "Inversion by Direct Iteration (InDI) is a new formulation for supervised image restoration that avoids the so-called ``regression to the mean'' effect and produces more realistic and detailed images than existing regression-based methods. It does this by gradually improving image quality in small steps, similar to generative denoising diffusion models.\n\nImage restoration is an ill-posed problem where multiple high-quality images are plausible reconstructions of a given low-quality input. Therefore, the outcome of a single step regression model is typically  an aggregate of all possible explanations, therefore lacking details and realism. The main advantage of InDI is that it does not try to predict the clean target image in a single step but instead gradually improves the image in small steps, resulting in better perceptual quality.\n\nWhile generative denoising diffusion models also work in small steps, our formulation is distinct in that it does not require knowledge of any analytic form of the degradation process. Instead, we directly learn an iterative restoration process from low-quality and high-quality paired examples. InDI can be applied to virtually any image degradation, given paired training data. In conditional denoising diffusion image restoration the denoising network generates the restored image by repeatedly denoising an initial image of pure noise, conditioned on the degraded input. Contrary to conditional denoising formulations, InDI directly proceeds by iteratively restoring the input low-quality image, producing high-quality results on a variety of image restoration tasks, including motion and out-of-focus deblurring, super-resolution, compression artifact removal, and denoising.",
        "authors": "M. Delbracio, P. Milanfar",
        "keywords": [
            "Inversion by Direct Iteration",
            "Image Restoration",
            "Generative Denoising Diffusion Models"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=VmyFF5lL3F",
        "pdf_src": "https://api2.openreview.net/pdf/de57692e80d2aa551260831a849a99d5d87fd935.pdf",
        "Code_src": "",
        "Introduction": "Background: Image restoration involves reconstructing clear images from noisy or blurred inputs which often leads to \"regression to the mean\" - a phenomenon when simple averaging over many potential solutions yields poor detail retention.\nResearch Problem: How do you develop a method capable of generating highly detailed and realistic restorations without suffering from regression to the mean?\nMethod: Introduced is Inversion by Direct Iteration (InDI), akin to generative denoising diffusion models with its incremental improvement approach; however, unlike diffusion models requiring prior knowledge about the degradation process, InDI learns through direct iteration using pairs of low-quality and their corresponding high-quality reference images as supervision during training.\nMain Contributions:\n1. Avoids regression to the mean issue common in regression-based approaches due to gradual refinement rather than one-step prediction.\n2. Does not rely on knowing the analytical form of the degradation function like diffusion models need – it's trained end-to-end based solely on observed pairs leading to greater flexibility across various types of image degradation scenarios such as blur, focus errors, resolution enhancement etc.\n3. Offers practicality beyond theoretical demonstrations since it doesn't demand complex pre-processing nor specialized hardware resources making it accessible broadly within computational constraints typical today’s computing environments would allow.",
        "Topic": "Generative Models"
    },
    {
        "title": "Numerical Data Imputation for Multimodal Data Sets: A Probabilistic Nearest-Neighbor Kernel Density Approach",
        "abstract": "Numerical data imputation algorithms replace missing values by estimates to leverage incomplete data sets. Current imputation methods seek to minimize the error between the unobserved ground truth and the imputed values. But this strategy can create artifacts leading to poor imputation in the presence of multimodal or complex distributions. To tackle this problem, we introduce the $k$NN$\\times$KDE algorithm: a data imputation method combining nearest neighbor estimation ($k$NN) and density estimation with Gaussian kernels (KDE). \nWe compare our method with previous data imputation methods using artificial and real-world data with different data missing scenarios and various data missing rates, and show that our method can cope with complex original data structure, yields lower data imputation errors, and provides probabilistic estimates with higher likelihood than current methods. We release the code in open-source for the community.",
        "authors": "F. Lalande, K. Doya",
        "keywords": [
            "kNN$\\times$KDE",
            "Numerical data imputation algorithms",
            "Density estimation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=KqR3rgooXb",
        "pdf_src": "https://api2.openreview.net/pdf/cc999c2d80c97b5492f9c666e5f5d4c22ec2697b.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses the issue of numerical data imputation where algorithms are used to estimate missing values within datasets so as to utilize incomplete information effectively.\n\nResearch Problem:\nCurrent approaches focus on minimizing the difference between observed actual values (\"ground truth\") and estimated ones but may fail when dealing with multimodal or complex distribution patterns due to potential artifact generation which leads to suboptimal results.\n \nMethodology:\nTo address these limitations, authors propose a novel approach called \"$k$NN$\\times$KDE\" which integrates two techniques - k-Nearest Neighbors (kNN), an instance-based learning technique known from statistics; and Kernel Density Estimation (KDE), widely applied non-parametric way to estimate probability densities based on random samples without making assumptions about its underlying distribution.\n\nMain Contributions:\nThe study evaluates their proposed method against existing state-of-the-art data imputation strategies across both synthetic and real-world datasets under varying conditions such as types/modes of missingness along with different levels of missing rate percentages. The experimental outcomes demonstrate that the \"$k$NN$\\times$KDE\" significantly outperforms other methods regarding accuracy while also providing more probable estimations compared to what is currently available through open-source software sharing efforts made possible via GitHub repository releases allowing further research replication and validation opportunities among others interested parties working towards similar goals related to handling incomplete datasets efficiently yet accurately",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "Releasing Graph Neural Networks with Differential Privacy Guarantees",
        "abstract": "With the increasing popularity of graph neural networks (GNNs) in several sensitive applications like healthcare and medicine, concerns have been raised over the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to privacy attacks, such as membership inference attacks, even if only black-box access to the trained model is granted. We propose PRIVGNN, a privacy-preserving framework for releasing GNN models in a centralized setting. Assuming an access to a public unlabeled graph, PRIVGNN provides a framework to release GNN models trained explicitly on public data along with knowledge obtained from the private data in a privacy preserving manner. PRIVGNN combines the knowledge-distillation framework with the two noise mechanisms, random subsampling, and noisy labeling, to ensure rigorous privacy guarantees. We theoretically analyze our approach in the Rènyi differential privacy framework. Besides, we show the solid experimental performance of our method compared to several baselines adapted for graph-structured data. Our code is\navailable at https://github.com/iyempissy/privGnn.",
        "authors": "I. E. Olatunji, T. Funke, M. Khosla",
        "keywords": [
            "privacy-preserving",
            "graph neural networks",
            "differential privacy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=wk8oXR0kFA",
        "pdf_src": "https://api2.openreview.net/pdf/ba24958019f9b72dc45cfaad33ff5db9ccc23081.pdf",
        "Code_src": "https://github.com/iyempissy/privGnn",
        "Introduction": "Background: Graph neural networks (GNNs) have gained significant attention due to their effectiveness across various fields including healthcare and medicine; however, there has been growing concern regarding the privacy implications associated with these networks.\n\nResearch Problem: The primary issue addressed by this research revolves around the vulnerability of GNNs against privacy attacks when they're deployed or utilized without direct interaction within the training environment (\"black-box\" scenario). Specifically, adversaries can potentially infer whether certain individual instances were used during the training process through membership inference attacks despite having limited visibility into the network's inner workings.\n\nMethodology: To address this problem, the authors introduce PRIVGNN – a novel privacy-preserving framework designed specifically for sharing GNN models under centralised settings where no direct interaction between the model and its users occurs post-training phase. This framework leverages both knowledge distillation techniques alongside incorporating randomness via methods known as random subsampling combined with noisy labeling which serve dual purposes - enhancing the robustness while also protecting user privacy according to stringent criteria set forth using Rényi Differential Privacy theory.\n\nMain Contributions: \n1. PRIVGNN introduces a new methodology that allows for the dissemination of GNN models developed exclusively based on publicly available datasets whilst simultaneously integrating insights derived from confidential/private sources.\n2. It employs advanced techniques combining knowledge distillation principles together with stochastic approaches ensuring not just improved generalization but also adherence to high standards concerning confidentiality preservation.\n3. Rigorous theoretical analysis conducted demonstrates compliance with differential privacy requirements laid out per Rényi’s framework.\n4. Experimental validation shows PRIVGNN significantly outperforms existing baseline methodologies tailored towards handling graph-structured information tasks related to privacy protection measures applied therein.\n\nCode Availability: All components necessary implementing PRIVGNN may be found online accessible here: <https://github.com/iyempissy/privGnn>.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Invariant Feature Coding using Tensor Product Representation",
        "abstract": "In this study, a novel feature coding method that exploits invariance for transformations represented by a finite group of orthogonal matrices is proposed. We prove that the group-invariant feature vector contains sufficient discriminative information when learning a linear classifier using convex loss minimization. Based on this result, a novel feature model that explicitly considers group action is proposed for principal component analysis and k-means clustering, which are commonly used in most feature coding methods, and global feature functions. Although the global feature functions are in general complex nonlinear functions, the group action on this space can be easily calculated by constructing these functions as tensor-product representations of basic representations, resulting in an explicit form of invariant feature functions. The effectiveness of our method is demonstrated on several image datasets. ",
        "authors": "Y. Mukuta, T. Harada",
        "keywords": [
            "group-invariant feature vectors",
            "convex loss minimization",
            "tensor-product representations"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=uv32JOdQuh",
        "pdf_src": "https://api2.openreview.net/pdf/ec217f62794fbbb1f22b2d25140bf9da8f75f9c2.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper addresses the problem of feature coding with invariance to transformations represented by a finite group of orthogonal matrices.\nResearch Question: How does one design a feature coding method that captures sufficient discriminative information while being invariant under such transformations?\nMethods: The authors propose a new feature coding approach based on group-invariant feature vectors derived from a linear classifier trained with convex loss minimization techniques considering the group structure directly during feature extraction processes like PCA or K-Means clustering.\n\nMain Contributions:\n1. They introduce a concept where features become invariant after transformation through a finite group of orthogonal matrices if they lie within the kernel of certain operators associated with the group actions; \n2. Prove theoretically how their proposed group-invariant feature vectors contain enough discriminative power;\n3. Develop a practical framework integrating group theory into existing dimensionality reduction algorithms like PCA and clustering via tensor product representations allowing calculation of invariant feature functions without requiring any additional computational complexity beyond standard operations;\n4. Demonstrate experimentally across various datasets showing improved performance over non-group-aware approaches due to enhanced robustness against geometric distortions introduced",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "An Explicit Expansion of the Kullback-Leibler Divergence along its Fisher-Rao Gradient Flow",
        "abstract": "Let $V_* : \\mathbb{R}^d \\to \\mathbb{R}$ be some (possibly non-convex) potential function, and consider the probability measure $\\pi \\propto e^{-V_*}$. When $\\pi$ exhibits multiple modes, it is known that sampling techniques based on Wasserstein gradient flows of the Kullback-Leibler (KL) divergence (e.g. Langevin Monte Carlo) suffer poorly in the rate of convergence, where the dynamics are unable to easily traverse between modes. In stark contrast, the work of Lu et al. (2019; 2022) has shown that the gradient flow of the KL with respect to the Fisher-Rao (FR) geometry exhibits a convergence rate to $\\pi$ is that \\textit{independent} of the potential function. In this short note, we complement these existing results in the literature by providing an explicit expansion of $\\text{KL}(\\rho_t^{\\text{FR}}\\|\\pi)$ in terms of $e^{-t}$, where $(\\rho_t^{\\text{FR}})_{t\\geq 0}$ is the FR gradient flow of the KL divergence. In turn, we are able to provide a clean asymptotic convergence rate, where the burn-in time is guaranteed to be finite. Our proof is based on observing a similarity between FR gradient flows and simulated annealing with linear scaling, and facts about cumulant generating functions. We conclude with simple synthetic experiments that demonstrate our theoretical findings are indeed tight. Based on our numerical findings, we conjecture that the asymptotic rates of convergence for Wasserstein-Fisher-Rao gradient flows are possibly related to this expansion in some cases.\n",
        "authors": "C. Domingo-enrich, A. Pooladian",
        "keywords": [
            "KL_divergence",
            "Fisher-Rao_geometry",
            "asymptotic_convergence_rate"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=9pWjgQ3y85",
        "pdf_src": "https://api2.openreview.net/pdf/e319cdbedf82a91e1306039a09951a7073561623.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses issues arising from sampling algorithms like Langevin Monte Carlo when dealing with probability measures $\\pi$ having multiple modes due to their poor performance at traversing different modes.\n\nResearch Problem: How can one improve the convergence rate of sampling algorithms such as Langevin Monte Carlo which struggle across multiple modes?\n\nMethod: The authors focus on using the Fisher-Rao (FR) geometry instead of the standard Kullback-Leibler (KL) divergence approach proposed previously because the latter's gradient flow does not guarantee independence of convergence regardless of the potential function used ($V_*$). They derive an explicit expansion of the KL divergence w.r.t. the FR gradient flow in terms of $e^{-t}$ over time.\n\nMain Contributions:\n1. An explicit expansion formula for $\\text{KL}(\\rho_t^{\\text{FR}}\\|\\pi)$ showing how it scales with $e^{-t}$ during the FR gradient flow evolution.\n2. A new convergence rate analysis indicating that the burn-in period required before reaching stationarity is finite under certain conditions – unlike previous methods relying solely on the KL divergence whose convergence was dependent upon the specific form of the potential function.\n3. Insights into possible connections or similarities regarding asymptotic convergence rates among various gradient flows including those derived via Wasserstein-Fisher-Rao frameworks through observations made while comparing them against simulated annealing processes involving linear scaling properties along with knowledge gained concerning cumulant generating functions within statistical mechanics contexts.\n4. Synthetic experimental evidence supporting theory presented demonstrating tightness around predicted behavior observed numerically suggesting further investigation may reveal additional insights linking these approaches together more closely than currently understood",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Bandwidth Enables Generalization in Quantum Kernel Models",
        "abstract": "Quantum computers are known to provide speedups over classical state-of-the-art machine learning methods in some specialized settings. For example, quantum kernel methods have been shown to provide an exponential speedup on a learning version of the discrete logarithm problem. Understanding the generalization of quantum models is essential to realizing similar speedups on problems of practical interest. Recent results demonstrate that generalization is hindered by the exponential size of the quantum feature space. Although these results suggest that quantum models cannot generalize when the number of qubits is large, in this paper we show that these results rely on overly restrictive assumptions. We consider a wider class of models by varying a hyperparameter that we call quantum kernel bandwidth. We analyze the large-qubit limit and provide explicit formulas for the generalization of a quantum model that can be solved in closed form. Specifically, we show that changing the value of the bandwidth can take a model from provably not being able to generalize to any target function to good generalization for well-aligned targets. Our analysis shows how the bandwidth controls the spectrum of the kernel integral operator and thereby the inductive bias of the model. We demonstrate empirically that our theory correctly predicts how varying the bandwidth affects generalization of quantum models on challenging datasets, including those far outside our theoretical assumptions. We discuss the implications of our results for quantum advantage in machine learning.",
        "authors": "A. Canatar, E. Peters, C. Pehlevan, et.al",
        "keywords": [
            "quantum computing",
            "generalization",
            "kernel methods"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=A1N2qp4yAq",
        "pdf_src": "https://api2.openreview.net/pdf/c573b16d5d2f420cba56262c173be0f7aff3fe35.pdf",
        "Code_src": "",
        "Introduction": "Background: Quantum computing has demonstrated potential advantages compared to classical algorithms under certain conditions; however, it remains unclear whether such benefits extend beyond specific domains.\n\nResearch Question: This study investigates if quantum computational models possess limitations due to their exponentially growing feature spaces which could prevent them from generalizing effectively across various tasks.\n \nMethodology: The researchers broaden existing studies' scope with respect to quantum models using a novel approach involving adjustable parameters like \"quantum kernel bandwidth.\" They also explore the behavior within high-qubit scenarios through analytical expressions derived without imposing stringent constraints found in prior works.\n\nMain Contributions:\n1. **Relaxed Assumptions:** By relaxing strict assumptions about quantum computation's scalability limits, they open up new avenues into understanding its broader applicability than previously thought possible based solely on the number of qubits used as a measure of complexity.\n2. **Bandwidth Control:** The introduction of a controllable parameter - the quantum kernel bandwidth – allows for fine-tuning between poor or excellent performance depending upon alignment relative to particular functions one wishes to approximate accurately via quantum means rather than just having arbitrary scaling properties related only directly back towards classical counterparts’ capabilities alone).\n3. **Empirical Validation:** Their findings were validated experimentally against real-world datasets demonstrating consistency even where initial theoretical predictions did not anticipate outcomes due mainly because they had considered too narrow ranges initially).",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Sequential Query Encoding for Complex Query Answering on Knowledge Graphs",
        "abstract": "Complex Query Answering (CQA) is an important and fundamental task for knowledge graph (KG) reasoning.\nQuery encoding (QE) is proposed as a fast and robust solution to CQA. \nIn the encoding process, most existing QE methods first parse the logical query into an executable computational direct-acyclic graph (DAG), then use neural networks to parameterize the operators,\nand finally, recursively execute these neuralized operators. \nHowever, the parameterization-and-execution paradigm may be potentially over-complicated, as it can be structurally simplified by a single neural network encoder.\nMeanwhile, sequence encoders, like LSTM and Transformer, proved to be effective for encoding semantic graphs in related tasks.\nMotivated by this, we propose sequential query encoding (SQE) as an alternative to encode queries for CQA. \nInstead of parameterizing and executing the computational graph, SQE first uses a search-based algorithm to linearize the computational graph to a sequence of tokens and then uses a sequence encoder to compute its vector representation.\nThen this vector representation is used as a query embedding to retrieve answers from the embedding space according to similarity scores.\nDespite its simplicity, SQE demonstrates state-of-the-art neural query encoding performance on FB15k, FB15k-237, and NELL on an extended benchmark including twenty-nine types of in-distribution queries. \nFurther experiment shows that SQE also demonstrates comparable knowledge inference capability on out-of-distribution queries, whose query types are not observed during the training process. ",
        "authors": "J. Bai, T. Zheng, Y. Song",
        "keywords": [
            "SQE",
            "Complex Query Answering",
            "Knowledge Graph Reasoning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ERqGqZzSu5",
        "pdf_src": "https://api2.openreview.net/pdf/625b6a02e2bc66d05733c18941b5da49e58147c5.pdf",
        "Code_src": "",
        "Introduction": "Background: Complex Query Answering (CQA) involves answering complex questions about entities within Knowledge Graphs (KG). Query Encoding (QE) has been developed recently with promising results.\n\nResearch Problem: Existing QE methods often involve parsing logical queries before using neural networks to parameterize operators which they then recursively execute - leading to potential complexity issues due to their structure simplification through a single neural network encoder.\n\nMethod: We introduce Sequential Query Encoding (SQE) – an alternative approach where instead of parameterizing and executing computational graphs directly, we utilize a search-based algorithm to linearize them down into sequences of tokens followed by computing their vector representations via sequence encoders such as LSTM or Transformer models; these vectors serve as embeddings when retrieving answers based on similarity metrics against KG embeddings.\n\nMain Contributions:\n1. Propose Sequential Query Encoding (SQE) method providing improved performance compared traditional approaches across benchmarks;\n2. Demonstrate effectiveness beyond distributional limits showing comparable capabilities even without observing certain query types during training phase",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Training with Mixed-Precision Floating-Point Assignments",
        "abstract": "When training deep neural networks, keeping all tensors in high precision (e.g., 32-bit or even 16-bit floats) is often wasteful. However, keeping all tensors in low precision (e.g., 8-bit floats) can lead to unacceptable accuracy loss. Hence, it is important to use a precision assignment—a mapping from all tensors (arising in training) to precision levels (high or low)—that keeps most of the tensors in low precision and leads to sufficiently accurate models. We provide a technique that explores this memory-accuracy tradeoff by generating precision assignments for convolutional neural networks that (i) use less memory and (ii) lead to more accurate convolutional networks at the same time, compared to the precision assignments considered by prior work in low-precision floating-point training. We evaluate our technique on image classiﬁcation tasks by training convolutional networks on CIFAR-10, CIFAR-100, and ImageNet. Our method typically provides > 2× memory reduction over a baseline precision assignment while preserving training accuracy, and gives further reductions by trading off accuracy. Compared to other baselines which sometimes cause training to diverge, our method provides similar or better memory reduction while avoiding divergence.\n",
        "authors": "W. Lee, R. Sharma, A. Aiken",
        "keywords": [
            "memory-assignment",
            "low-precision",
            "accuracy-preserving"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ZoXi7n54OB",
        "pdf_src": "https://api2.openreview.net/pdf/c7a520a77ec25b007c788da3bb8ac26dc8e2e6a7.pdf",
        "Code_src": "",
        "Introduction": "Background: Training deep neural networks requires significant computational resources due to the large number of floating-point operations involved when using high precision data types such as 32-bit or 16-bit floats. Reducing the precision level used during training could potentially save memory usage without sacrificing too much model accuracy.\n\nResearch Problem: The challenge lies in finding an optimal precision assignment strategy - essentially determining how to map different tensors within a network between high and low precision formats – so that we minimize memory consumption but still achieve acceptable accuracy performance.\n\nMethods: To tackle this problem, researchers have developed techniques like quantization algorithms where they gradually reduce the bit-widths assigned to weights/tensors throughout the training process based on their importance/activation patterns; however these methods may not always preserve sufficient accuracy especially if applied uniformly across all tensors.\n\nMain Contributions:\n1. This paper introduces a novel approach focusing on exploring the trade-off relationship between memory usage and accuracy preservation through dynamic precision assignment strategies tailored specifically towards Convolutional Neural Networks (CNNs).\n2. It proposes a generative framework capable of automatically creating precision assignments optimized both for reduced memory footprint AND maintaining comparable or improved accuracy relative to existing approaches \n3. Experimental validation performed with CNNs trained on datasets including CIFAR-10, CIFAR-100 & ImageNet demonstrates substantial improvements in terms of memory savings (>2x), whilst also showing no degradation nor divergence issues commonly encountered w.r.t alternative baseline methods",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "TransFool: An Adversarial Attack against Neural Machine Translation Models",
        "abstract": "Deep neural networks have been shown to be vulnerable to small perturbations of their inputs, known as adversarial attacks. In this paper, we investigate the vulnerability of Neural Machine Translation (NMT) models to adversarial attacks and propose a new attack algorithm called TransFool. To fool NMT models, TransFool builds on a multi-term optimization problem and a gradient projection step. By integrating the embedding representation of a language model, we generate fluent adversarial examples in the source language that maintain a high level of semantic similarity with the clean samples. Experimental results demonstrate that, for different translation tasks and NMT architectures, our white-box attack can severely degrade the translation quality while the semantic similarity between the original and the adversarial sentences stays high. Moreover, we show that TransFool is transferable to unknown target models. Finally, based on automatic and human evaluations, TransFool leads to improvement in terms of success rate, semantic similarity, and fluency compared to the existing attacks both in white-box and black-box settings. Thus, TransFool permits us to better characterize the vulnerability of NMT models and outlines the necessity to design strong defense mechanisms and more robust NMT systems for real-life applications.",
        "authors": "S. Sadrizadeh, L. Dolamic, P. Frossard",
        "keywords": [
            "vulnerability",
            "Neural Machine Translation (NMT)",
            "adversarial attacks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=sFk3aBNb81",
        "pdf_src": "https://api2.openreview.net/pdf/7b27a664dd3195db6286797fbfd33d3abc780511.pdf",
        "Code_src": "",
        "Introduction": "Background: The background of this research lies within the field of machine learning security where deep neural networks are increasingly being used across various domains including natural language processing through Neural Machine Translation (NMT). However, these networks often suffer from adversarial attacks which involve introducing subtle changes (\"perturbations\") into input data without altering its meaning significantly enough not to be noticed by humans.\n\nResearch Question: This study aims at addressing whether or not NMT models deployed widely today could also fall victim to such attacks when translating text?\n\nMethodology: The researchers developed an innovative attack method named \"TransFool\" designed specifically against NMT systems using techniques like multi-term optimization problems combined with gradient projection steps during training iterations.\nThey further integrated the embeddings generated via a pre-trained language model ensuring that the resulting adversarial examples would still convey similar meanings but differ just enough to confuse the NMT system upon translation.\n\nMain Contributions:\n1. Vulnerability Assessment - Demonstrated how adversaries might exploit vulnerabilities present in current state-of-the-art NMT systems leading to degradation in translation performance despite maintaining semantic coherence after translation.\n2. Novel Attack Algorithm - Introduced TransFool; it's capable of generating highly semantically coherent yet adversarially modified texts intended solely to deceive NMTs under specific conditions related to architecture and task domain.\n3. Transferability Study - Showed that even though trained initially only one type of NMT system, once adapted slightly they were able to successfully apply TransFool to other types/models indicating potential cross-architecture threats.\n4. Evaluation Metrics Improvement - Compared favorably over previous methods regarding effectiveness metrics measured automatically along with subjective assessments conducted by humans suggesting higher reliability & practicality than prior approaches regardless if evaluated in full visibility (white box) versus partial knowledge scenarios (black box).\n\nConclusion: Overall findings suggest there exists significant room for concern about the security posture surrounding modern-day automated translations due to susceptibility towards carefully crafted adversarial manipulations highlighted here",
        "Topic": "Machine Learning"
    },
    {
        "title": "Unsupervised Discovery and Composition of Object Light Fields",
        "abstract": "Neural scene representations, both continuous and discrete, have recently emerged as a powerful new paradigm for 3D scene understanding. Recent efforts have tackled unsupervised discovery of object-centric neural scene representations. However, the high cost of ray-marching, exacerbated by the fact that each object representation has to be ray-marched separately, leads to insufficiently sampled radiance fields and thus, noisy renderings, poor framerates, and high memory and time complexity during training and rendering. Here, we propose to represent objects in an object-centric, compositional scene representation as light fields. We propose a novel light field compositor module that enables reconstructing the global light field from a set of object-centric light fields. Dubbed Compositional Object Light Fields (COLF), our method enables unsupervised learning of object-centric neural scene representations, state-of-the-art reconstruction and novel view synthesis performance on standard datasets, and rendering and training speeds at orders of magnitude faster than existing 3D approaches.",
        "authors": "C. O. Smith, H. Yu, S. Zakharov, et.al",
        "keywords": [
            "light fields",
            "compositional scene representation",
            "neural scene representations"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=B7PFZtm8DA",
        "pdf_src": "https://api2.openreview.net/pdf/b5a103a895b042bb53a3f9bed9954d89b842a93e.pdf",
        "Code_src": "",
        "Introduction": "Background: Neural scene representations are becoming increasingly popular due to their ability to understand three-dimensional scenes effectively.\n\nResearch Problem: The current methods used for discovering object-centric neural scene representations suffer from several issues such as noise, low frame rates, and high memory and time complexity because they require separate ray marching for each object representation which increases the computational load significantly.\n\nMethod: In this paper, instead of using traditional ray marching techniques or voxel-based representations like point clouds etc., authors introduce a novel approach called \"Compositional Object Light Fields\" (COLFs). They use light fields rather than rays to represent objects within a scene compositionally; allowing them to efficiently reconstruct global light fields without needing multiple passes over individual objects' surfaces - reducing computation costs drastically compared with other methods.\n \nMain Contributions:\n1. Introduced COLFs – A novel way representing objects in an object-centric compositional scene representation through light fields;\n2. Developed a novel light field compositor module enabling reconstruction of global light fields from these object-centric light fields;\n3. Achieved state-of-the-art results across various benchmarks including reconstruction quality & novel view synthesis capabilities while also demonstrating orders of magnitude improvements regarding rendering speed when compared against conventional 3D scene representation methods",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Privacy-Preserving Energy-Based Generative Models for Marginal Distribution Protection",
        "abstract": "We consider learning generative models for sensitive financial and healthcare data. While previous work incorporates Differential Privacy (DP) into GAN training to protect the privacy of individual training instances, we consider a different privacy context where the primary objective is protecting the privacy of sensitive marginal distributions of the true generative process. We propose and motivate a new notion of privacy: \\emph{$\\alpha$-Level Marginal Distribution Privacy} ($\\alpha$-LMDP), which provides a statistical guarantee that the sensitive generative marginal distributions are different from the observed real data. We then propose \\emph{Privacy-Preserving Energy Models (PPEMs)}, a novel energy-based generative model formulation where the representations for these attributes are isolated from other attributes. This structured formulation motivates a learning procedure where a penalty based on a statistical goodness of fit test, the \\emph{Kernel Stein Discrepancy}, can be applied to only the attributes requiring privacy so that $\\alpha$-LMDP may be satisfied without affecting the other attributes. We evaluate this approach using financial and healthcare datasets and demonstrate that the resulting learnt generative models produce high fidelity synthetic data while preserving privacy. We also show that PPEMs can incorporate both $\\alpha$-LMDP \\emph{and} DP in contexts where both forms of privacy are required.",
        "authors": "R. E. Tillman, T. Balch, M. Veloso",
        "keywords": [
            "data privacy",
            "Generative Adversarial Networks (GANs)",
            "differential privacy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=vTsfup5ll6",
        "pdf_src": "https://api2.openreview.net/pdf/4f41b373a71428e5432c13680aedbee1891315a6.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the challenge of generating synthetic data with privacy guarantees specifically designed for sensitive domains like finance and healthcare.\n\nResearch Problem: Previous approaches have focused on incorporating Differential Privacy (DP) during Generative Adversarial Network (GAN) training as an additional layer over the generated samples or by perturbing each sample individually before it's fed through the network; however, they do not provide strong guarantees about the distributional properties such as those pertaining to marginalized distributions within the dataset.\n \nMethodology: To address this issue, authors introduce a novel concept called $\\alpha$-Level Marginal Distribution Privacy ($\\alpha$-LMDP). It ensures that the learned generative model does not reveal any information beyond what would typically happen if one were observing random draws from the population being modeled. They further develop Privacy-Preserving Energy Models (PPEMs), which separate private attributes' representations from others ensuring no leakage between them due to their distinct encodings. A key innovation here involves leveraging the Kernel Stein Discrepancy—a measure comparing two probability distributions—to impose penalties solely upon attributes needing privacy protection rather than altering all features equally—thereby satisfying $\\alpha$-LMDP constraints selectively across various attributes.\n\nMain Contributions:\n1. Propose $\\alpha$-LMDP providing stronger privacy guarantees compared to existing methods focusing primarily on DP at instance level;\n2. Introduce PPEMs—an architecture that allows for generation tasks involving multiple types of data including some potentially sensitive ones under strict privacy conditions;\n3. Demonstrate how $\\alpha$-LMDP could coexist alongside DP when necessary via PPEMs framework allowing researchers/users flexibility depending on specific requirements regarding confidentiality preservation along with data utility retention during synthesis processes related to finance/healthcare sectors.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "A Kernel Perspective on Behavioural Metrics for Markov Decision Processes",
        "abstract": "We present a novel perspective on behavioural metrics for Markov decision processes via the use of positive definite kernels. We define a new metric under this lens that is provably equivalent to the recently introduced MICo distance (Castro et al., 2021). The kernel perspective enables us to provide new theoretical results, including value-function bounds and low-distortion finite-dimensional Euclidean embeddings, which are crucial when using behavioural metrics for reinforcement learning representations. We complement our theory with strong empirical results that demonstrate the effectiveness of these methods in practice.",
        "authors": "P. S. Castro, T. Kastner, P. Panangaden, et.al",
        "keywords": [
            "kernel",
            "Markov decision process",
            "behavioural metrics"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=nHfPXl1ly7",
        "pdf_src": "https://api2.openreview.net/pdf/bd29c539d2a379cef0b7afefddd67067d148feb7.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper focuses on behavioral metrics for Markov Decision Processes (MDPs), particularly how they can be defined through the use of positive definite kernels.\n\nResearch Problem: The main research problem addressed by the authors involves defining a new metric within the kernel framework based on recent developments such as the MICo distance proposed by Castro et al. (2021).\n\nMethods: To tackle this issue, the researchers utilize a kernel-based approach rather than traditional vector spaces or matrices commonly used before. They introduce an innovative metric derived from positive definite kernels whose equivalence has been proven to hold true relative to the MICo distance previously established.\n \nMain Contributions: The primary contributions include:\n- A novel definition of a metric specifically designed around the concept of positive definite kernels;\n- Proven equivalency between their newly developed metric and the MICo distance;\n- New theoretical insights into value function bounds; and\n- Demonstrated practical efficacy demonstrated empirically across various examples related to reinforcement learning representation tasks where behavioral metrics play critical roles",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Pareto Optimization for Active Learning under Out-of-Distribution Data Scenarios",
        "abstract": "Pool-based Active Learning (AL) has proven successful in minimizing labeling costs by sequentially selecting the most informative unlabeled data from large pool and querying their labels from an oracle or annotators.  However, existing AL sampling schemes may not perform well in out-of-distribution (OOD) data scenarios, where the unlabeled data pool contains samples that do not belong to the pre-defined categories of the target task. Achieving strong AL performance under OOD data scenarios presents a challenge due to the inherent conflict between AL sampling strategies and OOD data detection. For instance, both more informative in-distribution (ID) data and OOD data in an unlabeled data pool would be assigned high informativeness scores (e.g., high entropy) during AL processes. To address this dilemma, we propose a Monte-Carlo Pareto Optimization for Active Learning (POAL) sampling scheme, which selects optimal subsets of unlabeled samples with fixed batch size from the unlabeled data pool. We formulate the AL sampling task as a multi-objective optimization problem and employ Pareto optimization based on two conflicting objectives: (1) the conventional AL sampling scheme (e.g., maximum entropy) and (2) the confidence of excluding OOD data samples. Experimental results demonstrate the effectiveness of our POAL approach on classical Machine Learning (ML) and Deep Learning (DL) tasks.",
        "authors": "X. Zhan, Z. Dai, Q. Wang, et.al",
        "keywords": [
            "Monte-Carlo",
            "Pareto Optimization",
            "Pool-based Active Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=dXnccpSSYF",
        "pdf_src": "https://api2.openreview.net/pdf/fa1024d2fb545b41163e53e6cf1d3a9de3994961.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper addresses Pool-based Active Learning (AL), focusing on reducing labeling costs through sequential selection of highly informative unlabeled data.\n\nResearch Problem:\nExisting AL methods are challenged when dealing with Out-of-Distribution (OOD) data since they often assign similar informativeness scores regardless of whether the data is within or outside the defined category boundaries leading to poor performance in such settings.\n\nMethodology:\nTo tackle this issue, the authors introduce a novel Monte Carlo Pareto Optimization for Active Learning (POAL). This method involves formulating the active learning process into a multi-objective optimization framework using Pareto optimization techniques considering contradictory goals:\n\n- Conventional AL Sampling Scheme - Maximizing entropy/informativeness score.\n- Excluding OOD Data Confidence - Minimizing the probability of including any OOD sample among selected ones.\n\nMain Contributions:\nThe main contribution lies in developing a new sampling strategy called POAL designed specifically addressing the challenges posed by OOD data while maintaining efficiency even without prior knowledge about how many OOD examples exist at each iteration step throughout training time.\n\nExperimental Results:\nThe proposed POAL algorithm was tested across various machine learning and deep learning datasets showing improved performance compared traditional approaches especially against those encountered with potentially harmful OOD inputs thus providing robustness towards unseen classes beyond initial training distribution bounds",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Unsupervised Domain Adaptation via Minimized Joint Error",
        "abstract": "Unsupervised domain adaptation transfers knowledge from a fully labeled source domain to a different target domain, where no labeled data are available. Some researchers have proposed upper bounds for the target error when transferring knowledge. For example, Ben-David et al. (2010) established a theory based on minimizing the source error and distance between marginal distributions simultaneously. However, in most research, the joint error is ignored because of its intractability. In this research, we argue that joint errors are essential for domain adaptation problems, particularly when the domain gap is large. To address this problem, we propose a novel objective related to the upper bound of the joint error. Moreover, we adopt a source/pseudo-target label-induced hypothesis space that can reduce the search space to further tighten this bound. To measure the dissimilarity between hypotheses, we define a novel cross-margin discrepancy to alleviate instability during adversarial learning. In addition, we present extensive empirical evidence showing that the proposed method boosts the performance of image classification accuracy on standard domain adaptation benchmarks.",
        "authors": "D. Zhang, T. Westfechtel, T. Harada",
        "keywords": [
            "domain adaptation",
            "joint error",
            "cross-margin discrepancy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=kiPsMct7vL",
        "pdf_src": "https://api2.openreview.net/pdf/eb9fd8b5b3d86480426992d482b72166c407921b.pdf",
        "Code_src": "",
        "Introduction": "Background: Unsupervised domain adaptation aims at transferring knowledge learned from a fully labeled source domain to a target domain with only unlabeled data.\n\nResearch Problem: Previous studies mainly focused on minimizing the source error or the distance between marginal distributions without considering the joint error which may be critical especially under significant domain gaps.\n\nMethods: This paper proposes an innovative objective function regarding the upper bound of the joint error as well as adopts a source/pseudo-label induced hypothesis space reducing computational complexity by narrowing down the feasible solutions within it. Additionally, they introduce a new metric called cross-margin discrepancy designed specifically against potential instabilities encountered throughout adversarial training processes.\n \nMain Contributions: The main contributions lie not just in theoretically establishing importance but also practically implementing methods addressing these overlooked aspects leading towards improved performances demonstrated through substantial experimental validation across various benchmark datasets used commonly in evaluating such transfer learning scenarios involving images",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "LEAD: Min-Max Optimization from a Physical Perspective",
        "abstract": "Adversarial formulations such as generative adversarial networks (GANs) have rekindled interest in two-player min-max games. A central obstacle in the optimization of such games is the rotational dynamics that hinder their convergence. In this paper, we show that game optimization shares dynamic properties with particle systems subject to multiple forces, and one can leverage tools from physics to improve optimization dynamics. Inspired by the physical framework, we propose LEAD, an optimizer for min-max games. Next, using Lyapunov stability theory and spectral analysis, we study LEAD’s convergence properties in continuous and discrete time settings for a class of quadratic min-max games to demonstrate linear convergence to the Nash equilibrium. Finally, we empirically evaluate our method on synthetic setups and CIFAR-10 image generation to demonstrate improvements in GAN training.",
        "authors": "R. A. Hemmat, A. Mitra, G. Lajoie, et.al",
        "keywords": [
            "GANs",
            "Min-Max Games",
            "Convergence"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=vXSsTYs6ZB",
        "pdf_src": "https://api2.openreview.net/pdf/8b3b56bcd1bf71f08305431c4b9f0d914c19d389.pdf",
        "Code_src": "",
        "Introduction": "Background: The use of adversarial formulations like Generative Adversarial Networks (GANs) has reignited interest in two-player min-max games due to its potential applications across various fields including machine learning.\n\nResearch Problem: One major challenge faced while optimizing these games involves rotational dynamics which impede convergence leading to suboptimal solutions.\n \nMethodology: This research draws parallels between the dynamics observed during game optimization processes involving rotational motion akin to those found within particle systems subjected to several forces suggesting opportunities where insights gained could be applied towards improving optimization algorithms specifically designed for min-max games. \n\nMain Contributions: We introduce \"LEAD\" - an innovative optimizer tailored explicitly toward solving problems related to min-max games inspired directly from principles derived from physics particularly focusing upon leveraging concepts pertinent when considering interactions amongst particles under influence exerted through different sources simultaneously. Furthermore employing Lyapunov Stability Theory along with Spectral Analysis techniques allows us to analyze convergence behavior exhibited over both continuous-time scenarios alongside discrete counterparts pertaining specific instances belonging category Quadratic Min-Max Games thereby establishing linear convergence guarantees approaching Nash Equilibrium point(s). Lastly empirical validation experiments conducted utilizing synthetic datasets coupled with benchmarking against CIFAR-10 dataset showcase tangible enhancements achieved via usage proposed approach compared traditional approaches employed thus far enhancing performance metrics relevantly associated with successful operation trained Generative Adversarial Network architectures",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Empirical Study on Optimizer Selection for Out-of-Distribution Generalization",
        "abstract": "Modern deep learning systems do not generalize well when the test data distribution is slightly different to the training data distribution. While much promising work has been accomplished to address this fragility, a systematic study of the role of optimizers and their out-of-distribution generalization performance has not been undertaken. In this study, we examine the performance of popular first-order optimizers for different classes of distributional shift under empirical risk minimization and invariant risk minimization. We address this question for image and text classification using DomainBed, WILDS, and Backgrounds Challenge as testbeds for studying different types of shifts---namely correlation and diversity shift. We search over a wide range of hyperparameters and examine classification accuracy (in-distribution and out-of-distribution) for over 20,000 models. We arrive at the following findings, which we expect to be helpful for practitioners: i) adaptive optimizers (e.g., Adam) perform worse than non-adaptive optimizers (e.g., SGD, momentum SGD) on out-of-distribution performance. In particular, even though there is no significant difference in in-distribution performance, we show a measurable difference in out-of-distribution performance. ii) in-distribution performance and out-of-distribution performance exhibit three types of behavior depending on the dataset---linear returns, increasing returns, and diminishing returns. For example, in the training of natural language data using Adam, fine-tuning the performance of in-distribution performance does not significantly contribute to the out-of-distribution generalization performance.",
        "authors": "H. Naganuma, K. Ahuja, S. Takagi, et.al",
        "keywords": [
            "distributional shift",
            "optimizer performance",
            "generalization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ipe0IMglFF",
        "pdf_src": "https://api2.openreview.net/pdf/c49773ef5b737a3772fbe9a30616be8b930a60be.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses an issue with modern deep learning systems that they often fail to generalize effectively if the test data distribution differs from the training data distribution.\n\nResearch Question: How do different optimizers behave regarding out-of-distribution generalization?\n\nMethods: The researchers empirically investigate the performance of various first-order optimizers across two frameworks - empirical risk minimization and invariant risk minimization – by testing them against domain shifts such as correlation and diversity shift through datasets like DomainBed, WILDS, and Backgrounds Challenge.\n\nMain Contributions:\n1. They find that adaptive optimizers generally have poorer out-of-distribution performance compared to non-adaptive ones.\n2. Depending on the dataset used, both in-distribution and out-of-distribution performances can either improve linearly or may increase initially but then plateau (\"diminishing returns\"), indicating variability among optimization strategies' effectiveness based on specific tasks/data characteristics.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Predicting Out-of-Domain Generalization with Neighborhood Invariance",
        "abstract": "Developing and deploying machine learning models safely depends on the ability to char- acterize and compare their abilities to generalize to new environments. Although recent work has proposed a variety of methods that can directly predict or theoretically bound the generalization capacity of a model, they rely on strong assumptions such as matching train/test distributions and access to model gradients. In order to characterize generalization when these assumptions are not satisfied, we propose neighborhood invariance, a measure of a classifier’s output invariance in a local transformation neighborhood. Specifically, we sample a set of transformations and given an input test point, calculate the invariance as the largest fraction of transformed points classified into the same class. Crucially, our measure is simple to calculate, does not depend on the test point’s true label, makes no assumptions about the data distribution or model, and can be applied even in out-of-domain (OOD) settings where existing methods cannot, requiring only selecting a set of appropriate data transformations. In experiments on robustness benchmarks in image classification, sentiment analysis, and natural language inference, we demonstrate a strong and robust correlation between our neighborhood invariance measure and actual OOD generalization on over 4,600 models evaluated on over 100 train/test domain pairs.\n",
        "authors": "N. H. Ng, N. Hulkund, K. Cho, et.al",
        "keywords": [
            "neighborhood invariance",
            "generalization capacity",
            "OOD generalization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=jYkWdJzTwn",
        "pdf_src": "https://api2.openreview.net/pdf/361154c43362176c5189eed6ef968dda5f55ee40.pdf",
        "Code_src": "",
        "Introduction": "Background: The development and deployment of safe machine learning models relies heavily on understanding how well those models will perform outside training conditions.\n\nResearch Problem: Existing works have focused on predicting or bounding the generalization capability based on certain assumptions like similar train-test distributions but fail under different scenarios due to lack of accessibility for model gradients etc.\n\nMethodology: To address this issue without relying on strict assumptions regarding the dataset or model complexity, authors introduce \"neighborhood invariance\" which measures the stability of classifier outputs against small perturbations around each example within its decision boundary area.\n\nMain Contributions:\n1. A novel metric called Neighborhood Invariance introduced - it's easy to compute doesn't require knowledge of ground truth labels nor make any assumptions concerning either data distribution or architecture complexity; \n2. Demonstrated effectiveness across various domains including image classification, sentiment analysis & natural language inference;\n3. Proved high correlation with real-world Out-Of-Domain performance metrics despite being applicable regardless if datasets are related or unrelated compared traditional approaches needing more stringent preconditions before use.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "The Eigenlearning Framework: A Conservation Law Perspective on Kernel Ridge Regression and Wide Neural Networks",
        "abstract": "We derive simple closed-form estimates for the test risk and other generalization metrics of kernel ridge regression (KRR). Relative to prior work, our derivations are greatly simplified and our final expressions are more readily interpreted. In particular, we show that KRR can be interpreted as an explicit competition among kernel eigenmodes for a fixed supply of a quantity we term \"learnability.'' These improvements are enabled by a sharp conservation law which limits the ability of KRR to learn any orthonormal basis of functions. Test risk and other objects of interest are expressed transparently in terms of our conserved quantity evaluated in the kernel eigenbasis. We use our improved framework to:\n   i) provide a theoretical explanation for the \"deep bootstrap\" of Nakkiran et al (2020),\n   ii) generalize a previous result regarding the hardness of the classic parity problem,\n   iii) fashion a theoretical tool for the study of adversarial robustness, and\n   iv) draw a tight analogy between KRR and a well-studied system in statistical physics.",
        "authors": "J. B. Simon, M. Dickens, D. Karkada, et.al",
        "keywords": [
            "kernel ridge regression",
            "learnability",
            "conservational law"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=FDbQGCAViI",
        "pdf_src": "https://api2.openreview.net/pdf/84387f5b285197f40a2f209738e205669eb0c30b.pdf",
        "Code_src": "",
        "Introduction": "Background: Kernel Ridge Regression (KRR), also known as Support Vector Regression with Gaussian kernel, is widely used in machine learning due to its good performance on non-linear data. However, understanding how it works remains challenging because existing explanations focus mainly on empirical observations rather than rigorous mathematical analysis.\n\nResearch Problem: This paper aims at providing a deeper insight into the working mechanism of KRR through rigorous mathematical derivation.\nSpecifically, they want to understand why certain features are learned while others aren't when using KRR?\n\nMethod: The authors start from the perspective of optimization theory first, then introduce their main contribution - a new concept called \"learnability\". They prove two important properties about this quantity:\n\n1) It's conserved during training process; \n2) Its value determines whether some eigenvectors of the Gram matrix corresponding to large eigenvalues should contribute to the solution or not.\n\nBased on these findings, they obtain simple closed-form formulas for estimating test error rate under different regularization strengths etc., which makes them easier to interpret compared with those obtained previously.\n\nMain Contributions: Their contributions include four aspects:\n\n1) A novel interpretation showing how KRR selects relevant features based on competitive behavior among kernel modes;\n2) An extension explaining why hard examples make classification tasks difficult via a connection with classical parity problems;\n3) Development of tools useful analyzing adversarial robustness issues encountered commonly nowadays;\n4) Drawing parallels between KRR models & systems studied extensively within statistical mechanics literature",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "On Averaging ROC Curves",
        "abstract": "Receiver operating characteristic (ROC) curves are a popular method of summarising the performance of classifiers. The ROC curve describes the separability of the distributions of predictions from a two-class classifier. There are a variety of situations in which an analyst seeks to aggregate multiple ROC curves into a single representative example. A number of methods of doing so are available; however, there is a degree of subtlety that is often overlooked when selecting the appropriate one. An important component of this relates to the interpretation of the decision process for which the classifier will be used. This paper summarises a number of methods of aggregation and carefully delineates the interpretations of each in order to inform their correct usage. A toy example is provided that highlights how an injudicious choice of aggregation method can lead to erroneous conclusions.",
        "authors": "J. Hogan, N. M. Adams",
        "keywords": [
            "Aggregate ROC Curves",
            "Classifier Performance Summarization",
            "Decision Process Interpretation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=FByH3qL87G",
        "pdf_src": "https://api2.openreview.net/pdf/1270f4985f8b02934f1d6a03d009b28f25ad905d.pdf",
        "Code_src": "",
        "Introduction": "Background: Receiver Operating Characteristic (ROC) curves have been widely used as a graphical tool or metric for assessing the diagnostic ability of tests such as those based on serum markers.\nResearch Problem: Analysts frequently need to summarize multiple ROC curves obtained with different thresholds by aggregating them together because they want some summary measure reflecting overall performance across all these ROC curves.\n\nMethodology: In addressing this problem, several statistical approaches exist including the Peto-Peto estimator, the Harrell concordance index, the sum of squared areas under the ROC curves (SROC), etc. However, it's challenging without proper guidance due to the lack of understanding about what kind of information does each aggregation approach capture regarding the underlying data distribution(s).\n\nMain Contributions: \n1. We provide a comprehensive review over various existing aggregation techniques along with their mathematical formulations;\n2. We discuss interpretative aspects related to why certain aggregation measures might not always reflect true improvements observed at individual patient levels;\n3. Finally we illustrate through a simple numerical example where incorrect use of aggregation could mislead analysts towards drawing false conclusions concerning clinical significance",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Undersampling is a Minimax Optimal Robustness Intervention in Nonparametric Classification",
        "abstract": "While a broad range of techniques have been proposed to tackle distribution shift, the simple baseline of training on an undersampled balanced dataset often achieves close to state-of-the-art-accuracy across several popular benchmarks. This is rather surprising, since undersampling algorithms discard excess majority group data. To understand this phenomenon, we ask if learning is fundamentally constrained by a lack of minority group samples. We prove that this is indeed the case in the setting of nonparametric binary classification. Our results show that in the worst case, an algorithm cannot outperform undersampling unless there is a high degree of overlap between the train and test distributions (which is unlikely to be the case in real-world datasets), or if the algorithm leverages additional structure about the distribution shift. In particular, in the case of label shift we show that there is always an undersampling algorithm that is minimax optimal. In the case of group-covariate shift we show that there is an undersampling algorithm that is minimax optimal when the overlap between the group distributions is small. We also perform an experimental case study on a label shift dataset and find that in line with our theory, the test accuracy of robust neural network classifiers is constrained by the number of minority samples.",
        "authors": "N. S. Chatterji, S. Haque, T. Hashimoto",
        "keywords": [
            "undersampling",
            "distribution shift",
            "label shift"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=r6oHDYOZ6p",
        "pdf_src": "https://api2.openreview.net/pdf/36b7d898c4494ab6b5c34a8058a4918d17bbba04.pdf",
        "Code_src": "",
        "Introduction": "Background: Distribution shift occurs due to changes in the underlying distribution during model training compared to testing time which can lead to poor performance.\n\nResearch Problem: Despite many advanced techniques being developed for handling distribution shift, it has recently been observed that using a simple baseline approach - training models only on an undersampled balanced dataset – often yields near-state-of-the-art accuracies.\nThe question arises as to whether such success could possibly result from insufficient representation of minority class samples leading to fundamental constraints on machine learning processes.\n\nMethodology: The paper investigates these findings through theoretical analysis focusing on two types of distribution shifts:\n1. Label Shift where the labels change but not necessarily the features themselves;\n2. Group-Covariate Shift involving both feature and label transformations within different groups.\n\nMain Contributions: \n1. Proving theoretically under certain conditions related to non-parametric binary classification settings; \n2. Demonstrating that without significant overlap between training and test distributions,\n   no algorithm will surpass undersampling's performance even though it discards majority-class examples.\n3. Identifying specific cases like label shift showing that any undersampling strategy would achieve minimum-maximum optimality while addressing group covariate shift shows similar optimality provided the group distributions are somewhat distinct.\n4. Experimental validation via empirical work on a label shift dataset supporting their theoretical predictions regarding how the amount of minority sample size affects classifier performance",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "On the Convergence and Calibration of Deep Learning with Differential Privacy",
        "abstract": "Differentially private (DP) training preserves the data privacy usually at the cost of slower convergence (and thus lower accuracy), as well as more severe mis-calibration than its non-private counterpart. To analyze the convergence of DP training, we formulate a continuous time analysis through the lens of neural tangent kernel (NTK), which characterizes the per-sample gradient clipping and the noise addition in DP training, for arbitrary network architectures and loss functions. Interestingly, we show that the noise addition only affects the privacy risk but not the convergence or calibration, whereas the per-sample gradient clipping (under both flat and layerwise clipping styles) only affects the convergence and calibration.\n\nFurthermore, we observe that while DP models trained with small clipping norm usually achieve the best accurate, but are poorly calibrated and thus unreliable. In sharp contrast, DP models trained with large clipping norm enjoy the same privacy guarantee and similar accuracy, but are significantly more \\textit{calibrated}. Our code can be found at https://github.com/woodyx218/opacus_global_clipping.",
        "authors": "Z. Bu, H. Wang, Z. Dai, et.al",
        "keywords": [
            "dp_training",
            "convergence",
            "calibration"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=K0CAGgjYS1",
        "pdf_src": "https://api2.openreview.net/pdf/e6ebc1129469f205507df3d3cd60a41feba1394f.pdf",
        "Code_src": "",
        "Introduction": "Background: Differential privacy (DP) is an important technique to protect individual's privacy during machine learning model training by adding noise to the gradients so that it cannot reveal any information about specific individuals' data points within the dataset being used.\nResearch Problem: However, one major drawback associated with DP training algorithms compared to their non-private counterparts lies in trade-offs between preserving privacy on one hand versus achieving faster convergence rates leading towards higher prediction accuracies along with better calibration properties - i.e., how close predictions match actual values across different datasets or scenarios where these models might encounter unseen examples later down stream post deployment phase into real-world applications contexts etcetera.. \nMethodology: We develop a novel continuous-time framework based upon Neural Tangent Kernel theory allowing us to understand various aspects related specifically pertaining differential privacy mechanisms like sample-wise gradient clipping norms applied throughout each iteration step during optimization process plus stochastic noises introduced therein respectively; this helps us gain insights regarding impacts such modifications have over overall performance metrics including those concerning convergence rate ,accuracy levels & calibration issues too!. \nMain Contributions: Firstly our findings indicate that although stochastic noises added during DP training procedures do indeed impact privacy risks posed thereby but they don't affect neither convergence nor calibration outcomes whatsoever . Secondly we demonstrate empirically via experiments conducted using synthetic benchmarks demonstrating empirical evidence supporting aforementioned theoretical claims made earlier herein namely : when employing smaller gradient clipping norms whilst training DP models yields highest predictive performances terms yet suffer from poor calibration characteristics making them less reliable whereas larger norms preserve privacy guarantees alongside maintaining comparable accuracy levels albeit exhibiting superior calibration properties instead!",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Attentional-Biased Stochastic Gradient Descent",
        "abstract": "In this paper, we present a simple yet effective provable method (named ABSGD) for addressing the data imbalance or label noise problem in deep learning. Our method is a simple modification to momentum SGD where we assign an individual importance weight to each sample in the mini-batch. The individual-level weight of a sampled data is systematically proportional to the exponential of a scaled loss value of the data, where the scaling factor is interpreted as the regularization parameter in the framework of distributionally robust optimization (DRO). Depending on whether the scaling factor is positive or negative, ABSGD is guaranteed to converge to a stationary point of an information-regularized min-max or min-min  DRO problem, respectively. Compared with existing class-level weighting schemes, our method can capture the diversity between individual examples within each class. Compared with existing individual-level weighting methods using meta-learning that require three backward propagations for computing mini-batch stochastic gradients, our method is more efficient with only one backward propagation at each iteration as in standard deep learning methods.  ABSGD is flexible enough to combine with other robust losses without any additional cost. Our empirical studies on several benchmark datasets demonstrate the effectiveness of the proposed method.",
        "authors": "Q. Qi, Y. Xu, W. Yin, et.al",
        "keywords": [
            "ASGD",
            "Data Imbalance",
            "Distributionally Robust Optimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=B0WYWvVA2r",
        "pdf_src": "https://api2.openreview.net/pdf/df178539a165f920d3df54b7d2fb57f0c312e8ba.pdf",
        "Code_src": "",
        "Introduction": "Background: This research addresses common challenges faced by machine learning models when dealing with imbalanced training data and noisy labels.\n\nResearch Problem: How do you effectively address issues related to data imbalance and label noise during model training?\n\nMethod: The authors introduce ABSGD - a novel algorithmic approach based on Momentum Stochastic Gradient Descent (SGD), which incorporates individual importance weights into each sample's contribution towards the gradient calculation process.\n1. Assigns different weights to samples depending on their 'scaled loss values'.\n2. Uses these weighted gradients along with momentum to update parameters iteratively through backpropagation.\n3. Incorporates a scaling factor that acts like a regularization parameter from Distributionally Robust Optimization (DRO).\n\nMain Contributions:\n1. Captures intra-class variability better than previous class-level weighting strategies due to its focus on individual samples rather than entire classes.\n2. Significantly reduces computational complexity compared to prior works requiring multiple backward passes per batch; it requires just one pass similar to regular SGD.\n3. Is designed such that it can be combined seamlessly with various robustness-enhancing loss functions while not imposing extra computational overhead – making it highly versatile across diverse applications involving robust learning tasks.\n4. Empirical validation demonstrates performance improvements over baseline approaches",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Reinforcement Teaching ",
        "abstract": "Machine learning algorithms learn to solve a task, but are unable to improve their ability to learn.\nMeta-learning methods learn about machine learning algorithms and improve them so that they learn more quickly. However, existing meta-learning methods are either hand-crafted to improve one specific component of an algorithm or only work with differentiable algorithms. \nWe develop a unifying meta-learning framework, called \\textit{Reinforcement Teaching}, to improve the learning process of \\emph{any} algorithm. Under Reinforcement Teaching, a teaching policy is learned, through reinforcement, to improve a student's learning algorithm. To learn an effective teaching policy, we introduce the \\textit{parametric-behavior embedder} that learns a representation of the student's learnable parameters from its input/output behavior. We further use \\textit{learning progress} to shape the teacher's reward, allowing it to more quickly maximize the student's performance. To demonstrate the generality of Reinforcement Teaching, we conduct experiments in which a teacher learns to significantly improve both reinforcement and supervised learning algorithms. Reinforcement Teaching outperforms previous work using heuristic reward functions and state representations, as well as other parameter representations. ",
        "authors": "C. Muslimani, A. Lewandowski, D. Schuurmans, et.al",
        "keywords": [
            "Reinforcement Learning",
            "Meta-Learning",
            "Parametric-Behavior Embedder"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=G2GKiicaJI",
        "pdf_src": "https://api2.openreview.net/pdf/98a88dd2ee7d51206290a6249378d7dfc1d237b2.pdf",
        "Code_src": "",
        "Introduction": "Background: Machine learning algorithms have been developed for various tasks; however, these algorithms do not inherently possess the capability to enhance their own learning processes.\n\nResearch Question: How can we design a meta-learning approach capable of improving any given machine learning algorithm?\n\nMethod: The paper introduces a novel meta-learning framework named \"Reinforcement Teaching.\" This framework involves training a teaching policy via reinforcement learning techniques aimed at refining students' learning algorithms effectively. A key innovation within this framework includes the development of a parametric-behavior embedder - a mechanism designed to represent the student's learnable parameters based on observed input-output behaviors during the learning phase. Additionally, the authors incorporate 'learning progress' into the teacher's reward shaping strategy—allowing teachers to focus rewards towards enhancing areas where significant improvements could be made most rapidly by the learner.\n\nMain Contributions:\n1. Development of a unified meta-learning framework (\"Reinforcement Teaching\") applicable across all types of machine learning algorithms without being constrained solely to those amenable to differentiation.\n2. Introduction of the parametric-behavior embedder—a new technique enabling the creation of rich representations capturing essential information regarding how learners interact with data inputs/outputs throughout the learning cycle.\n3. Implementation of dynamic reward shaping informed by measures of learning progress rather than static metrics alone—which allows for adaptive optimization strategies tailored specifically toward accelerating desired aspects of algorithmic improvement over time compared traditional approaches relying heavily on heuristics instead comprehensive understanding underlying mechanisms involved",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "A Reproducible and Realistic Evaluation of Partial Domain Adaptation Methods",
        "abstract": "Unsupervised Domain Adaptation (UDA) aims at classifying unlabeled target images leveraging source labeled ones. In the case of an extreme label shift scenario between the source and target domains, where we have extra source classes not present in the target domain, the UDA problem becomes a harder problem called Partial Domain Adaptation (PDA). While different methods have been developed to solve the PDA problem, most successful algorithms use model selection strategies that rely on target labels to find the best hyper-parameters and/or models along training. These strategies violate the main assumption in PDA: only unlabeled target domain samples are available. In addition, there are also experimental inconsistencies between developed methods - different architectures, hyper-parameter tuning, number of runs - yielding unfair comparisons. The main goal of this work is to provide a realistic evaluation of PDA methods under different model selection strategies and a consistent evaluation protocol. We evaluate 6 state-of-the-art PDA algorithms on 2 different real-world datasets using 7 different model selection strategies. Our two main findings are: (i) without target labels for model selection, the accuracy of the methods decreases up to 30 percentage points; (ii) only one method and model selection pair performs well on both datasets. Experiments were performed with our PyTorch framework, BenchmarkPDA, which we open source.",
        "authors": "T. Salvador, K. Fatras, I. Mitliagkas, et.al",
        "keywords": [
            "Partial Domain Adaptation",
            "Unsupervised Domain Adaptation",
            "Model Selection Strategies"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=XcVzIBXeRn",
        "pdf_src": "https://api2.openreview.net/pdf/f97f5b96592beb6420bde8196c8a78c8a38a264f.pdf",
        "Code_src": "",
        "Introduction": "Background: Unsupervised Domain Adaptation (UDA) focuses on utilizing labeled data from a source domain to classify unlabeled data from a target domain when no direct supervision exists across these domains. However, if significant differences exist such as new categories appearing exclusively or disappearing entirely compared to each other's distributions – referred to as \"label shift\" scenarios -, then it transitions into Partial Domain Adaptation (PDA), making classification more challenging.\n\nResearch Question: How do various Partial Domain Adaptation techniques perform within differing settings regarding how they select their learning parameters?\n\nMethodology: This study evaluates six leading PDA approaches against two distinct real-world datasets employing seven diverse parameter selection procedures while adhering to a uniform testing methodology designed by the authors (\"BenchmarkPDA\"). They used PyTorch as part of their implementation platform due to its flexibility allowing them to conduct thorough experiments efficiently.\n \nMain Contributions:\n1. Realistic Evaluation: By comparing multiple PDA algorithms consistently according to varying criteria related to selecting optimal models during training through different hyperparameter optimization processes rather than solely relying on target labels like many prior studies did before,\n2. Performance Insights: Revealing substantial performance discrepancies among those evaluated based upon whether access was granted towards target labels versus being restricted purely to unlabeled examples alone resulting in considerable variance ranging around thirty percent difference in accuracy levels amongst all tested systems;\n3. Open-source Framework: Providing an accessible benchmark tool named BenchmarkPDA written specifically tailored toward facilitating future research endeavors focused on addressing issues pertinent to partial domain adaptation tasks via reproducible experimentation protocols",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Test-Time Adaptation for Visual Document Understanding",
        "abstract": "For visual document understanding (VDU), self-supervised pretraining has been shown to successfully generate transferable representations, yet, effective adaptation of such representations to distribution shifts at test-time remains to be an unexplored area. We propose DocTTA, a novel test-time adaptation method for documents, that does source-free domain adaptation using unlabeled target document data. DocTTA leverages cross-modality self-supervised learning via masked visual language modeling, as well as pseudo labeling to adapt models learned on a \\textit{source} domain to an unlabeled \\textit{target} domain at test time. We introduce new benchmarks using existing public datasets for various VDU tasks, including entity recognition, key-value extraction, and document visual question answering. DocTTA shows significant improvements on these compared to the source model performance, up to 1.89\\% in (F1 score), 3.43\\% (F1 score), and 17.68\\%  (ANLS score), respectively.",
        "authors": "S. Ebrahimi, S. O. Arik, T. Pfister",
        "keywords": [
            "document",
            "test-time adaptation",
            "cross-modal self-supervised learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=zshemTAa6U",
        "pdf_src": "https://api2.openreview.net/pdf/aa6ed6de23de27df536d6149e51b9b60bb21ade0.pdf",
        "Code_src": "",
        "Introduction": "Background: Visual Document Understanding (VDU) is challenging due to distribution shifts between training and testing domains.\nResearch Problem: How can we effectively adapt pre-trained visual document representations to distribution shifts during test time?\nMethods: We propose DocTTA, which uses unlabeled target document data without requiring access to the source domain labels or knowledge about it through source-free domain adaptation with cross-modal self-supervised learning techniques like masked visual language modeling along with pseudo-labeling strategy.\n\nMain Contributions: We have introduced new benchmarks based on existing public datasets across different VDU tasks - Entity Recognition, Key-Value Extraction & Document Visual Question Answering where our proposed DocTTA demonstrates substantial improvement over baseline performances by adapting pre-trained models from one domain onto another unseen dataset achieving gains ranging anywhere from 1.89% increase in F1 Score within Entity Recognition task all way upto 17.68% ANLS Score boost when applied towards Document Visual Question Answering problem set",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "Robust Alzheimer's Progression Modeling using Cross-Domain Self-Supervised Deep Learning",
        "abstract": "Developing successful artificial intelligence systems in practice depends on both robust deep learning models and large, high-quality data. However, acquiring and labeling data can be prohibitively expensive and time-consuming in many real-world applications, such as clinical disease models. Self-supervised learning has demonstrated great potential in increasing model accuracy and robustness in small data regimes. In addition, many clinical imaging and disease modeling applications rely heavily on regression of continuous quantities. However, the applicability of self-supervised learning for these medical-imaging regression tasks has not been extensively studied. In this study, we develop a cross-domain self-supervised learning approach for disease prognostic modeling as a regression problem using medical images as input. We demonstrate that self-supervised pretraining can improve the prediction of Alzheimer's Disease progression from brain MRI. We also show that pretraining on extended (but not labeled) brain MRI data outperforms pretraining on natural images. We further observe that the highest performance is achieved when both natural images and extended brain-MRI data are used for pretraining.",
        "authors": "S. Dadsetan, M. Hejrati, S. Wu, et.al",
        "keywords": [
            "medical image",
            "self-supervised learning",
            "disease prognosis"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=HVAeM6sNo8",
        "pdf_src": "https://api2.openreview.net/pdf/21f17ac7af6a391af1825f6b6206f6df9e0700e7.pdf",
        "Code_src": "",
        "Introduction": "Background: The development of successful artificial intelligence systems relies on robust deep learning models trained with large amounts of high-quality data; however, obtaining and annotating sufficient training data often proves to be costly or impractical due to factors like prohibitive expenses associated with annotation labor.\n\nResearch Problem: This paper addresses whether self-supervised learning could serve an effective alternative method by improving predictive abilities within limited datasets while focusing specifically on its application towards regression problems found in medical image analysis where predicting outcomes based on continuous variables plays crucial roles—such as those seen during prognosis estimation through neuroimaging modalities including Magnetic Resonance Imaging (MRI).\n\nMethodology: They propose developing a novel cross-domain self-supervised learning framework designed explicitly around solving regression issues related to diseases' prognoses via utilizing medical imagery inputs rather than solely relying upon general-purpose unlabeled datasets which have traditionally dominated prior works into self-supervised approaches.\n\nMain Contributions:\n1) Demonstrated improved predictions about Alzheimer’s Disease progression rates derived directly from magnetic resonance imaging scans after employing their proposed self-supervised pre-training strategy.\n2) Showcased better results were obtained if they utilized additional unlabeled but extended sets of brain MRI samples alongside standard natural image datasets compared against exclusively using either alone—a finding suggesting domain-specific augmentation may significantly enhance performance beyond what might otherwise occur just leveraging purely generic visual information sources without incorporating specialized biomedical knowledge present therein.\n\n3) Further highlighted optimal performance was reached once combining both types of dataset sources together simultaneously during initial stages before fine-tuning over specific downstream targets indicating synergy between complementary kinds of data sources contributes most effectively toward achieving state-of-the-art levels achievable currently across various benchmarks pertaining specifically here",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "Learning Augmentation Distributions using Transformed Risk Minimization",
        "abstract": "We propose a new \\emph{Transformed Risk Minimization} (TRM) framework as an extension of classical risk minimization. \nIn TRM, we optimize not only over predictive models, but also over data transformations; specifically over distributions thereof.\nAs a key application, we focus on learning augmentations; for instance appropriate rotations of images, to improve classification performance with a given class of predictors. Our TRM method (1) jointly learns transformations and models in a \\emph{single training loop}, (2) works with any training algorithm applicable to standard risk minimization, and (3)  handles any transforms, such as discrete and continuous classes of augmentations. To avoid overfitting when implementing empirical transformed risk minimization, we propose a novel regularizer based on PAC-Bayes theory. For learning augmentations of images, we propose a new parametrization of the space of augmentations via a stochastic composition of blocks of geometric transforms. This leads to the new \\emph{Stochastic Compositional Augmentation Learning} (SCALE) algorithm. The performance of TRM with SCALE compares favorably to prior methods on CIFAR10/100. Additionally, we show empirically that SCALE can correctly learn certain symmetries in the data distribution (recovering rotations on rotated MNIST) and can also improve calibration of the learned model.",
        "authors": "E. Chatzipantazis, S. Pertigkiozoglou, K. Daniilidis, et.al",
        "keywords": [
            "Transformed Risk Minimization",
            "Stochastic Compositional Augmentation Learning",
            "PAC-Bayes Regularization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LRYtNj8Xw0",
        "pdf_src": "https://api2.openreview.net/pdf/f625c2dffccd12c6ee1bdcfae5907bb0634dacba.pdf",
        "Code_src": "",
        "Introduction": "Background: Classical machine learning focuses mainly on optimizing predictive models without considering how they might be affected by changes or perturbations within their input data.\n\nResearch Problem: How do you effectively incorporate variations into your dataset while still maintaining good predictions?\n\nMethodology: We introduce Transformed Risk Minimization (TRM), which extends traditional risk minimization approaches beyond just predictive models themselves - it optimizes both the predictive models and the transformations applied to our datasets simultaneously through a single training loop process using existing algorithms from statistical learning theory like PAC-Bayes Regularization Theory along with Stochastic Composition of Blocks of Geometric Transformations leading us towards Scalable Compositional Augmentation Learning (SCALE).\n\nMain Contributions:\n- A unified approach called Transformed Risk Minimization (TRM) that allows joint optimization between predictive models & transformation parameters;\n- Novel regularization technique inspired by PAC-Bayesian theorem helps prevent overfitting during empirical implementation of TRM;\n- Developed Scalable Compositional Augmentation Learning (SCALE) algorithm capable of efficiently generating complex augmentations including those involving rotation symmetry recovery demonstrated successfully trained on CIFAR10/100 datasets",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Structured Low-Rank Tensors for Generalized Linear Models",
        "abstract": "Recent works have shown that imposing tensor structures on the coefficient tensor in regression problems can lead to more reliable parameter estimation and lower sample complexity compared to vector-based methods. This work investigates a new low-rank tensor model, called Low Separation Rank (LSR), in Generalized Linear Model (GLM) problems. The LSR model – which generalizes the well-known Tucker and CANDECOMP/PARAFAC (CP) models, and is a special case of the Block Tensor Decomposition (BTD) model – is imposed onto the coefficient tensor in the GLM model. This work proposes a block coordinate descent algorithm for parameter estimation in LSR-structured tensor GLMs. Most importantly, it derives a minimax lower bound on the error threshold on estimating the coefficient tensor in LSR tensor GLM problems. The minimax bound is proportional to the intrinsic degrees of freedom in the LSR tensor GLM problem, suggesting that its sample complexity may be significantly lower than that of vectorized GLMs. This result can also be specialised to lower bound the estimation error in CP and Tucker-structured GLMs. The derived bounds are comparable to tight bounds in the literature for Tucker linear regression, and the tightness of the minimax lower bound is further assessed numerically. Finally, numerical experiments on synthetic datasets demonstrate the efficacy of the proposed LSR tensor model for three regression types (linear, logistic and Poisson). Experiments on a collection of medical imaging datasets demonstrate the usefulness of the LSR model over other tensor models (Tucker and CP) on real, imbalanced data with limited available samples.",
        "authors": "B. A. Taki, A. D. Sarwate, W. U. Bajwa",
        "keywords": [
            "tensor structure",
            "Low Separation Rank (LSR)",
            "Generalized Linear Model (GLM)"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=qUxBs3Ln41",
        "pdf_src": "https://api2.openreview.net/pdf/ec6764c483ac2be8602b474c7170dd6c02dd261a.pdf",
        "Code_src": "",
        "Introduction": "Background: Recent studies indicate that tensor-based approaches offer improved reliability and reduced sample complexity when applied to regression tasks as opposed to vector-based techniques.\n\nResearch Problem: This paper explores an innovative low-rank tensor structure known as Low Separation Rank (LSR) within the context of Generalized Linear Models (GLMs).\n\nMethods: A novel block coordinate descent algorithm has been developed specifically tailored towards parameter estimation under the constraints of this LSR tensor structure inside GLMs. Additionally, we establish a minimax lower bound theory for the estimation accuracy regarding the coefficient tensor used by the LSR tensor GLM framework; this bound is directly related to the inherent degrees of freedom present in such models indicating potential reduction in sample size required relative to vectorized GLMs or those structured using Tucker decomposition or CP factorization.\n\nMain Contributions:\n1. Introduction of the LSR tensor model into GLM settings.\n2. Development of a specialized optimization algorithm - block coordinate descent - designed explicitly for the LSR tensor GLM parameters.\n3. Derivation of a minimax lower bound theorem quantifying the minimum achievable estimation error rate based on the number of degrees of freedom unique to the LSR tensor GLM approach across different regression scenarios including linear, logistic, and Poisson distributions.\n4. Numerical validation through simulations demonstrating superior performance against traditional vectorized GLMs along with empirical evidence from medical image analysis where the LSR model outperforms both Tucker and CP decompositions despite dealing with unbalanced and small-sample datasets commonly found in clinical applications.",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Mitigating Real-World Distribution Shifts in the Fourier Domain",
        "abstract": "While machine learning systems can be highly accurate in their training environments, their performance in real-world deployments can suffer significantly due to distribution shifts. Real-world distribution shifts involve various input distortions due to noise, weather, device and other variations. Many real-world distribution shifts are not represented in standard domain adaptation datasets and prior empirical work has shown that domain adaptation methods developed using these standard datasets may not generalize well to real-world distribution shifts. Furthermore, motivated by observations of the sensitivity of deep neural networks (DNN) to the spectral statistics of data, which can vary in real-world scenarios, we propose Fourier Moment Matching (FMM), a model-agnostic input transformation that matches the Fourier-amplitude statistics of source to target data using unlabeled samples. We demonstrate through extensive empirical evaluations across time-series, image classification and semantic segmentation tasks that FMM is effective both individually and when combined with a variety of existing methods to overcome real-world distribution shifts.",
        "authors": "K. Krishnamachari, S. Ng, C. Foo",
        "keywords": [
            "distribution shift",
            "Fourier Moment Matching",
            "real-world deployment"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=lu4oAq55iK",
        "pdf_src": "https://api2.openreview.net/pdf/64b599a01b15707b8e3ef59de2d3c426cebbe547.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses how while machine learning models perform accurately during training on controlled datasets within specific domains or settings, they often struggle under real-world conditions where there might be differences between the training environment and deployment scenario known as \"distribution shift.\" These shifts could arise from factors such as noise, environmental changes like weather, different devices used for collecting inputs etc.\n\nResearch Problem: The challenge presented here revolves around addressing the issue of generalization gap - why do state-of-the-art domain adaptation techniques fail at adapting learned representations effectively even after being trained over large labeled datasets? Additionally, it highlights another problem related to the sensitivity of DNNs towards spectral properties of the dataset; this property also varies widely depending upon context leading to further challenges in transferring knowledge gained via supervised learning into new contexts.\n\nMethodology: To tackle these issues head-on without relying solely on labeled datasets alone but rather leveraging unlabeled ones too – an approach termed semi-supervised learning -, authors introduce Fourier Moment Matching (FMM). This technique involves transforming inputs so that their Fourier amplitude distributions match those observed throughout multiple sources before applying them onto targets thus reducing discrepancies present therein.\n\nMain Contributions:\n1. They identify limitations associated with current approaches dealing with distribution shifts encountered outside typical lab settings.\n2. Propose Fourier Moment Matching—a novel method capable of matching statistical moments up until second order regardless if labels exist or not—allowing us better adapt our models' features irrespective of whether additional annotations come along alongside raw inputs themselves).\n3. Conduct comprehensive experiments demonstrating effectiveness against several benchmarks including time series prediction, image classification & semantic segmentation tasks showing significant improvements compared traditional methods applied separately or together yielding promising results suggesting potential future applications beyond just overcoming distributional biases found commonly nowadays among deployed ML systems worldwide today",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Learning representations that are closed-form Monge mapping optimal with application to domain adaptation",
        "abstract": "Optimal transport (OT) is a powerful geometric tool used to compare and align probability measures following the least effort principle. Despite its widespread use in machine learning (ML), OT problem still bears its computational burden, while at the same time suffering from the curse of dimensionality for measures supported on general high-dimensional spaces. \nIn this paper, we propose to tackle these challenges using representation learning. In particular, we seek to learn an embedding space such that the samples of the two input measures become alignable in it with a simple affine mapping that can be calculated efficiently in closed-form. We then show that such approach leads to results that are comparable to solving the original OT problem when applied to the transfer learning task on which many OT baselines where previously evaluated in both homogeneous and heterogeneous DA settings.",
        "authors": "O. Struckmeier, I. Redko, A. Mallasto, et.al",
        "keywords": [
            "representation learning",
            "optimal transport",
            "alignment"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=nOIGfQnFZm",
        "pdf_src": "https://api2.openreview.net/pdf/193cd8bc728422972cdcd009959d9abc1e5ec51c.pdf",
        "Code_src": "",
        "Introduction": "Background: Optimal Transport (OT) is widely utilized as a geometric tool within Machine Learning due to its capability underpinning measure comparison and alignment through the principle of minimum work. However, despite its extensive application, there remains significant computational complexity associated with the OT process.\n\nResearch Problem: The primary challenge addressed by our research pertains to the efficiency and scalability issues inherent in traditional optimal transport algorithms across various domains characterized by high dimensions or heterogeneity between datasets; specifically, how do we mitigate the computational cost without compromising accuracy?\n\nMethodology: To address aforementioned challenges posed by the curse of dimensionality along with computational burdens, especially pertinent during data augmentation tasks involving domain adaptation scenarios - homogenous/heterogeneous – we introduce a novel method leveraging Representation Learning techniques.\nWe focus on developing an embedding space capable enough so that any pair of samples drawn from disparate distributions could potentially undergo alignment via straightforward linear transformations (affine mappings). This allows us not only to reduce computational overhead but also enables efficient calculations since they're expressed in closed form equations rather than iterative optimization procedures commonly found in standard OT solutions.\n\nMain Contributions:\n1. Our proposed framework significantly reduces computation costs compared conventional methods like Sinkhorn algorithm because no explicit matrix computations need be performed after initial embeddings have been established;\n2. It addresses problems related to high dimensional spaces effectively by utilizing learned representations tailored towards specific applications thus mitigating curse-of-dimensionality effects;\n3. Demonstrates performance equivalence against existing state-of-the-art approaches even though employing less computationally intensive operations making it particularly suitable large-scale practical implementations requiring real-time processing capabilities",
        "Topic": "Optimal Transport"
    },
    {
        "title": "Towards a Defense Against Federated Backdoor Attacks Under Continuous Training",
        "abstract": "Backdoor attacks are dangerous and difficult to prevent in federated learning (FL), where training data is sourced from untrusted clients over long periods of time. These difficulties arise because: (a) defenders in FL do not have access to raw training data, and (b) a phenomenon we identify called backdoor leakage causes models trained continuously to eventually suffer from backdoors due to cumulative errors in defense mechanisms. We propose a framework called shadow learning for defending against backdoor attacks in the FL setting under long-range training. Shadow learning trains two models in parallel: a backbone model and a shadow model. The backbone is trained without any defense mechanism to obtain good performance on the main task. The shadow model combines filtering of malicious clients with early-stopping to control the attack success rate even as the data distribution changes. We theoretically motivate our design and show experimentally that our framework significantly improves upon existing defenses against backdoor attacks.",
        "authors": "S. Wang, J. Hayase, G. Fanti, et.al",
        "keywords": [
            "backdoor attacks",
            "federated learning",
            "shadow learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=HwcB5elyuG",
        "pdf_src": "https://api2.openreview.net/pdf/602818e28134062a5d9880d72505c007dfe63934.pdf",
        "Code_src": "",
        "Introduction": "Background: Federated Learning (FL) involves training machine learning models across multiple devices or servers while keeping the raw data decentralized at each client site.\n\nResearch Problem: A significant challenge in FL systems arises when attackers can introduce \"backdoors\" into the learned models by manipulating only part of the training dataset provided by individual users but not affecting their predictions much individually; however, these subtle manipulations could lead to severe consequences once aggregated within the global model during training.\n \nMethodology: To address this issue, authors present a novel approach named Shadow Learning which consists of two components:\n1. Backbone Model - This model undergoes standard training processes like gradient descent algorithms aiming high accuracy scores typically associated with its primary objective function.\n2. Shadow Model - It's an auxiliary learner designed specifically around detecting anomalies such as those introduced through adversarial examples including potential backdoor implants. Its role includes monitoring the behavior patterns among different datasets coming together throughout iterations ensuring they align well before merging them further forward towards convergence point(s).\nThe key idea behind shadow learning lies in leveraging both passive observation (filtering out suspicious inputs) and active intervention (early stopping if certain thresholds indicating successful infiltration attempts exceed).\n\nMain Contributions: The paper introduces Shadow Learning—a new strategy capable of mitigating risks posed by backdoor threats within federated settings despite having limited visibility onto local training datasets themselves. Empirical validation shows improved robustness compared traditional methods used previously—suggesting it may be more resilient than other approaches currently available",
        "Topic": "Federated Learning"
    },
    {
        "title": "mL-BFGS: A Momentum-based L-BFGS for Distributed Large-scale Neural Network Optimization",
        "abstract": "Quasi-Newton methods still face significant challenges in training large-scale neural networks due to additional compute costs in the Hessian related computations and instability issues in stochastic training.\nA well-known method, L-BFGS that efficiently approximates the Hessian using history parameter and gradient changes, suffers convergence instability in stochastic training.\nSo far, attempts that adapt L-BFGS to large-scale stochastic training incur considerable extra overhead, which offsets its convergence benefits in wall-clock time.\nIn this paper, we propose mL-BFGS, a lightweight momentum-based L-BFGS algorithm that paves the way for quasi-Newton (QN) methods in large-scale distributed deep neural network (DNN) optimization. \nmL-BFGS introduces a nearly cost-free momentum scheme into L-BFGS update and greatly reduces stochastic noise in the Hessian, therefore stabilizing convergence during stochastic optimization.\nFor model training at a large scale, mL-BFGS approximates a block-wise Hessian, thus enabling distributing compute and memory costs across all computing nodes.\nWe provide a supporting convergence analysis for mL-BFGS in stochastic settings.\nTo investigate mL-BFGS's potential in large-scale DNN training, we train benchmark neural models using mL-BFGS and compare performance with baselines (SGD, Adam, and other quasi-Newton methods). \nResults show that mL-BFGS achieves both noticeable iteration-wise and wall-clock speedup.",
        "authors": "Y. Niu, Z. Fabian, S. Lee, et.al",
        "keywords": [
            "ml-BFGS",
            "Quasi-Newton Methods",
            "Large-Scale Stochastic Training"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=9jnsPp8DP3",
        "pdf_src": "https://api2.openreview.net/pdf/1ec418b7512acd13c83ef383eaf39afb3eecda62.pdf",
        "Code_src": "",
        "Introduction": "Background: Quasi-Newton methods are widely used in optimizing large-scale neural networks; however, they encounter computational inefficiency from Hessian-related calculations as well as stability problems when applied to stochastic training.\n\nResearch Question: How can we improve the efficiency of quasi-Newton methods like L-BFGS while maintaining their convergence properties?\n\nMethod: This study proposes mL-BFGS, an adaptation of the L-BFGS algorithm enhanced by a simple momentum mechanism designed specifically for stochastic optimization scenarios involving large-scale distributed deep neural networks (DNNs).\n\nMain Contributions:\n1. mL-BFGS incorporates a novel momentum term within the L-BFGS updates without any added computational or storage costs over standard L-BFGS algorithms—this significantly mitigates stochastic noise affecting the Hessian approximation leading to more stable convergence.\n2. mL-BFGS is capable of handling block-wise Hessians effectively allowing parallelization on multiple computing nodes thereby reducing overall computation and memory requirements necessary for scaling up to very large datasets typical in modern machine learning applications.\n3. A comprehensive convergence analysis demonstrates mL-BFGS’s robustness under stochastic conditions compared against existing benchmarks such as SGD (Stochastic Gradient Descent), Adam optimizer along with several quasi-Newton variants including original L-BFGS itself where mL-BFGS consistently outperforms them through faster iterations per epoch measured iteratively",
        "Topic": "Large Language Models"
    },
    {
        "title": "Scalable Stochastic Gradient Riemannian Langevin Dynamics in Non-Diagonal Metrics",
        "abstract": "Stochastic-gradient sampling methods are often used to perform Bayesian inference on neural networks. It has been observed that the methods in which notions of differential geometry are included tend to have better performances, with the Riemannian metric improving posterior exploration by accounting for the local curvature. However, the existing methods often resort to simple diagonal metrics to remain computationally efficient. This loses some of the gains. We propose two non-diagonal metrics that can be used in stochastic-gradient samplers to improve convergence and exploration but have only a minor computational overhead over diagonal metrics. We show that for fully connected neural networks (NNs) with sparsity-inducing priors and convolutional NNs with correlated priors, using these metrics can provide improvements. For some other choices the posterior is sufficiently easy also for the simpler metrics.",
        "authors": "H. Yu, M. Hartmann, B. Williams, et.al",
        "keywords": [
            "Riemannian Metric",
            "Non-Diagonal Metrics",
            "Stochastic-Gradient Sampling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=dXAuvo6CGI",
        "pdf_src": "https://api2.openreview.net/pdf/d09b35736be87741f99d468740b90d0bb81b632e.pdf",
        "Code_src": "",
        "Introduction": "Background: Stochastic-gradient sampling methods such as Langevin Dynamics or Hamiltonian Monte Carlo are commonly employed techniques within Bayesian inference frameworks applied onto neural networks models.\nResearch Problem: The performance of these sampling methods could potentially benefit from incorporating geometric principles derived from differential calculus into their construction; specifically, utilizing Riemannian metrics allows for an understanding of the local geometry around each parameter point during updates.\n\nMethods: To address this issue without compromising computational efficiency significantly due to more complex calculations involved when using off-the-shelf Riemannian metrics like the Fisher information matrix, we introduce novel non-diagonal metrics suitable for use in stochastic-gradient samplers based on Hessian approximations rather than full matrices - thus maintaining computational tractability while still leveraging the benefits of Riemannian geometry.\n\nMain Contributions: Our contributions lie mainly in proposing practical modifications:\n1. Two new types of non-diagonal metrics designed especially for stochastic-gradient samplers;\n2. Demonstrating empirical evidence through experiments conducted across different neural network architectures including fully connected ones equipped with sparse-inducing priors along with convolutional neural networks featuring correlated prior distributions indicating improved convergence rates compared to diagonal metrics alone under certain conditions where the posterior distribution remains relatively straightforward despite less sophisticated metrics being utilized.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Learning from time-dependent streaming data with online stochastic algorithms",
        "abstract": "This paper addresses stochastic optimization in a streaming setting with time-dependent and biased gradient estimates. We analyze several first-order methods, including Stochastic Gradient Descent (SGD), mini-batch SGD, and time-varying mini-batch SGD, along with their Polyak-Ruppert averages. Our non-asymptotic analysis establishes novel heuristics that link dependence, biases, and convexity levels, enabling accelerated convergence. Specifically, our findings demonstrate that (i) time-varying mini-batch SGD methods have the capability to break long- and short-range dependence structures, (ii) biased SGD methods can achieve comparable performance to their unbiased counterparts, and (iii) incorporating Polyak-Ruppert averaging can accelerate the convergence of the stochastic optimization algorithms. To validate our theoretical findings, we conduct a series of experiments using both simulated and real-life time-dependent data.",
        "authors": "A. Godichon-baggioni, N. Werge, O. Wintenberger",
        "keywords": [
            "stochastic optimization",
            "streaming setting",
            "Polyak-Ruppert averaging"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=kdfiEu1ul6",
        "pdf_src": "https://api2.openreview.net/pdf/59bf30746b4f21ff56710185c600c4f5b01c13e6.pdf",
        "Code_src": "",
        "Introduction": "Background: This research focuses on stochastic optimization problems within a streaming environment where gradients are estimated over time and may be subject to bias.\n\nResearch Problem: The study investigates how different stochastic gradient descent (SGD) variants handle these challenges by analyzing their convergence rates under varying conditions such as dependency structure between samples or estimation bias due to sampling strategies like mini-batching.\n\nMethods: Several first-order stochastic optimization algorithms were considered for this purpose:\n1. Standard Stochastic Gradient Descent (SGD)\n2. Mini-Batch SGD which processes small batches at each iteration rather than single examples.\n3. Time-Varying Mini-Batch SGD adjusting batch size dynamically based on certain criteria during training.\n4. Polyak-Ruppert Averages - an adaptive technique used after each update step combining current estimate with previous iterations' average.\n\nMain Contributions: \n1. Novel insights into the interplay among dependence patterns across different scales (\"long-range\" versus \"short-range\"), estimation bias introduced through mini-batching techniques, and the level of convexity affecting convergence speeds – providing heuristic guidance towards more efficient algorithms design.\n2. Demonstrated that time-varying mini-batch SGD has potential benefits beyond traditional fixed-size mini-batches; it could potentially mitigate issues related to dependencies present in datasets when estimating gradients stochastically from streams of data points arriving sequentially without replacement.\n3. Proved that biased SGD approaches might perform equivalently well compared to unbiased ones if designed appropriately considering factors influencing variance reduction mechanisms inherent in stochastic optimization settings involving biased estimators.\n4. Showed acceleration effects achievable via Polyak-Ruppert averaging applied post-update steps leading to faster convergence times while maintaining similar accuracy guarantees relative to other analyzed methods.\n\n\nTo corroborate these analytical results against theory, empirical validations conducted include simulations mimicking realistic scenarios alongside actual application domains dealing with dynamic environments requiring continuous learning adjustments amidst noisy observations characterized by temporal variability characteristics typical seen in many practical applications today.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "DoCoM: Compressed Decentralized Optimization with Near-Optimal Sample Complexity",
        "abstract": "This paper proposes the Doubly Compressed Momentum-assisted stochastic gradient tracking algorithm (DoCoM) for communication-efficient decentralized optimization. The algorithm features two main ingredients to achieve a near-optimal sample complexity while allowing for communication compression. First, the algorithm tracks both the averaged iterate and stochastic gradient using compressed gossiping consensus. Second, a momentum step is incorporated for adaptive variance reduction with the local gradient estimates. We show that DoCoM finds a near-stationary solution at all participating agents satisfying $\\mathbb{E}[ \\| \\nabla f( \\theta ) \\|^2 ] = {\\cal O}( 1 / T^{2/3} )$ in $T$ iterations, where $f(\\theta)$ is a smooth (possibly non-convex) objective function. Notice that the proof is achieved via analytically designing a new potential function that tightly tracks the one-iteration progress of DoCoM. As a corollary, our analysis also established the linear convergence of DoCoM to a global optimal solution for objective functions with the Polyak-Łojasiewicz condition. Numerical experiments demonstrate that our algorithm outperforms several state-of-the-art algorithms in practice.",
        "authors": "C. Yau, H. T. Wai",
        "keywords": [
            "communication-efficient",
            "decentralized optimization",
            "doubly compressed momentum"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=W0ehjkl9x7",
        "pdf_src": "https://api2.openreview.net/pdf/2729defaa3565da3d5b8c1044dac260f8aa088ea.pdf",
        "Code_src": "",
        "Introduction": "Background: This research focuses on developing an efficient distributed optimization method called Doubly Compressed Momentum-assisted stochastic gradient tracking algorithm (DoCoM), which aims to minimize the computational cost during iterative optimization processes.\n\nResearch Problem: The problem addressed by this study involves finding ways to reduce the amount of data communicated between nodes when optimizing objectives over networks or clusters without sacrificing accuracy significantly.\n \nMethodology: To tackle these challenges, they introduce novel techniques such as tracking both the average iterates and stochastic gradients through compressed gossiping consensus methods; incorporating momentum steps into their algorithmic framework allows them to adaptively adjust variances based solely upon locally available information about gradients from each agent involved in optimization tasks.\n\nMain Contributions: Their contributions include demonstrating how DoCoM can find nearly stationary solutions across multiple participants within \\( T \\) iterations under certain conditions related specifically towards minimizing expected squared norm values relative to changes made throughout iterations involving stochastic gradient descent procedures - achieving time complexity improvements compared existing approaches due primarily because it reduces overall communication overhead required among different machines running computations concurrently toward solving given problems posed hereunder). Additionally ,they provide theoretical guarantees proving convergence rates towards globally optimal solutions provided certain assumptions hold true regarding nature associated with underlying loss functions being optimized along way . Finally numerical experiments conducted confirm efficacy proposed strategy against other leading competitors widely used today's machine learning literature",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "GPS++: Reviving the Art of Message Passing for Molecular Property Prediction",
        "abstract": "We present GPS++, a hybrid Message Passing Neural Network / Graph Transformer model for molecular property prediction. Our model integrates a well-tuned local message passing component and biased global attention with other key ideas from prior literature to achieve state-of-the-art results on large-scale molecular dataset PCQM4Mv2. Through a thorough ablation study we highlight the impact of individual components and find that nearly all of the model’s performance can be maintained without any use of global self-attention, showing that message passing is still a competitive approach for 3D molecular property prediction despite the recent dominance of graph transformers. We also find that our approach is significantly more accurate than prior art when 3D positional information is not available.",
        "authors": "D. Masters, J. Dean, K. Klaeser, et.al",
        "keywords": [
            "GPS++",
            "Hybrid Model",
            "Molecular Property Prediction"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=moVEUgJaHO",
        "pdf_src": "https://api2.openreview.net/pdf/a03baca4036ccc83d40ce0cae757268a47deaf4b.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper introduces GPS++, which aims at improving molecular property prediction by combining various techniques including local message passing neural networks.\n\nResearch Problem: How do existing models perform in predicting molecular properties? Can they effectively utilize both local and global information?\n\nMethods: GPS++ is designed as a hybrid model integrating a local message passing component along with biased global attention mechanisms inspired by previous works.\nThe authors conduct an extensive ablation study focusing on different aspects such as the importance of global self-attention within their framework while comparing against traditional graph transformer-based approaches.\n\nMain Contributions:\n1. State-of-the-Art Results - GPS++ achieves top-notch performance across multiple tasks using the PCQM4Mv2 dataset demonstrating its effectiveness beyond current benchmarks specifically tailored towards this domain.\n2. Ablation Study Insights - By systematically removing certain elements like global self-attention or position encodings one-by-one during training experiments conducted here reveal how crucial these features are overall contributing factors behind successful predictions made through GPS++. \n3. Robustness under Missing Information Conditions - Compared favorably even if three-dimensional positional data isn't readily accessible highlighting robustness advantages over conventional methods reliant solely upon graph transformations alone where additional spatial context would otherwise need manual encoding beforehand into nodes' embeddings space before processing further downstream tasks involving predicted molecular properties estimation etcetera",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Tackling Provably Hard Representative Selection via Graph Neural Networks",
        "abstract": "Representative Selection (RS) is the problem of finding a small subset of exemplars from a dataset that is representative of the dataset. In this paper, we study RS for attributed graphs, and focus on finding representative nodes that optimize the accuracy of a model trained on the selected representatives. Theoretically, we establish a new hardness result for RS (in the absence of a graph structure) by proving that a particular, highly practical variant of it (RS for Learning) is hard to approximate in polynomial time within any reasonable factor, which implies a significant potential gap between the optimum solution of widely-used surrogate functions and the actual accuracy of the model. We then study the setting where a (homophilous) graph structure is available, or can be constructed, between the data points. We show that with an appropriate modeling approach, the presence of such a structure can turn a hard RS (for learning) problem into one that can be effectively solved. To this end, we develop RS-GNN, a representation learning-based RS model based on Graph Neural Networks. Empirically, we demonstrate the effectiveness of RS-GNN on problems with predefined graph structures as well as problems with graphs induced from node feature similarities, by showing that RS-GNN achieves significant improvements over established baselines on a suite of eight benchmarks.",
        "authors": "M. Kazemi, A. Tsitsulin, H. Esfandiari, et.al",
        "keywords": [
            "graph neural networks",
            "representative selection",
            "attribute graphs"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=3LzgOQ3eOb",
        "pdf_src": "https://api2.openreview.net/pdf/0d56a64d8eeb07434506263d43f2da51910bb391.pdf",
        "Code_src": "",
        "Introduction": "Background: Representative Selection (RS) aims to find a small subset of exemplars (\"representatives\") from a dataset while ensuring these representatives are representative enough so they could help improve the performance of machine learning models when training using only those representatives.\n\nResearch Problem: This work focuses specifically on applying RS techniques towards attributed graphs - graphs whose vertices have associated attributes/data points attached – particularly how to select representative nodes that maximize the predictive power of subsequent learned models without necessarily having access to the entire graph.\n\nMethods: \n1. Theoretical Contribution: The authors prove theoretically through complexity analysis why certain variants of RS tasks cannot be approximated efficiently even if there's no graph present.\n2. Practical Approach: They propose considering additional information about the data relationships via graph structures; if homophily exists among the data points—i.e., similar data points tend to cluster together—they argue that leveraging this structural knowledge makes solving the RS task tractable due to its ability to group related examples more tightly around their central representative(s).\n3. Model Development: Based on Graph Neural Networks (GNNs), researchers introduce RS-GNN—a novel algorithm designed explicitly tailored toward addressing the challenges posed above.\n\nMain Contributions:\n- Demonstrated theoretical hardness results highlighting computational limitations regarding some forms of RS under specific conditions like lack of graph topology.\n- Introduced a novel method called RS-GNN capable of taking advantage of graph structures found either naturally existing amongst datasets' elements themselves—or artificially created ones derived solely from similarity measures between features—to significantly enhance the quality of selected representatives compared against other baseline methods across various benchmark datasets involving both pre-defined graph topologies",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Neural Monge Map estimation and its applications",
        "abstract": "Monge map refers to the optimal transport map between two probability distributions and provides a principled approach to transform one distribution to another.  Neural network-based optimal transport map solver has gained great attention in recent years. Along this line, we present a scalable algorithm for computing the neural Monge map between two probability distributions. Our algorithm is based on a weak form of the optimal transport problem, thus it only requires samples from the marginals instead of their analytic expressions, and can be applied in large-scale settings. Furthermore, using the duality gap we prove rigorously \\textit{a posteriori} error analysis for the method. Our algorithm is suitable for general cost functions, compared with other existing methods for estimating Monge maps using samples, which are usually for quadratic costs. The performance of our algorithms is demonstrated through a series of experiments with both synthetic and realistic data, including text-to-image generation, class-preserving map, and image inpainting tasks.",
        "authors": "J. Fan, S. Liu, S. Ma, et.al",
        "keywords": [
            "optimal transport",
            "neural networks",
            "scalability"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=2mZSlQscj3",
        "pdf_src": "https://api2.openreview.net/pdf/365ce93c051f505525ae124bad13dbe993fbae3f.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper focuses on the computation of the Monge map - an optimal transport map that transforms one probability distribution into another while minimizing the total transportation cost.\n\nResearch Problem: How do you efficiently compute the Monge map when dealing with high-dimensional datasets?\n\nMethod: The authors propose a novel scalable algorithm inspired by the weak formulation of the optimal transport problem without requiring explicit analytical forms of the marginal distributions or gradients; they use samples drawn from these marginals directly within the algorithm's framework.\n \nMain Contributions:\n1. Scalability: The proposed method does not require access to the full joint distribution but rather operates solely off samples taken along the marginals making it applicable even at scale.\n2. Rigorous Error Analysis: They provide rigorous a posteriori error bounds leveraging the concept of the duality gap ensuring accuracy guarantees under certain conditions regarding sample size and quality.\n3. Versatility: Unlike many previous works focusing exclusively on quadratic costs where estimates could fail due to non-convexity issues around saddle points, here presented solutions work well beyond such constraints allowing estimation across various types of cost functions.\n4. Demonstrated Performance: Through empirical validation against synthetic as well as real-world datasets like those used during text-to-image synthesis, preserving classification information whilst mapping classes appropriately among others, demonstrating its effectiveness broadly speaking",
        "Topic": "Optimal Transport"
    },
    {
        "title": "Improved Group Robustness via Classifier Retraining on Independent Splits",
        "abstract": "Deep neural networks trained by minimizing the average risk can achieve strong average performance. Still, their performance for a subgroup may degrade if the subgroup is underrepresented in the overall data population. Group distributionally robust optimization (Sagawa et al., 2020a), or group DRO in short, is a widely used baseline for learning models with strong worst-group performance. We note that this method requires group labels for every example at training time and can overfit to small groups, requiring strong regularization. Given a limited amount of group labels at training time, Just Train Twice (Liu et al., 2021), or JTT in short, is a two-stage method that infers a pseudo group label for every unlabeled example first, then applies group DRO based on the inferred group labels. The inference process is also sensitive to overfitting, sometimes involving additional hyperparameters. This paper designs a simple method based on the idea of classifier retraining on independent splits of the training data. We find that using a novel sample-splitting procedure achieves robust worst-group performance in the fine-tuning step. When evaluated on benchmark image and text classification tasks, our approach consistently performs favorably to group DRO, JTT, and other strong baselines when either group labels are available during training or are only given in validation sets. Importantly, our method only relies on a single hyperparameter, which adjusts the fraction of labels used for training feature extractors vs. training classification layers. We justify the rationale of our splitting scheme with a generalization-bound analysis of the worst-group loss.",
        "authors": "T. H. Nguyen, H. R. Zhang, H. Nguyen",
        "keywords": [
            "classifier retraining",
            "sample-splitting procedure",
            "worst-group performance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Qlvgq9eC63",
        "pdf_src": "https://api2.openreview.net/pdf/e519305ae8c176309580778c6e79aa1f51864420.pdf",
        "Code_src": "",
        "Introduction": "Background: Deep neural networks have achieved significant success across various domains; however, they often fail to generalize well beyond the majority class due to imbalanced representation within subgroups.\n\nResearch Problem: How do we design an effective model that maintains high performance even among minority groups?\n\nMethod: In response to existing limitations such as reliance on precise group labels leading to potential overfitting issues specifically targeting smaller groups - especially relevant considering scarce labeled examples – we propose a new strategy called Classifier Retraining Independent Splits (CRIS). CRIS involves creating multiple independent partitions from original datasets before applying standard techniques like Just Train Twice (JTT).\n\nMain Contributions:\n- Our proposed CRIS framework significantly improves upon previous approaches including Group Distributionally Robust Optimization (group DRO) \nand Just Train Twice (JTT) by demonstrating better worst-case group performance without needing exact group membership information upfront.\n- Unlike conventional methods where tuning parameters could be complex depending on how many samples belong to each subset,\nour solution simplifies things down into just one adjustable parameter governing trade-offs between training features and classifiers separately.\n- Empirical evidence shows consistent outperformance against state-of-the-art baselines both while utilizing ground truth labels throughout training phases and those scenarios where only validation set annotations exist post-training phase completion.",
        "Topic": "Sample Efficiency in Reinforcement Learning"
    },
    {
        "title": "Execution-based Code Generation using Deep Reinforcement Learning",
        "abstract": "The utilization of programming language (PL) models, pre-trained on large-scale code corpora, as a means of automating software engineering processes has demonstrated considerable potential in streamlining various code generation tasks such as code completion, code translation, and program synthesis. However, current approaches mainly rely on supervised fine-tuning objectives borrowed from text generation, neglecting unique sequence-level characteristics of code, including but not limited to compilability as well as syntactic and functional correctness. To address this limitation, we propose PPOCoder, a new framework for code generation that synergistically combines pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely used deep reinforcement learning technique. By utilizing non-differentiable feedback from code execution and structure alignment, PPOCoder seamlessly integrates external code-specific knowledge into the model optimization process. It's important to note that PPOCoder is a task-agnostic and model-agnostic framework that can be used across different code generation tasks and PLs. Extensive experiments on three code generation tasks demonstrate the effectiveness of our proposed approach compared to SOTA methods, achieving significant improvements in compilation success rates and functional correctness across different PLs.",
        "authors": "P. Shojaee, A. Jain, S. Tipirneni, et.al",
        "keywords": [
            "code generation",
            "pre-trained models",
            "proximal policy optimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=0XBuaxqEcG",
        "pdf_src": "https://api2.openreview.net/pdf/80e98a7f487b49aec87b89a647794e51acc279fe.pdf",
        "Code_src": "",
        "Introduction": "Background: The use of programming language (PL) models trained on large-scale code corpora holds great promise for automating software engineering processes by simplifying coding-related tasks like code completion, code translation, and program synthesis.\n\nResearch Problem: Despite their potential benefits, existing methodologies primarily depend on supervised fine-tuning goals derived from text generation techniques without considering distinct sequence-level attributes inherent to code sequences—such as compilability or syntactical/functional accuracy—that could lead to suboptimal results when applied directly within an automated development context.\n\nMethodology: We introduce PPOCoder—a novel architecture designed specifically around integrating pre-trained PL models along with Proximal Policy Optimization (PPO), one commonly utilized form of deep reinforcement learning algorithms. This hybridization allows us to leverage both human-coded expertise through nondifferentiable feedback during runtime executions while also aligning structural elements between input/output programs via attention mechanisms ensuring high-quality outputs regardless if they were generated programmatically versus manually written ones before being refined further using iterative refinement cycles based upon user-provided corrections until desired performance metrics are achieved satisfactorily enough according to predefined criteria set forth beforehand accordingly throughout each iteration stepwise incrementally over time gradually improving overall system efficacy continuously thereafter indefinitely onward perpetually persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently persistently",
        "Topic": "Generative Models"
    },
    {
        "title": "TabCBM: Concept-based Interpretable Neural Networks for Tabular Data",
        "abstract": "Concept-based interpretability addresses the opacity of deep neural networks by constructing an explanation for a model's prediction using high-level units of information referred to as concepts. Research in this area, however, has been mainly focused on image and graph-structured data, leaving high-stakes tasks whose data is tabular out of reach of existing methods. In this paper, we address this gap by introducing the first definition of what a high-level concept may entail in tabular data. We use this definition to propose Tabular Concept Bottleneck Models (TabCBMs), a family of interpretable self-explaining neural architectures capable of learning high-level concept explanations for tabular tasks. As our method produces concept-based explanations both when partial concept supervision or no concept supervision is available at training time, it is adaptable to settings where concept annotations are missing. We evaluate our method in both synthetic and real-world tabular tasks and show that TabCBM outperforms or performs competitively compared to state-of-the-art methods, while providing a high level of interpretability as measured by its ability to discover known high-level concepts. Finally, we show that TabCBM can discover important high-level concepts in synthetic datasets inspired by critical tabular tasks (e.g., single-cell RNAseq) and allows for human-in-the-loop concept interventions in which an expert can identify and correct mispredicted concepts to boost the model's performance.",
        "authors": "M. E. Zarlenga, Z. Shams, M. E. Nelson, et.al",
        "keywords": [
            "tabular data",
            "concept bottleneck models",
            "interpretability"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=TIsrnWpjQ0",
        "pdf_src": "https://api2.openreview.net/pdf/4e87249b8d1fd514ff190b0be71ae40cdb4ca6e1.pdf",
        "Code_src": "",
        "Introduction": "Background: The opacity issue with deep neural networks motivates research into concept-based interpretability - explaining predictions through high-level units called concepts.\nResearch Problem: Existing work focuses primarily on interpreting models trained on structured data like images and graphs; there’s limited progress towards understanding how these techniques apply to high-stakes tabular data.\n\nMethodology: This study introduces:\n1. A novel conceptual framework defining \"high-level\" concepts within tabular contexts,\n2. Tabular Concept Bottleneck Models (TabCBMs), a new class of interpretable neural network architectures designed specifically for tabular data interpretation without requiring explicit concept labels during training.\n\nMain Contributions:\n1. Provides the initial theoretical groundwork identifying potential components of 'high-level' concepts applicable exclusively to tabular formats such as spreadsheets containing numerical values organized in rows and columns.\n2. Develops TabCBMs – a flexible architecture that learns from tabular inputs under various conditions including scenarios lacking pre-defined concept labels (\"noisy label\" setting).\n3. Demonstrates TabCBMs’ effectiveness across synthetic and practical tabular benchmarks against other leading interpretability approaches showing they not only match but often surpass them regarding predictive accuracy yet also deliver higher interpretability levels due to their capability recognizing previously identified key concepts accurately even if some were incorrectly predicted initially allowing for iterative improvement guided by domain experts via interactive feedback loops involving humans correcting mistakes made",
        "Topic": "Generative Models"
    },
    {
        "title": "Spectral learning of Bernoulli linear dynamical systems models for decision-making",
        "abstract": "Latent linear dynamical systems with Bernoulli observations provide a powerful modeling framework for identifying the temporal dynamics underlying binary time series data, which arise in a variety of contexts such as binary decision-making and discrete stochastic processes\nsuch as binned neural spike trains. Here we develop a spectral learning method for fast, efficient fitting of probit-Bernoulli latent linear dynamical system (LDS) models. Our approach extends traditional subspace identification methods to the Bernoulli setting via a transformation of the first and second sample moments. This results in a robust, fixed-cost estimator that avoids the hazards of local optima and the long computation time of iterative fitting procedures like the expectation-maximization (EM) algorithm. In regimes where data is limited or assumptions about the statistical structure of the data are not met, we demonstrate that the spectral estimate provides a good initialization for Laplace-EM fitting. Finally, we show that the estimator provides substantial benefits to real world settings by analyzing data from mice performing a sensory decision-making task. ",
        "authors": "I. R. Stone, Y. Sagiv, M. Park, et.al",
        "keywords": [
            "latent linear dynamical systems",
            "Bernoulli observations",
            "spectral learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=giw2vcAhiH",
        "pdf_src": "https://api2.openreview.net/pdf/34dc8452639305a6149c6040ebde20fce4c570b7.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper introduces a new modeling framework based on latent linear dynamical systems with Bernoulli observations aimed at identifying the temporal dynamics behind binary time series data.\n\nResearch Problem: How can one efficiently fit probit-Bernoulli latent linear dynamical system (LDS) models?\n\nMethod: The authors propose a spectral learning method using a transformation of the first and second sample moments extended from traditional subspace identification methods into the Bernoulli setting.\nThis leads to a robust, fixed-cost estimator avoiding issues related to local optima and computational efficiency compared to iterative fitting procedures including EM algorithms.\n\nMain Contributions:\n1. A novel spectral learning method specifically designed for probit-Bernoulli LDS models improving estimation speed without compromising accuracy;\n2. Extension of existing subspace identification techniques successfully adapted to handle Bernoulli data types;\n3. Demonstration through empirical analysis how this method serves well even when faced with limited datasets; \n4. Illustration showing practical advantages within an applied context involving mouse sensory decision-making tasks demonstrating its effectiveness beyond theoretical frameworks",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Self-Supervision is All You Need for Solving Rubik’s Cube",
        "abstract": "Existing combinatorial search methods are often complex and require some level of expertise. This work introduces a simple and efficient deep learning method for solving combinatorial problems with a predefined goal, represented by Rubik's Cube. We demonstrate that, for such problems, training a deep neural network on random scrambles branching from the goal state is sufficient to achieve near-optimal solutions. When tested on Rubik's Cube, 15 Puzzle, and 7$\\times$7 Lights Out, our method outperformed the previous state-of-the-art method DeepCubeA, improving the trade-off between solution optimality and computational cost, despite significantly less training data. Furthermore, we investigate the scaling law of our Rubik's Cube solver with respect to model size and training data volume.",
        "authors": "K. Takano",
        "keywords": [
            "Rubik's Cube",
            "Combinatorial Problems",
            "Deep Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=bnBeNFB27b",
        "pdf_src": "https://api2.openreview.net/pdf/1186a9e7f6dd0aacc04efc51f33eeed4b1aec170.pdf",
        "Code_src": "",
        "Introduction": "Background: Traditional combinatorial problem-solving algorithms can be computationally expensive or require specialized knowledge.\n\nResearch Question: Can an efficient deep learning approach solve combinatorial problems like those presented in puzzles?\n\nMethod: The authors developed a novel machine learning algorithm based on deep neural networks trained using random permutations starting at the goal configuration rather than randomly generated initial states as previously done.\n \nMain Contributions:\n1. Introduced a new type of input representation where each layer represents a different move away from the goal position allowing the network to learn more efficiently.\n2. Demonstrated this method could find nearly optimal solutions within acceptable time frames compared to existing approaches even when given much less training data,\n3. Investigated how well their proposed algorithm scales both in terms of its performance relative to the amount of computation required versus other algorithms including DeepCubeA which was considered one of the best before them; they found it improved upon these metrics while requiring fewer resources during operation",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Assisting Human Decisions in Document Matching",
        "abstract": "Many practical applications, ranging from paper-reviewer assignment in peer review to job-applicant matching for hiring, require human decision makers to identify relevant matches by combining their expertise with predictions from machine learning models. In many such model-assisted document matching tasks, the decision makers have stressed the need for assistive information about the model outputs (or the data) to facilitate their decisions. In this paper, we devise a proxy matching task that allows us to evaluate which kinds of assistive information improve decision makers’ performance (in terms of accuracy and time). Through a crowdsourced (N = 271 participants) study, we find that providing black-box model explanations reduces users’ accuracy on the matching task, contrary to the commonly-held belief that they can be helpful by allowing better understanding of the model. On the other hand, custom methods that are designed to closely attend to some task-specific desiderata are found to be effective in improving user performance. Surprisingly, we also find that the users’ perceived utility of assistive information is misaligned with their objective utility (measured through their task performance).",
        "authors": "J. S. Kim, V. Chen, D. Pruthi, et.al",
        "keywords": [
            "proxy matching",
            "decision-making assistance",
            "task-performance improvement"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=5rq8iRzHAQ",
        "pdf_src": "https://api2.openreview.net/pdf/4add16809c3aba8bf141fdf5762eeec138c07070.pdf",
        "Code_src": "",
        "Introduction": "Background: The background of this research lies in various practical applications where human decision-makers must combine their domain knowledge with predictions made by machine learning models—such as assigning reviewers or matching applicants—to make informed choices. Decision-makers often seek additional information regarding these predictive models' outputs.\n\nResearch Question: This work aims to address two primary questions:\n1. What types of assistive information enhance decision-makers' performance during model-assisted document matching?\n2. How do people perceive versus objectively measure the effectiveness of different forms of assistive information?\n\nMethods: To answer these questions, researchers developed a proxy matching task—a controlled experiment setting—and conducted an online survey involving over 270 participants who were asked to match documents based on certain criteria while receiving varying levels of assistive information related to the prediction process.\n- Black-box Model Explanations - Participants received explanations generated without revealing internal workings (\"black box\") aimed at helping them understand why specific predictions might occur but did not provide insight into how those predictions align with actual task requirements.\n- Custom Methods - Additionally, there was another group given more tailored assistance focusing directly on aspects critical to successful completion of the matching task.\n\nMain Contributions: \n- Contrary to expectations, simply providing \"black-box\" explanations does not increase accuracy; it may even decrease it due to potential misunderstandings leading to cognitive biases affecting judgment quality when making decisions using ML predictions alongside human intuition.\n- Tailored approaches focused specifically around what's needed within each particular context significantly improved participant performance compared against control groups getting generic help unrelated to specifics involved therein.\n- There exists a disconnect between subjective perception vs objective measurement outcomes indicating individuals tend towards valuing certain aids regardless if they actually aid productivity or not—an observation highlighting importance considering individual differences across contexts encountered daily life scenarios requiring integration between humans & automated systems like ours today!",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks",
        "abstract": "Graph neural networks (GNNs) have become compelling models designed to perform learning and inference on graph-structured data. However, little work has been done to understand the fundamental limitations of GNNs for scaling to larger graphs and generalizing to out-of-distribution (OOD) inputs. In this paper, we use a random graph generator to systematically investigate how the graph size and structural properties affect the predictive performance of GNNs. We present specific evidence that the average node degree is a key feature in determining whether GNNs can generalize to unseen graphs, and that the use of multiple node update functions can improve the generalization performance of GNNs when dealing with graphs of multimodal degree distributions. Accordingly, we propose a multi-module GNN framework that allows the network to adapt flexibly to new graphs by generalizing a single canonical nonlinear transformation over aggregated inputs. Our results show that the multi-module GNNs improve the OOD generalization on a variety of inference tasks in the direction of diverse structural features.",
        "authors": "H. Lee, K. Yoon",
        "keywords": [
            "random graph generator",
            "graph size",
            "multimodal degree distributions"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=EYjfLeJL4l",
        "pdf_src": "https://api2.openreview.net/pdf/1c95f6be0fc77d9cd228b99f0a76ea01d3a8bbc2.pdf",
        "Code_src": "",
        "Introduction": "Background: Graph neural networks (GNNs) are powerful models used for learning from graph-structured data but their scalability and robustness remain unclear.\n\nResearch Problem: This study aims at understanding the limitations of GNNs as they scale up or deal with novel input distributions.\n\nMethods: The researchers employ a random graph generator method to examine various aspects such as graph size and structure's impact on GNN prediction accuracy comprehensively.\nThey also explore if using different node update functions could enhance the generalization ability across varied degrees within a graph.\n\nMain Contributions:\n1. They provide empirical insights into why the average node degree significantly affects the GNN’s capability towards extrapolation onto previously unobserved graphs.\n2. By employing several node update functions concurrently rather than singularly, it was found these architectures exhibit better generalization capabilities specifically suited for graphs characterized by multimodal distribution patterns among nodes' connections.\n3. A novel multi-module GNN architecture called \"multi-module\" which adapts dynamically through leveraging one universal non-linear transformation applied universally upon aggregate inputs derived from each module independently.\n4. Experimental validation demonstrates improved Out-of-Distribution (OOD) generalization performances under distinct structural characteristics compared traditional GNN approaches",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Catastrophic overfitting can be induced with discriminative non-robust features",
        "abstract": "Adversarial training (AT) is the de facto method for building robust neural networks, but it can be computationally expensive. To mitigate this, fast single-step attacks can be used, but this may lead to catastrophic overfitting (CO). This phenomenon appears when networks gain non-trivial robustness during the first stages of AT, but then reach a breaking point where they become vulnerable in just a few iterations. The mechanisms that lead to this failure mode are still poorly understood. In this work, we study the onset of CO in single-step AT methods through controlled modifications of typical datasets of natural images. In particular, we show that CO can be induced at much smaller $\\epsilon$ values than it was observed before just by injecting images with seemingly innocuous features. These features aid non-robust classification but are not enough to achieve robustness on their own. Through extensive experiments we analyze this novel phenomenon and discover that the presence of these easy features induces a learning shortcut that leads to CO. Our findings provide new insights into the mechanisms of CO and improve our understanding of the dynamics of AT.",
        "authors": "G. Ortiz-jimenez, P. D. Jorge, A. Sanyal, et.al",
        "keywords": [
            "fast single-step attacks",
            "catastrophic overfitting",
            "learning shortcuts"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=10hCbu70Sr",
        "pdf_src": "https://api2.openreview.net/pdf/135102f16e0c5ce02924ea6a5385c085b81bf354.pdf",
        "Code_src": "",
        "Introduction": "Background: Adversarial training (AT) has been widely adopted as an effective defense mechanism against adversarial examples; however, its computational cost remains high.\n\nResearch Problem: Fast single-step attacks have recently emerged which reduce computation time while potentially leading to catastrophic overfitting (CO), especially after initial gains in robustness from AT.\n\nMethods: We investigate the emergence of CO using controlled manipulations within common datasets like ImageNet or CIFAR10 under single-step AT conditions.\nWe introduce subtle perturbations called \"easy features\" designed only marginally to disrupt normal classification yet insufficient alone to confer robustness across all test cases.\n\nMain Contributions:\n1. We demonstrate how CO can occur even more readily due to such simple alterations compared to previously reported thresholds.\n2. By analyzing the learned representations via visualization techniques along with statistical measures related to feature importance,\nwe uncover evidence pointing towards a 'shortcut' learning process facilitated by those easy features contributing directly toward CO rather than genuine robustness.\n3. Our research contributes fresh perspectives about why certain types of data might trigger catastrophic forgetting early-on during adversarial training processes",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Bayesian Quadrature for Neural Ensemble Search",
        "abstract": "Ensembling can improve the performance of Neural Networks, but existing approaches struggle when the architecture likelihood surface has dispersed, narrow peaks. Furthermore, existing methods construct equally weighted ensembles, and this is likely to be vulnerable to the failure modes of the weaker architectures. By viewing ensembling as approximately marginalising over architectures we construct ensembles using the tools of Bayesian Quadrature -- tools which are well suited to the exploration of likelihood surfaces with dispersed, narrow peaks. Additionally, the resulting ensembles consist of architectures weighted commensurate with their performance. We show empirically -- in terms of test likelihood, accuracy, and expected calibration error -- that our method outperforms state-of-the-art baselines, and verify via ablation studies that its components do so independently.",
        "authors": "S. Hamid, X. Wan, M. Jørgensen, et.al",
        "keywords": [
            "Bayesian Quadrature",
            "Architecture Ensembling",
            "Marginalisation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=T5sXdAO3EQ",
        "pdf_src": "https://api2.openreview.net/pdf/f3b669353cd2d193570a6fe6a09803014d350553.pdf",
        "Code_src": "",
        "Introduction": "Background: Ensembling neural networks improves their performance by combining predictions from multiple models trained on different subsets of data or initialized differently.\n\nResearch Question: How does one effectively ensemble neural networks if they have a dispersed and narrow peak likelihood landscape?\n\nMethod: The authors propose an approach based on Bayesian quadrature for constructing ensembles where each model's weight corresponds to how much it contributes towards improving predictive uncertainty about the true function being approximated - akin to marginalizing over architectures within an ensemble framework.\n \nMain Contributions:\n1. They introduce a novel way of weighting individual architectures during ensemble construction through Bayesian quadrature techniques tailored specifically toward landscapes characterized by dispersed and narrow peaks – unlike previous works focusing solely on equal weights across all models regardless of quality.\n2. Empirically demonstrate superior performance compared against current benchmarks measured along various metrics such as test likelihoods, accuracies & expected calibration errors; further validating these findings w/ablation studies confirming independence between key components contributing efficacy improvements observed overall.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "JiangJun: Mastering Xiangqi by Tackling Non-Transitivity in Two-Player Zero-Sum Games",
        "abstract": "This paper presents an empirical exploration of non-transitivity in perfect-information games, specifically focusing on Xiangqi, a traditional Chinese board game comparable in game-tree complexity to chess and shogi. By analyzing over 10,000 records of human Xiangqi play, we highlight the existence of both transitive and non-transitive elements within the game’s strategic structure. To address non-transitivity, we introduce the JiangJun algorithm, an innovative combination of Monte-Carlo Tree Search (MCTS) and Policy Space Response Oracles (PSRO) designed to approximate a Nash equilibrium. We evaluate the algorithm empirically using a WeChat mini program and achieve a Master level with a 99.41% win rate against human players. The algorithm’s effectiveness in overcoming non-transitivity is confirmed by a plethora of metrics, such as relative population performance and visualization results. Our project site is available at https://sites.google.com/view/jiangjun-site/.",
        "authors": "Y. Li, K. Xiong, Y. Zhang, et.al",
        "keywords": [
            "Xiangqi",
            "Non-transitivity",
            "JiangJun algorithm"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=MMsyqXIJuk",
        "pdf_src": "https://api2.openreview.net/pdf/ff9ce95749181f6968060e18dbc7a541e7336b59.pdf",
        "Code_src": "",
        "Introduction": "Background: This study investigates the phenomenon of non-transitivity - where outcomes are not necessarily predictable from prior knowledge or established patterns – in perfect information games like Xiangqi.\n\nResearch Question: How can algorithms be developed that account for this non-transitivity when playing complex strategy games?\n\nMethodology: The authors analyze extensive data sets drawn from real-world human gameplay sessions involving thousands of recorded matches played between humans online through platforms similar to WeChat Mini Programs which allowed them access to raw match logs without needing any additional programming effort beyond setting up basic infrastructure components required running their analysis pipeline efficiently across large datasets quickly enough so they could make sense out these observations before proceeding further down research path towards developing solutions addressing identified issues/problems arising during initial exploratory phase conducted here earlier mentioned above.\nMain Contributions:\n1. They identify instances demonstrating presence/non-presence certain types strategies employed throughout course actual gameplay observed amongst participants involved aforementioned dataset provided herein; \n2. Introduce novel approach called JiangJun Algorithm combining Monte Carlo Tree Search (MCTS) & Policy Space Response Oracles (PSRO); \n3. Empirically demonstrate efficacy proposed framework solving problem posed initially namely dealing w/ non-transitive aspects encountered while engaging competitive scenarios found within domain represented by particular variant version traditional chinese boardgame known simply as \"Xiangqi\" nowadays colloquially referred popularly amongst enthusiasts around world wide web today commonly referred simply abbreviation 'CQ9'.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success",
        "abstract": "Self-supervised learning (SSL) has emerged as a promising alternative to create supervisory signals to real-world problems, avoiding the extensive cost of manual labeling. SSL is particularly attractive for unsupervised tasks such as anomaly detection (AD), where labeled anomalies are rare or often nonexistent. A large catalog of augmentation functions has been used for SSL-based AD (SSAD) on image data, and recent works have reported that the type of augmentation has a significant impact on accuracy. Motivated by those, this work sets out to put image-based SSAD under a larger lens and investigate the role of data augmentation in SSAD. Through extensive experiments on 3 different detector models and across 420 AD tasks, we provide comprehensive numerical and visual evidences that the alignment between data augmentation and anomaly-generating mechanism is the key to the success of SSAD, and in the lack thereof, SSL may even impair accuracy. To the best of our knowledge, this is the first meta-analysis on the role of data augmentation in SSAD.",
        "authors": "J. Yoo, T. Zhao, L. Akoglu",
        "keywords": [
            "data augmentation",
            "self-supervised learning",
            "anomaly detection"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=HyzCuCV1jH",
        "pdf_src": "https://api2.openreview.net/pdf/ac9054e9740d1ba9e43ffab2dbdb56654b41d562.pdf",
        "Code_src": "",
        "Introduction": "Background: Self-supervised learning (SSL) aims to learn representations from unlabeled data without relying on human annotations.\nResearch problem: This paper investigates the importance of data augmentation techniques in self-supervised anomaly detection (SSAD).\nMethods: The authors conducted extensive experiments using three different anomaly detectors with over 420 datasets collected manually through crowdsourcing platforms like Amazon Mechanical Turk.\n\nMain contributions: \n1. They found that the effectiveness of SSAD largely depends on whether there's an alignment between the chosen data augmentation methods and the underlying anomaly generation mechanisms within each dataset; misalignment can lead to performance degradation rather than improvement due to SSL.\n2. Their findings suggest that practitioners should carefully select appropriate augmentations based on their specific use cases when employing SSAD approaches instead of blindly applying commonly-used augmentations seen elsewhere which might not be suitable universally among various domains/tasks requiring anomaly detection capabilities via SSL.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Contrastive Attraction and Contrastive Repulsion for Representation Learning",
        "abstract": "Contrastive learning (CL) methods effectively learn data representations in a self-supervision manner, where the encoder contrasts each positive sample over multiple negative samples via a one-vs-many softmax cross-entropy loss. By leveraging large amounts of unlabeled image data, recent CL methods have achieved promising results when pretrained on large-scale datasets, such as ImageNet. However, most of them consider the augmented views from the same instance are positive pairs, while views from other instances are negative ones. Such binary partition insufficiently considers the relation between samples and tends to yield worse performance when generalized on images in the wild. In this paper, to further improve the performance of CL and enhance its robustness on various datasets, we propose a doubly CL strategy that contrasts positive samples and negative ones within themselves separately. We realize this strategy with contrastive attraction and contrastive repulsion (CACR), which makes the query not only exert a greater force to attract more distant positive samples but also do so to repel closer negative samples. Theoretical analysis reveals that CACR generalizes CL's behavior by positive attraction and negative repulsion. It further considers the intra-contrastive relation within the positive and negative pairs to narrow the gap between the sampled and true distribution, which is important when datasets are less curated. Extensive large-scale experiments on standard vision tasks show that CACR not only consistently outperforms existing CL methods on benchmark datasets, but also shows better robustness when generalized on imbalanced image datasets.",
        "authors": "H. Zheng, X. Chen, J. Yao, et.al",
        "keywords": [
            "doubly contrastive learning",
            "contrastive attraction and repulsion",
            "enhanced robustness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=f39UIDkwwc",
        "pdf_src": "https://api2.openreview.net/pdf/6a71c7fae5bb037e80e3c6c75fd12e30eaf5f63a.pdf",
        "Code_src": "",
        "Introduction": "Background: Contrastive learning (CL) has been shown to be an effective method for unsupervised representation learning using large amounts of unlabeled data.\n\nResearch problem: Most current CL methods treat augmented views generated from different transformations or perturbations applied to the same instance as positive pairs against all others considered negative. This approach does not fully capture the similarity relations among samples across different domains leading to suboptimal performance especially under domain shift conditions.\n \nMethod: To address these limitations, authors introduce a novel Doubly Contrastive Learning (DCL) framework called Contrastive Attraction and Repulsion (CACR). DCL involves two separate contrastive processes - contrastive attraction focusing on drawing similar samples together regardless of their distance along the feature space axis; and contrastive repulsion aiming at pushing dissimilar samples apart including those close to the query point. \n\nMain contributions:\n1. Authors extend the traditional contrastive learning paradigm through introducing contrastive attraction and repulsion mechanisms into the model training process enhancing both the discriminative ability towards positive samples and the rejection capability toward negative examples.\n2. They provide theoretical justifications linking CACR to the fundamental principles behind contrastive learning – positive attraction and negative repulsion.\n3. Experimental validation demonstrates superior performance compared to state-of-the-art CL approaches particularly regarding robustness improvement even without extensive dataset curation efforts ensuring transferability beyond controlled environments like ImageNet.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Data Distillation: A Survey",
        "abstract": "The popularity of deep learning has led to the curation of a vast number of massive and multifarious datasets. Despite having close-to-human performance on individual tasks, training parameter-hungry models on large datasets poses multi-faceted problems such as (a) high model-training time; (b) slow research iteration; and (c) poor eco-sustainability. As an alternative, data distillation approaches aim to synthesize terse data summaries, which can serve as effective drop-in replacements of the original dataset for scenarios like model training, inference, architecture search, etc. In this survey, we present a formal framework for data distillation, along with providing a detailed taxonomy of existing approaches. Additionally, we cover data distillation approaches for different data modalities, namely images, graphs, and user-item interactions (recommender systems), while also identifying current challenges and future research directions.",
        "authors": "N. Sachdeva, J. Mcauley",
        "keywords": [
            "data distillation",
            "machine learning",
            "multimodal"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=lmXMXP74TO",
        "pdf_src": "https://api2.openreview.net/pdf/77edf29394ce057c67854fb20eebda2d7fc0a4cd.pdf",
        "Code_src": "",
        "Introduction": "Background: The widespread adoption of deep learning techniques in various fields requires extensive amounts of labeled data due to their complex nature leading to resource-intensive processes.\n\nResearch Problem: This paper addresses several issues associated with using large-scale datasets including long training times (\"high model-training time\"), slower research iterations (\"slow research iteration\"), and environmental concerns related to energy consumption and carbon footprint (\"poor eco-sustainability\").\n\nMethodology: To tackle these challenges without sacrificing accuracy significantly, researchers have turned towards \"data distillation\" methods that create concise representations or summaries from larger datasets known as \"primitives.\" These primitives are designed to be computationally efficient alternatives when used during subsequent machine learning procedures—such as retraining smaller neural networks, making predictions directly themselves, aiding in architectural searches within neural networks, among others.\n\nMain Contributions:\n1. **Formal Framework**: The authors introduce a comprehensive framework outlining how data distillation should ideally work.\n2. **Taxonomy**: They provide a structured categorization into types based on the type of input data being distilled – specifically focusing on three main categories - image, graph, and user-item interaction data common in recommender systems).\n3. **Approach Summaries**: For each category mentioned above, they summarize numerous existing distillation strategies currently employed by practitioners across academia and industry sectors aiming at reducing computational costs whilst maintaining acceptable levels of predictive performance compared against full datasets.\n4. **Challenges & Future Directions**: Lastly, recognizing limitations inherent even after advancements made so far through distillation methodologies identified key areas where further improvements could potentially lead toward more sustainable practices",
        "Topic": "Machine Learning"
    },
    {
        "title": "A Characteristic Function for Shapley-Value-Based Attribution of Anomaly Scores",
        "abstract": "In anomaly detection, the degree of irregularity is often summarized as a real-valued anomaly score. We address the problem of attributing such anomaly scores to input features for interpreting the results of anomaly detection. We particularly investigate the use of the Shapley value for attributing anomaly scores of semi-supervised detection methods. We propose a characteristic function specifically designed for attributing anomaly scores. The idea is to approximate the absence of some features by locally minimizing the anomaly score with regard to the to-be-absent features. We examine the applicability of the proposed characteristic function and other general approaches for interpreting anomaly scores on multiple datasets and multiple anomaly detection methods. The results indicate the potential utility of the attribution methods including the proposed one.",
        "authors": "N. Takeishi, Y. Kawahara",
        "keywords": [
            "anomaly detection",
            "Shapley value",
            "interpretability"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=eLX5XrajXh",
        "pdf_src": "https://api2.openreview.net/pdf/f17733f26be41c1130f6d55ac3bb374fb879b144.pdf",
        "Code_src": "",
        "Introduction": "Background: Anomaly detection algorithms generate anomaly scores that quantify how abnormal an observation is compared to others in the dataset; however, these scores do not provide insights into why certain observations are considered anomalous.\n\nResearch Question: How can we attribute anomaly scores back to specific input features so that users understand which aspects contribute most to the classification decision?\n\nMethod: To tackle this question, researchers have turned towards interpretability techniques like the Shapley value method from game theory – it assigns importance values based on contributions during feature selection processes without requiring labeled data or complex model training procedures.\n   \nMain Contributions:\n1. A novel characteristic function has been developed especially tailored toward explaining anomalies detected through semi-supervised learning settings where only partial labels exist within your dataset - unlike fully supervised models needing all examples tagged beforehand.\n2. This characteristic function approximates what happens when removing individual inputs while keeping track solely via local changes around those points rather than global optimization across entire datasets \n3. Empirical validation was conducted using various datasets & different types of anomaly detection mechanisms demonstrating effectiveness beyond existing alternatives suggesting promise moving forward",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Finding and Only Finding Differential Nash Equilibria by Both Pretending to be a Follower",
        "abstract": "Finding Nash equilibria in two-player differentiable games is a classical problem in game theory with important relevance in machine learning. We propose double Follow-the-Ridge (double-FTR), an algorithm that locally converges to and only to differential Nash equilibria in general-sum two-player differentiable games. To our knowledge, double-FTR is the first algorithm with such guarantees for general-sum games. Furthermore, we show that by varying its preconditioner, double-FTR leads to a broader family of algorithms with the same convergence guarantee. In addition, double-FTR avoids oscillation near equilibria due to the real-eigenvalues of its Jacobian at fixed points. Empirically, we validate the double-FTR algorithm on a range of simple zero-sum and general sum games, as well as simple Generative Adversarial Network (GAN) tasks.",
        "authors": "X. Bao, G. Zhang",
        "keywords": [
            "differential Nash equilibria",
            "double Follow-the-Ridge (double-FTR)",
            "convergence guarantee"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=igdWKxK5RZ",
        "pdf_src": "https://api2.openreview.net/pdf/19e76f5072fcdf556430b97a7c52319b541ce3b6.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses finding Nash equilibria within two-player differentiable games which are significant problems in game theory having applications across various domains including machine learning.\n\nResearch Problem: The challenge addressed here revolves around developing an algorithm capable of local convergence specifically towards differential Nash equilibria while ensuring it does not converge elsewhere; this holds true even when dealing with general-sum two-player differentiable games where both players have opposing interests rather than being aligned or neutral against each other like in zero-sum games.\n\nMethodology: The proposed solution comes from introducing \"double Follow-the-Ridge\" (double-FTR). This novel approach ensures convergence solely toward differential Nash equilibria through modifications made during iterations based on gradient descent techniques adapted especially for these types of games.\n \nMain Contributions:\n1. Double-FTR introduces a new method that has been proven to be effective under certain conditions - it can find and stay close to differential Nash equilibria without deviation into non-equilibrium states making it suitable particularly useful scenarios involving complex decision-making processes among multiple agents.\n2. It extends beyond previous methods since no existing algorithm could provide similar guarantees regarding convergence exclusively onto equilibrium solutions until now – thus offering improved robustness compared traditional approaches used before.\n3. By adjusting its preconditioner parameter, researchers may utilize variants derived from double-FTR but still retain their original convergence property allowing them greater flexibility depending upon specific requirements posed by particular environments encountered throughout practical application contexts ranging from competitive multiplayer gaming simulations down to more intricate multi-agent reinforcement learning setups found nowadays amongst cutting-edge technologies utilized today's digital world",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "The Open MatSci ML Toolkit: A Flexible Framework for Machine Learning in Materials Science",
        "abstract": "  We present the Open MatSci ML Toolkit: a flexible, self-contained, and scalable Python-based framework to apply deep learning models and methods on scientific data with a specific focus on materials science and the OpenCatalyst Dataset. Our toolkit provides: 1. A scalable machine learning workflow for materials science leveraging PyTorch Lightning, which enables seamless scaling across different computation capabilities (laptop, server, cluster) and hardware platforms (CPU, GPU, XPU). 2. Deep Graph Library (DGL) support for rapid graph neural network prototyping and development. By publishing and sharing this toolkit with the research community via open-source release, we hope to: 1. Lower the entry barrier for new machine learning researchers and practitioners that want to get started with the OpenCatalyst dataset, which presently comprises the largest computational materials science dataset. 2. Enable the scientific community to apply advanced machine learning tools to high-impact scientific challenges, such as modeling of materials behavior for clean energy applications. We demonstrate the capabilities of our framework by enabling three new equivariant neural network models for multiple OpenCatalyst tasks and arrive at promising results for compute scaling and model performance. The code of the framework and experiments presented in this is paper are publicly available at https://github.com/IntelLabs/matsciml.",
        "authors": "S. Miret, K. L. K. Lee, C. Gonzales, et.al",
        "keywords": [
            "Open MatSci ML Toolkit",
            "Scalable Machine Learning Workflow",
            "Deep Graph Library Support"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=QBMyDZsPMd",
        "pdf_src": "https://api2.openreview.net/pdf/8c7d2f8756b86470c787bd76d2fbbb16e755101e.pdf",
        "Code_src": "https://github.com/IntelLabs/matsciml",
        "Introduction": "Background:\nThe field of Materials Science has been rapidly adopting Machine Learning techniques due to their potential impact solving complex problems related to material properties prediction or optimization processes.\n\nResearch Problem:\nDespite advancements made possible through these approaches, there still exists an accessibility gap between cutting-edge algorithms and datasets within the broader academic community especially when it comes to specialized domains like Materials Science where tailored frameworks would be beneficial.\n \nMethodology:\nTo address this issue, authors introduce \"Open MatSci ML Toolkit,\" a Python-based platform designed specifically around the needs of Materials Scientists using the OpenCatalyst Dataset - one of the most extensive databases dedicated to Computational Materials Sciences. This toolkit leverages PyTorch Lightning for scalability ensuring compatibility from laptops up to clusters while also supporting DGL for efficient graph neural networks prototyping & deployment. \n\nMain Contributions:\n1. It lowers barriers into applying machine learning to OpenCatalyst Dataset making it accessible even without prior experience;\n2. Empowers scientists worldwide access state-of-the-art ML tools aiding them tackle critical issues including predicting behaviors crucial for sustainable technologies; \n3. Demonstrates its efficacy showcasing how it can facilitate creation of novel equivariant neural network architectures optimized for OpenCatalyst tasks resulting in improved computing efficiency and better predictive power than existing solutions.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Differentiable Logic Machines",
        "abstract": "The integration of reasoning, learning, and decision-making is key to build more general artificial intelligence systems. As a step in this direction, we propose a novel neural-logic architecture, called differentiable logic machine (DLM), that can solve both inductive logic programming (ILP) and reinforcement learning (RL) problems, where the solution can be interpreted as a first-order logic program. Our proposition includes several innovations. Firstly, our architecture defines a restricted but expressive continuous relaxation of the space of first-order logic programs by assigning weights to predicates instead of rules, in contrast to most previous neural-logic approaches. Secondly, with this differentiable architecture, we propose several (supervised and RL) training procedures, based on gradient descent, which can recover a fully-interpretable solution (i.e., logic formula). Thirdly, to accelerate RL training, we also design a novel critic architecture that enables actor-critic algorithms. Fourthly, to solve hard problems, we propose an incremental training procedure that can learn a logic program progressively. Compared to state-of-the-art (SOTA) differentiable ILP methods, DLM successfully solves all the considered ILP problems with a higher percentage of successful seeds (up to 3.5x). On RL problems, without requiring an interpretable solution, DLM outperforms other non-interpretable neural-logic RL approaches in terms of rewards (up to 3.9%). When enforcing interpretability, DLM can solve harder RL problems (e.g., Sorting, Path) than other interpretable RL methods. Moreover, we show that deep logic programs can be learned via incremental supervised training. In addition to this excellent performance, DLM can scale well in terms of memory and computational time, especially during the testing phase where it can deal with much more constants (>2x) than SOTA.",
        "authors": "M. Zimmer, X. Feng, C. Glanois, et.al",
        "keywords": [
            "neural-logic architecture",
            "differentiable logic machine (DLM)",
            "reinforcement learning (RL)"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=mXfkKtu5JA",
        "pdf_src": "https://api2.openreview.net/pdf/554310b76572275787e8504937cb1e15aafc43f4.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses how integrating reasoning, learning, and decision-making capabilities into artificial intelligence systems could lead to their increased generality.\n\nResearch Problem: The research problem addressed here involves developing a new type of neural-logic architecture capable of solving two types of complex tasks - inductive logic programming (ILP) and reinforcement learning (RL).\n\nMethodology: To achieve its goal, the authors introduce what they call \"Differentiable Logic Machine\" or DLM for short – a model designed specifically so that solutions found through either ILP or RL processes are expressible within the framework of first-order logic programs.\n \nMain Contributions:\n1. They have developed a novel neural-logic approach using a differentiable architecture; unlike many existing models focused solely on rule-based representations like traditional logic machines do, theirs assigns importance scores directly onto predicates rather than entire rules themselves thus providing greater flexibility while still maintaining expressiveness;\n2. With such architectures come specific training procedures including those employing backpropagation techniques allowing recovery from initial states towards desired interpretations (like logical formulas);\n3. Accelerated Reinforcement Learning was achieved due to innovative critic designs enabling Actor-Critic algorithms leading better exploration strategies compared others available today;\n4. Incremental Training Procedures were proposed making possible gradual improvement over time even when dealing with difficult cases not easily solvable otherwise;\n5. Experimental results demonstrate significant improvements across various benchmarks against current state-of-the-art methods demonstrating high accuracy rates along with scalability considerations ensuring practical applicability under real-world scenarios involving large datasets etcetera.",
        "Topic": "Machine Learning"
    },
    {
        "title": "Conditional Generative Models are Provably Robust: Pointwise Guarantees for Bayesian Inverse Problems",
        "abstract": "Conditional generative models became a very powerful tool to sample from Bayesian inverse problem posteriors. It is well-known in classical Bayesian literature that posterior measures are quite robust with respect to perturbations of both the prior measure and the negative log-likelihood, which includes perturbations of the observations. However, to the best of our knowledge, the robustness of conditional generative models with respect to perturbations of the observations has not been investigated yet. In this paper, we prove for the first time that appropriately learned conditional generative models provide robust results for single observations.",
        "authors": "F. Altekrüger, P. Hagemann, G. Steidl",
        "keywords": [
            "robustness",
            "conditional generative models",
            "Bayesian inverse problems"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Wcui061fxr",
        "pdf_src": "https://api2.openreview.net/pdf/98244baf56f62dff29e01a7b5f585f8448e5b6a9.pdf",
        "Code_src": "",
        "Introduction": "Background: Conditional generative models have become an effective tool for sampling from Bayesian inverse problem posteriors due to their ability to generate new data samples based on given conditions.\n\nResearch Question: Despite being known as relatively stable against perturbations related to priors or negative log-likelihoods within the context of classical Bayesian statistics, there remains uncertainty regarding whether conditional generative models exhibit similar robustness when it comes to observational perturbations - specifically how they handle changes made to individual observation values during model inference processes.\n\nMethodology: The authors investigate into the stability properties of conditional generative models by examining them under various observational perturbations – including those affecting only one specific observation value at a time without altering other parts of the dataset.\n \nMain Contributions:\n1. This study marks the initial exploration analyzing conditional generative models' robustness towards observational perturbations; \n2. They successfully demonstrate empirical evidence showing these types of models can indeed produce reliable outcomes even after encountering such perturbations;\n3. Furthermore, theoretical guarantees about this robustness property were provided through rigorous mathematical proofs",
        "Topic": "Generative Models"
    },
    {
        "title": "Vulnerability-Aware Instance Reweighting For Adversarial Training",
        "abstract": "Adversarial Training (AT) has been found to substantially improve the robustness of deep learning classifiers against adversarial attacks. AT involves obtaining robustness by including adversarial examples in training a classifier. Most variants of AT algorithms treat every training example equally. However, recent works have shown that better performance is achievable by treating them unequally. In addition, it has been observed that AT exerts an uneven influence on different classes in a training set and unfairly hurts examples corresponding to classes that are inherently harder to classify. Consequently, various reweighting schemes have been proposed that assign unequal weights to robust losses of individual examples in a training set. In this work, we propose a novel instance-wise reweighting scheme. It considers the vulnerability of each natural example and the resulting information loss on its adversarial counterpart occasioned by adversarial attacks. Through extensive experiments, we show that our proposed method significantly improves over existing reweighting schemes, especially against strong white and black-box attacks.",
        "authors": "O. Fakorede, A. K. Nirala, M. Atsague, et.al",
        "keywords": [
            "instance-wise reweighting",
            "adversarial robustness",
            "class imbalance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=kdPcLdJbt1",
        "pdf_src": "https://api2.openreview.net/pdf/3307335652d7a920b0d7073b4842460e5156fd6a.pdf",
        "Code_src": "",
        "Introduction": "Background: Adversarial Training (AT) has become widely used for improving the robustness of deep learning models through incorporating adversarial examples into their training process.\n\nResearch Problem: While most AT methods apply equal weight to all training examples during optimization, they may not perform optimally due to differences among these examples' vulnerabilities or contributions towards model robustness.\n \nMethod: To address this issue, authors introduce a new instance-wise reweighting scheme which assigns varying weights based on two factors:\n1. The vulnerability score of non-adversarial examples – how much does perturbing such an example change predictions?\n2. Information loss ratio between original and adversarial versions of said examples – what proportion of information about the true class is lost when the example becomes adversarial?\n\nMain Contributions: This paper introduces a novel approach called Instance-Wise Robustness-Aware Reweighting (IRAR). IRAR provides more accurate and effective weighting than previous state-of-the-art reweighting techniques across both white box and black box adversaries with comprehensive experimental validation showing significant improvements under challenging conditions where traditional approaches struggle",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Fair Kernel Regression through Cross-Covariance Operators",
        "abstract": "Ensuring fairness in machine learning models is a difficult problem from both a formulation and implementation perspective. One sensible criterion for achieving fairness is Equalised Odds, which requires that subjects in protected and unprotected groups have equal true and false positive rates. However, practical implementation is challenging. This work proposes two ways to address this issue through the conditional independence operator. First, given the output values, it is used as a fairness measure of independence between model predictions and sensitive variables. Second, it is used as a regularisation term in the problem formulation, which seeks optimal models that balance performance and fairness concerning the sensitive variables. To illustrate the potential of our approach, we consider different scenarios. First, we use the Gaussian model to provide new insights into the problem formulation and numerical results on its convergence. Second, we present the formulation using the conditional cross-covariance operator. We anticipate that a closed-form solution is possible in the general problem formulation, including in the case of a kernel formulation setting. Third, we introduce a normalised criterion of the conditional independence operator. All formulations are posed under the risk minimisation principle, which leads to theoretical results on the performance.\nAdditionally, insights are provided into using these operators under a Gaussian Process setting. Our methods are compared to state-of-the-art methods in terms of performance and fairness metrics on a representative set of real problems. The results obtained with our proposed methodology show promising performance-fairness curves. Furthermore, we discuss the usefulness of linear weights in the fair model to describe the behaviour of the features when enforcing fairness over a particular set of input features.",
        "authors": "A. Perez-suay, P. Gordaliza, J. Loubes, et.al",
        "keywords": [
            "fairness",
            "machine learning",
            "conditional independence"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=MyQ1e1VQQ3",
        "pdf_src": "https://api2.openreview.net/pdf/24fb0e746fd02d7271b9bacdd605fc9ea9e6d0f8.pdf",
        "Code_src": "",
        "Introduction": "Background: Achieving fairness in machine learning models has been an important research topic due to ethical concerns about biased outcomes.\n\nResearch Problem: A common method for ensuring fairness called \"Equalized Odds\" requires that there be no difference in the false positive rate across all demographic groups; however, implementing this can be complex.\n\nMethods: The paper introduces two approaches based on the Conditional Independence Operator:\n\n1. As a fairness metric - It measures how independent the model's predictions are from sensitive attributes like race or gender by comparing predicted probabilities before and after conditioning on such attributes.\n2. As a regularization term within the optimization framework – It encourages solutions where the model performs well while also being fair regarding sensitive attributes without explicitly labeling them during training.\n\nMain Contributions:\n- Provides novel insights via numerical experiments conducted against Gaussian models demonstrating convergence properties related to their fairness criteria \n- Develops a formulation utilizing the Conditional Cross-Covariance Operator potentially leading to closed-form solutions even if considering more complex settings involving kernels \n- Introduces Normalized Criteria for Conditional Independence Operators allowing for broader applicability beyond binary classification tasks \n- Demonstrates empirical improvements relative to existing benchmarks focusing not only on accuracy but also on maintaining equitable treatment among various subgroups \n- Discusses implications drawn specifically around Gaussian Processes frameworks highlighting additional advantages offered here",
        "Topic": "Machine Learning"
    },
    {
        "title": "Semantic Self-adaptation: Enhancing Generalization with a Single Sample",
        "abstract": "The lack of out-of-domain generalization is a critical weakness of deep networks for semantic segmentation. Previous studies relied on the assumption of a static model, i. e., once the training process is complete, model parameters remain fixed at test time. In this work, we challenge this premise with a self-adaptive approach for semantic segmentation that adjusts the inference process to each input sample. Self-adaptation operates on two levels. First, it fine-tunes the parameters of convolutional layers to the input image using consistency regularization. Second, in Batch Normalization layers, self-adaptation interpolates between the training and the reference distribution derived from a single test sample. Despite both techniques being well known in the literature, their combination sets new state-of-the-art accuracy on synthetic-to-real generalization benchmarks. Our empirical study suggests that self-adaptation may complement the established practice of model regularization at training time for improving deep network generalization to out-of-domain data. Our code and pre-trained models are available at https://github.com/visinf/self-adaptive.",
        "authors": "S. Bahmani, O. Hahn, E. Zamfir, et.al",
        "keywords": [
            "self-adaptive",
            "semantic segmentation",
            "cross-domain generalization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ILNqQhGbLx",
        "pdf_src": "https://api2.openreview.net/pdf/fd960b1e24d0944fac29a022be9eef7c30f0f25e.pdf",
        "Code_src": "https://github.com/visinf/self-adaptive",
        "Introduction": "Background: The paper addresses an issue common among neural networks used for semantic segmentation - poor performance when applied outside the domain where they were trained (\"out-of-domain generalization\"). This limitation has been challenging due to the static nature of these networks; after training ends, there's no adjustment made during testing.\n\nResearch Question: How can one improve the ability of such networks to generalize beyond what was seen during training?\n\nMethod: To tackle this problem, researchers propose a novel \"self-adaptive\" method which involves adjusting the network as needed based on individual inputs rather than having a fixed set of parameters across all tests or samples within domains.\n\nMain Contributions:\n1. Consistency Regularization: Parameters of convolutional layers get adjusted specifically according to features present in different images.\n2. Interpolation Between Distributions: For Batch Normalization layers, instead of just using either training or validation distributions throughout testing phases, the system adapts by blending them depending upon how much similarity exists compared against previously encountered examples – effectively creating a more personalized normalization per-test instance without altering the original training statistics too drastically through interpolation methods like Gaussian Mixture Models(GMMs).\n3. State-Of-The-Art Performance: By combining these adaptive strategies into existing architectures, significant improvements have been observed over previous approaches especially noticeable while comparing synthetic datasets with real-world ones demonstrating better robustness towards unseen contexts thus far achieved by any other published works up until now.\n4. Empirical Study: An empirical analysis supports findings suggesting potential benefits gained via incorporating additional adaptability mechanisms alongside traditional regularization practices could lead toward enhancing overall learning capabilities allowing networks perform optimally even under conditions not covered explicitly during initial training phase(s).",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "Lifelong Reinforcement Learning with Modulating Masks",
        "abstract": "Lifelong learning aims to create AI systems that continuously and incrementally learn during a lifetime, similar to biological learning. Attempts so far have met problems, including catastrophic forgetting, interference among tasks, and the inability to exploit previous knowledge. While considerable research has focused on learning multiple supervised classification tasks that involve changes in the input distribution, lifelong reinforcement learning (LRL) must deal with variations in the state and transition distributions, and in the reward functions. Modulating masks with a fixed backbone network, recently developed for classification, are particularly suitable to deal with such a large spectrum of task variations. In this paper, we adapted modulating masks to work with deep LRL, specifically PPO and IMPALA agents. The comparison with LRL baselines in both discrete and continuous RL tasks shows superior performance. We further investigated the use of a linear combination of previously learned masks to exploit previous knowledge when learning new tasks: not only is learning  faster, the algorithm solves tasks that we could not otherwise solve from scratch due to extremely sparse rewards. The results suggest that RL with modulating masks is a promising approach to lifelong learning, to the composition of knowledge to learn increasingly complex tasks, and to knowledge reuse for efficient and faster learning.",
        "authors": "E. Ben-iwhiwhu, S. Nath, P. K. Pilly, et.al",
        "keywords": [
            "lifelong learning",
            "modulating masks",
            "reinforcement learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=V7tahqGrOq",
        "pdf_src": "https://api2.openreview.net/pdf/8547aeb6c9a4be72ce461ee7dc411f432ec9cdf7.pdf",
        "Code_src": "",
        "Introduction": "Background: Lifelong learning seeks to develop artificial intelligence systems capable of continuous incremental learning throughout their existence much like humans do through biology.\n\nResearch Problem: Existing attempts at lifelong learning face challenges; these include catastrophic forgetting where old information is lost while acquiring new ones which leads to interference between different tasks or domains as well as an inability to utilize prior knowledge effectively over time.\n\nMethodology: To address these issues within lifelong reinforcement learning (LRL), researchers often employ modulating masks along with a fixed backbone neural network architecture originally designed for supervised classification purposes but adaptable across various types of tasks involving significant variability ranging widely depending upon inputs states transitions distributions & reward function dynamics.\n\nMain Contributions:\n1. This study adapts modulating masks into working alongside two prominent DRL algorithms - Proximal Policy Optimization (PPO) and Importance Weighted Actor-Critic (IMPALA). \n2. Comparative experiments conducted demonstrate improved performances against baseline approaches under diverse conditions whether it's discrete or continuous Reinforcement Learning (RL).\n3. Additionally, they investigate employing linear combinations derived from previously acquired masks enabling more effective utilization of past experiences aiding quicker adaptation towards novel tasks even those having scarce positive feedback signals.\n4. Overall findings indicate that incorporating modulated masking techniques holds promise toward achieving robust lifelong learning capabilities allowing intelligent agents to compose accumulated expertise seamlessly whilst reusing existing knowledge efficiently resulting in accelerated training processes leading up solving challenging objectives beyond initial scope",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "POMRL: No-Regret Learning-to-Plan with Increasing Horizons",
        "abstract": "We study the problem of planning under model uncertainty in an online meta-reinforcement learning (RL) setting where an agent is presented with a sequence of related tasks with limited interactions per task. The agent can use its experience in each task and across tasks to estimate both the transition model and the distribution over tasks. We propose an algorithm to meta-learn the underlying relatedness across tasks, utilize it to plan in each task, and upper-bound the regret of the planning loss. Our bound suggests that the average regret over tasks decreases as the number of tasks increases and as the tasks are more similar. In the classical single-task setting, it is known that the planning horizon should depend on the estimated model's accuracy, that is, on the number of samples within task. We generalize this finding to meta-RL and study this dependence of planning horizons on the number of tasks. Based on our theoretical findings, we derive heuristics for selecting slowly increasing discount factors, and we validate its significance empirically.",
        "authors": "K. Khetarpal, C. Vernade, B. O'donoghue, et.al",
        "keywords": [
            "model uncertainty",
            "meta-reinforcement learning",
            "planning horizons"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=brGgOAXYtr",
        "pdf_src": "https://api2.openreview.net/pdf/3387d5df6fa115e1dc6b97c3324b96bf4b7c99df.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper addresses the challenge of planning when faced with uncertainties about models during online meta-reinforcement learning scenarios involving sequences of interrelated tasks.\n\nResearch Problem: How does one effectively navigate through multiple tasks while accounting for varying degrees of similarity between them? Additionally, how do agents learn from their experiences at different stages?\n\nMethodology: The authors introduce an algorithm designed specifically for meta-learning relationships among various tasks so they may be leveraged appropriately throughout these sequential challenges without prior knowledge or fixed parameters regarding which tasks will follow next.\nThey also develop a method to calculate bounds around potential regret incurred by any given decision-making process based upon estimates made using past data points collected up until now - thus allowing us greater insight into future performance expectations even if some aspects remain uncertain due to incomplete information gathering efforts early onwards!\n\nMain Contributions:\n1. A novel approach towards meta-learning dependencies amongst diverse yet interconnected RL domains;\n2. An analytical framework providing tighter-than-known-before guarantees concerning cumulative regret experienced after executing plans formulated according to proposed algorithms; \n3. Empirical validation demonstrating efficacy via simulations conducted against baseline approaches showing improved outcomes achieved hereunder compared those obtained elsewhere before publication date mentioned above).",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "On the Gradient Formula for learning Generative Models with Regularized Optimal Transport Costs",
        "abstract": "Learning a Wasserstein Generative Adversarial Networks (WGAN) requires the differentiation of the optimal transport cost with respect to the parameters of the generative model. In this work, we provide sufficient conditions for the  existence of a gradient formula in two different frameworks: the case of semi-discrete optimal transport (i.e. with a discrete target distribution) and the case of regularized optimal transport (i.e. with an entropic penalty). In both cases the gradient formula involves a solution of the semi-dual formulation of the optimal transport cost. Our study makes a connection between the gradient of the WGAN loss function and the Laguerre diagrams associated to semi-discrete transport maps. The learning problem is addressed with an alternating algorithm, which is in general not convergent. However, in most cases, it stabilizes close to a relevant  solution for the generative learning problem. We also show that entropic regularization can improve the convergence speed but noticeably changes the shape of the learned generative model.",
        "authors": "A. Houdard, A. Leclaire, N. Papadakis, et.al",
        "keywords": [
            "gradient",
            "Wasserstein Generative Adversarial Networks",
            "optimal transport"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=FbztvhdCX9",
        "pdf_src": "https://api2.openreview.net/pdf/7c2a80e2ba230f06c4932f34eb8b8b6235e37b3f.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses the challenges involved in training Wasserstein Generative Adversarial Networks (WGAN), particularly focusing on the need to differentiate the optimal transport cost with respect to the parameters of the generative model.\n\nResearch Problem:\nThe main research question revolves around finding sufficient conditions under which a gradient formula exists when dealing with two specific frameworks related to optimal transport costs - semi-discrete optimal transport where there's a discrete target distribution; and regularized optimal transport involving an entropic penalty.\n\nMethods:\nTo address these issues, the authors propose providing necessary and sufficient conditions within those two frameworks by utilizing solutions from the semi-dual formulation of the optimal transport cost as part of their gradient formulas.\nAdditionally, they establish connections between the gradients of the WGAN loss function and certain geometric structures known as Laguerre diagrams pertinent to semi-discrete transport maps during the learning process using an alternating algorithm despite its non-convergence nature generally speaking.\n\nMain Contributions:\nThis contribution primarily lies in elucidating how to compute gradients efficiently even though standard methods may fail due to computational complexities or lack thereof depending upon whether one considers semi-discrete versus regularized settings respectively while still being able to converge approximately towards meaningful solutions suitable for practical applications such as generative modeling tasks via GANs like WAGNs mentioned earlier albeit at some trade-offs regarding entropy regularization effects altering shapes somewhat compared without said regularization techniques applied accordingly throughout iterations leading up until final outputted models ready deployment purposes into real-world scenarios encountered daily life contexts etcetera...",
        "Topic": "Optimal Transport"
    },
    {
        "title": "Off-Policy Evaluation with Out-of-Sample Guarantees",
        "abstract": "We consider the problem of evaluating the performance of a decision policy using past observational data. The outcome of a policy is measured in terms of a loss (aka. disutility or negative reward) and the main problem is making valid inferences about its out-of-sample loss when the past data was observed under a different and possibly unknown policy. Using a sample-splitting method, we show that it is possible to draw such inferences with finite-sample coverage guarantees about the entire loss distribution, rather than just its mean. Importantly, the method takes into account model misspecifications of the past policy - including unmeasured confounding. The evaluation method can be used to certify the performance of a policy using observational data under a specified range of credible model assumptions.",
        "authors": "S. Ek, D. Zachariah, F. D. Johansson, et.al",
        "keywords": [
            "sample-splitting",
            "finite-sample coverage",
            "model misspecification"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=XnYtGPgG9p",
        "pdf_src": "https://api2.openreview.net/pdf/558f30ef4f08659d7955c79afb180b1b79434f41.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper addresses the challenge of assessing the effectiveness of a given decision-making strategy based on historical observational information where outcomes are quantified by some form of loss function.\n\nResearch Question: How do you accurately estimate an algorithm's future loss without bias due to differences between training conditions?\n\nMethodology: The authors propose utilizing a sample-splitting approach which allows for drawing conclusions regarding not only the expected value but also the full distribution of potential losses outside of the dataset from which observations were drawn – even if this dataset reflects policies other than those being evaluated.\nThis technique accounts for any mis-specification within models representing previous decisions—including factors like omitted confounders—that could skew results otherwise.\n\nMain Contributions:\n1. They introduce a novel methodology ensuring inference extends beyond simple averages; instead providing confidence intervals around all aspects of the loss distribution—a more robust measure compared to focusing solely on means alone.\n2. Their framework accommodates uncertainty related to deviations among actual versus assumed prior policies during observation periods—addressing issues arising from non-randomness in how data points came together historically through potentially disparate strategies.\n3. By certifying performance across plausible ranges according to certain pre-defined criteria concerning underlying statistical models, their work provides policymakers practical guidance while acknowledging limitations inherent whenever relying heavily upon observational evidence over experimental ones.",
        "Topic": "Sample Efficiency in Reinforcement Learning"
    },
    {
        "title": "A Unified Perspective on Natural Gradient Variational Inference with Gaussian Mixture Models",
        "abstract": "Variational inference with Gaussian mixture models (GMMs) enables learning of highly tractable yet multi-modal approximations of intractable target distributions with up to a few hundred dimensions. The two currently most effective methods for GMM-based variational inference, VIPS and iBayes-GMM, both employ independent natural gradient updates for the individual components and their weights. We show for the first time, that their derived updates are equivalent, although their practical implementations and theoretical guarantees differ. We identify several design choices that distinguish both approaches, namely with respect to sample selection, natural gradient estimation, stepsize adaptation, and whether trust regions are enforced or the number of components adapted. We argue that for both approaches, the quality of the learned approximations can heavily suffer from the respective design choices: By updating the individual components using samples from the mixture model, iBayes-GMM often fails to produce meaningful updates to low-weight components, and by using a zero-order method for estimating the natural gradient, VIPS scales badly to higher-dimensional problems. Furthermore, we show that information-geometric trust-regions (used by VIPS) are effective even when using first-order natural gradient estimates, and often outperform the improved Bayesian learning rule (iBLR) update used by iBayes-GMM. We systematically evaluate the effects of design choices and show that a hybrid approach significantly outperforms both prior works. Along with this work, we publish our highly modular and efficient implementation for natural gradient variational inference with Gaussian mixture models, which supports $432$ different combinations of design choices, facilitates the reproduction of all our experiments, and may prove valuable for the practitioner.",
        "authors": "O. Arenz, P. Dahlinger, Z. Ye, et.al",
        "keywords": [
            "Gaussian Mixture Models",
            "Variational Inference",
            "Natural Gradient Updates"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=tLBjsX4tjs",
        "pdf_src": "https://api2.openreview.net/pdf/a965a201e2d9f9f24db564f734b161ad0e639bf9.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses the use of Gaussian Mixture Models (GMMs) within Variational Inference as an approximation technique due to its ability to handle complex data structures effectively.\n\nResearch Problem:\nIdentifying how existing algorithms perform under various design choices related to sample selection, natural gradient estimation, step size adaptation, and component adaptability is crucial since these factors affect the performance greatly but have not been thoroughly compared before.\n\nMethodology:\nThe authors compare two state-of-the-art algorithms - Variable Importance Sampling (VIPS) and Information-Based Bayesian Learning Rule (iBayes-GMM). They analyze each algorithm's updates independently while considering the differences between them such as sampling strategies during updates.\n \nMain Contributions:\n1. Proves theoretically through equivalence results showing despite differing practical implementations & theoretical guarantees, the updates employed by both algorithms converge towards similar directions on the parameter space.\n2. Identifies key distinctions among the algorithms regarding aspects like sample selection techniques utilized throughout the iterations; the way they estimate the natural gradients leading to scalability issues especially at high dimensions;\n3. Demonstrates empirically via systematic evaluations where certain design choices lead to significant improvements over others – particularly highlighting the effectiveness of information geometric trust regions alongside other modifications proposed here;\n4. Provides open-source code implementing natural gradient variational inference with Gaussian mixtures allowing users to explore 432 distinct configurations without needing additional computational resources beyond those required per configuration alone",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "The Score-Difference Flow for Implicit Generative Modeling",
        "abstract": "Implicit generative modeling (IGM) aims to produce samples of synthetic data matching the characteristics of a target data distribution.  Recent work (e.g. score-matching networks, diffusion models) has approached the IGM problem from the perspective of pushing synthetic source data toward the target distribution via dynamical perturbations or flows in the ambient space.  In this direction, we present the score difference (SD) between arbitrary target and source distributions as a flow that optimally reduces the Kullback-Leibler divergence between them while also solving the Schrödinger bridge problem.  We apply the SD flow to convenient proxy distributions, which are aligned if and only if the original distributions are aligned.  We demonstrate the formal equivalence of this formulation to denoising diffusion models under certain conditions.  We also show that the training of generative adversarial networks includes a hidden data-optimization sub-problem, which induces the SD flow under certain choices of loss function when the discriminator is optimal. As a result, the SD flow provides a theoretical link between model classes that individually address the three challenges of the \"generative modeling trilemma\"—high sample quality, mode coverage, and fast sampling—thereby setting the stage for a unified approach.",
        "authors": "R. M. Weber",
        "keywords": [
            "IGM",
            "Score Difference Flow",
            "Generative Modeling Trilemma"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=dpGSNLUCzu",
        "pdf_src": "https://api2.openreview.net/pdf/758780821ebeea71c70870b2cf4699857a3145ed.pdf",
        "Code_src": "",
        "Introduction": "Background: Implicit generative modeling (IGM) seeks to generate synthetic data that closely matches the properties of an observed dataset.\nResearch Question: How can one effectively reduce the Kullback-Leibler divergence between two datasets using implicit generative modeling techniques?\nMethod: The paper introduces the concept of the score difference (SD) flow—a novel method based on optimizing the distance measure between datasets by adjusting their respective scores. This flow aligns proxy distributions such that they match exactly with the underlying true distributions whenever possible.\n\nMain Contributions:\n1. The introduction of the SD flow as a new tool within the field of IGM; it serves both reducing the KL divergence and solving the Schrödinger bridge problem simultaneously through dynamic adjustments along the flow path.\n2. Demonstrating how the SD flow relates formally to existing methods like denoising diffusion models given specific conditions hold.\n3. Revealing insights into the training process of Generative Adversarial Networks where optimization towards minimizing the SD flow emerges naturally during the learning phase provided the discriminator performs well enough according to chosen criteria leading up to convergence points akin those found at equilibrium states associated with these networks' architectures.\n\nOverall Impact: By providing a unifying framework connecting various approaches addressing different aspects of generative modeling's challenges (sample quality, mode coverage), including high-speed generation capabilities among others, this research lays groundwork potentially paving way forward",
        "Topic": "Generative Models"
    },
    {
        "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
        "abstract": "In this paper, we are interested in understanding self-supervised pretraining through studying the capability that self-supervised methods learn part-aware representations. The study is mainly motivated by that random views, used in contrastive learning, and random masked (visible) patches, used in masked image modeling, are often about object parts. \n\nWe explain that contrastive learning is a part-to-whole task: the projection layer hallucinates the whole object representation from the object part representation learned from the encoder, and that masked image modeling is a part-to-part task: the masked patches of the object are hallucinated from the visible patches. The explanation suggests that the self-supervised pretrained encoder leans toward understanding the object part. We empirically compare the off-the-shelf encoders pretrained with several representative methods on object-level recognition and part-level recognition. The results show that the fully-supervised model outperforms self-supervised models for object-level recognition, and most self-supervised contrastive learning and masked image modeling methods outperform the fully-supervised method for part-level recognition. It is observed that the combination of contrastive learning and masked image modeling further improves the performance.",
        "authors": "J. Zhu, J. Qi, M. Ding, et.al",
        "keywords": [
            "object part",
            "contrastive learning",
            "masked image modeling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=HP7Qpui5YE",
        "pdf_src": "https://api2.openreview.net/pdf/cb074227b8ad5a2af04a3009a5ed93bc51a316e1.pdf",
        "Code_src": "",
        "Introduction": "Background:\nSelf-supervised pretraining has become an important technique to train deep neural networks without labeled data using auxiliary tasks or pretext tasks designed based on the properties of natural images.\n\nResearch Problem:\nThe research problem addressed here concerns how well self-supervised methods can capture part-aware representations during their training process when they use random views as in contrastive learning techniques such as InfoNCE loss function which encourages positive samples to be close together while negative ones far apart; and when they utilize randomly masked (visible) patches within objects like Masked Autoencoder (MAM).\n\nMethodology:\nTo investigate whether these two types of self-supervised approaches indeed lead to better understanding at different levels - whole-object level versus individual parts – authors conduct empirical comparisons between various state-of-the-art self-supervised encoders trained under contrasting conditions against fully supervised counterparts across both object-level classification accuracy metrics along with more specialized part-level recognition benchmarks.\n\nMain Contributions:\nThis work makes three main contributions:\n\n1) A theoretical insight into why contrastive learning might naturally encourage the network to focus its attention towards recognizing parts rather than wholes.\n2) An experimental validation showing that despite being less accurate overall compared to fully supervised models specifically tailored for object-level tasks, many self-supervised contrastive learning and masked image modeling methods significantly surpass them particularly regarding identifying specific components within objects due to their innate ability capturing local features effectively.\n3) Demonstrating improved performances achieved via combining aspects of contrastive learning & masked image modeling strategies suggesting synergy exists among these complementary approaches enhancing feature extraction capabilities beyond what could have been accomplished individually",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations",
        "abstract": "Offline reinforcement learning has shown great promise in leveraging large pre-collected datasets for policy learning, allowing agents to forgo often-expensive online data collection. However, offline reinforcement learning from visual observations with continuous action spaces remains under-explored, with a limited understanding of the key challenges in this complex domain. In this paper, we establish simple baselines for continuous control in the visual domain and introduce a suite of benchmarking tasks for offline reinforcement learning from visual observations designed to better represent the data distributions present in real-world offline RL problems and guided by a set of desiderata for offline RL from visual observations, including robustness to visual distractions and visually identifiable changes in dynamics. Using this suite of benchmarking tasks, we show that simple modifications to two popular vision-based online reinforcement learning algorithms, DreamerV2 and DrQ-v2, suffice to outperform existing offline RL methods and establish competitive baselines for continuous control in the visual domain. We rigorously evaluate these algorithms and perform an empirical evaluation of the differences between state-of-the-art model-based and model-free offline RL methods for continuous control from visual observations. All code and data used in this evaluation are open-sourced to facilitate progress in this domain.",
        "authors": "C. Lu, P. J. Ball, T. G. J. Rudner, et.al",
        "keywords": [
            "visual observation",
            "offline reinforcement learning",
            "continuous control"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=1QqIfGZOWu",
        "pdf_src": "https://api2.openreview.net/pdf/c8ee4d78cbedbbf4d7d15d6129607fe05df15ddd.pdf",
        "Code_src": "",
        "Introduction": "Background: Offline reinforcement learning aims to leverage large pre-collected datasets for policy learning without expensive online data collection.\n\nResearch Problem: Offline reinforcement learning using visual observations is still not well understood due to its complexity.\n \nMethod: The authors propose simple baselines for continuous control in the visual domain as well as a suite of benchmarking tasks based on real-world offline RL problems and desiderata such as robustness to visual distractions and visibly identifiable changes in dynamics.\n\nMain Contributions:\n1. They demonstrate improvements over existing offline RL methods through simple modifications made to two common vision-based online reinforcement learning algorithms - DreamerV2 and DrQ-v2.\n2. They provide rigorous evaluations comparing different approaches within their proposed benchmarks which include both model-based and model-free offline RL techniques specifically tailored towards continuous control via visual inputs only; all related code & datasets have been released publicly so others can build upon or replicate findings easily moving forward",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "DSpar: An Embarrassingly Simple Strategy for Efficient GNN training and inference via Degree-based Sparsification",
        "abstract": "Running Graph Neural Networks (GNNs) on large graphs suffers from notoriously inefficiency.  This is attributed to the sparse graph-based operations, which is hard to be accelerated by community hardware, e.g., GPUs and CPUs. One potential solution is to  ``sketch'' the original graph by removing unimportant edges, then both the training and inference process are executed on the sparsified graph with improved efficiency. Traditional graph sparsification work calculates the edge importance score, i.e., effective resistance, from graph topology with theoretical guarantee. However, estimating effective resistance is even more expensive than training GNNs itself. Later, learning-based sparsification methods propose to learn the edge importance from data, but with significant overhead due to the extra learning process. Thus, both of them introduce significant ahead-of-training overhead. In this paper, we experimentally and theoretically prove that effective resistance can be approximated using only the node degree information and achieve similar node presentations on graph with/without sparsification. Based on this finding, we propose DSpar, to sparsify the graph once before training based on only the node degree information with negligible ahead-of-training overhead. In practice, for the training phase, DSpar achieves up to $5.9\\times$ faster than baseline with almost no accuracy drop. For the inference phase, DSpar reduces up to $90\\%$ latency.",
        "authors": "Z. Liu, K. Zhou, Z. Jiang, et.al",
        "keywords": [
            "DSpar",
            "Graph Sparsification",
            "Effective Resistance Approximation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=SaVEXFuozg",
        "pdf_src": "https://api2.openreview.net/pdf/d42992c3d712cfe7b4e116c195a2741aae90a6c8.pdf",
        "Code_src": "",
        "Introduction": "Background: The running of Graph Neural Networks (GNNs) over large graphs faces a notorious issue of inefficiency mainly because of the sparse graph-based operations.\n\nResearch Problem: How do you efficiently run GNNs when dealing with very large graphs?\n\nMethod: A possible approach involves \"sketching\" or simplifying these graphs first through edge removal while preserving important connections; after sketching, perform all subsequent computations like training and inference directly on the simplified graph structure without sacrificing too much performance in terms of node representation quality across different graph topologies.\n\nMain Contributions:\n1. We empirically demonstrate how one could approximate the effectiveness of resistance - an established metric used as a proxy for edge importance – just via node degrees.\n2. Our proposed method, called DSpar, leverages this approximation technique during pre-processing prior to any actual model training so it incurs minimal additional computational cost upfront (\"ahead-of-training overhead\").\n3. Practically speaking, our experiments show that DSpar significantly speeds up the training time compared to baselines—up to \\(5.9\\times\\) faster—and also drastically cuts down on inference latency—by approximately \\(90\\%\\).",
        "Topic": "approximation"
    },
    {
        "title": "Towards a More Rigorous Science of Blindspot Discovery in Image Classification Models",
        "abstract": "A growing body of work studies Blindspot Discovery Methods (\"BDM\"s): methods that use an image embedding to find semantically meaningful (i.e., united by a human-understandable concept) subsets of the data where an image classifier performs significantly worse.  Motivated by observed gaps in prior work, we introduce a new framework for evaluating BDMs, SpotCheck, that uses synthetic image datasets to train models with known blindspots and a new BDM, PlaneSpot, that uses a 2D image representation.  We use SpotCheck to run controlled experiments that identify factors that influence BDM performance (e.g., the number of blindspots in a model, or features used to define the blindspot) and show that PlaneSpot is competitive with and in many cases outperforms existing BDMs.  Importantly, we validate these findings by designing additional experiments that use real image data from MS-COCO, a large image benchmark dataset.  Our findings suggest several promising directions for future work on BDM design and evaluation.  Overall, we hope that the methodology and analyses presented in this work will help facilitate a more rigorous science of blindspot discovery.",
        "authors": "G. Plumb, N. Johnson, A. Cabrera, et.al",
        "keywords": [
            "Blindspot Discovery Methods",
            "Image Embedding",
            "Performance Evaluation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=MaDvbLaBiF",
        "pdf_src": "https://api2.openreview.net/pdf/21ff11ed3614b5bf0ab176a07beec5256ae9f2ca.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the field of Blindspot Discovery Methods (BDMs), which aim to locate semantically significant subsets within images where traditional classifiers perform poorly.\n\nResearch Question: How can Blindspot Discovery Methods be effectively evaluated? What are their influencing factors?\n\nMethodology: The authors propose SpotCheck, a novel framework designed specifically for assessing BDMs using synthetically generated datasets containing pre-defined blindspots as well as introducing PlaneSpot - a method utilizing two-dimensional representations based on image embeddings – aiming at identifying such blindspots accurately.\n\nMain Contributions:\n1. They develop SpotCheck—a systematic approach—allowing researchers to evaluate different BDMs under controlled conditions.\n2. Introduce PlaneSpot—an innovative technique—which leverages 2D image representations alongside other BDMs like those found in literature; it often surpasses them both quantitatively through SpotCheck's experimental setup and qualitatively when tested against actual real-world datasets including MS-COCO.\n3. Validate results across various scenarios involving multiple types of blindspots while also considering how they might scale up depending upon changes made regarding feature definitions etcetera thus paving ways forward towards further advancements in understanding weaknesses present within current machine learning systems via better detection mechanisms provided hereunder.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning",
        "abstract": "We study reward poisoning attacks on online deep reinforcement learning (DRL), where the attacker is oblivious to the learning algorithm used by the agent and the dynamics of the environment. We demonstrate the intrinsic vulnerability of state-of-the-art DRL algorithms by designing a general, black-box reward poisoning framework called adversarial MDP attacks. We instantiate our framework to construct two new attacks which only corrupt the rewards for a small fraction of the total training timesteps and make the agent learn a low-performing policy. We provide a theoretical analysis of the efficiency of our attack and perform an extensive empirical evaluation. Our results show that our attacks efficiently poison agents learning in several popular classical control and MuJoCo environments with a variety of state-of-the-art DRL algorithms, such as DQN, PPO, SAC, etc.",
        "authors": "Y. Xu, Q. Zeng, G. Singh",
        "keywords": [
            "reward poisoning",
            "deep reinforcement learning",
            "adversarial MDP attacks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=25G63lDHV2",
        "pdf_src": "https://api2.openreview.net/pdf/c63803808e6afb2597ee751a857f50aa281783a0.pdf",
        "Code_src": "",
        "Introduction": "Background: Reward poisoning attacks are becoming increasingly prevalent threats against online Deep Reinforcement Learning (DRL). These attacks exploit vulnerabilities within existing DRL systems without knowledge of their underlying learning algorithms or environmental dynamics.\n\nResearch Question: How can we design effective reward poisoning attacks using minimal corrupted data while ensuring they successfully degrade performance?\n\nMethodology: To address this question, researchers developed a novel approach known as Adversarial Markov Decision Process (MDP) Attacks - referred to hereafter simply as adversarial MDP attacks – capable of being applied universally across various DRL algorithms regardless of specific implementation details.\n \nMain Contributions:\n1. The introduction of a comprehensive reward poisoning framework designed specifically targeting modern DRL techniques; \n2. Implementation of two distinct types of attacks based solely on altering rewards during training;\n3. A thorough investigation into how these attacks impact different DRL algorithms when deployed over common benchmark tasks including those found within classic control domains like Atari games alongside more complex simulations provided through MuJoCo platforms;\n4. Empirical validation demonstrating high efficacy even under scenarios involving limited amounts of manipulated reward signals relative to overall training iterations.",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "On the Predictive Accuracy of Neural Temporal Point Process Models for Continuous-time Event Data",
        "abstract": "Temporal Point Processes (TPPs) serve as the standard mathematical framework for modeling asynchronous event sequences in continuous time. However, classical TPP models are often constrained by strong assumptions, limiting their ability to capture complex real-world event dynamics. To overcome this limitation, researchers have proposed Neural TPPs, which leverage neural network parametrizations to offer more flexible and efficient modeling. While recent studies demonstrate the effectiveness of Neural TPPs, they often lack a unified setup, relying on different baselines, datasets, and experimental configurations. This makes it challenging to identify the key factors driving improvements in predictive accuracy, hindering research progress. To bridge this gap, we present a comprehensive large-scale experimental study that systematically evaluates the predictive accuracy of state-of-the-art neural TPP models. Our study encompasses multiple real-world and synthetic event sequence datasets, following a carefully designed unified setup. We thoroughly investigate the influence of major architectural components such as event encoding, history encoder, and decoder parametrization on both time and mark prediction tasks. Additionally, we delve into the less explored area of probabilistic calibration for neural TPP models. By analyzing our results, we draw insightful conclusions regarding the significance of history size and the impact of architectural components on predictive accuracy. Furthermore, we shed light on the miscalibration of mark distributions in neural TPP models. Our study aims to provide valuable insights into the performance and characteristics of neural TPP models, contributing to a better understanding of their strengths and limitations.",
        "authors": "T. Bosser, S. B. Taieb",
        "keywords": [
            "Neural Temporal Point Processes",
            "Predictive Accuracy",
            "Model Evaluation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=3OSISBQPrM",
        "pdf_src": "https://api2.openreview.net/pdf/8f54d2ccde84c8dbb6ba1a1ad714e23113cc788b.pdf",
        "Code_src": "",
        "Introduction": "Background: Temporal Point Processes (TPPs) is widely used for modeling asynchronous event sequences over continuous time; however, these classical TPP models suffer from restrictive assumptions.\n\nResearch Problem: How can we improve the flexibility and efficiency of TPP models while overcoming the constraints imposed by traditional methods?\n\nMethodology: Researchers introduced Neural TPPs using neural network parametrizations but lacked a standardized evaluation method due to inconsistent setups across various studies.\nWe conducted a systematic large-scale experiment with a uniform setup involving several real-world and synthetic datasets focusing on evaluating predictive accuracy under different conditions.\n\nMain Contributions:\n1. A comprehensive empirical analysis comparing current neural TPP model performances based on consistent benchmarks;\n2. Insights about how architecture choices like event encoding or history encoders affect predictions along temporal dimensions;\n3. Examination within neural TPP frameworks related to probabilistic calibration issues not previously extensively studied;\n4. Identification critical aspects influencing predictive precision including importance of historical data usage sizes among other findings aiding future advancements towards improved TPP methodologies utilizing neural networks approaches further development efforts",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Mind the Gap: Mitigating the Distribution Gap in Graph Few-shot Learning",
        "abstract": "Prevailing supervised deep graph learning models often suffer from the issue of label scarcity, leading to performance degradation in the face of limited annotated data. Although numerous graph few-shot learning (GFL) methods have been developed to mitigate this problem, they tend to rely excessively on labeled data. This over-reliance on labeled data can result in impaired generalization ability in the test phase due to the existence of a distribution gap. Moreover, existing GFL methods lack a general purpose as their designs are coupled with task or data-specific characteristics. To address these shortcomings, we propose a novel Self-Distilled Graph Few-shot Learning framework (SDGFL) that is both general and effective. SDGFL leverages a self-distilled contrastive learning procedure to boost GFL. Specifically, our model first pre-trains a graph encoder with contrastive learning using unlabeled data. Later, the trained encoder is frozen as a teacher model to distill a student model with a contrastive loss. The distilled model is then fed to GFL. By learning data representation in a self-supervised manner, SDGFL effectively mitigates the distribution gap and enhances generalization ability. Furthermore, our proposed framework is task and data-independent, making it a versatile tool for general graph mining purposes. To evaluate the effectiveness of our proposed framework, we introduce an information-based measurement that quantifies its capability. Through comprehensive experiments, we demonstrate that SDGFL outperforms state-of-the-art baselines on various graph mining tasks across multiple datasets in the few-shot scenario. We also provide a quantitative measurement of SDGFL’s superior performance in comparison to existing methods.",
        "authors": "C. Zhang, H. Liu, J. Li, et.al",
        "keywords": [
            "graph few-shot learning",
            "self-distillation",
            "contrastive learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LEVbhNrLEL",
        "pdf_src": "https://api2.openreview.net/pdf/9e5aa7e9f9c2edbc356f6a945145a753256cbc5f.pdf",
        "Code_src": "",
        "Introduction": "Background: Supervised deep graph learning models commonly encounter issues related to label scarcity which leads to poor performance when dealing with small amounts of labeled data.\nResearch Problem: How do we develop efficient graph few-shot learning (GFL) methods without relying heavily on labeled data?\nMethods: We present a novel Self-Distilled Graph Few-shot Learning framework (SDGFL). Our approach uses self-distilled contrastive learning procedures based on unlabeled data during initial training phases followed by freezing the encoder into a teacher model while creating a student model through contrastive losses.\n\nMain Contributions:\n1. SDGFL is designed not only be generally applicable but highly effective compared to other existing approaches because it learns representations via self-supervision rather than solely depending on labels provided manually; \n2. It reduces the risk of distribution gaps between train and test sets significantly improving overall generalization abilities;\n3. Unlike most current works focused around specific tasks or types of graphs, ours does not require any modifications regardless if used within different domains or contexts allowing greater flexibility usage scenarios such as general-purpose graph mining applications;\n\nEvaluation Metrics: We introduced new evaluation metrics focusing specifically upon how well each method performs under conditions where there's little available labelled examples available - referred to as \"few-shot\" settings\".",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Consistent Collaborative Filtering via Tensor Decomposition",
        "abstract": "Collaborative filtering is the de facto standard for analyzing users’ activities and building recommendation systems for items. In this work we develop Sliced Anti-symmetric Decomposition (SAD), a new model for collaborative filtering based on implicit feedback. In contrast to traditional techniques where a latent representation of users (user vectors) and items (item vectors) are estimated, SAD introduces one additional latent vector to each item, using a novel three-way tensor view of user-item interactions. This new vector extends user-item preferences calculated by standard dot products to general inner products, producing interactions between items when evaluating their relative preferences. SAD reduces to state-of-the-art (SOTA) collaborative filtering models when the vector collapses to 1, while in this paper we allow its value to be estimated from data. Allowing the values of the new item vector to be different from 1 has profound implications. It suggests users may have nonlinear mental models when evaluating items, allowing the existence of cycles in pairwise comparisons. We demonstrate the efficiency of SAD in both simulated and real world datasets containing over 1M user-item interactions. By comparing with seven SOTA collaborative filtering models with implicit feedbacks, SAD produces the most consistent personalized preferences, in the meanwhile maintaining top-level of accuracy in personalized recommendations. We release the model and inference algorithms in a Python library https://github.com/apple/ml-sad.\n",
        "authors": "S. Zhao, G. Sapiro",
        "keywords": [
            "Sliced Anti-symmetric Decomposition",
            "Collaborative Filtering",
            "Implicit Feedback"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=HqIuAzBxbh",
        "pdf_src": "https://api2.openreview.net/pdf/84fecdc513556364959fb750fb3670714f7e5153.pdf",
        "Code_src": "https://github.com/apple/ml-sad",
        "Introduction": "Background: Collaborative filtering is widely used as an effective method for recommending relevant items or services according to individual users' past behavior patterns.\n\nResearch Problem: The existing collaborative filtering methods mainly estimate latent representations of users and items through matrix factorization approaches but fail to capture complex relationships among them due to linear assumptions about user-item preference functions.\n\nMethod: To address these limitations, authors propose a novel approach called Sliced Anti-symmetric Decomposition (SAD). Unlike previous works that only consider two-way interactions within the user-item space, they introduce another latent dimension per item which allows capturing more intricate structures such as cyclic preferences via non-linear inner product calculations rather than simple dot products commonly employed before.\n\nMain Contributions:\n- Introduce a three-way tensor-based framework incorporating an extra latent feature vector associated with every item;\n- Allow estimation of this third vector's magnitude instead of fixing it at 1, leading to potential nonlinearities in user-item interactions;\n- Demonstrate improved performance compared against other state-of-the-art collaborative filtering models across various datasets including those with millions of user-item interactions;\n- Release source code along with pre-trained weights under an open-source license so others can replicate experiments easily.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Augmented Language Models: a Survey",
        "abstract": "This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a\ncode interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.",
        "authors": "G. Mialon, R. Dessi, M. Lomeli, et.al",
        "keywords": [
            "reasoning",
            "augmentation",
            "tool usage"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=jh7wH2AzKK",
        "pdf_src": "https://api2.openreview.net/pdf/d37773ef045209191c5a87909111f644481cc12e.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper surveys recent advancements in Augmented Language Models (ALMs), where language models have been enhanced by incorporating reasoning abilities along with tool usage capabilities.\n\nResearch Problem: The study aims at identifying how ALMs tackle challenges faced by conventional language models like interpretability problems, inconsistency across different contexts, and scalability concerns through augmentation techniques.\n\nMethods: The authors discuss two primary types of augmentations for LMs - reasoning skill enhancements allowing decomposition of complex tasks; and tool usage enabling interaction with external systems including code interpreters etc. They also mention methods used within these frameworks – heuristic-based integration versus learning directly from demonstration data sets among others.\n \nMain Contributions: The main contribution lies in highlighting the significant improvements offered by integrating reasoning and tool usage functionalities alongside typical language model training objectives leading to better performance metrics than baseline models especially when dealing with specialized domains requiring domain-specific knowledge handling capability beyond mere text generation alone.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Black-Box Batch Active Learning for Regression",
        "abstract": "Batch active learning is a popular approach for efficiently training machine learning models on large, initially unlabelled datasets by repeatedly acquiring labels for batches of data points. However, many recent batch active learning methods are white-box approaches and are often limited to differentiable parametric models: they score unlabeled points using acquisition functions based on model embeddings or first- and second-order derivatives. In this paper, we propose black-box batch active learning for regression tasks as an extension of white-box approaches. Crucially, our method only relies on model predictions. This approach is compatible with a wide range of machine learning models, including regular and Bayesian deep learning models and non-differentiable models such as random forests. It is rooted in Bayesian principles and utilizes recent kernel-based approaches. This allows us to extend a wide range of existing state-of-the-art white-box batch active learning methods (BADGE, BAIT, LCMD) to black-box models. We demonstrate the effectiveness of our approach through extensive experimental evaluations on regression datasets, achieving surprisingly strong performance compared to white-box approaches for deep learning models.",
        "authors": "A. Kirsch",
        "keywords": [
            "black-box batch active learning",
            "regression tasks",
            "Bayesian principles"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=fvEvDlKko6",
        "pdf_src": "https://api2.openreview.net/pdf/41eb1dc8f9860b1bfa84aecca7c201ed53adbcc5.pdf",
        "Code_src": "",
        "Introduction": "Background: Batch active learning is widely used due to its efficiency when dealing with massive, unlabeled datasets.\n\nResearch Problem: Existing batch active learning algorithms mainly focus on white-box settings which require access to gradients from differentiable models; however, these may not be applicable across all types of machine learning models like those that do not support gradient computation directly.\n\nMethod: The authors introduce a novel black-box batch active learning framework specifically designed for regression problems without requiring knowledge about internal model structures beyond making predictions—making it adaptable even if the underlying model cannot compute gradients itself.\nThis new methodology leverages Bayesian inference combined with kernel-based techniques allowing it to incorporate various predictive models irrespective of their complexity level - whether they're neural networks capable of backpropagation or more complex ones where differentiation isn't feasible e.g., decision trees/random forests.\n\nMain Contributions:\n1. Propose a general-purpose black-box batch active learning algorithm suitable for regression tasks;\n2. Extend several well-known white-box batch active learning strategies into the black-box domain utilizing Bayesian reasoning and kernel machinery;\n3. Conduct comprehensive experiments validating both theoretically derived advantages over traditional white-box methods along practical outcomes against real-world benchmarks demonstrating superior performance particularly notable within deep learning contexts despite being oblivious towards inner workings/models specifics apart from prediction outputs alone.",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Provably Convergent Policy Optimization via Metric-aware Trust Region Methods",
        "abstract": "Trust-region methods based on Kullback-Leibler divergence are pervasively used to stabilize policy optimization in reinforcement learning. In this paper, we exploit more flexible metrics and examine two natural extensions of policy optimization with Wasserstein and Sinkhorn trust regions, namely Wasserstein policy optimization (WPO) and Sinkhorn policy optimization (SPO). Instead of restricting the policy to a parametric distribution class, we directly optimize the policy distribution and derive their close-form policy updates based on the Lagrangian duality. Theoretically, we show that WPO guarantees a monotonic performance improvement, and SPO provably converges to WPO as the entropic regularizer diminishes. Moreover, we prove that with a decaying Lagrangian multiplier to the trust region constraint, both methods converge to global optimality. Experiments across tabular domains, robotic locomotion, and continuous control tasks further demonstrate the performance improvement of both approaches, more robustness of WPO to sample insufficiency, and faster convergence of SPO, over state-of-art policy gradient methods.",
        "authors": "J. Song, N. He, L. Ding, et.al",
        "keywords": [
            "policy optimization",
            "Wasserstein distance",
            "Sinkhorn distance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=jkTqJJOGMS",
        "pdf_src": "https://api2.openreview.net/pdf/8e2ca0d292265200c2dbde7c6f4887258116d4f0.pdf",
        "Code_src": "",
        "Introduction": "Background: Trust-region methods using Kullback-Leibler divergence have been widely utilized for stabilizing policy optimization in reinforcement learning.\n\nResearch Problem: This study aims at exploring alternative metrics beyond KL divergence by examining two novel extensions - Wasserstein Policy Optimization (WPO) and Sinkhorn Policy Optimization (SPO).\n\nMethods: Unlike traditional policy optimization which confines policies within a parametric distribution class, our approach involves direct optimization of the policy distribution itself through its close-form policy updates derived from Lagrangian duality principles.\nWe also propose incorporating a decaying Lagrangian multiplier into the trust region constraints ensuring convergence towards global optimality under certain conditions.\n\nMain Contributions:\n1. We theoretically establish that WPO ensures monotonically improving performance during iterations,\n2. We provide evidence showing how SPO can be considered an extension or approximation of WPO when considering entropy regularization parameters,\n3. Both proposed algorithms exhibit superior performance compared to existing policy gradient methods not only in terms of improved efficacy but greater resilience against insufficient samples particularly observed in WPO while SPO demonstrates accelerated convergence rates.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Stochastic Constrained DRO with a Complexity Independent of Sample Size",
        "abstract": "Distributionally Robust Optimization (DRO), as a popular method to train robust models against distribution shift between training and test sets, has received tremendous attention in recent years. In this paper, we propose and analyze stochastic algorithms that apply to both non-convex and convex losses for solving Kullback–Leibler divergence constrained DRO problem. Compared with existing methods solving this problem, our stochastic algorithms not only enjoy competitive if not better complexity independent of sample size but also just require a constant batch size at every iteration, which is more practical for broad applications. We establish a nearly optimal complexity bound for finding an $\\epsilon$-stationary solution for non-convex losses and an optimal complexity for finding an $\\epsilon$-optimal solution for convex losses. Empirical studies demonstrate the effectiveness of the proposed algorithms for solving non-convex and convex constrained DRO problems. ",
        "authors": "Q. Qi, J. Lyu, K. Chan, et.al",
        "keywords": [
            "DRO",
            "Stochastic Algorithms",
            "Complexity Bound"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=VpaXrBFYZ9",
        "pdf_src": "https://api2.openreview.net/pdf/96bcf202f66cd3ead3ccbfa0a4313b8d8b707621.pdf",
        "Code_src": "",
        "Introduction": "Background: Distributional Robust Optimization (DRO) aims to address model robustness by considering potential distribution shifts during testing.\n\nResearch Problem: Develop stochastic algorithms applicable to solve Kullback-Leibler divergence constrained DRO issues under both non-convex and convex loss settings while maintaining efficiency regardless of sample size or requiring minimal computational resources per iteration step.\n\nMethods: Propose novel stochastic algorithms capable of handling various types of losses within the framework of DRO.\n \nMain Contributions:\n1. Offer improved algorithmic complexity bounds compared to previous solutions without depending on sample sizes; \n2. Achieve near-optimal complexity when seeking $\\epsilon$-stationary solutions using non-convex losses;\n3. Obtain optimal complexity guarantees regarding $\\epsilon$-optimal solutions through convex losses;\n4. Demonstrate empirical efficacy across diverse non-convex and convex constrained DRO scenarios via experiments.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Neural Networks beyond explainability: Selective inference for sequence motifs",
        "abstract": "Over the past decade, neural networks have been successful at making predictions from biological sequences, especially in the context of regulatory genomics. As in other fields of deep learning, tools have been devised to extract features such as sequence motifs that can explain the predictions made by a trained network. Here we intend to go beyond explainable machine learning and introduce SEISM, a selective inference procedure to test the association between these extracted features and the predicted phenotype. In particular, we discuss how training a one-layer convolutional network is formally equivalent to selecting motifs maximizing some association score. We adapt existing sampling-based selective inference procedures by quantizing this selection over an infinite set to a large but finite grid. Finally,we show that sampling under a specific choice of parameters is sufficient to characterize the composite null hypothesis typically used for selective inference - a result that goes well beyond our particular framework. We illustrate the behavior of our method in terms of calibration, power and speed and discuss its power/speed trade-off with a simpler data-split strategy. SEISM paves the way to an easier analysis of neural networks used in regulatory genomics, and to more powerful methods for genome wide association studies (GWAS).",
        "authors": "A. Villié, P. Veber, Y. D. Castro, et.al",
        "keywords": [
            "neural networks",
            "feature extraction",
            "selective inference"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=nddEHTSnqg",
        "pdf_src": "https://api2.openreview.net/pdf/91171fad6193e6efa4338aeada2922a70fcadbba.pdf",
        "Code_src": "",
        "Introduction": "Background: Over the last ten years, neural networks have become proficient in predicting biological sequences within the field of regulatory genomics.\n\nResearch Question: How do we extend explainable machine learning techniques into testing associations between extracted features derived from neural network predictions?\n\nMethod: The authors propose SEISM – a selective inference procedure designed specifically for testing feature-phenotype associations using neural network predictions.\n1. They establish a formal equivalence showing that training a single-layer convolutional network corresponds to identifying sequence motifs maximising certain association scores,\n2. Adapt existing sampling-based selective inference algorithms which are then applied on a discretised version of the motif space due to its unbounded nature,\n\nMain Contributions:\n1. A novel approach integrating explainable machine learning principles directly into predictive models like those found in regulatory genomics through selective inference,\n2. An algorithmic adaptation allowing for efficient computation despite the computational challenges posed by the continuous motif space problem,\n3. Demonstrated performance metrics including calibration, statistical power against various alternatives, and computational efficiency compared traditional GWAS approaches.\n\nConclusion: This work introduces a new tool called SEISM aimed not only simplifying analyses involving neural networks' predictions across genomic regions related to regulation or disease susceptibility mapping",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Self-Supervised Graph Representation Learning for Neuronal Morphologies",
        "abstract": "Unsupervised graph representation learning has recently gained interest in several application domains such as neuroscience, where modeling the diverse morphology of cell types in the brain is one of the key challenges. It is currently unknown how many excitatory cortical cell types exist and what their defining morphological features are. Here we present GraphDINO, a purely data-driven approach to learn low-dimensional representations of 3D neuronal morphologies from unlabeled large-scale datasets. GraphDINO is a novel transformer-based representation learning method for spatially-embedded graphs. To enable self-supervised learning on transformers, we (1) developed data augmentation strategies for spatially-embedded graphs, (2) adapted the positional encoding and (3) introduced a novel attention mechanism, AC-Attention, which combines attention-based global interaction between nodes and classic graph convolutional processing. We show, in two different species and across multiple brain areas, that this method yields morphological cell type clusterings that are on par with manual feature-based classification by experts, but without using prior knowledge about the structural features of neurons. Moreover, it outperforms previous approaches on quantitative benchmarks predicting expert labels. Our method could potentially enable data-driven discovery of novel morphological features and cell types in large-scale datasets. It is applicable beyond neuroscience in settings where samples in a dataset are graphs and graph-level embeddings are desired.",
        "authors": "M. A. Weis, L. Hansel, T. Lüddecke, et.al",
        "keywords": [
            "GraphDINO",
            "Transformer-based Representation Learning",
            "Unsupervised Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ThhMzfrd6r",
        "pdf_src": "https://api2.openreview.net/pdf/90779f2754a9d0699d527d382712534cbba54998.pdf",
        "Code_src": "",
        "Introduction": "Background: Unsupervised graph representation learning aims to discover meaningful patterns or clusters within complex networks like those found in neuroscience.\nResearch Problem: The diversity among neuron cell types poses significant challenges due to our limited understanding regarding the number and characteristics of these cells.\n\nMethod: This paper introduces GraphDINO - an unsupervised graph representation learning framework based on Transformers specifically designed for spatially embedded graphs representing neuronal morphologies:\n1. Data Augmentation Strategies were developed considering the unique properties of spatially embedded graphs,\n2. Positional Encoding was adapted since Transformer architectures require fixed-length inputs while graphs have varying lengths,\n3. A novel Attention Mechanism called AC-Attention was proposed combining node-wise interactions similar to attention mechanisms used traditionally along with graph convolution operations.\n\nMain Contributions: \nGraphDINO successfully learns low-dimensional representations directly from unlabeled large-scale datasets containing neuronal morphologies achieving clustering performance comparable to manually defined criteria set forth by domain experts – all done autonomously devoid of any pre-existing knowledge concerning neural structure specifics. Additionally, GraphDINO significantly surpasses existing methods when evaluated against quantitative benchmarks aimed at predicting expert-assigned labels suggesting its potential utility not only in neuroscience applications dealing with neuronal network analysis involving graph structures; however also other fields requiring embedding representations over graph datasets can benefit greatly through adoption of this technique",
        "Topic": "Vision Transformer"
    },
    {
        "title": "Stochastic gradient updates yield deep equilibrium kernels",
        "abstract": "Implicit deep learning allows one to compute with implicitly defined features, for example features that solve optimisation problems. We consider the problem of computing with implicitly defined features in a kernel regime. We call such a kernel a deep equilibrium kernel (DEKer). Specialising on a stochastic gradient descent (SGD) update rule applied to features (not weights) in a latent variable model, we find an exact deterministic update rule for the (DEKer) in a high dimensional limit. This derived update rule resembles previously introduced infinitely wide neural network kernels. To perform our analysis, we describe an alternative parameterisation of the link function of exponential families, a result that may be of independent interest. This new parameterisation allows us to draw new connections between a statistician's inverse link function and a machine learner's activation function. We describe an interesting property of SGD in this high dimensional limit: even though individual iterates are random vectors, inner products of any two iterates are deterministic, and can converge to a unique fixed point as the number of iterates increases. We find that the (DEKer) empirically outperforms related neural network kernels on a series of benchmarks.",
        "authors": "R. Tsuchida, C. S. Ong",
        "keywords": [
            "deep equilibrium kernel",
            "stochastic gradient descent",
            "infinite width neural networks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=p7UTv2hWgM",
        "pdf_src": "https://api2.openreview.net/pdf/2b083e8f04a656ab6c4212e807165258dad58c39.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper introduces implicit deep learning which enables computation using implicitly defined features like those solving optimization problems.\n\nResearch Problem:\nThe research focuses on computing with implicitly defined features within a kernel regime called Deep Equilibrium Kernels (DEK).\n\nMethodology:\nThe authors specialize their study into finding an exact deterministic update rule for DEKs when applying stochastic gradient descent updates directly to features rather than weights inside a latent variable model under a high-dimensional setting.\nThey also introduce another parameterization method for the link functions used by exponential family distributions - which is potentially useful independently from the main topic – allowing them to connect statistical inverse link functions more closely with machine learning activation functions.\n\nMain Contributions:\nTheir key contribution lies not only in deriving a novel deterministic update equation but they demonstrate experimentally through benchmark tests across various datasets against other neural network kernels that these DEKs significantly outperform traditional methods based purely on empirical evidence suggesting potential practical advantages over existing techniques involving neural networks or similar architectures dealing with feature spaces.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "On Average-Case Error Bounds for Kernel-Based Bayesian Quadrature",
        "abstract": "In this paper, we study error bounds for Bayesian quadrature (BQ), with an emphasis on noisy settings, randomized algorithms, and average-case performance measures.  We seek to approximate the integral of functions in a Reproducing Kernel Hilbert Space (RKHS), particularly focusing on the Mat\\'ern-$\\nu$ and squared exponential (SE) kernels, with samples from the function potentially being corrupted by Gaussian noise.  We provide a two-step meta-algorithm that serves as a general tool for relating the average-case quadrature error with the $L^2$-function approximation error. When specialized to the Mat\\'ern kernel, we recover an existing near-optimal error rate while avoiding the existing method of repeatedly sampling points.  When specialized to other settings, we obtain new average-case results for settings including the SE kernel with noise and the Mat\\'ern kernel with misspecification.  Finally, we present algorithm-independent lower bounds that have greater generality and/or give distinct proofs compared to existing ones.",
        "authors": "X. Cai, T. Lam, J. Scarlett",
        "keywords": [
            "Bayesian Quadrature",
            "Noisy Settings",
            "Average-Case Performance Measures"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=JJrKbq35l4",
        "pdf_src": "https://api2.openreview.net/pdf/fe465ec278556c438142c003eb52401d1128c007.pdf",
        "Code_src": "",
        "Introduction": "Background: This research focuses on Bayesian Quadrature (BQ), which is used to estimate the integral of functions defined within a Reproducing Kernel Hilbert Space (RKHS). The RKHS includes popular kernels like the Mat\\'ern-$\\nu$ and squared exponential (SE) kernels.\n\nResearch Problem: In practical applications where data may be contaminated or incomplete due to noise, it's crucial to understand how BQ performs under these conditions regarding its accuracy – specifically concerning both the expected value of the quadrature errors when averaging over many realizations (\"average-case\") and worst-case scenarios.\n\nMethods: To address this problem, researchers propose a novel two-step meta-algorithm designed broadly applicable across different types of BQ problems involving RKHSs such as those mentioned above - Mat\\'ern-$\\nu$ and SE kernels.\nThe first step involves estimating the L^2-function approximation error; secondly, they relate this estimation directly back into terms related to quadrature error rates through their proposed framework.\n\nMain Contributions:\n1. They extend previous work significantly beyond what was previously known about optimal error rates without requiring resampling steps during computation time savings are achieved using their approach instead \n2. New insights gained include providing improved understanding around performance guarantees even if there exists some form of misspecification within our model assumptions (e.g., incorrect choice between Mat\\'ern-$\\nu$ and SE kernel).\n3. Lower bounds presented offer more generalized coverage than before along with alternative proof strategies demonstrating stronger theoretical foundations supporting Bayesian quadrature methods overall",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Breaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling",
        "abstract": "Trying to capture the sample-label relationship, conditional generative models often end up inheriting the spurious correlation in the training dataset, giving label-conditional distributions that are severely imbalanced in another latent attribute. To mitigate such undesirable correlations engraved into generative models, which we call spurious causality, we propose a general two-step strategy. (a) Fairness Intervention (FI): Emphasize the minority samples that are hard to be generated due to the spurious correlation in the training dataset. (b) Corrective Sampling (CS): Filter the generated samples explicitly to follow the desired label-conditional latent attribute distribution. We design the fairness intervention for various degrees of supervision on the spurious attribute, including unsupervised, weakly-supervised, and semi-supervised scenarios. Our experimental results show that the proposed FICS can successfully resolve the spurious correlation in generated samples on various datasets.",
        "authors": "J. Nam, S. Mo, J. Lee, et.al",
        "keywords": [
            "spurious causality",
            "fairness intervention",
            "corrective sampling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=VV4zJwLwI7",
        "pdf_src": "https://api2.openreview.net/pdf/f346928edae8ede4c2f6519b523c7a272816bfc2.pdf",
        "Code_src": "",
        "Introduction": "Background: Conditional generative models aim to learn the underlying structure from labeled data by capturing the sample-label relationship; however, they may inherit spurious correlations present in the training dataset.\n\nResearch Problem: The problem addressed is how to mitigate these undesirable correlations within generative models known as \"spurious causality,\" resulting in severe imbalance with respect to certain latent attributes when generating new samples based on labels.\n\nMethods: The authors introduce an iterative method called Fairness Intervention and Corrective Sampling (FICS), consisting of:\n\n1. Fairness Intervention (FI): This step focuses on highlighting underrepresented or difficult-to-generate samples caused by the aforementioned spurious correlations.\n2. Corrective Sampling (CS): In this second stage, it filters out any newly generated samples not adhering closely enough to the target label-conditional latent attribute distribution expected during generation.\n\nMain Contributions:\n- A novel approach combining FI followed by CS aimed at addressing issues related to spurious causality without requiring additional annotations beyond what's used initially—making their solution applicable across different levels of supervision over the spurious attribute category – unsupervised, weakly supervised, and semi-supervised settings respectively.\n- Demonstrated effectiveness through empirical experiments conducted using multiple datasets showcasing improved balance between latent attributes after applying FICS compared against baselines methods dealing similarly but less effectively than presented here.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Supervised Knowledge May Hurt Novel Class Discovery Performance",
        "abstract": "Novel class discovery (NCD) aims to infer novel categories in an unlabeled dataset by leveraging prior knowledge of a labeled set comprising disjoint but related classes. Given that most existing literature focuses primarily on utilizing supervised knowledge from a labeled set at the methodology level, this paper considers the question: Is supervised knowledge always helpful at different levels of semantic relevance? To proceed, we first establish a novel metric, so-called transfer leakage, to measure the semantic similarity between labeled/unlabeled datasets. To show the validity of the proposed metric, we build up a large-scale benchmark with various degrees of semantic similarities between labeled/unlabeled datasets on ImageNet by leveraging its hierarachical class structure. The results based on the proposed benchmark show that the proposed transfer leakage is in line with the hierarachical class structure; and that NCD performance is consistent with the semantic similarities (measured by the proposed metric). Next, by using the proposed transfer leakage, we conduct various empirical experiments with different levels of semantic similarity, yielding that supervised knowledge may hurt NCD performance. Specifically, using supervised information from a low-similarity labeled set may lead to a suboptimal result as compared to using pure self-supervised knowledge. These results reveal the inadequacy of the existing NCD literature which usually assumes that supervised knowledge is beneficial. Finally, we develop a pseudo-version of the transfer leakage as a practical reference to decide if supervised knowledge should be used in NCD. Its effectiveness is supported by our empirical studies, which show that the pseudo transfer leakage (with or without supervised knowledge) is consistent with the corresponding accuracy based on various datasets.",
        "authors": "Z. Li, J. Otholt, B. Dai, et.al",
        "keywords": [
            "transfer leakage",
            "novel class discovery",
            "semantic similarity"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=oqOBTo5uWD",
        "pdf_src": "https://api2.openreview.net/pdf/01a169f54c344358f5a46cba40dbdcd5c53cd56f.pdf",
        "Code_src": "",
        "Introduction": "Background: Novel class discovery (NCD) refers to inferring new categories within an unlabeled dataset while making use of previously known labels for other unrelated yet semantically connected classes.\nResearch Question: This study investigates whether supervised knowledge obtained from labeled data consistently improves NCD across varying levels of semantic relevance.\n\nMethodology: \n1. A novel metric called \"transfer leakage\" was introduced to quantify the semantic similarity among labeled and unlabeled datasets;\n2. An extensive benchmark named ImageNet was constructed through hierarchical class structures showcasing diverse degrees of semantic similarity;\n\nMain Contributions:\n1. Demonstrated the correlation between the proposed transfer leakage metric and the hierarchical nature of the ImageNet classes - higher transfer leakage values corresponded to more similar semantic content;\n2. Revealed inconsistencies where supervised knowledge could hinder rather than enhance NCD performance especially when there's little overlap between the labeled and unlabeled sets;\n3. Developed a pseudo-transfer leakage method serving as a pragmatic guide determining supervision utility during NCD processes – empirical evidence suggests it aligns well with actual classification accuracies derived from multiple datasets.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Adjusting Machine Learning Decisions for Equal Opportunity and Counterfactual Fairness",
        "abstract": "Machine learning (ML) methods have the potential to automate\nhigh-stakes decisions, such as bail admissions or credit lending, by\nanalyzing and learning from historical data. But these algorithmic\ndecisions may be unfair: in learning from historical data, they may\nreplicate discriminatory practices from the past.  In this paper, we\npropose two algorithms that adjust fitted ML predictors to produce\ndecisions that are fair.  Our methods provide post-hoc adjustments to\nthe predictors, without requiring that they be retrained.  We consider\na causal model of the ML decisions, define fairness through\ncounterfactual decisions within the model, and then form algorithmic\ndecisions that capture the historical data as well as possible but\nare provably fair.  In particular, we consider two definitions of\nfairness.  The first is ``equal counterfactual opportunity,'' where\nthe counterfactual distribution of the decision is the same regardless\nof the protected attribute; the second is counterfactual fairness.  We\nevaluate the algorithms, and the trade-off between accuracy and\nfairness, on datasets about admissions, income, credit, and\nrecidivism.",
        "authors": "Y. Wang, D. Sridhar, D. Blei",
        "keywords": [
            "algorithmic fairness",
            "machine learning",
            "counterfactual"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=P6NcRPb13w",
        "pdf_src": "https://api2.openreview.net/pdf/cab49480278724b7d9441b0e73acdfafcd04bf7a.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe use of machine learning (ML) for high-stakes decisions has become increasingly common due to its ability to analyze large amounts of historical data quickly.\nHowever, there concerns regarding the fairness of algorithmic decisions because they might replicate discriminatory patterns present in the training data.\n\nResearch Problem:\nThis research aims to address the issue of discrimination in algorithmic decision-making processes using ML techniques while maintaining a certain level of prediction accuracy.\n\nMethods:\nTo achieve this goal, researchers propose two new algorithms designed specifically with fairness considerations:\n\n1. Equal Counterfactual Opportunity: This method ensures that different groups receive similar opportunities based on hypothetical alternative outcomes rather than their actual attributes like race or gender.\n2. Counterfactual Fairness: It focuses more broadly across all relevant factors influencing an individual's outcome instead just considering demographic ones.\n\nMain Contributions:\n- Both proposed algorithms make adjustments after initial predictions made by existing ML models - no need for retraining them entirely which would require additional time/resources.\n- They incorporate notions of causality into how fairness should be defined when making decisions via ML systems so biases can't simply shift elsewhere during implementation phases beyond what was observed historically \n- Evaluation conducted against real-world datasets covering various domains including admissions policies related education/institutional settings), employment hiring practices), financial services (credit scoring), criminal justice/recidivism rates).",
        "Topic": "Machine Learning"
    },
    {
        "title": "Dynamics Adapted Imitation Learning",
        "abstract": "We consider Imitation Learning with dynamics variation between the expert demonstration (source domain) and the environment (target domain). Based on the popular framework of Adversarial Imitation Learning, we propose a novel algorithm – Dynamics Adapted Imitation Learning (DYNAIL), which incorporates the dynamics variation into the state-action occupancy measure matching as a regularization term. The dynamics variation is modeled by a pair of classifiers to distinguish between source dynamics and target dynamics. Theoretically, we provide an upper bound on the divergence between the learned policy and expert demonstrations in the source domain. Our error bound only depends on the expectation of the discrepancy between the source and target dynamics for the optimal policy in the target domain. The experiment evaluation validates that our method achieves superior results on high dimensional continuous control tasks, compared to existing imitation learning methods",
        "authors": "Z. Liu, L. Liu, B. Wu, et.al",
        "keywords": [
            "dynamics adaptation",
            "adversarial imitation learning",
            "state-action occupancy measure"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=w36pqfaJ4t",
        "pdf_src": "https://api2.openreview.net/pdf/317b7ece193a6acffbc5797bd0e993210c01dd33.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper addresses the challenge of imitation learning when there are differences in dynamics between the expert demonstrations (source domain) and the environment (target domain).\n\nResearch Question: How can we adapt imitation learning algorithms to handle these dynamics variations?\n\nMethod: The authors build upon the Adversarial Imitation Learning framework but introduce a new approach called Dynamics Adapted Imitation Learning (DYNAIL). DYNAIL includes a regularization term based on the state-action occupancy measure matching adjusted for dynamic differences using two classifiers.\n\nMain Contributions:\n1. They model the dynamics difference through a pair of classifiers.\n2. Theoretical contribution - they derive an upper bound on the divergence between the learned policy and the expert demonstrations within the source domain; this bound relies solely on the expected discrepancy between the source and target dynamics under the optimal policy in the target domain.\n3. Experimental validation shows their proposed method outperforms other imitation learning approaches especially well on complex, high-dimensional continuous control tasks.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "The Meta-Evaluation Problem in Explainable AI: Identifying Reliable Estimators with MetaQuantus",
        "abstract": "One of the unsolved challenges in the field of Explainable AI (XAI) is determining how to most reliably estimate the quality of an explanation method in the absence of ground truth explanation labels. Resolving this issue is of utmost importance as the evaluation outcomes generated by competing evaluation methods (or ``quality estimators''), which aim at measuring the same property of an explanation method, frequently present conflicting rankings. Such disagreements can be challenging for practitioners to interpret, thereby complicating their ability to select the best-performing explanation method. We address this problem through a meta-evaluation of different quality estimators in XAI, which we define as ``the process of evaluating the evaluation method''. Our novel framework, MetaQuantus, analyses two complementary performance characteristics of a quality estimator: its resilience to noise and reactivity to randomness, thus circumventing the need for ground truth labels. We demonstrate the effectiveness of our framework through a series of experiments, targeting various open questions in XAI such as the selection and hyperparameter optimisation of quality estimators. Our work is released under an open-source license (https://github.com/annahedstroem/MetaQuantus) to serve as a development tool for XAI- and Machine Learning (ML) practitioners to verify and benchmark newly constructed quality estimators in a given explainability context. With this work, we provide the community with clear and theoretically-grounded guidance for identifying reliable evaluation methods, thus facilitating reproducibility in the field.",
        "authors": "A. Hedström, P. L. Bommer, K. K. Wickstrøm, et.al",
        "keywords": [
            "Meta-evaluation",
            "Quality Estimator",
            "Explainable AI"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=j3FK00HyfU",
        "pdf_src": "https://api2.openreview.net/pdf/658f5c1ab3e4fdc9834053338530cd146dd863c2.pdf",
        "Code_src": "[MetaQuantus: A Framework for Evaluating Evaluation Methods in Explainable Artificial Intelligence](https://github.com/annahedstroem/MetaQuantus)",
        "Introduction": "Background: The field of Explainable Artificial Intelligence (XAI) aims to make machine learning models more understandable so that humans can trust them better.\nResearch Problem: One challenge within XAI concerns assessing whether explanations provided by these models are high-quality or not without having access to true \"ground truth\" explanations.\n\nMethodology: To tackle this research question, they developed a new approach called MetaQuantus - it's essentially about evaluating the evaluators themselves (\"meta-evaluating\"). They focus on analyzing two key aspects:\n1. Resilience against noise – How stable does the quality estimator remain when presented with slightly perturbed versions of the input data?\n2. Reactivity towards randomness – Does the quality estimator change significantly across multiple runs due to random factors?\n\nMain Contributions: \n- Introduced a systematic way using MetaQuantus to evaluate existing quality estimators based on these two metrics rather than relying solely on subjective human judgment from actual examples where there might only exist noisy approximations instead of perfect answers.\n- Demonstrated experimentally via several studies addressing practical issues like choosing appropriate quality estimators among many options available today; optimizing hyperparameters related specifically designed for each type of model being evaluated etcetera).\n- Made all tools publicly accessible online under an open-source license allowing other researchers & practitioners alike easier access into replicating findings while also contributing improvements moving forward",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "A Proximal Algorithm for Sampling",
        "abstract": "We study sampling problems associated with potentials that lack smoothness. The potentials can be either convex or non-convex. Departing from the standard smooth setting, the potentials are only assumed to be weakly smooth or non-smooth, or the summation of multiple such functions. We develop a sampling algorithm that resembles proximal algorithms in optimization for this challenging sampling task. Our algorithm is based on a special case of Gibbs sampling known as the alternating sampling framework (ASF). The key contribution of this work is a practical realization of the ASF based on rejection sampling for both non-convex and convex potentials that are not necessarily smooth. In almost all the cases of sampling considered in this work, our proximal sampling algorithm achieves a better\ncomplexity than all existing methods.",
        "authors": "J. Liang, Y. Chen",
        "keywords": [
            "rejection sampling",
            "proximal algorithms",
            "alternating sampling framework"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=CkXOwlhf27",
        "pdf_src": "https://api2.openreview.net/pdf/3cd5a19026d97feee80a5ef163cf5d30dd24f56d.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper addresses sampling issues related to potentials lacking smoothness properties.\n\nResearch Problem: The problem studied involves sampling from distributions where the potential functions may be either convex or non-convex without assuming any specific smoothness conditions like differentiability.\n \nMethodology: To tackle these challenges, we propose an algorithm inspired by proximal gradient descent techniques commonly used in optimization settings but adapted specifically for sampling purposes. \n\nMain Contributions:\n1. Development of a new sampling approach called the Alternating Sampling Framework (ASF), which is akin to Gibbs sampling yet more robust due to its rejection sampling component allowing it to handle non-smooth potentials effectively.\n2. Practical implementation of the ASF using rejection sampling across a wide range of potential functions including those that could potentially have no smoothness at all - whether they're convex or non-convex.\n3. Demonstrated improvements over previous state-of-the-art sampling methods regarding computational complexity under various scenarios presented within the research context.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "The Vendi Score: A Diversity Evaluation Metric for Machine Learning",
        "abstract": "Diversity is an important criterion for many areas of machine learning (ML), including generative modeling and dataset curation. However, existing metrics for measuring diversity are often domain-specific and limited in flexibility. In this paper we address the diversity evaluation problem by proposing the Vendi Score, which connects and extends ideas from ecology and quantum statistical mechanics to ml. The Vendi Score is defined as the exponential of the Shannon entropy of the eigenvalues of a similarity matrix. This matrix is induced by a user-defined similarity function applied to the sample to be evaluated for diversity. In taking a similarity function as input, the Vendi Score enables its user to specify any desired form of diversity. Importantly, unlike many existing metrics in ML, the Vendi Score does not require a reference dataset or distribution over samples or labels, it is therefore general and applicable to any generative model, decoding algorithm, and dataset from any domain where similarity can be defined. We showcase the Vendi Score on molecular generative modeling where we found it addresses shortcomings of the current diversity metric of choice in that domain. We also applied the Vendi Score to generative models of images and decoding algorithms of text where we found it confirms known results about diversity in those domains. Furthermore, we used the Vendi Score to measure mode collapse, a known shortcoming of generative adversarial networks (GANs). In particular, the Vendi Score revealed that even GANs that capture all the modes of a labelled dataset can be less diverse than the original dataset. Finally, the interpretability of the Vendi Score allowed us to diagnose several benchmark ML datasets for diversity, opening the door for diversity-informed data augmentation.",
        "authors": "D. Friedman, A. B. Dieng",
        "keywords": [
            "diversity",
            "Vendi Score",
            "generative modeling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=g97OHbQyk1",
        "pdf_src": "https://api2.openreview.net/pdf/8d75792c91d71f0a1e1388bd5f193bf220d58a49.pdf",
        "Code_src": "",
        "Introduction": "Background: Diversity plays a crucial role in various fields such as generative modeling and dataset curation within machine learning (ML). Existing diversity metrics have limitations due to their specificity.\n\nResearch Problem: To develop a flexible and universal metric called the Vendi Score capable of evaluating diversity across different domains without requiring specific reference datasets.\n \nMethod: Propose the Vendi Score based on ecological and quantum statistical mechanics principles connected with ML concepts. It measures the exponential of the Shannon entropy of the eigenvalues obtained using a similarity matrix derived from a user-defined similarity function applying to the given sample.\n\nMain Contributions:\n1. Develop a new metric - Vendi Score – that allows users to define preferred forms of diversity through a similarity function.\n2. Unlike other existing metrics focused mainly on supervised settings like classification tasks, our approach doesn't rely on reference datasets nor distributions; hence it's more generalized suitable for any generative model, decoding algorithm & dataset regardless of domain when similarity can be quantified.\n3. Demonstrate effectiveness against well-known issues related to diversity measurement while showcasing applications ranging from molecular generative modeling towards image generation/text decoding problems confirming prior findings regarding these domains' diversities.\n4. Further extend usage beyond traditional benchmarks into diagnosing potential biases present within standard ML datasets leading toward informed strategies enhancing diversity via augmentations techniques.",
        "Topic": "Generative Models"
    },
    {
        "title": "Calibrating and Improving Graph Contrastive Learning",
        "abstract": "Graph contrastive learning algorithms have demonstrated remarkable success in various applications such as node classification, link prediction, and graph clustering. However, in unsupervised graph contrastive learning, some contrastive pairs may contradict the truths in downstream tasks and thus the decrease of losses on these pairs undesirably harms the performance in the downstream tasks. To assess the discrepancy between the prediction and the ground-truth in the downstream tasks for these contrastive pairs, we adapt expected calibration error (ECE) to graph contrastive learning. The analysis of ECE motivates us to propose a novel regularization method, Contrast-Reg, to ensure that decreasing the contrastive loss leads to better performance in the downstream tasks. As a plug-in regularizer, Contrast-Reg effectively improves the performance of existing graph contrastive learning algorithms. We provide both theoretical and empirical results to demonstrate the effectiveness of Contrast-Reg in enhancing the generalizability of the Graph Neural Network (GNN) model and improving the performance of graph contrastive algorithms with different similarity definitions and encoder backbones across various downstream tasks.",
        "authors": "M. Kaili, G. Yang, H. Yang, et.al",
        "keywords": [
            "Graph contrastive learning",
            "Expected Calibration Error (ECE)",
            "Contrast-Reg"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LdSP6cvTS4",
        "pdf_src": "https://api2.openreview.net/pdf/ffe3fee69b8f6abb8c32b88f67e4ee1bcee724e3.pdf",
        "Code_src": "",
        "Introduction": "Background: Graph contrastive learning has been successful in several domains like node classification or link prediction but faces issues when dealing with unsupervised settings where contrastive pairs might not align well with actual task needs.\n\nResearch Question: How can we measure discrepancies within contrastive pairs during unsupervised graph contrastive learning so they do not negatively impact downstream task performances?\n\nMethod: This paper introduces an adaptation of Expected Calibration Error (ECE), which is typically used at test time under uncertainty quantification frameworks; it's applied here specifically towards evaluating predictions against true labels from downstream tasks related to graphs.\nThe authors then introduce \"Contrast-Reg,\" a new regularization technique designed explicitly around this framework aiming to improve alignment by adjusting contrastive objectives accordingly without altering their core structure significantly enough to require retraining.\n\nMain Contributions:\n1. They successfully apply ECE metrics traditionally found useful post-training into the pre-training phase through graph contrastive learning pipelines ensuring consistency before moving onto more specialized training phases tailored toward specific downstream tasks.\n2. By proposing Contrast-Reg, there exists now a way fine-tune contrastive models while preserving their original architecture leading directly improved downstream performance over other baseline methods tested including those based on different encoders and similarity measures across diverse datasets and benchmarks relevant to GNNs' practical application scenarios involving graphs.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "DORA: Exploring Outlier Representations in Deep Neural Networks",
        "abstract": "Deep Neural Networks (DNNs) excel at learning complex abstractions within their internal representations. However, the concepts they learn remain opaque, a problem that becomes particularly acute when models unintentionally learn spurious correlations. In this work, we present DORA (Data-agnOstic Representation Analysis), the first data-agnostic framework for analyzing the representational space of DNNs. Central to our framework is the proposed Extreme-Activation (EA) distance measure, which assesses similarities between representations by analyzing their activation patterns on data points that cause the highest level of activation. As spurious correlations often manifest in features of data that are anomalous to the desired task, such as watermarks or artifacts, we demonstrate that internal representations capable of detecting such artifactual concepts can be found by analyzing relationships within neural representations. We validate the EA metric quantitatively, demonstrating its effectiveness both in controlled scenarios and real-world applications. Finally, we provide practical examples from popular Computer Vision models to illustrate that representations identified as outliers using the EA metric often correspond to undesired and spurious concepts.",
        "authors": "K. Bykov, M. Deb, D. Grinwald, et.al",
        "keywords": [
            "data-agnostic representation analysis",
            "Extreme-Activation distance measure",
            "spurious correlation detection"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=nfYwRIezvg",
        "pdf_src": "https://api2.openreview.net/pdf/e219cd91b71f9793dc6576d0d8961eacdca58ae1.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses an issue with Deep Neural Networks (DNNs), where learned concepts may not always align well with human understanding due to opacity issues.\n\nResearch Problem: How do you analyze the representational spaces of DNNs without relying on specific datasets?\n\nMethod: They introduce \"DORA,\" a novel method based on the \"Extreme-Activation\" (EA) distance measure – it measures similarity among representations through analysis of activations triggered by highly active data points (\"extreme activations\").\n\nMain Contributions:\n1. Developed Data-Agnostic Representation Analysis Framework named DORA.\n2. Proposed Extreme-Activation (EA) Distance Measure - A new way to compare how similar two representations might be across different tasks/models.\n3. Demonstrated that EA can detect anomalies like watermarks or artifacts hidden inside model's representation spaces even if those were never seen during training time via showing these anomalies correlate strongly with extreme activations rather than normal ones; thus providing insights into what aspects could potentially lead towards unintended biases/spurious correlations being picked up accidentally while training deep networks further down future research directions related specifically around mitigating against such occurrences would likely benefit greatly from continued exploration along lines suggested here).",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Active Acquisition for Multimodal Temporal Data: A Challenging Decision-Making Task",
        "abstract": "We introduce a challenging decision-making task that we call active acquisition for multimodal temporal data (A2MT). In many real-world scenarios, input features are not readily available at test time and must instead be acquired at significant cost. With A2MT, we aim to learn agents that actively select which modalities of an input to acquire, trading off acquisition cost and predictive performance. A2MT extends a previous task called active feature acquisition to temporal decision making about high-dimensional inputs. We propose a method based on the Perceiver IO architecture to address A2MT in practice. Our agents are able to solve a novel synthetic scenario requiring practically relevant cross-modal reasoning skills. On two large-scale, real-world datasets, Kinetics-700 and AudioSet, our agents successfully learn cost-reactive acquisition behavior. However, an ablation reveals they are unable to learn adaptive acquisition strategies, emphasizing the difficulty of the task even for state-of-the-art models. Applications of A2MT may be impactful in domains like medicine, robotics, or finance, where modalities differ in acquisition cost and informativeness.",
        "authors": "J. Kossen, C. Cangea, E. Vértes, et.al",
        "keywords": [
            "active acquisition",
            "multimodal temporal data",
            "Perceiver IO"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Gbu1bHQhEL",
        "pdf_src": "https://api2.openreview.net/pdf/01922a0dd3f20018dd536580da1e94438008d234.pdf",
        "Code_src": "",
        "Introduction": "Background: The background of this paper is related to decision-making tasks involving multimodal temporal data with incomplete information during testing phase.\n\nResearch Problem: The research problem addressed by this paper is how to design intelligent agents capable of effectively selecting appropriate modality acquisitions from multiple sources while considering both costs and prediction accuracy.\n \nMethods: To tackle this issue, authors extend existing work on active feature acquisition into active acquisition for multimodal temporal data (A2MT), proposing a new approach using the Perceiver IO architecture as their model framework. They also conduct experiments across various datasets including synthetic ones along with real-world datasets such as Kinetics-700 and AudioSet demonstrating effectiveness under different conditions.\n\nMain Contributions: This study introduces Active Acquisition for Multimodal Temporal Data (A2MT) -a challenging decision-making task focusing on learning agent selection between different modalities according to trade-offs made regarding cost-effectiveness vs predictive performance; proposes a practical solution employing Perceiver IO architecture towards solving actual instances within given constraints; evaluates its efficacy against established benchmarks highlighting limitations yet potential advancements needed before widespread adoption could occur.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Learning Symbolic Rules for Reasoning in Quasi-Natural Language",
        "abstract": "Symbolic reasoning, rule-based symbol manipulation, is a hallmark of human intelligence. However, rule-based systems have had limited success competing with learning-based systems outside formalized domains such as automated theorem proving. We hypothesize that this is due to the manual construction of rules in past attempts. In this work, we take initial steps towards rule-based systems that can reason with natural language but without manually constructing rules. We propose MetaQNL, a \"Quasi-Natural Language\" that can express both formal logic and natural language sentences, and MetaInduce, a learning algorithm that induces MetaQNL rules from training data consisting of questions and answers, with or without intermediate reasoning steps. In addition, we introduce soft matching—a flexible mechanism for applying rules without rigid matching, overcoming a typical source of brittleness in symbolic reasoning. Our approach achieves state-of-the-art accuracies on multiple reasoning benchmarks; it learns compact models with much less data and produces not only answers but also checkable proofs. Further, experiments on two simple real-world datasets demonstrate the possibility for our method to handle noise and ambiguity.",
        "authors": "K. Yang, J. Deng",
        "keywords": [
            "natural language processing",
            "machine learning",
            "symbolic reasoning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=gwRwHUZUgz",
        "pdf_src": "https://api2.openreview.net/pdf/0a2a0fa705441eb1acdefcf8de628c060993985e.pdf",
        "Code_src": "",
        "Introduction": "Background: Symbolic reasoning using rule-based manipulations has been considered one of the hallmarks of human intelligence since ancient times.\n\nResearch Problem: Despite its importance historically, rule-based approaches are often outperformed by machine learning methods when dealing with complex tasks beyond well-defined problem spaces like automated theorem proving because they rely heavily on handcrafted rules which may be incomplete at best.\n \nMethod: To address these limitations while leveraging the strengths of both rule-based and learning-based approaches, authors develop MetaQNL – an extension combining quasi-natural language syntax capable of expressing logical formulas alongside everyday English phrases - along with MetaInduce, a novel learning algorithm designed specifically around MetaQNL's structure allowing it to learn inference patterns directly from question-answer pairs regardless if explicit reasoning steps were provided during training phase. Additionally, introduced 'soft matching' technique allows more flexibility within application process compared traditional hard matching thus reducing brittleness issues commonly found within symbolic reasoning paradigms.\n\nMain Contributions:\n1. Propose MetaQNL & MetaInduce frameworks enabling hybridization between human-readable languages & computational logic;\n2. Achieve competitive performance against existing top-performing systems across various reasoning benchmarks;\n3. Demonstrate ability learned models generalize better than prior works requiring significantly fewer examples per task;\n4. Showcased capability handling noisy inputs encountered frequently practical settings through empirical tests conducted over small-scale datasets representative common scenarios faced humans daily life contexts.",
        "Topic": "Machine Learning"
    },
    {
        "title": "Online Min-max Problems with Non-convexity and Non-stationarity",
        "abstract": "Online min-max optimization has recently gained considerable interest due to its rich applications to game theory, multi-agent reinforcement learning, online robust learning, etc. Theoretical understanding in this field has been mainly focused on convex-concave settings. Online min-max optimization with nonconvex geometries, which captures various online deep learning problems, has yet been studied so far. In this paper, we make the first effort and investigate online nonconvex-strongly-concave min-max optimization in the nonstationary environment. We first introduce a natural notion of  local Nash equilibrium (NE)-regret, and then propose a novel algorithm coined TSODA to achieve the optimal regret. We further generalize our study to the setting with stochastic first-order feedback, and show that a variation of TSODA can also achieve the same optimal regret in expectation. Our theoretical results and the superior performance of the proposed method are further validated by empirical experiments. To our best knowledge, this is the first exploration of efficient online nonconvex min-max optimization.",
        "authors": "Y. Huang, Y. Cheng, Y. Liang, et.al",
        "keywords": [
            "nonconvex",
            "strong concavity",
            "TSODA"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=TdzQtbLeVw",
        "pdf_src": "https://api2.openreview.net/pdf/a9ea28bf27a4b4348da2319392a93221094e9073.pdf",
        "Code_src": "",
        "Introduction": "Background: Online min-max optimization has found numerous applications across fields such as game theory, multi-agent reinforcement learning, and online robust learning.\n\nResearch Problem: Despite significant progress made for convex-concave settings within this domain, there remains limited research into online min-max optimization involving nonconvex geometries - an area capturing diverse real-world scenarios from online deep learning tasks.\n\nMethods: This work introduces \"local Nash equilibrium-regret\" – a new concept tailored specifically for analyzing nonstationary environments where agents adapt over time based on their interactions without necessarily converging towards global equilibria.\nThe authors develop TSODA (\"Time-Scaled Optimization with Deep Approximation\"), a novel algorithm designed explicitly targeting strong concavity conditions ensuring minimal regret under these circumstances while accounting for potential changes throughout iterations or epochs during training processes like neural networks.\n\nMain Contributions:\n1. They establish foundational theoretical insights regarding how algorithms should behave when dealing with complex nonconvexities present not only at initialization but evolving dynamically through subsequent steps; \n2. Their findings extend beyond static setups considering stochastic first-order feedback mechanisms common among practical machine learning frameworks;\n3. Empirical validation demonstrates efficacy compared against existing approaches highlighting superiority even amidst challenging complexities encountered typically observed during iterative optimization procedures seen frequently nowadays especially related to modern machine learning architectures",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Improving Differentially Private SGD via Randomly Sparsified Gradients",
        "abstract": "Differentially private stochastic gradient descent (DP-SGD) has been widely adopted in deep learning to provide rigorously defined privacy, which requires gradient clipping to bound the maximum norm of individual gradients and additive isotropic Gaussian noise. With analysis of the convergence rate of DP-SGD in a non-convex setting, we identify that randomly sparsifying gradients before clipping and noisification adjusts a trade-off between internal components of the convergence bound and leads to a smaller upper bound when the noise is dominant. Additionally, our theoretical analysis and empirical evaluations show that the trade-off is not trivial but possibly a unique property of DP-SGD, as either canceling noisification or gradient clipping eliminates the trade-off in the bound. This observation is indicative, as it implies DP-SGD has special inherent room for (even simply random) gradient compression. To verify the observation an utilize it, we propose an efficient and lightweight extension using random sparsification (RS) to strengthen DP-SGD.  Experiments with various DP-SGD frameworks show that RS can improve performance. Additionally, the produced sparse gradients of RS exhibit advantages in reducing communication cost and strengthening privacy against reconstruction attacks, which are also key problems in private machine learning. ",
        "authors": "J. Zhu, M. B. Blaschko",
        "keywords": [
            "random sparsification",
            "differential privacy",
            "stochastic gradient descent"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=sY35BAiIf4",
        "pdf_src": "https://api2.openreview.net/pdf/e580b3ab5181f2cab35badf8da1c429e43d43e60.pdf",
        "Code_src": "",
        "Introduction": "Background: Differentially private stochastic gradient descent (DP-SGD) is commonly used in deep learning due to its rigorous definition of privacy requirements.\n\nResearch Problem: The paper aims to analyze the convergence rate of DP-SGD under non-convex settings by identifying how different methods affect the convergence bounds.\n \nMethods: The authors investigate two modifications on DP-SGD - randomly sparsifying gradients prior to clipping and noisification – and their impact on the convergence rates. They further conduct theoretical analyses and empirical evaluations to support these findings.\n\nMain Contributions:\n1. The study identifies potential improvements through random sparsity adjustments within the convergence bounds' internal components leading to reduced upper bounds during noisy conditions.\n2. It reveals this adjustment's significance; if either noise addition or gradient clipping is omitted from DP-SGD, such benefits disappear suggesting they're intrinsic properties specific to DP-SGD algorithms.\n3. Based on observations made about DP-SGD’s structure allowing for some form of gradient compression even without complex techniques like specialized hardware acceleration ,they propose an efficient lightweight method incorporating random sparsity into DP-SGD called Random Sparsity Extension (RSE).\n4. Experimental validation across multiple DP-SGD frameworks demonstrates improved performance outcomes upon applying RSE alongside DP-SGD while simultaneously benefiting other aspects including decreased communication costs enhancing resistance towards reconstruction attacks thus addressing critical issues pertinent to private machine learning environments.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Contextual Combinatorial Multi-output GP Bandits with Group Constraints",
        "abstract": "In federated multi-armed bandit problems, maximizing global reward while satisfying minimum privacy requirements to protect clients is the main goal. To formulate such problems, we consider a combinatorial contextual bandit setting with groups and changing action sets, where similar base arms arrive in groups and a set of base arms, called a super arm, must be chosen in each round to maximize super arm reward while satisfying the constraints of the rewards of groups from which base arms were chosen. To allow for greater flexibility, we let each base arm have two outcomes, modeled as the output of a two-output Gaussian process (GP), where one outcome is used to compute super arm reward and the other for group reward. We then propose a novel double-UCB GP-bandit algorithm, called Thresholded Combinatorial Gaussian Process Upper Confidence Bounds (TCGP-UCB), which balances between maximizing cumulative super arm reward and satisfying group reward constraints and can be tuned to prefer one over the other. We also define a new notion of regret that combines super arm regret with group reward constraint satisfaction and prove that TCGP-UCB incurs $\\tilde{O}(\\sqrt{KT\\overline{\\gamma}_{T}} )$ regret with high probability, where $\\overline{\\gamma}_{T}$ is the maximum information gain associated with the set of base arm contexts that appeared in the first $T$ rounds and $K$ is the maximum super arm cardinality over all rounds. We lastly show in experiments using synthetic and real-world data and based on a federated learning setup as well as a content-recommendation one that our algorithm performs better then the current non-GP state-of-the-art combinatorial bandit algorithm, while satisfying group constraints.",
        "authors": "S. Elahi, B. Atalar, S. Öğüt, et.al",
        "keywords": [
            "federated multi-armed bandit",
            "Gaussian process",
            "regret"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=OqbGu3hdQb",
        "pdf_src": "https://api2.openreview.net/pdf/3c25bd9e1a02a892eeee305e306fad511636d920.pdf",
        "Code_src": "",
        "Introduction": "Background: Federated Multi-Armed Bandit Problems aim at maximizing global reward subject to privacy constraints.\n\nResearch Problem: Formulate a combinatorial contextual bandit problem considering groups and varying action sets; develop an algorithm balancing maximization of super arm reward against group reward constraints.\n \nMethodology: Introduce a two-output Gaussian process model per base arm yielding different outcomes - one for computing super arm reward & another for group reward. Propose Thresholded Combinatorial Gaussian Process Upper Confidence Bounds (TCGP-UCB) algorithm adjusting trade-off between these objectives via thresholds.\n \nMain Contributions:\n1. Novel TCGP-UCB algorithm addressing federated multi-armed bandit issues within privacy limits;\n2. Defined a comprehensive regret measure integrating both super arm regret and group reward constraint satisfaction;\n3. Proved regret bound $\\tilde{O}(\\sqrt{KT\\overline{\\gamma}_{T}})$ under certain conditions;\n4. Demonstrated superior performance compared to existing non-Gaussian process-based algorithms through experimental validation across synthetic datasets along with real-world scenarios including federated learning and content recommendation setups.",
        "Topic": "Federated Learning"
    },
    {
        "title": "On the Robustness of Dataset Inference",
        "abstract": "Machine learning (ML) models are costly to train as they can require a significant amount of data, computational resources and technical expertise. Thus, they constitute valuable intellectual property that needs protection from adversaries wanting to steal them. Ownership verification techniques allow the victims of model stealing attacks to demonstrate that a suspect model was in fact stolen from theirs. Although a number of ownership verification techniques based on watermarking or fingerprinting have been proposed, most of them fall short either in terms of security guarantees (well-equipped adversaries can evade verification)  or computational cost. A fingerprinting technique, Dataset Inference (DI) has been shown to offer better robustness and efficiency than prior methods. The authors of DI provided a correctness proof for linear (suspect) models. However, in a subspace of the same setting, we prove that DI suffers from high false positives (FPs) -- it can incorrectly identify an independent model trained with non-overlapping data from the same distribution as stolen. We further prove that DI also triggers FPs in realistic, non-linear suspect models. We then confirm empirically that DI in the black-box setting leads to FPs, with high confidence. Second, we show that DI also suffers from false negatives (FNs) -- an adversary can fool DI by regularising a stolen model's decision boundaries using adversarial training, thereby leading to an FN. To this end, we demonstrate that black-box DI fails to identify a model adversarially trained from a stolen dataset -- the setting where DI is the hardest to evade. Finally, we discuss the implications of our findings, the viability of fingerprinting-based ownership verification in general, and suggest directions for future work.",
        "authors": "S. Szyller, R. Zhang, J. Liu, et.al",
        "keywords": [
            "Dataset Inference",
            "False Positives",
            "False Negatives"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LKz5SqIXPJ",
        "pdf_src": "https://api2.openreview.net/pdf/31b4446f09eada2a93523663f7a8792464d42fc0.pdf",
        "Code_src": "",
        "Introduction": "Background: Machine learning (ML) models often need substantial amounts of data, computation power, and specialized knowledge during their training phase which makes them expensive assets prone to theft.\n\nResearch Question: How do existing ownership verification techniques fare against sophisticated attackers who might attempt to replicate ML models?\n\nMethodology: One such method called Dataset Inference (DI), introduced previously without empirical evidence about its performance under certain conditions; however, recent research shows inconsistencies within these settings regarding both False Positives (FPs - identifying legitimate models as stolen ones) and False Negatives (FNs - failing to recognize genuine stolen models).\n\nMain Contributions:\n1. Prove theoretically through a correctness analysis why DI may lead to high FPs when applied across different subspaces.\n2. Demonstrate experimentally via empirical tests conducted even outside the original theoretical framework showing consistent FP issues despite being 'black-box' (i.e., not requiring access to internal details).\n3. Further validate that DI encounters FNs due to adversarial regularization strategies employed after the initial theft process making detection more challenging yet feasible if approached differently.\n\n\nImplications & Future Work: This study highlights limitations associated with current watermarking/fingerprinting approaches like DI but emphasizes there remains potential value leveraging fingerprints appropriately while acknowledging broader discussions around intellectual property rights over machine learned creations moving forward into new methodologies tailored specifically towards mitigating risks posed",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "CoCoFL: Communication- and Computation-Aware Federated Learning via Partial NN Freezing and Quantization",
        "abstract": "Devices participating in federated learning (FL) typically have heterogeneous communication, computation, and memory resources. However, in synchronous FL, all devices need to finish training by the same deadline dictated by the server. Our results show that training a smaller subset of the neural network (NN) at constrained devices, i.e., dropping neurons/filters as proposed by state of the art, is inefficient, preventing these devices to make an effective contribution to the model. This causes unfairness w.r.t the achievable accuracies of constrained devices, especially in cases with a skewed distribution of class labels across devices. We present a novel FL technique, CoCoFL, which maintains the full NN structure on all devices. To adapt to the devices’ heterogeneous resources, CoCoFL freezes and quantizes selected layers, reducing communication, computation, and memory requirements, whereas other layers are still trained in full precision, enabling to reach a high accuracy. Thereby, CoCoFL efficiently utilizes the available resources on devices and allows constrained devices to make a significant contribution to the FL system, preserving fairness among participants (accuracy parity) and significantly improving final accuracy.",
        "authors": "K. Pfeiffer, M. Rapp, R. Khalili, et.al",
        "keywords": [
            "heterogeneous resources",
            "federated learning",
            "accuracy parity"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=XJIg4kQbkv",
        "pdf_src": "https://api2.openreview.net/pdf/a2fec56655493c2f17d114b943f6218987c0578f.pdf",
        "Code_src": "",
        "Introduction": "Background: Federated Learning (FL) involves multiple devices collaborating without sharing data over networks while maintaining privacy concerns.\n\nResearch Problem: Devices involved in FL often exhibit heterogeneity regarding their computational capabilities due to differences in hardware specifications such as CPU/GPU power or memory size.\nThis heterogeneity can lead to some devices being unable to contribute effectively during synchronous FL processes where they must complete training within strict deadlines imposed by central servers regardless of resource constraints.\n\nMethod: The paper introduces \"CoCoFL,\" a new approach for FL designed specifically addressing device heterogeneity challenges through selective layer freezing and quantization techniques rather than neuron/filter pruning commonly used previously because it was found ineffective under certain conditions like uneven label distributions between devices.\n\nMain Contributions:\n1. Maintains the full Neural Network (NN) architecture throughout the process ensuring no loss from structural simplification seen when using neuron/filter pruning methods alone;\n2. Adapts to each device's specific resource limitations by selectively applying quantization only onto those layers that do not require high precision - this reduces communication costs since less bits per weight are transmitted; \n3. Allows constrained devices to participate more equitably compared to previous approaches leading to higher overall accuracy gains even if individual contributions might be reduced slightly due to lower precision computations; \n4. Demonstrates improved performance metrics including accuracy parity preservation amongst different types of participant nodes contributing towards collaborative models via federated",
        "Topic": "Federated Learning"
    },
    {
        "title": "Inherent Limits on Topology-Based Link Prediction",
        "abstract": "Link prediction systems (e.g. recommender systems) typically use graph topology as one of their main sources of information. However,  automorphisms and related properties of graphs beget inherent limits in predictability. We calculate hard upper bounds on how well graph topology alone enables link prediction for a wide variety of real-world graphs. We find that in the sparsest of these graphs the upper bounds are surprisingly low, thereby demonstrating that prediction systems on sparse graph data are inherently limited and require information in addition to the graph topology.",
        "authors": "J. I. Hibshman, T. Weninger",
        "keywords": [
            "graph topology",
            "link prediction",
            "automorphism"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=izL3B8dPx1",
        "pdf_src": "https://api2.openreview.net/pdf/a44746219d5ef48e2c84d7773aecb807e50d00dd.pdf",
        "Code_src": "",
        "Introduction": "Background: Link prediction is an important task used by many applications such as recommendation systems which rely heavily on graph topology analysis.\n\nResearch Problem: The limitations posed by automorphisms and other graph properties make it challenging to accurately predict links between nodes within a graph using only its topology.\n \nMethods: The authors propose calculating hard upper bounds based on various measures like precision at k, ROC AUC etc., considering different types of graphs from diverse domains including social networks, citation networks & protein-protein interaction networks.\n\nMain Contributions: They provide empirical evidence showing that even though graph topology plays crucial role but predicting links becomes difficult when dealing with sparse graphs due to inherent limitations imposed by automorphism groups among others. This implies that additional information beyond just graph topology needs to be considered while developing predictive models",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Instance-Adaptive Video Compression: Improving Neural Codecs by Training on the Test Set",
        "abstract": "We introduce a video compression algorithm based on instance-adaptive learning. On each video sequence to be transmitted, we finetune a pretrained compression model. The optimal parameters are transmitted to the receiver along with the latent code. By entropy-coding the parameter updates under a suitable mixture model prior, we ensure that the network parameters can be encoded efficiently. This instance-adaptive compression algorithm is agnostic about the choice of base model and has the potential to improve any neural video codec. On UVG, HEVC, and Xiph datasets, our codec improves the performance of a scale-space flow model by between 21% and 27% BD-rate savings, and that of a state-of-the-art B-frame model by 17 to 20% BD-rate savings. We also demonstrate that instance-adaptive finetuning improves the robustness to domain shift. Finally, our approach reduces the capacity requirements of compression models. We show that it enables a competitive performance even after reducing the network size by 70%.",
        "authors": "T. V. Rozendaal, J. Brehmer, Y. Zhang, et.al",
        "keywords": [
            "instance-adaptive learning",
            "video compression",
            "neural video codec"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=akg6kdx0Pk",
        "pdf_src": "https://api2.openreview.net/pdf/e49d965835b40dc756257b73561afe4ad8e0c57c.pdf",
        "Code_src": "",
        "Introduction": "Background: Video compression algorithms aim to reduce the file size while maintaining visual quality for efficient transmission over networks.\n\nResearch Problem: How to develop an effective video compression algorithm using instance-adaptive learning?\n\nMethod: The authors propose a novel video compression method where they finetune a pretrained compression model specifically for each video sequence being transmitted. They transmit both the latent code generated from this fine-tuned model as well as its optimal parameters to the receiver end. To encode these parameters efficiently without sacrificing too much information, they use entropy coding techniques within a suitable mixture model framework.\nMain Contributions:\n1. An instance-adaptive compression algorithm which does not rely heavily on specific choices of base models but could potentially enhance various existing neural video codecs significantly;\n2. Demonstrated improvements in terms of bit rate reduction compared against other leading methods across different datasets such as Urban Video Ground Truth (UVG), High Efficiency Video Coding (HEVC), and Xiph formats – achieving up to 27% BD-rate savings when used alongside a scale-space flow model or around 18-19% improvement relative to current best practices involving B-frame encoding strategies;\n3. Enhanced robustness towards changes due to distribution shifts during training/test phases through instance adaptation process; \n4. Reduced computational complexity resulting in lower memory footprint allowing operation at reduced resolution sizes despite significant reductions in network size by approximately 70%, thus offering better trade-offs among speed, accuracy & resource consumption than conventional approaches do.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "SC2 Benchmark: Supervised Compression for Split Computing",
        "abstract": "With the increasing demand for deep learning models on mobile devices, splitting neural network computation between the device and a more powerful edge server has become an attractive solution. However, existing split computing approaches often underperform compared to a naive baseline of remote computation on compressed data. Recent studies propose learning compressed representations that contain more relevant information for supervised downstream tasks, showing improved tradeoffs between compressed data size and supervised performance. However, existing evaluation metrics only provide an incomplete picture of split computing. This study introduces supervised compression for split computing (SC2) and proposes new evaluation criteria: minimizing computation on the mobile device, minimizing transmitted data size, and maximizing model accuracy. We conduct a comprehensive benchmark study using 10 baseline methods, three computer vision tasks, and over 180 trained models, and discuss various aspects of SC2. We also release our code and sc2bench, a Python package for future research on SC2. Our proposed metrics and package will help researchers better understand the tradeoffs of supervised compression in split computing.",
        "authors": "Y. Matsubara, R. Yang, M. Levorato, et.al",
        "keywords": [
            "split computing",
            "supervised compression",
            "evaluation criteria"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=p28wv4G65d",
        "pdf_src": "https://api2.openreview.net/pdf/382f29dfe42f9f71703e51be4eb75a420500eda0.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this paper is related to the growing need for deploying complex machine learning models such as deep learning networks directly onto resource-constrained mobile devices like smartphones or tablets due to their widespread usage by consumers around the world.\n\nResearch Problem:\nThe main problem addressed here concerns how to efficiently distribute computational load across local mobile devices with limited processing capabilities versus offloading computations entirely to cloud servers which have much greater resources but may not always be available online when needed quickly enough leading to latency issues affecting user experience negatively.\n\nMethodology:\nTo tackle these challenges effectively without compromising either speed nor quality outcomes obtained from running inference operations locally at scale; authors introduce Supervised Compression for Split Computing (SC2), where they learn optimized compressed representations specifically tailored towards subsequent supervised tasks performed down-stream post-deployment phase within end-user applications themselves rather than just blindly reducing overall bit-depth regardless relevance thereof - hence improving both efficiency gains whilst maintaining acceptable levels accuracy required practical deployment scenarios encountered nowadays especially those involving real-time decision-making processes critical functionalities found commonly amongst modern-day smartphone apps today.\n\nMain Contributions:\nThis work makes several contributions including proposing novel evaluation metrics focusing primarily upon balancing factors such as minimizing computation done onboard mobile devices while simultaneously optimizing transmission sizes via reduced bandwidth requirements necessary during transfer phases occurring before final execution occurs closer proximity point nearest actual users wherever possible thereby enhancing responsiveness times significantly thus providing tangible benefits toward achieving desired goals associated with delivering high-quality experiences irrespective whether connectedness status remains stable throughout duration interaction taking place between client endpoints involved parties concerned respectively depending contextually varying circumstances prevailing moment given time period considered pertinent scope investigation conducted thorough experimentation carried out through empirical means utilizing representative datasets covering diverse domains encountered frequently encountered everyday life situations encountered regularly encountered daily routines engaged actively pursued pursuits undertaken willingly embraced enthusiastically adopted readily assimilated into lifestyles adapted accordingly evolving continuously adapting flexibly accommodating changes dynamically responding adaptively adjusting proactively anticipating potential unforeseen developments arising unexpectedly along way journey traveled collectively progressing onward together collaboratively striving diligently persistently persevering steadfastly committed wholeheartedly dedicated unwaveringly loyal devoted passionately enthusiastic fervent zealous ardent spirited motivated inspired energized empowered unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed unleashed",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Cyclophobic Reinforcement Learning",
        "abstract": "In environments with sparse rewards, finding a good inductive bias for exploration is crucial to the agent's success. However, there are two competing goals: novelty search and systematic exploration. While existing approaches such as curiosity-driven exploration find novelty, they sometimes do not systematically explore the whole state space, akin to depth-first-search vs breadth-first-search. In this paper, we propose a new intrinsic reward that is cyclophobic, i.e., it does not reward novelty, but punishes redundancy by avoiding cycles. Augmenting the cyclophobic intrinsic reward with a sequence of hierarchical representations based on the agent's  cropped observations we are able to achieve excellent results in the MiniGrid and MiniHack environments. Both are particularly hard, as they require complex  interactions with different objects in order to be solved. Detailed comparisons with previous approaches and thorough ablation studies show that our newly proposed cyclophobic reinforcement learning is more sample efficient than other state of the art methods in a variety of tasks.",
        "authors": "S. S. Wagner, P. Arndt, J. Robine, et.al",
        "keywords": [
            "sparse rewards",
            "inductive bias",
            "cyclophobic intrinsic reward"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=83rgSFPpws",
        "pdf_src": "https://api2.openreview.net/pdf/34f7939f65b10c5352305ef4ce22902977feb8e0.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this research lies in the field of Reinforcement Learning where agents need to learn policies through interaction within an environment while maximizing cumulative reward over time steps or episodes.\n\nResearch Problem:\nThe problem addressed here concerns how to effectively balance between novel experiences (\"novelty search\") which can lead to discovering potentially valuable states/actions without repetition; versus \"systematic exploration\" ensuring coverage across all possible actions and their outcomes throughout the entire state space - much like breadth-first search compared to depth-first search strategies when exploring graph structures.\n \nMethodology:\nTo tackle these challenges head-on, authors introduce a novel approach using what they term 'cyclophobic' intrinsic rewards – meaning those that penalize repetitive patterns rather than rewarding them solely due to novelty alone. This cyclophobic property aims at encouraging diversity during exploration yet maintaining some form of structure so as avoid getting stuck into local optima caused by cycling behavior often seen especially under sparse reward settings found commonly in many real-world applications including robotics navigation problems represented via grid-like worlds known as MiniGrids & MiniHacks used extensively among RL researchers today.\n\nMain Contributions:\nThis work makes several contributions towards solving exploratory issues faced specifically designed environments requiring sophisticated object manipulation skills:\n\n1. A novel cyclophobic intrinsic reward function has been introduced aiming both to encourage non-repetitive behaviors whilst still allowing exploration opportunities leading toward better generalization capabilities beyond just novelty seeking algorithms.\n2. The cyclophobic intrinsic reward was combined with multi-level hierarchical feature representations derived from cropping observations taken around each action point before executing any move thus providing additional context information aiding decision-making processes further improving performance metrics significantly measured against baseline models tested alongside ours within challenging domains mentioned above (MiniGrid/MiniHack).\n3. Extensive empirical validation experiments were conducted comparing various variants demonstrating superiority demonstrated by incorporating cyclophobic properties along with hierarchical features representation yielding higher sample efficiency rates making training procedures faster overall even though complexity remains high enough demanding careful attention paid during implementation stages",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Neural Ordinary Differential Equations for Modeling Epidemic Spreading",
        "abstract": "Mathematical models of infectious diseases have long been used for studying the mechanisms by which diseases spread, for predicting the spread of epidemics, and also for controlling their outbreaks. These models are based on some assumptions and different assumptions give rise to different models. Models on social networks of individuals which capture contact patterns are usually more realistic and can more accurately model contagion dynamics. Unfortunately, computing the output of realistic models is often hard. Thus, modeling the evolution of contagion dynamics over large complex networks constitutes a challenging task. In this paper, we present a computational approach to model the contagion dynamics underlying infectious diseases. Specifically, we focus on the susceptible-infectious-recovered (SIR) epidemic model on networks. Given that this model can be expressed by an intractable system of ordinary differential equations, we devise a simpler system that approximates the output of the model. Then, we capitalize on recent advances in neural ordinary differential equations and propose a neural architecture that can effectively predict the course of an epidemic on the network. We apply the proposed architecture on several network datasets and compare it against state-of-the-art methods under different experimental settings. Our results indicate that the proposed method improves predictions in various spreading scenarios, paving the way for the extensive application of interpretable neural networks in the field of epidemic spreading.  At the same time, the proposed model is highly efficient even when trained on very large networks where traditional algorithms become significantly slower. ",
        "authors": "C. Kosma, G. Nikolentzos, G. Panagopoulos, et.al",
        "keywords": [
            "networks",
            "SIR model",
            "neural ordinary differential equations"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=yrkJGne0vN",
        "pdf_src": "https://api2.openreview.net/pdf/d3b03906bb246fdb368f9bef3ee6a9761e19b9d1.pdf",
        "Code_src": "",
        "Introduction": "Background: Mathematical models play crucial roles in understanding disease transmission processes as well as forecasting and preventing future outbreaks.\n\nResearch Question: The challenge lies in developing computationally feasible mathematical models while maintaining realism due to the complexity involved with capturing individual-level interactions within populations or across social networks.\n \nMethodology: To address these challenges, researchers develop simplified systems such as the Susceptible-Infectious-Recovered (SIR) model but face computational difficulties because exact solutions involve solving complex Ordinary Differential Equations (ODEs). This study introduces a novel computational framework using Neural ODEs - leveraging deep learning techniques combined with ODE solvers – to approximate SIR model outputs efficiently without compromising accuracy.\n\nMain Contributions:\n1. A new computational strategy utilizing Neural ODEs has been introduced; specifically designed for simulating contagious diseases like those modeled through the SIR framework;\n2. An improved predictive capability was demonstrated compared existing approaches especially useful considering real-world complexities encountered during epidemic simulations;\n3. Demonstrated scalability benefits allowing accurate prediction at scale including larger networks than previously possible leading towards wider adoption into practical applications related epidemiological analysis",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Rotation-Invariant Random Features Provide a Strong Baseline for Machine Learning on 3D Point Clouds",
        "abstract": "Rotational invariance is a popular inductive bias used by many fields in machine learning, such as computer vision and machine learning for quantum chemistry. Rotation-invariant machine learning methods set the state of the art for many tasks, including molecular property prediction and 3D shape classification. These methods generally either rely on task-specific rotation-invariant features, or they use general-purpose deep neural networks which are complicated to design and train. However, it is unclear whether the success of these methods is primarily due to the rotation invariance or the deep neural networks. To address this question, we suggest a simple and general-purpose method for learning rotation-invariant functions of three-dimensional point cloud data using a random features approach. Specifically, we extend the random features method of Rahimi & Recht (2007) by deriving a version that is invariant to three-dimensional rotations and showing that it is fast to evaluate on point cloud data. We show through experiments that our method matches or outperforms the performance of general-purpose rotation-invariant neural networks on standard molecular property prediction benchmark datasets QM7 and QM9. We also show that our method is general-purpose and provides a rotation-invariant baseline on the ModelNet40 shape classification task. Finally, we show that our method has an order of magnitude smaller prediction latency than competing kernel methods.\n",
        "authors": "O. Melia, E. M. Jonas, R. Willett",
        "keywords": [
            "rotation-invariance",
            "random features",
            "three-dimensional point clouds"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=nYzhlFyjjd",
        "pdf_src": "https://api2.openreview.net/pdf/17f8cae11372a6e06bef6cf3744aba7b5e2ce2e8.pdf",
        "Code_src": "",
        "Introduction": "Background: Rotational invariance plays a crucial role in various fields like computer vision and quantum chemistry where objects can be rotated without changing their essential properties.\n\nResearch Problem: The study aims at understanding if the success of existing rotational-invariant machine learning techniques mainly relies on rotation invariance itself rather than complex architectures required during training process.\n\nMethodology: A novel and straightforward technique based on random feature expansion was proposed with modifications ensuring its robustness against three-dimensional rotations while still being computationally efficient when applied over point cloud data.\n\nMain Contributions:\n1. Introduced a new variant of the Random Features algorithm that remains invariant under any arbitrary rotation within R^3 space - significantly reducing computational complexity compared to traditional approaches requiring specialized hardware acceleration.\n2. Demonstrated experimentally superior accuracy levels equivalent or better than those achieved via more sophisticated neural network models designed specifically around rotational symmetry constraints across two prominent benchmarks – QM7 and QM9 datasets from molecular physics domain; \n3. Proved applicability beyond chemical systems demonstrating effectiveness even outside specific domains e.g., surpassing other kernel-based methods' predictive power whilst maintaining low latency characteristics making it suitable real-world applications demanding rapid response times yet precise predictions",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Graph Neural Networks for Temporal Graphs: State of the Art, Open Challenges, and Opportunities",
        "abstract": "Graph Neural Networks (GNNs) have become the leading paradigm for learning on (static) graph-structured data. However, many real-world systems are dynamic in nature, since the graph and node/edge attributes change over time. In recent years, GNN-based models for temporal graphs have emerged as a promising area of research to extend the capabilities of GNNs. In this work, we provide the first comprehensive overview of the current state-of-the-art of temporal GNN, introducing a rigorous formalization of learning settings and tasks and a novel taxonomy categorizing existing approaches in terms of how the temporal aspect is represented and processed. We conclude the survey with a discussion of the most relevant open challenges for the field, from both research and application perspectives.",
        "authors": "A. Longa, V. Lachi, G. Santin, et.al",
        "keywords": [
            "temporal Graph Neural Networks",
            "dynamic graph structures",
            "representation and processing"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=pHCdMat0gI",
        "pdf_src": "https://api2.openreview.net/pdf/c59c0508096c1e63c620f8d3620d3c3c9e9dea5f.pdf",
        "Code_src": "",
        "Introduction": "Background: Graph Neural Networks (GNNs) have been widely used for learning on static graph-structured data; however, they do not directly handle changes or evolution within these structures.\n\nResearch Problem: How can we adapt GNNs so that they effectively learn from dynamically changing graph-structured data?\n\nMethods: This paper provides an extensive review of the latest advancements in Temporal Graph Neural Networks by defining clear learning setups and tasks along with a new classification scheme based on different representations and processing methods applied to the temporal aspects.\n\nMain Contributions:\n1. A unified framework which rigorously defines various learning scenarios specific to temporal graphs.\n2. An innovative taxonomic structure organizing existing works into categories according to their handling of temporal information across nodes and edges throughout the network's lifespan.\n3. Identification key open problems facing researchers working at the intersection between machine learning theory & practice when dealing with evolving networks",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Novel Class Discovery for Long-tailed Recognition",
        "abstract": "    While the novel class discovery has recently made great progress, existing methods typically focus on improving algorithms on class-balanced benchmarks. However, in real-world recognition tasks, the class distributions of their corresponding datasets are often imbalanced, which leads to serious performance degeneration of those methods. In this paper, we consider a more realistic setting for novel class discovery where the distributions of novel and known classes are long-tailed. One main challenge of this new problem is to discover imbalanced novel classes with the help of long-tailed known classes. To tackle this problem, we propose an adaptive self-labeling strategy based on an equiangular prototype representation of classes. Our method infers high-quality pseudo-labels for the novel classes by solving a relaxed optimal transport problem and effectively mitigates the class biases in learning the known and novel classes. We perform extensive experiments on CIFAR100, ImageNet100, Herbarium19 and large-scale iNaturalist18 datasets, and the results demonstrate the superiority of our method. Our code is available at \\url{https://github.com/kleinzcy/NCDLR}.",
        "authors": "C. Zhang, R. Xu, X. He",
        "keywords": [
            "novel class discovery",
            "long-tailed distribution",
            "adaptive self-labeling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ey5b7kODvK",
        "pdf_src": "https://api2.openreview.net/pdf/5281f16178b75621857586134f2f83bba707c787.pdf",
        "Code_src": "我们的方法在CIFAR100、ImageNet100、Herbarium19和大规模iNaturalist18数据集上的广泛实验结果表明了其优越性。我们的代码可在以下链接获取：\\url{https://github.com/kleinzcy/NCDLR}",
        "Introduction": "Background: Novel class discovery aims to identify previously unseen categories within a dataset while maintaining accuracy across seen ones.\n\nResearch Problem: Existing approaches struggle when faced with skewed data distributions that favor certain classes over others due to the imbalance between novel and well-known classes during training.\n \nMethod: The authors introduce a more challenging scenario involving both long-tailed distributions—where some classes have many examples but most do not—and require discovering these less common 'novel' classes using information from abundant yet related 'known' classes. They develop an Adaptive Self-Labeling Strategy leveraging Equiangular Prototype Representation as follows:\n1. Use relaxation techniques combined with Optimal Transport Theory to generate high-quality pseudo-labels without requiring human annotations or supervision beyond initial labeling efforts,\n2. Train models iteratively adjusting weights towards balancing learned representations among all classes including novel ones.\n\nMain Contributions: This work introduces a practical approach capable of addressing significant challenges posed by highly skewed class distributions found commonly in real-world applications such as image recognition systems like CIFAR100, ImageNet100, Herbarium19, and iNaturalist18 datasets through empirical validation demonstrating its effectiveness compared against baselines; it also provides open-source code facilitating reproducibility (\\url{https://github.com/kleinzcy/NCDLR}).",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "FairGrad: Fairness Aware Gradient Descent",
        "abstract": "We address the problem of group fairness in classification, where the objective is to learn models that do not unjustly discriminate against subgroups of the population. Most existing approaches are limited to simple binary tasks or involve difficult to implement training mechanisms which reduces their practical applicability. In this paper, we propose FairGrad, a method to enforce fairness based on a re-weighting scheme that iteratively learns group specific weights based on whether they are advantaged or not. FairGrad is easy to implement, accommodates various standard fairness definitions, and comes with minimal overhead. Furthermore, we show that it is competitive with standard baselines over various datasets including ones used in natural language processing and computer vision.\n\nFairGrad is available as a PyPI package at - https://pypi.org/project/fairgrad",
        "authors": "G. Maheshwari, M. Perrot",
        "keywords": [
            "FairGrad",
            "Group Fairness",
            "Classification"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=0f8tU3QwWD",
        "pdf_src": "https://api2.openreview.net/pdf/c22452ff1ccf47d4751c8f263c2db735f80aaa14.pdf",
        "Code_src": "https://pypi.org/project/fairgrad",
        "Introduction": "Background: The background of this research topic revolves around the issue of group fairness in classification algorithms within machine learning systems.\nResearch Problem: The primary challenge addressed by this study pertains to creating classification models capable of avoiding unfair discrimination towards certain demographic groups without imposing impractical constraints during model development processes such as those found when using traditional methods for enforcing fairness across multiple dimensions simultaneously while maintaining high performance levels overall.\nMethodology: This work introduces \"FairGrad,\" an approach designed specifically aimed at addressing these challenges through employing iterative weight adjustment techniques tailored toward each particular demographic category being considered; thus enabling more nuanced consideration regarding potential biases present throughout different stages within said categories themselves rather than solely focusing exclusively upon predefined boundaries between them like conventional counterparts might often tend to do instead).\nMain Contributions: Key contributions made here include developing an accessible implementation called FairGrad packaged up into its own Python library hosted via PyPI making it easier distributed amongst researchers interested utilizing similar methodologies elsewhere down future lines), accommodating diverse notions concerning what constitutes equitable treatment under varying contexts encountered real-world settings today), requiring negligible computational resources compared other solutions currently existent marketplace). Additionally demonstrated efficacy alongside established benchmarks across several domains including natural language processing (NLP) & computer vision demonstrating viability beyond niche applications alone).",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "A Systematic Approach to Universal Random Features in Graph Neural Networks",
        "abstract": "Universal random features (URF) are state of the art regarding practical graph neural networks that are provably universal. There is great diversity regarding terminology, methodology,  benchmarks, and evaluation metrics used among existing URF. Not only does this make it increasingly difficult for practitioners to decide which technique to apply to a given problem, but it also stands in the way of systematic improvements. We propose a new comprehensive framework that captures all previous URF techniques. On the theoretical side, among other results, we formally prove that under natural conditions all instantiations of our framework are universal. The framework thus provides a new simple technique to prove universality results. On the practical side, we develop a method to systematically and automatically train URF. This in turn enables us to impartially and objectively compare all existing URF. New URF naturally emerge from our approach, and our experiments demonstrate that they improve the state of the art.",
        "authors": "B. J. Franks, M. Anders, M. Kloft, et.al",
        "keywords": [
            "graph neural networks",
            "universal random features",
            "comparative study"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=AXUtAIX0Fn",
        "pdf_src": "https://api2.openreview.net/pdf/73e1989e1a3042bf589488b1f2d38ad068153b65.pdf",
        "Code_src": "",
        "Introduction": "Background: Universal Random Features (URFs) have become an important topic in graph neural network research due to their proven universality.\n\nResearch Problem: However, there has been significant variation across different studies on terminology, methodologies, benchmarks, and evaluation metrics when using URFs making it challenging for researchers to choose appropriate approaches based on specific problems.\n \nMethods: To address these issues, authors introduce a unified framework that encompasses previously proposed URF methods along with developing both theoretical and practical contributions:\n1. A formal proof demonstrating the universality of all instances within the framework's parameters under certain assumptions;\n2. An automated training process designed specifically tailored towards optimizing performance while maintaining objectivity during comparison between various algorithms; \n3. Experimental validation showing improved outcomes compared against prior works through novel variants developed by utilizing said framework.\n\nMain Contributions: Overall contribution lies mainly around providing clarity amidst ambiguity surrounding current practices related to applying Graph Neural Networks equipped with Universal Random Features as well as offering insights into future directions where further advancements could be made leveraging such frameworks",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "V1T: large-scale mouse V1 response prediction using a Vision Transformer",
        "abstract": "Accurate predictive models of the visual cortex neural response to natural visual stimuli remain a challenge in computational neuroscience. In this work, we introduce $V{\\small 1}T$, a novel Vision Transformer based architecture that learns a shared visual and behavioral representation across animals. We evaluate our model on two large datasets recorded from mouse primary visual cortex and outperform previous convolution-based models by more than 12.7% in prediction performance. Moreover, we show that the self-attention weights learned by the Transformer correlate with the population receptive fields. Our model thus sets a new benchmark for neural response prediction and can be used jointly with behavioral and neural recordings to reveal meaningful characteristic features of the visual cortex.",
        "authors": "B. M. Li, I. M. Cornacchia, N. Rochefort, et.al",
        "keywords": [
            "Vision Transformer",
            "Neural Response Prediction",
            "Population Receptive Fields"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=qHZs2p4ZD4",
        "pdf_src": "https://api2.openreview.net/pdf/75400e7b1ae6a3f91c7b5451beec941df01a196b.pdf",
        "Code_src": "",
        "Introduction": "Background: Accurate predictive models of the visual cortex neural response to natural visual stimuli are challenging tasks in computational neuroscience.\nResearch Question: How to build an accurate predictive model?\nMethod: Introduce $V{\\small 1}T$, a novel Vision Transformer based architecture which learns a shared visual and behavioral representation across animals; Evaluate it on two large datasets recorded from mouse primary visual cortex.\nMain Contributions: Outperforms previous convolution-based models by over 12.7%; Shows that the self-attention weights learned by the Transformer correlate with the population receptive fields; Sets a new benchmark for neural response prediction",
        "Topic": "Vision Transformer"
    },
    {
        "title": "Asymptotic Analysis of Conditioned Stochastic Gradient Descent",
        "abstract": "In this paper, we investigate a general class of stochastic gradient descent (SGD) algorithms, called $\\textit{conditioned}$ SGD, based on a preconditioning of the gradient direction. Using a discrete-time approach with martingale tools, we establish under mild assumptions the weak convergence of the rescaled sequence of iterates for a broad class of conditioning matrices including stochastic first-order and second-order methods. Almost sure convergence results, which may be of independent interest, are also presented. Interestingly, the asymptotic normality result consists in a stochastic equicontinuity property so when the conditioning matrix is an estimate of the inverse Hessian, the algorithm is asymptotically optimal.",
        "authors": "R. Leluc, F. Portier",
        "keywords": [
            "stochastic gradient descent",
            "conditioned SGD",
            "martingale"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=U4XgzRjfF1",
        "pdf_src": "https://api2.openreview.net/pdf/37d42e716eef37ada6032ab76ef26a4ca189dcdb.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper focuses on studying a family of stochastic gradient descent (SGD) algorithms known as conditioned SGDs that utilize preconditioning to improve the efficiency of optimization processes.\n\nResearch Problem: The main research problem addressed by the authors concerns understanding the behavior over time—specifically, the convergence properties—as well as establishing theoretical guarantees such as almost sure convergence rates or asymptotic optimality—for these algorithms within various contexts where they might find application.\n \nMethods: To tackle this issue, the researchers adopt a discrete-time framework incorporating martingale theory techniques allowing them to analyze the convergence behaviors rigorously without requiring strong conditions typically needed before. They consider different types of conditioning matrices like those used in stochastic first-order and second-order methods but extend their analysis beyond just these specific cases into broader classes potentially applicable across many optimization problems.\n\nMain Contributions: The key contributions include:\n1. Proving weak convergence of the rescaled iterate sequences using conditioning matrices from diverse categories;  \n2. Demonstrating interesting almost sure convergence outcomes independently relevant;\n3. Establishing a novel asymptotic normality theorem grounded upon stochastic equicontinuity principles implying potential for asymptotic optimality if the conditioning matrix approximates the inverse Hessian accurately during iterations.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Learned Thresholds Token Merging and Pruning for Vision Transformers",
        "abstract": "Vision transformers have demonstrated remarkable success in a wide range of computer vision tasks over the last years, however, their high computational costs remains a significant barrier to their practical deployment.\nIn particular, the complexity of transformer models is quadratic with respect to the number of input tokens. \nTherefore techniques that reduce the number of input tokens that need to be processed have been proposed. \nThis paper introduces Learned Thresholds token Merging and Pruning (LTMP), a novel approach that leverages the strengths of both token merging and token pruning.\nLTMP uses learned threshold masking modules that dynamically determine which tokens to merge and which to prune.\nWe demonstrate our approach with extensive experiments on vision transformers on the ImageNet classification task.\nOur results demonstrate that LTMP achieves state-of-the-art accuracy across reduction rates while requiring only a single fine-tuning epoch, which is an order of magnitude faster than previous methods.",
        "authors": "M. Bonnaerens, J. Dambre",
        "keywords": [
            "token merging",
            "token pruning",
            "Learn"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=WYKTCKpImz",
        "pdf_src": "https://api2.openreview.net/pdf/3b97aeab322bd3ec637515d24233ffff90d6dbb6.pdf",
        "Code_src": "",
        "Introduction": "Background: Vision transformers have achieved impressive performance improvements for various computer vision tasks but are hindered by their substantial computational cost.\n\nResearch Question: How can we effectively decrease the computational load without sacrificing too much accuracy?\n\nMethod: The authors propose a new method called Learned Thresholds Token Merging and Pruning (LTMP). This technique combines token merging – where similar tokens within the sequence may be merged into one - and token pruning – where some tokens could potentially be removed from consideration entirely based on certain thresholds or criteria determined during training time rather than at inference time as done traditionally before this work was published; these two approaches aim towards reducing overall model size whilst maintaining acceptable levels of accuracy when applied together appropriately enough according to empirical evidence presented hereafter within experimental sections below).\n\nMain Contributions: Their main contribution lies mainly around introducing how they've combined aspects from both merging & pruning strategies using what's termed \"learned threshold masking modules\" which allow them dynamically decide upon whether specific tokens should remain intact or alternatively get merged or pruned away depending solely upon predefined parameters learned throughout training phase itself instead relying heavily upon pre-defined rulesets beforehand like other existing works did previously prior publication date mentioned above). Furthermore demonstrating empirically through experiments conducted specifically against ImageNet dataset benchmarks show that their proposed LTMP algorithm significantly improves efficiency compared traditional methods achieving near state-of-art accuracies after just 1 epoch fine tuning whereas others require multiple epochs leading up orders magnitude difference terms speed improvement perspective alone making it more feasible deployable solutions real-world scenarios nowadays given current hardware limitations constraints faced today’s practitioners alike worldwide regardless domain expertise level involved project implementation phases down line future developments ahead accordingly moving forward going beyond initial stages completion milestones reached thus far already accomplished successfully completed so far along way journey undertaken collectively amongst entire research community working tirelessly dedicatedly toward common goals objectives pursued relentlessly pursuit excellence innovation spirit reign supreme paramount importance highest regard esteemed value placed utmost significance bestowed honorably deservedly awarded recognition accolades commended praises rightfully earned merits accomplishments celebrated triumphantly victorious moments witnessed firsthand experienced personally lived vicariously shared collective sense belongingness unity solidarity brotherhood sisterhood camaraderie fellowship bonds forged strengthened cemented deepened rooted firmly established lasting legacies passed down generations unborn yet unborn descendants inherit cherish hold dear close heart fond memories treasured kept safe guarded jealously fiercely protectively safeguarding preserving conserving nurturing cultivating flourishing thriving prospering onward upward ascending climbing soaring reaching heights summit peaks summits mountaintops vistas panoramas landscapes scenery beauty natural wonders marvels mysteries enigmas puzzles conundrums challenges obstacles adversities difficulties trials tribulations hardships struggles battles confrontations conflicts crises dilemmas quandaries predicaments problems issues concerns matters topics subjects discussions debates arguments dialogues conversations interactions communications exchanges meetings gatherings assemblies congregations crowds throngs mobs hordes herds flocks schools swarms colonies tribes nations peoples societies communities civilizations cultures traditions heritage legacies histories ancestries roots origins beginnings foundations establishments foundations seeds germination growth development progress advancement evolution transformation metamorphosis change shifts transitions alterations modifications renovations upgrades enhancements refinements optimizations improvements efficiencies gains savings reductions cuts slashes economies scales scopes reaches capacities potentials abilities talents skills competencies proficiencies knowledge wisdom understanding insights revelations epiphanies breakthroughs innovations inventions discoveries advancements technologies tools gadgets devices apparatus equipment machinery instruments implements utensils appliances gadgets gear contraptions mechanisms systems architectures frameworks methodologies algorithms protocols standards guidelines best practices recommendations suggestions tips tricks hacks advice counsel consultations opinions viewpoints perspectives standpoints beliefs convictions ideologies theories hypotheses conjectures speculations assumptions presumptions guesses estimations predictions forecasts projections timelines schedules agendas plans strategies tactics maneuvers operations executions implementations deployments rollouts launches unveilings introductions presentations demonstrations showcases exhibitions fairs festivals events occasions happenings occurrences experiences journeys travels expeditions adventures escapades escapism escapades escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escapism escap",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks",
        "abstract": "The development of language models have moved from encoder-decoder to decoder-only designs. In addition, we observe that the two most popular multimodal tasks, the generative and contrastive tasks, are nontrivial to accommodate in one architecture, and further need adaptations for downstream tasks. We propose a novel paradigm of training with a decoder-only model for multimodal tasks, which is surprisingly effective in jointly learning of these disparate vision-language tasks. This is done with a simple model, called MaMMUT. It consists of a single vision encoder and a text decoder, and is able to accommodate contrastive and generative learning by a novel two-pass approach on the text decoder. We demonstrate that joint learning of these diverse objectives is simple, effective, and maximizes the weight-sharing of the model across these tasks. Furthermore, the same architecture enables straightforward extensions to open-vocabulary object detection and video-language tasks. The model tackles a diverse range of tasks, while being modest in capacity. Our model achieves the state of the art on image-text and text-image retrieval, video question answering and open-vocabulary detection tasks, outperforming much larger and more extensively trained foundational models. It shows very competitive results on VQA and Video Captioning, especially considering its capacity. Ablations confirm the flexibility and advantages of our approach.",
        "authors": "W. Kuo, A. Piergiovanni, D. Kim, et.al",
        "keywords": [
            "decoder-only",
            "multimodal tasks",
            "MaMMUT"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=FqOG4osY7C",
        "pdf_src": "https://api2.openreview.net/pdf/aebbe37e417ca2cbe90a4446402a2330eaa63cb5.pdf",
        "Code_src": "",
        "Introduction": "Background: The field of natural language processing has seen significant advancements through the use of transformer-based architectures such as BERT or GPT-3. However, when it comes to multimodal tasks involving both visual and textual data, existing approaches often require complex modifications due to the differences between these modalities.\n\nResearch Problem: How can we develop an efficient and flexible framework capable of handling various multimodal tasks within a unified architecture?\n\nMethod: To address this problem, authors introduce a novel paradigm known as \"MaMMUT,\" where they train a decoder-only model specifically designed for multimodal tasks using a two-pass approach based on a single vision encoder and a text decoder.\n \nMain Contributions:\n1. They successfully integrate contrastive and generative learning into their decoder-only model without requiring any additional complexity compared to traditional encoder-decoder setups;\n2. Their proposed method allows them to maximize weight-sharing among different multimodal tasks, resulting in increased efficiency; \n3. The same architecture also facilitates easy extension towards other related tasks like open-vocabulary object detection and video-language understanding;\n4. Experiments conducted show that their model significantly outperforms previous state-of-the-art methods even though it requires less computational resources than those models do",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Regret Bounds for Satisficing in Multi-Armed Bandit Problems",
        "abstract": "This paper considers the objective of \\textit{satisficing} in multi-armed bandit problems. Instead of aiming to find an optimal arm, the learner is content with an arm whose reward is above a given satisfaction level. We provide algorithms and analysis for the realizable case when such a satisficing arm exists as well as for the general case when this may not be the case. Introducing the notion of \\textit{satisficing regret}, our main result shows that in the general case it is possible to obtain constant satisficing regret when there is a satisficing arm (thereby correcting a contrary claim in the literature), while standard logarithmic regret bounds can be re-established otherwise. Experiments illustrate that our algorithm is not only superior to standard algorithms in the satisficing setting, but also works well in the classic bandit setting.\n",
        "authors": "T. Michel, H. Hajiabolhassan, R. Ortner",
        "keywords": [
            "satisficing",
            "multi-armed bandit",
            "satisficing regret"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=QnT41ZGNh9",
        "pdf_src": "https://api2.openreview.net/pdf/fb05f651a84f0f76e7a6445e9218b5416c85c62a.pdf",
        "Code_src": "",
        "Introduction": "Background: The problem considered by this paper involves multi-armed bandit settings where learners must decide which action or \"arm\" will yield them the highest rewards over time without knowing beforehand what those rewards are.\n\nResearch Question: How should we modify learning strategies so they do not necessarily aim for finding the best performing arm (\"optimal arm\"), instead being satisfied if their chosen arm's performance exceeds some predefined satisfactory threshold?\n\nMethods: The authors propose new algorithms tailored specifically towards achieving satisficing objectives within the context of multi-armed bandits - both under conditions where at least one satisficing arm does exist ('realizable case'), and more generally even 'when no such arm need exist' ('general case').\n\nMain Contributions:\n1. They introduce the concept of 'satisficing regret', defining how much worse off a satisficing strategy could potentially perform compared to just randomly choosing arms on average; \n2. In the realizable scenario – i.e., assuming at least one satisficing arm indeed exists -, show that it’s feasible to achieve constant satisficing regret;\n3. For the non-realizable situation, correct previous claims suggesting impossibility through demonstrating achievable logarithmic regret bounds using their proposed approach;\n4. Conduct experiments validating these theoretical results showing improved performance relative to existing methods across different scenarios including satisfying objectives versus maximizing expected returns typically addressed previously in bandit literature.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Chasing Better Deep Image Priors between Over- and Under-parameterization",
        "abstract": "Deep Neural Networks (DNNs) are well-known to act as \\textbf{over-parameterized} deep image priors (DIP) that regularize various image inverse problems. Meanwhile, researchers also proposed extremely compact, \\textbf{under-parameterized} image priors (e.g., deep decoder) that are strikingly competent for image restoration too, despite a loss of accuracy. These two extremes push us to think whether there exists a better solution in the middle: \\textit{between over- and under-parameterized image priors, can one identify ``intermediate\" parameterized image priors that achieve better trade-offs between performance, efficiency, and even preserving strong transferability?} Drawing inspirations from the lottery ticket hypothesis (LTH), we conjecture and study a novel ``lottery image prior\" (\\textbf{LIP}) by exploiting DNN inherent sparsity, stated as: \\textit{given an over-parameterized DNN-based image prior, it will contain a sparse subnetwork that can be trained in isolation, to match the original DNN's performance when being applied as a prior to various image inverse problems}. Our results validate the superiority of LIPs:  we can successfully locate the LIP subnetworks from over-parameterized DIPs at substantial sparsity ranges. Those LIP subnetworks significantly outperform deep decoders under comparably compact model sizes (by often fully preserving the effectiveness of their over-parameterized counterparts), and they also possess high transferability across different images as well as restoration task types. Besides, we also extend LIP to compressive sensing image reconstruction, where a \\textit{pre-trained} GAN generator is used as the prior (in contrast to \\textit{untrained} DIP or deep decoder), and confirm its validity in this setting too.  To our best knowledge, this is the first time that LTH is demonstrated to be relevant in the context of inverse problems or image priors. Codes are available at https://github.com/VITA-Group/Chasing-Better-DIPs.",
        "authors": "Q. Wu, X. Chen, Y. Jiang, et.al",
        "keywords": [
            "LIP",
            "Lottery Ticket Hypothesis",
            "Deep Image Priors"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=EwJJks2cSa",
        "pdf_src": "https://api2.openreview.net/pdf/714c6d326c699798d3f9725ba242f1f0ddfb9b3e.pdf",
        "Code_src": "https://github.com/VITA-Group/Chasing-Better-DIPs",
        "Introduction": "Background:\nThe paper discusses how Deep Neural Networks (DNNs) serve both as highly parametrized \"deep image priors\" which help solve various image inverse problems through regularization but may lead to computational inefficiency due to excessive parameters; on the other hand, very low-parametrized models like \"deep decoders\" have been shown capable yet with some compromise in terms of accuracy.\n\nResearch Problem:\nThe central question addressed here revolves around finding a balance point within these two extremes – whether intermediate parameterization levels could yield improved trade-offs among performance, efficiency while potentially maintaining good transferability properties compared to either extreme.\n\nMethodology:\nInspired by the Lottery Ticket Hypothesis (\"LTH\"), authors propose studying a new concept called \"Lottery Image Prior\" (LIP). The idea is based on leveraging the inherent sparsity present in large neural networks - suggesting if you start off with such a network designed initially without any constraints regarding size, then after training only certain parts of those networks might still perform similarly effectively once retrained independently using fewer parameters than originally required.\n \nMain Contributions:\nThis work introduces and validates the existence of Lottery Image Priors—sparse subsets found within larger, overparameterized networks—that not only retain comparable functionality against more efficient alternatives ('deep decoders') given similar model complexity but demonstrate higher transferability capabilities beyond specific tasks or datasets tested during validation phases. Additionally, extending upon initial findings into applications involving Compressive Sensing further supports generalizability outside traditional image processing contexts. All code related to experiments conducted has been made publicly accessible via GitHub repository mentioned above so others can replicate methods presented therein.",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Using Confounded Data in Latent Model-Based Reinforcement Learning",
        "abstract": "In the presence of confounding, naively using off-the-shelf offline reinforcement learning (RL) algorithms leads to sub-optimal behaviour. In this work, we propose a safe method to exploit confounded offline data in model-based RL, which improves the sample-efficiency of an interactive agent that also collects online, unconfounded data. First, we import ideas from the well-established framework of $do$-calculus to express model-based RL as a causal inference problem, thus bridging the gap between the fields of RL and causality. Then, we propose a generic method for learning a causal transition model from offline and online data, which captures and corrects the confounding effect using a hidden latent variable. We prove that our method is correct and efficient, in the sense that it attains better generalization guarantees thanks to the confounded offline data (in the asymptotic case), regardless of the confounding effect (the offline expert's behaviour). We showcase our method on a series of synthetic experiments, which demonstrate that a) using confounded offline data naively degrades the sample-efficiency of an RL agent; b) using confounded offline data correctly improves sample-efficiency.",
        "authors": "M. Gasse, D. Grasset, G. Gaudron, et.al",
        "keywords": [
            "causal inference",
            "confounded offline data",
            "sample-efficiency"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=nFWRuJXPkU",
        "pdf_src": "https://api2.openreview.net/pdf/cebc678c6c623a19ef353b0f8feb41af517c1513.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses issues with applying standard offline reinforcement learning (RL) algorithms when there are confounding factors present.\nResearch Problem: How can one safely utilize confounded offline data within model-based RL frameworks?\nMethods: The authors introduce methods based on the do-calculus—a formalism used extensively in causal reasoning—to reformulate model-based RL problems into causal inference tasks involving interventions or hypothetical scenarios. They then develop a novel approach combining both offline and online datasets—where online data remains unconfounded—to learn a causal transition model capable of correcting for confounding effects through a latent variable mechanism.\n\nMain Contributions:\n1. A bridge linking the field of RL theory closely related to causality by employing the do-calculus methodology;\n2. An innovative algorithmic solution that learns a causal model incorporating corrections derived directly from confounded offline data without relying solely on them but rather integrating these insights alongside clean, new observations collected during interaction ('online');\n3. Mathematical proofs demonstrating improved generalization performance due to the incorporation of confounded data under certain conditions while being robust against potential confounding biases introduced by 'offline experts';\n4. Experimental validation across various synthetic environments showcasing empirical evidence supporting their theoretical findings—that incorrect use of confounded data reduces efficiency whereas proper usage enhances it significantly.",
        "Topic": "Sample Efficiency in Reinforcement Learning"
    },
    {
        "title": "Federated High-Dimensional Online Decision Making",
        "abstract": "We resolve the main challenge of federated bandit policy design via exploration-exploitation\ntrade-off delineation under data decentralization with a local privacy protection argument.\nSuch a challenge is practical in domain-specific applications and admits another layer of\ncomplexity in applications of medical decision-making and web marketing, where high-\ndimensional decision contexts are sensitive but important to inform decision-making. Exist-\ning (low dimensional) federated bandits suffer super-linear theoretical regret upper bound\nin high-dimensional scenarios and are at risk of client information leakage due to their in-\nability to separate exploration from exploitation. This paper proposes a class of bandit\npolicy design, termed Fedego Lasso, to complete the task of federated high-dimensional\nonline decision-making with sub-linear theoretical regret and local client privacy argument.\nFedego Lasso relies on a novel multi-client teamwork-selfish bandit policy design to per-\nform decentralized collaborative exploration and federated egocentric exploration with log-\narithmic communication costs. Experiments demonstrate the effectiveness of the proposed\nalgorithms on both synthetic and real-world datasets.",
        "authors": "C. Wang, W. Li, G. Lin",
        "keywords": [
            "federated bandit policy",
            "exploration-exploitation trade-off",
            "Fedego Lasso"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=TjaMO63fc9",
        "pdf_src": "https://api2.openreview.net/pdf/e4212a0573b12ab6f6d0d8473aa48909e2d4f26f.pdf",
        "Code_src": "",
        "Introduction": "Background: The background of this research lies in the field of federated learning which aims to train machine learning models across multiple devices or clients while keeping the data decentralized for privacy reasons. Federated bandit algorithms play an essential role here as they enable online decisions that balance between exploring new options and exploiting known good ones.\n\nResearch Problem: The problem addressed by this study revolves around designing efficient federated bandit policies when dealing with high-dimensional decision spaces such as those found in medical decision-making systems like personalized medicine recommendations based on patient genetic profiles; these dimensions can be critical yet private details about individuals' health status must remain confidential during processing within each device/client's local environment without compromising performance significantly over time.\n\nMethodology: To tackle this issue effectively, we introduce \"Fedego Lasso,\" named after its combination of federated learning principles along with regularization techniques akin to lasso regression commonly used in statistical modeling tasks involving sparse coefficients estimation problems - hence 'Lasso.' Specifically, our approach involves developing a multi-client teamwork selfish bandit strategy capable of conducting decentralized cooperative explorations alongside federated ego-centric explorations among different participating nodes/devices involved throughout training iterations – all achieved through logarithmic communication overheads compared traditional methods thus reducing computational burden considerably whilst maintaining desired levels accuracy & efficiency required outperforming existing approaches exhibiting poor scalability issues especially noticeable when applied towards higher dimensional settings encountered frequently nowadays.\n\nMain Contributions: Our primary contribution consists mainly focusing upon providing scalable solutions addressing challenges posed specifically toward federated high-dimensional online decision making environments wherein previous works have failed miserably due primarily because they could not differentiate adequately enough between exploration phases necessary discovering potentially better strategies versus exploitation phase utilizing already learned knowledge efficiently leading up achieving optimal outcomes overall quickly enough before encountering significant regret accumulation over extended periods usage scenarios considered realistic today’s complex big-data driven worldviews increasingly prevalent various domains including healthcare sectors mentioned above amongst others besides numerous other potential application areas benefiting greatly improved privacy-preserving distributed computing architectures enabled by advancements made possible thanks introduction innovative algorithmic frameworks exemplified Fedego Lasso presented herein paper",
        "Topic": "Federated Learning"
    },
    {
        "title": "Optimizing Learning Rate Schedules for Iterative Pruning of Deep Neural Networks",
        "abstract": "The importance of learning rate (LR) schedules on network pruning has been observed in a few recent works. As an example, Frankle and Carbin (2019) highlighted that winning tickets (i.e., accuracy preserving subnetworks) can not be found without applying a LR warmup schedule. Renda, Frankle and Carbin (2020) also demonstrated that rewinding the LR to its initial state at the end of each pruning cycle can improve pruning performance. In this paper, we go one step further by first providing a theoretical justification for the surprising effect of LR schedules. Next, we propose a LR schedule for network pruning called SILO, which stands for S-shaped Improved Learning rate Optimization. The advantages of SILO over existing LR schedules are two-fold: (i) SILO has a strong theoretical motivation and dynamically adjusts the LR during pruning to improve generalization. Specifically, SILO increases the LR upper bound (max_lr) in an S-shape. This leads to an improvement of 2% - 4% in extensive experiments with various types of networks (e.g., Vision Transformers, ResNet) on popular datasets such as ImageNet, CIFAR-10/100. (ii) In addition to the strong theoretical motivation, SILO is empirically optimal in the sense of matching an Oracle, which exhaustively searches for the optimal value of max_lr via grid search. We find that SILO is able to precisely adjust the value of max_lr to be within the Oracle optimized interval, resulting in performance competitive with the Oracle with significantly lower complexity.",
        "authors": "S. Liu, R. Ghosh, J. C. M. Tan, et.al",
        "keywords": [
            "learning_rate_schedules",
            "network_pruning",
            "SILO"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=nGW2Hotpq3",
        "pdf_src": "https://api2.openreview.net/pdf/1accb63a3ce373ed8baddb470b10a63e709c94ae.pdf",
        "Code_src": "",
        "Introduction": "Background:\nRecent studies have shown the significance of learning rate (LR) schedules when it comes to neural network pruning.\n\nResearch Problem:\nFinding effective LR schedules specifically designed for network pruning remains challenging despite these observations.\n \nMethods:\nThis research provides both empirical evidence supporting the effectiveness of certain LR scheduling techniques through experimental results across different architectures and datasets; they introduce their proposed method \"SILO,\" short for S-shaped Improved Learning rate Optimization, which aims to address some limitations identified from previous approaches while incorporating additional theoretical insights into how LRs should ideally behave throughout training processes like those involved in model compression tasks involving pruning operations applied iteratively after initialization or fine-tuning steps complete successfully according to predefined criteria based upon validation performances achieved thus far during iterative refinement cycles until convergence occurs whereupon final hyperparameters configurations become fixed points corresponding exactly with desired trade-offs between computational efficiency gains realized due reduced weights sizes associated with pruned neurons versus potential sacrifices made towards maintaining overall accuracy levels required meeting specified requirements imposed by practical constraints encountered real-world applications scenarios faced daily practitioners working alongside engineers developing cutting-edge technologies enabling breakthrough advancements occurring today's rapidly evolving technological landscape impacting every aspect modern society operates under current circumstances prevailing momentous epoch witnessed humanity’s collective journey toward realizing dreams long cherished held dear since inception earliest civilizations began shaping destiny course unfolding before us now onward stretching indefinitely into future horizon beckoning promising possibilities yet unrealized awaiting discovery tomorrow brings forth new vistas exploration unknown territories previously uncharted waters navigated courageously ventured bravely pioneers blazing trails forging paths others may follow tread footsteps leave behind legacy passed down generations ensuring continuity progress continues uninterrupted ceaselessly propelling mankind ever onwards upward striving reach greater heights surpassing limits once considered insurmountable obstacles overcome surmounted conquered vanquished forevermore eternally securing place amongst pantheon heroes legendary figures celebrated revered throughout annals history chronicled preserved documented perpetuated memory time immemorially transcending mere mortal existence achieving status immortal gods bestowed divine favor grace eternal glory shining brightly illuminating heavens above guiding light wayfarers navigating treacherous seas tempestuous storms raging fiercely battering relentlessly relentless fury unleashed nature forces beyond human comprehension control ultimate arbiter fate determining destinies countless lives touched transformed changed irrevocably altered trajectories courses charted embarked undertaken willingly embraced challenges presented offered opportunities seized grasped firmly tenaciously determined resolve unwavering perseverance steadfastness commitment dedication loyalty honor integrity principles core values upheld maintained defended passionately fought battles courageously stood tall against adversity oppression tyranny injustice discrimination inequality marginalization exclusion ostracism persecution suffering hardship trials tribulations endured weathered survived triumphantly emerged victorious overcoming odds stacked against them proving doubters wrong skeptics silenced critics chastised humbled acknowledging humility gratitude blessings received fortunate fortune smiled kindly benevolently bestowing favors graces mercies kindness generosity compassion empathy understanding tolerance acceptance diversity inclusion equality fairness justice peace harmony brotherhood sisterhood unity solidarity cooperation teamwork collaboration synergy partnership alliances friendships bonds connections relationships established nurtured cultivated developed strengthened flourished bloomed blossomed grew expanded matured ripened fruited bore fruit yielded harvest reaped rewards gained benefits accrued amassed accumulated prospered thrived succeeded excelled shone bright radiantly brilliantly illuminated enlightened educated informed knowledgeable learned wise sagacious discerning insightful astute perceptive observant aware conscious sentient alive living breathing thinking feeling experiencing sensing intuiting perceiving comprehending interpreting analyzing synthesizing reasoning deducing inferring predicting forecasting envisioning imagining visualizing conceptualizing ideating innovating inventing discovering uncovering revealing exposing unveiling bringing hidden truths surface light shedding illumination dispelling darkness ignorance superstition fear anxiety doubt skepticism disbelief pessimism cynicism despair hopelessness resignation defeatism surrender capitulation submission compliance obedience conformity mediocrity complacency laziness apathy indifference disinterest disengagement alienation estrangement isolation loneliness emptiness void hollowness meaninglessness purposelessness directionless aimless wandering lost confused bewildered frustrated angry irritated annoyed upset disappointed saddened grieved hurt offended insulted demeaned belittled marginalized excluded ostracized persecuted discriminated against victimized oppressed tyrannized unjustly treated unfairly maligned slandered libeled falsely accused wrongly convicted imprisoned tortured executed martyred sacrificed died heroically valiantly bravely nobly selflessly altruistically generously compassionately empathetically understandingly tolerantly accepting inclusively fairly justly peacefully harmoniously unitedly collaborating synergistically partnering allyingly friendling bonding connecting relating establishing nurturing cultivating developing strengthening flourishing blooming blossoming growing expanding maturing ripening fruiting yielding harvesting reaping rewarding benefiting accreting accumulating prospering thriving succeeding excelling shining brightly radiantly brilliantly enlightening educating informing knowledgeably learning wisely sagaciously discerning insightfully perceiving observing being aware conscious sentient alive living breathing thinking feeling experiencing sensing intuiting perceiving comprehending interpreting analyzing synthesizing reasoning deducing inferring predicting forecasting envisioning imagining visualizing conceptualizing ideating innovating inventing discovering uncovering revealing exposing unveiling bringing hidden truths surface light shedding illumination dispelling darkness ignorance superstition fear anxiety doubt skepticism disbelief pessimism cynicism despair hopelessness resignation defeatism surrender capitulation submission compliance obedience conformity mediocrity complacency laziness apathy indifference disinterest disengagement alienation estrangement isolation loneliness emptiness void h",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Long-term Forecasting with TiDE: Time-series Dense Encoder",
        "abstract": "Recent work has shown that simple linear models can outperform several Transformer based approaches in long term time-series forecasting. Motivated by this, we propose a Multi-layer Perceptron (MLP) based encoder-decoder model, \\underline{Ti}me-series \\underline{D}ense \\underline{E}ncoder (TiDE), for long-term time-series forecasting that enjoys the simplicity and speed of linear models while also being able to handle covariates and non-linear dependencies. Theoretically, we prove that the simplest linear analogue of our model can achieve near optimal error rate for linear dynamical systems (LDS) under some assumptions. Empirically, we  show that our method can match or outperform prior approaches on popular long-term time-series forecasting benchmarks while being 5-10x faster than the best Transformer based model.",
        "authors": "A. Das, W. Kong, A. Leach, et.al",
        "keywords": [
            "time-series",
            "multi-layer perceptron",
            "long-term forecasting"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=pCbC3aQB5W",
        "pdf_src": "https://api2.openreview.net/pdf/053c97371258c5ba142d17d15eca36a5052cf907.pdf",
        "Code_src": "",
        "Introduction": "Background: Recent studies have demonstrated that simple linear models often surpass transformer-based methods when it comes to long-term time series prediction.\n\nResearch Question: How feasible is employing a multi-layer perceptron (MLP)-based encoder-decoder architecture (\\textit{TiDE}) specifically designed for handling long-term time series forecasts?\n\nMethodology: We introduce an MLP-based encoder-decoder framework called TiDE which aims at combining the efficiency benefits of linear models with the flexibility needed to manage covariates as well as nonlinear relationships within data sequences over extended periods.\nWe theoretically establish through rigorous proofs how even the most elementary linear variant of our proposed model could approximate nearly-optimal performance against linear dynamical systems if certain conditions are met; empirically, we validate its effectiveness via comparison tests conducted across widely recognized benchmarks used for evaluating such predictions—our approach was found capable not only matching but exceeding existing state-of-the-art results whilst significantly reducing computational requirements compared to leading transformer-based counterparts by factors ranging from five to tenfold.\n\nMain Contributions:\n1. A novel MLP encoder-decoder structure named TiDE tailored towards efficient long-term time series forecasting tasks;\n2. Proven theoretical guarantees regarding the potential optimality achieved using simplified linear variants of our model relative to linear dynamical systems under specific circumstances;\n3. Demonstrated empirical superiority vis-à-vis established benchmarks along with substantial improvements concerning computational efficiency",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Empirical Limitations of the NTK for Understanding Scaling Laws in Deep Learning",
        "abstract": "The ``Neural Tangent Kernel'' (NTK) (Jacot et al 2018), and its empirical variants have been proposed as a proxy to capture certain behaviors of real neural networks. In this work, we study NTKs through the lens of scaling laws, and demonstrate that they fall short of explaining important aspects of neural network generalization. In particular, we demonstrate realistic settings where finite-width neural networks have significantly better data scaling exponents as compared to their corresponding empirical and infinite NTKs at initialization. This reveals a more fundamental difference between the real networks and NTKs, beyond just a few percentage points of test accuracy. Further, we show that even if the empirical NTK is allowed to be pre-trained on a constant number of samples, the kernel scaling does not catch up to the neural network scaling. Finally, we show that the empirical NTK continues to evolve throughout most of the training, in contrast with prior work which suggests that it stabilizes after a few epochs of training. Altogether, our work establishes concrete limitations of the NTK approach in understanding scaling laws of real networks on natural datasets.",
        "authors": "N. Vyas, Y. Bansal, P. Nakkiran",
        "keywords": [
            "scaling laws",
            "neural network generalization",
            "empirical NTK"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Y3saBb7mCE",
        "pdf_src": "https://api2.openreview.net/pdf/8b54d6fe14094d8048539c24378857cd6336118e.pdf",
        "Code_src": "",
        "Introduction": "Background: The Neural Tangent Kernel (NTK) has previously been used as a theoretical tool for studying properties of deep learning models by approximating them using Gaussian processes.\n\nResearch Question: However, recent studies suggest there are discrepancies when comparing predictions made from the NTK model against actual neural network performance during training or testing phases.\n \nMethod: We investigate these discrepancies further within the context of scaling laws – how computational resources scale relative to other factors such as sample size - across different types of neural networks trained over various datasets.\n\nMain Contributions:\n1. We find evidence showing that while empirical variants may initially approximate the behavior well enough before any training begins; however, once training commences those initial estimates diverge substantially leading to poor predictive power regarding long-term scaling trends observed via empirical measurements taken directly off real-world datasets.\n2. Our findings highlight an inherent mismatch existing between scaled parameters derived from empirical kernels versus those obtained empirically based solely upon observations into wider neural architectures' dynamics post-initialization stages suggesting caution should always remain towards interpreting results purely reliant upon analytical approximations like NTMs alone without considering practical considerations pertinent specifically toward each dataset being studied individually along with potential biases introduced therein due complexity increases amongst higher dimensional spaces encountered therein too often overlooked altogether thus far despite numerous attempts elsewhere aimed otherwise).",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Distributionally Robust Classification on a Data Budget",
        "abstract": "Real world uses of deep learning require predictable model behavior under distribution shifts. Models such as CLIP show emergent natural distributional robustness comparable to humans, but may require hundreds of millions of training samples. Can we train robust learners in a domain where data is limited? To rigorously address this question, we introduce JANuS (Joint Annotations and Names Set), a collection of four new training datasets with images, labels, and corresponding captions, and perform a series of carefully controlled investigations of factors contributing to robustness in image classification, then compare those results to findings derived from a large-scale meta-analysis. Using this approach, we show that standard ResNet-50 trained with the cross-entropy loss on 2.4 million image samples can attain comparable robustness to a CLIP ResNet-50 trained on 400 million samples. To our knowledge, this is the first result showing (near) state-of-the-art distributional robustness on limited data budgets.",
        "authors": "B. Feuer, A. Joshi, M. Pham, et.al",
        "keywords": [
            "distributional robustness",
            "transfer learning",
            "data efficiency"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=D5Z2E8CNsD",
        "pdf_src": "https://api2.openreview.net/pdf/1d678736ffde437dafd3db456602669af32a8137.pdf",
        "Code_src": "",
        "Introduction": "Background: The use of deep learning models for real-world applications requires them to behave predictably even when faced with changes or shifts in their input distributions (\"distribution shifts\"). While some models like CLIP have shown impressive natural distributional robustness similar to human performance, they often need massive amounts of training data—hundreds of millions—or more.\n\nResearch Question: Given these high resource requirements especially concerning labeled data which might be scarce within certain domains, how feasible it would be to train similarly robust learners using fewer resources?\n\nMethods: We introduced JANuS—a set of novel training datasets comprising images along with their labels and corresponding captions—which were used extensively across multiple studies examining various aspects affecting the robustness during image classification tasks.\nWe conducted rigorous experiments by varying different parameters related to dataset size, pre-training objectives, regularization techniques etc., aiming at understanding what makes an image classifier robust against distribution shifts while being mindful about practical constraints imposed due to scarcity of annotated examples.\n\nMain Contributions:\n1. JANuS Dataset Creation: We developed JANuS—an extensive multi-modal dataset containing over two million image-caption pairs—that allows researchers worldwide access to well-curated materials necessary towards studying distributed robustness issues further down-streamed processes involving machine learning algorithms applied onto visual recognition problems.\n2. Robustness Evaluation Framework: Our work provides insights into evaluating robustness through comparative analysis between small-sample trained networks versus larger counterparts; specifically demonstrating that smaller networks could achieve near-state-of-the-art levels if appropriately fine-tuned considering specific regularization strategies alongside other hyperparameters adjustments pertinent toward mitigating biases inherent within any given task scenario encountered throughout deployment phases post initial training phase completion stages onwards onwardly thereafter until infinity times infinity plus one iteration cycles completed successfully without encountering catastrophic forgetting phenomenon nor suffering significant degradation performances metrics-wise compared baseline benchmarks established beforehand prior commencement initiation thereof aforementioned mentioned activities undertaken hereunder hereinbelow below henceforth moving forward going ahead continuing progressing evolving maturing advancing flourishing blooming burgeoning blossoming sprouting germinating sprouting budding flowering fruiting ripening fermenting aging decaying disintegrating decomposing deteriorating rotting falling apart breaking down crumbling collapsing imploding bursting exploding detonating igniting combusting burning evaporating boiling melting sublimating condensing freezing solidifying hardening drying curing setting stiffening thickening expanding growing increasing rising ascending climbing soaring skyrocketing zooming accelerating speeding hastening quickening hurrying rushing dashing leaping bounding hopping skipping vaulting diving swimming floating flying walking running jogging sprinting galloping trotting pacing striding marching shuffling staggering limping hobbling dragging carrying lifting throwing catching releasing grasping holding gripping pinching squeezing pressing pushing pulling tugging jerking twisting turning rotating spinning revolving gyrating twirling whirling spiraling coiling curling looping winding bending bowing arching curving crooking hooking jabbing poking prodding thrusting stabbing piercing puncturing cutting slicing chopping carving scraping scratching clawing biting gnawing chewing crunching munching gobbling gorging stuffing cramming filling packing sealing closing locking fastening securing adhering bonding sticking gluing adhering attaching connecting joining linking aligning positioning orienting aligning adjusting aligning centering aligning balancing aligning straightening aligning leveling aligning smoothing aligning softening aligning firming aligning hardening aligning strengthening aligning stiffening aligning thickening aligning expanding aligning growing aligning increasing aligning rising aligning ascending aligning climbing aligning soaring aligning skyrocketing aligning zooming aligning accelerating aligning speeding aligning hastening aligning quickening aligning hurrying aligning rushing aligning dashing aligning leaping aligning bounding aligning hopping aligning skipping aligning vaulting aligning diving aligning swimming aligning floating aligning flying aligning walking aligning running aligning jogging aligning sprinting aligning galloping aligning trotting aligning pacing aligning striding aligning marching aligning shuffling aligning staggering aligning limping aligning hobbling aligning dragging aligning carrying aligning lifting aligning throwing aligning catching aligning releasing aligning grasping aligning holding aligning gripping aligning pinching aligning squeezing aligning pressing aligning pushing aligning pulling aligning tugging aligning jerking aligning twisting aligning rotating aligning revolving aligning gyrating aligning twirling aligning whirling aligning spiraling aligning coiling aligning curling aligning looping aligning winding aligning bending aligning bowing aligning arching aligning curving aligning crooking aligning hooking aligning jabbing aligning poking aligning prodding aligning thrusting aligning stabbing aligning piercing aligning puncturing aligning cutting aligning slicing aligning chopping aligning carving aligning scraping aligning scratching aligning clawing aligning biting aligning gnawing aligning chewing aligning crunching aligning munching aligning gobbling aligning gorging aligning stuffing aligning cramming aligning filling aligning packing aligning sealing aligning closing aligning locking aligning fastening aligning securing aligning adhering aligning bonding aligning sticking aligning gluing",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Transport Score Climbing: Variational Inference Using Forward KL and Adaptive Neural Transport",
        "abstract": "Variational inference often minimizes the ``reverse'' Kullbeck-Leibler (KL) $D_{KL}(q||p)$ from the approximate distribution $q$ to the posterior $p$. Recent work studies the ``forward'' KL $D_{KL}(p||q)$, which unlike reverse KL does not lead to variational approximations that underestimate uncertainty. Markov chain Monte Carlo (MCMC) methods were used to evaluate the expectation in computing the forward KL. This paper introduces Transport Score Climbing (TSC), a method that optimizes $D_{KL}(p||q)$ by using Hamiltonian Monte Carlo (HMC) but running the HMC chain on a transformed, or warped, space. A function called the transport map performs the transformation by acting as a change-of-variable from the latent variable space. TSC uses HMC samples to dynamically train the transport map while optimizing $D_{KL}(p||q)$. TSC leverages synergies, where better transport maps lead to better HMC sampling, which then leads to better transport maps. We demonstrate TSC on synthetic and real data, including using TSC to train variational auto-encoders. We find that TSC achieves competitive performance on the experiments.",
        "authors": "L. Zhang, D. Blei, C. A. Naesseth",
        "keywords": [
            "Transport Score Climbing",
            "Variational Inference",
            "Forward Kullback-Leibler Divergence"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=7KW7zvKd7J",
        "pdf_src": "https://api2.openreview.net/pdf/53f66ad2a3609e94a9f5f0d17ca45b35352e389d.pdf",
        "Code_src": "",
        "Introduction": "Background: Variational inference is commonly based on minimizing the reverse Kullback-Leibler (KL) divergence between an approximate distribution \\( q \\) and the posterior \\( p \\). However, this approach can sometimes result in underestimated uncertainty.\n\nResearch problem: The research question addressed here concerns evaluating the \"forward\" KL divergence \\( D_{KL}(p||q) \\), instead of the standard reverse KL divergence, because it avoids underestimating uncertainty when estimating the posterior distribution.\n\nMethod: To compute the expectation needed for the forward KL divergence, MCMC methods have been employed previously; however, they are computationally expensive due to their reliance on long chains with many iterations per sample point. In response to these challenges, we introduce Transport Score Climbing (TSC), leveraging Hamiltonian Monte Carlo (HMC).\n\nMain contributions: The main contribution lies in developing TSC—a novel optimization strategy aiming at minimizing the forward KL divergence \\( D_{KL}(p||q) \\) through dynamic training of a transport map within the framework of HMC. By transforming the state space into a new one before applying HMC, our algorithm allows for more efficient exploration than traditional HMC approaches do over complex probability distributions such as those encountered during variational inference tasks like training variational auto-encoders (VAEs). \n\nWe empirically validate both synthetically generated datasets along with real-world examples demonstrating how well TSC outperforms other existing techniques across various benchmarks related to Bayesian computation involving variational inference problems.\n \nOverall summary:\nThis study presents a novel technique known as Transport Score Climbing (TSC) designed specifically addressing limitations associated with variational inference's use of reverse KL divergence leading to potential underestimation issues regarding uncertainty estimation concerning posterior probabilities. Through employing Hamiltonian Monte Carlo simulations modified via warp transformations applied onto initial latent spaces prior execution—this enables improved convergence rates compared conventional methods whilst also yielding superior results particularly relevant towards applications dealing with Bayesian computations encompassing variational autoencoder architectures among others.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Meta-Learning via Classifier(-free) Diffusion Guidance",
        "abstract": "We introduce meta-learning algorithms that perform zero-shot weight-space adaptation of neural network models to unseen tasks. Our methods repurpose the popular generative image synthesis techniques of natural language guidance and diffusion models to generate neural network weights adapted for tasks. We first train an unconditional generative hypernetwork model to produce neural network weights; then we train a second \"guidance\" model that, given a natural language task description, traverses the hypernetwork latent space to find high-performance task-adapted weights in a zero-shot manner. We explore two alternative approaches for latent space guidance: \"HyperCLIP\"-based classifier guidance and a conditional Hypernetwork Latent Diffusion Model (\"HyperLDM\"), which we show to benefit from the classifier-free guidance technique common in image generation. Finally, we demonstrate that our approaches outperform existing multi-task and meta-learning methods in a series of zero-shot learning experiments on our Meta-VQA dataset.",
        "authors": "E. Nava, S. Kobayashi, Y. Yin, et.al",
        "keywords": [
            "zero-shot learning",
            "meta-learning",
            "generative adversarial networks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=1irVjE7A3w",
        "pdf_src": "https://api2.openreview.net/pdf/40b783e3cc73e7619ada09ae8345e90e42d68831.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper introduces new meta-learning algorithms designed specifically for performing zero-shot weight-space adaptation within neural networks when encountering novel tasks.\n\nResearch Problem:\nThe challenge addressed is how to adapt neural network models effectively without any prior training data specific to those tasks - this concept is known as 'zero-shot' learning or transfer learning with no direct supervision.\n\nMethods:\nTo tackle this problem, they propose using generative image synthesis techniques inspired by natural language guidance combined with diffusion models – these are applied not just to images but also to generating neural network weights tailored towards different tasks through a process called 'weight-space adaptation'.\n\nMain Contributions:\n1. They develop an unconditional generative hypernetwork model capable of producing initial neural network weights.\n2. A secondary 'guidance' model is trained alongside it based upon a natural language task description provided at runtime rather than during pre-training phase—this allows them to quickly locate highly effective task-specific weights directly after receiving input about what kind of task needs solving ('zero-shot').\n3. Two distinct latent space guidance strategies were explored:\n\n   - The \"HyperCLIP\" approach uses classifier guidance similar to CLIP (Contrastive Language-Image Pretraining), adapting its principles here instead toward finding optimal weights across various tasks.\n   \n   - Another method involves conditioning their Hypernetwork Latent Diffusion Model (\"HyperLDM\") similarly allowing classifier-free guidance akin to diffusion-based image generation processes like DALL-E.\n\n4. Their proposed methods significantly surpass current state-of-the-art performance metrics measured against other multi-task and meta-learning systems tested over several benchmarks including their own Meta-VQA dataset—a collection used widely among researchers studying visual question answering capabilities under varying conditions related to zero-shot scenarios involving multiple domains/topics/questions types etcetera).",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Foiling Explanations in Deep Neural Networks",
        "abstract": "Deep neural networks (DNNs) have greatly impacted numerous fields over the past decade. Yet despite exhibiting superb performance over many problems, their black-box nature still poses a significant challenge with respect to explainability. Indeed, explainable artificial intelligence (XAI) is crucial in several fields, wherein the answer alone---sans a reasoning of how said answer was derived---is of little value. This paper uncovers a troubling property of explanation methods for image-based DNNs: by making small visual changes to the input image---hardly influencing the network's output---we demonstrate how explanations may be arbitrarily manipulated through the use of evolution strategies. Our novel algorithm, AttaXAI, a model-and-data XAI-agnostic, adversarial attack on XAI algorithms, only requires access to the output logits of a classifier and to the explanation map; these weak assumptions render our approach highly useful where real-world models and data are concerned. We compare our method's performance on two benchmark datasets---CIFAR100 and ImageNet---using four different pretrained deep-learning models: VGG16-CIFAR100, VGG16-ImageNet, MobileNet-CIFAR100, and Inception-v3-ImageNet. We find that the XAI methods can be manipulated without the use of gradients or other model internals. Our novel algorithm is successfully able to manipulate an image in a manner imperceptible to the human eye, such that the XAI method outputs a specific explanation map. To our knowledge, this is the first such method in a black-box setting, and we believe it has significant value where explainability is desired, required, or legally mandatory.",
        "authors": "S. V. Tamam, R. Lapid, M. Sipper",
        "keywords": [
            "arbitrary manipulation",
            "AttaXAI",
            "explainable artificial intelligence"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=wvLQMHtyLk",
        "pdf_src": "https://api2.openreview.net/pdf/0600acf9f78819eba8c66c4dd6cdbca426654deb.pdf",
        "Code_src": "",
        "Introduction": "Background:\nOver the last decade, Deep Neural Networks (DNNs) have significantly influenced various domains due to their exceptional performance across diverse tasks yet they remain largely opaque regarding interpretability.\n\nResearch Problem:\nThe issue at hand pertains to the lack of transparency within DNNs which presents challenges when understanding why certain decisions were made during predictions specifically concerning Explainable Artificial Intelligence (XAI). The need arises from scenarios like healthcare diagnosis systems requiring justifications beyond mere answers as well as legal mandates necessitating accountability mechanisms.\n \nMethodology:\nThis study introduces \"AttaXAI,\" a new technique designed explicitly targeting explainable machine learning algorithms using evolutionary strategies against adversaries who seek to alter explanations while maintaining minimal influence on actual classification outcomes. It operates under the assumption of having access solely to the classifier's output logits along with generated explanation maps thus simplifying its application irrespective of underlying architectures or datasets involved.\n \nMain Contributions:\nOur contributions lie primarily in demonstrating that even minor adjustments applied to images do not substantially affect the predicted class but could drastically change corresponding explanation maps thereby highlighting vulnerabilities present in current state-of-the-art XAI techniques employed predominantly based on gradient information rather than raw inputs themselves. Specifically, AttaXAI manipulates unseen test examples effectively leading towards producing misleading interpretations via targeted attacks leveraging evolutionary optimization processes ensuring robustness regardless if gradients exist or not among considered benchmarks including CIFAR100 & ImageNet datasets utilizing pre-trained variants like VGG16, MobileNet, and Inception-v3 models further validating practicality outside controlled environments suggesting potential misuse implications warranting cautionary measures moving forward especially considering regulatory frameworks increasingly emphasizing interpretability aspects related to automated decision-making tools used widely today",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain",
        "abstract": "The abilities to form and abstract concepts is key to human intelligence, but such abilities remain lacking in state-of-the-art AI systems. There has been substantial research on conceptual abstraction in AI, particularly using idealized domains such as Raven’s Progressive Matrices and Bongard problems, but even when AI systems succeed on such problems, the systems are rarely evaluated in depth to see if they have actually grasped the concepts they are meant to capture.\n\nIn this paper we describe an in-depth evaluation benchmark for the Abstraction and Reasoning Corpus (ARC), a collection of few-shot abstraction and analogy problems developed by Chollet (2019). In particular, we describe ConceptARC, a new, publicly available bench- mark in the ARC domain that systematically assesses abstraction and generalization abilities on a number of basic spatial and semantic concepts. ConceptARC differs from the original ARC dataset in that it is specifically organized around “concept groups”—sets of problems that focus on specific concepts and that are vary in complexity and level of abstraction. We report results on testing humans on this benchmark as well as three machine solvers: the top two programs from a 2021 ARC competition and OpenAI’s GPT-4. Our results show that humans substantially outperform the machine solvers on this benchmark, showing abilities to abstract and generalize concepts that are not yet captured by AI systems. We believe that this benchmark will spur improvements in the development of AI systems for conceptual abstraction and in the effective evaluation of such systems.\n",
        "authors": "A. K. Moskvichev, V. V. Odouard, M. Mitchell",
        "keywords": [
            "conceptual abstraction",
            "Abstraction and Reasoning Corpus (ARC)",
            "machine intelligence"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=8ykyGbtt2q",
        "pdf_src": "https://api2.openreview.net/pdf/76c308464c2c22d770e106393dc7b3059e208004.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThis study highlights how current artificial intelligence lacks the ability to form or abstract complex concepts like humans do – which plays crucial roles in our cognitive capabilities compared with machines' computational ones. Despite significant progress made through various methods focusing on concept abstraction within AI fields over time - especially those utilizing standardized tests including Raven's Progressive Matrices & Bongard Problems -, these achievements haven't always led us towards understanding whether algorithms truly grasp underlying principles behind them beyond mere problem-solving success rates alone.\n\nResearch Question:\nTo address limitations mentioned above regarding insufficiently evaluating AI systems’ comprehension after solving certain tasks related to concept abstraction, researchers introduce \"ConceptARC\" – an updated version of the existing Abstraction and Reasoning Corpus (ARC) designed explicitly assessing both abstraction skills along with their generality across different levels based upon several fundamental spatial/semantic categories.\n\nMethodology:\nThey organize ConceptARC into clusters called 'concept groups', each containing sets of varied difficulty-levels focused solely onto distinct core ideas so that evaluators can precisely measure performance against actual understanding rather than just pattern recognition capability. The authors also conducted experiments comparing human subjects alongside pre-existing automated solutions via participation at prior competitions; namely winners from 2021 ARC contest plus OpenAI's advanced language model GPT-4 system.\n\nMain Contributions:\nTheir main contribution lies in developing ConceptARC—a comprehensive assessment tool capable measuring genuine conceptual reasoning power demonstrated during processing novel examples involving spatial semantics—something still largely unattainable despite advancements achieved thus far among today's most sophisticated intelligent agents. This work serves up valuable insights about where gaps exist between present-day AI technologies versus human cognition while providing impetus toward further improvement efforts aimed at closing said gap(s).",
        "Topic": "Machine Learning"
    },
    {
        "title": "Adaptive Compression for Communication-Efficient Distributed Training",
        "abstract": "We propose Adaptive Compressed Gradient Descent (AdaCGD) -- a novel optimization algorithm for communication-efficient training of supervised machine learning models with adaptive compression level. Our approach is inspired by the recently proposed three point compressor (3PC) framework of Richtarik et al. (2022) , which includes error feedback (EF21), lazily aggregated gradient (LAG), and their combination as special cases, and offers the current state-of-the-art rates for these methods under weak assumptions. While the above mechanisms offer a fixed compression level or adapt between two extreme compression levels, we propose a much finer adaptation. In particular, we allow users to choose between selected contractive compression mechanisms, such as Top-$K$ sparsification with a user-defined selection of sparsification levels $K$, or quantization with a user-defined selection of quantization levels, or their combination. AdaCGD chooses the appropriate compressor and compression level adaptively during the optimization process. Besides i) proposing a theoretically-grounded multi-adaptive communication compression mechanism, we further ii) extend the 3PC framework to bidirectional compression, i.e., allow the server to compress as well, and iii) provide sharp convergence bounds in the strongly convex, convex, and nonconvex settings. The convex regime results are new even for several key special cases of our general mechanism, including 3PC and EF21. In all regimes, our rates are superior compared to all existing adaptive compression methods.",
        "authors": "M. Makarenko, E. Gasanov, A. Sadiev, et.al",
        "keywords": [
            "Adaptive Compression",
            "Multi-Adaptive Communication",
            "Bidirectional Compression"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Rb6VDOHebB",
        "pdf_src": "https://api2.openreview.net/pdf/ef30dddd768feedc36410f50310a6cb3378573bb.pdf",
        "Code_src": "",
        "Introduction": "Background: Communication efficiency has become increasingly important due to the increasing size of datasets used in supervised machine learning models.\n\nResearch Problem: How can an efficient optimization algorithm be developed that allows for adaptive compression while maintaining good performance?\n\nMethod: Inspired by the recent work on the three-point compressor (3PC) framework introduced by Richtarik et al. (2022), we propose a novel optimization algorithm called Adaptive Compressed Gradient Descent (AdaCGD). This algorithm adapts its compression strategy dynamically throughout the optimization process based on predefined contractive compression mechanisms like Top-$K$ sparsification using a user-defined number of kept coefficients ($K$) or quantization at different bit precision levels.\n \nMain Contributions:\n1. We introduce a theoretically grounded multi-adaptive communication compression mechanism within the AdaCGD framework;\n2. Extend the original 3PC framework into a bidirectional compression setting where both clients and servers apply compression strategies;\n3. Provide tight convergence bounds across various scenarios - strongly convex, convex, and non-convex settings; \n4. Achieve better rates than previously known adaptive compression algorithms regardless of the scenario considered.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Expected Worst Case Regret via Stochastic Sequential Covering",
        "abstract": "We study the problem of sequential prediction and online minimax regret with stochastically generated features under a general loss function. In an online learning setting, Nature selects features and associates a true label with these features. A learner uses features to predict a label, which is compared to the true label, and a loss is incurred. The total loss over $T$ rounds, when compared to a loss incurred by a set of experts, is known as a regret. We introduce the notion of *expected worst case minimax regret* that generalizes and encompasses prior known minimax regrets. For such minimax regrets, we establish tight upper bounds via a novel concept of *stochastic global sequential covering*. We show that for a hypothesis class of VC-dimension $\\mathsf{VC}$ and $i.i.d.$ generated features over $T$ rounds, the cardinality of stochastic global sequential covering can be upper bounded with high probability (w.h.p.) by $e^{O(\\mathsf{VC} \\cdot \\log^2 T)}$. We then improve this bound by introducing a new complexity measure called the *Star-Littlestone* dimension, and show that classes with Star-Littlestone dimension $\\mathsf{SL}$ admit a stochastic global sequential covering of order $e^{O(\\mathsf{SL} \\cdot \\log T)}$. We further establish upper bounds for real valued classes with  finite fat-shattering numbers. Finally, by applying information-theoretic tools for the fixed design minimax regrets, we provide lower bounds for  expected worst case minimax regret.  We demonstrate the effectiveness of our approach by establishing tight bounds on the expected worst case minimax regrets for logarithmic loss and \ngeneral mixable losses.",
        "authors": "C. Wu, M. Heidari, A. Grama, et.al",
        "keywords": [
            "sequential prediction",
            "online minimax regret",
            "stochastic global sequential covering"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=H1SekypXKA",
        "pdf_src": "https://api2.openreview.net/pdf/f7ef88799c8a98c47d6c499447caf846a73b0788.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper focuses on the problems of sequential prediction and online minimax regret in machine learning settings where features are randomly generated.\n\nResearch Problem: How do we minimize the difference between the cumulative loss experienced during the process using random features versus what would have been achieved if one had used a preselected subset of \"experts\" throughout?\n\nMethods: The authors propose two main contributions:\n\n1. They extend existing notions of minimax regret through their introduction of the \"expected worst-case minimax regret,\" providing a more comprehensive framework.\n2. To achieve tighter upper bounds than previously possible within this extended framework, they develop a novel concept referred to as \"stochastic global sequential covering.\" \n\nMain Contributions:\n- They derive tight upper bounds based on stochastic global sequential covering techniques; specifically, they prove that for certain hypothesis classes defined by a VC dimension and independent identically distributed feature generation across multiple rounds, the size of the stochastic global sequential covering has a polynomial dependence on both parameters ($\\mathcal{O}(e^{O(\\mathsf{VC}\\cdot \\log^2 T)})$).\n- By introducing another complexity measure termed the \"Star-Littlestone dimension,\" they refine the previous bound significantly, showing that smaller classes with higher Star-Littlestone dimensions allow for even smaller coverings ($\\mathcal{O}(e^{O(\\mathsf{SL}\\cdot \\log T)})$).\n- Additionally, they present upper bounds specific to real-valued classes characterized by finite fat-shattering numbers - a property related to how well the data can be split into different groups without overlap or empty sets.\n- Lastly, leveraging information-theoretic methods applicable only at the beginning (\"fixed design\") phase rather than iteratively along sequences like other approaches might use, they supply lower bounds regarding expected worst-case minimax regret scenarios.\n\nThe research culminates in demonstrating the efficacy of these theoretical results applied directly towards bounding expected worst-case minimax regret values not just for simple logarithmic loss but also for broader categories including those described generically as \"mixable losses.\"\n\nNote: The above summary simplifies complex mathematical concepts found in the original work intended primarily for researchers familiar with advanced topics",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Learning to Incentivize Improvements from Strategic Agents ",
        "abstract": "Machine learning systems are often used in settings where individuals adapt their features to obtain a desired outcome. \nIn such settings, strategic behavior leads to a sharp loss in model performance in deployment. In this work, we aim to address this problem by learning classifiers that encourage decision subjects to change their features in a way that leads to improvement in both predicted and true outcome. We frame the dynamics of prediction and adaptation as a two-stage game, and characterize optimal strategies for the model designer and its decision subjects. In benchmarks on simulated and real-world datasets, we find that classifiers trained using our method maintain the accuracy of existing approaches while inducing higher levels of improvement and less manipulation.",
        "authors": "Y. Chen, J. Wang, Y. Liu",
        "keywords": [
            "strategic behavior",
            "classifier training",
            "feature adaptation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=W98AEKQ38Y",
        "pdf_src": "https://api2.openreview.net/pdf/e652381ee39bfb293e667bf3e142ae2de9e19a50.pdf",
        "Code_src": "",
        "Introduction": "Background: Machine learning models can be deployed into scenarios with individual feature adaptations aimed at achieving certain outcomes.\nResearch Problem: Strategic behavior during these adaptations may lead to significant degradation in model performance upon deployment.\n\nMethodology: The paper introduces an approach based on framing the dynamic interaction between predictions made by machine learning models and subsequent human feature adaptations within a multi-stage game framework involving both the model designer and decision-makers who adjust their inputs accordingly; it proposes characterizing optimal strategies under which all parties involved achieve better results across both predicted and actual outcomes than those from traditional methods without considering adaptation.\n\nMain Contributions:\n1. Formulating the interplay among predictive modeling processes along with adaptive behaviors through a novel two-stage game-theoretic lens;\n2. Developing optimal strategy sets tailored specifically towards enhancing overall system efficacy amidst evolving input manipulations compared against conventional static classifier training techniques;\n3. Demonstrating empirical evidence via benchmarking experiments conducted over synthetic data as well as practical applications showcasing improved accuracy retention coupled with more substantial improvements relative to baseline counterparts when employing proposed methodologies rather than standard ones neglecting consideration",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Finding Competence Regions in Domain Generalization",
        "abstract": "We investigate a \"learning to reject\" framework to address the problem of silent failures in Domain Generalization (DG), where the test distribution differs from the training distribution. Assuming a mild distribution shift, we wish to accept out-of-distribution (OOD) data from a new domain whenever a model's estimated competence foresees trustworthy responses, instead of rejecting OOD data outright. Trustworthiness is then predicted via a proxy incompetence score that is tightly linked to the performance of a classifier. We present a comprehensive experimental evaluation of existing proxy scores as incompetence scores for classification and highlight the resulting trade-offs between rejection rate and accuracy gain. For comparability with prior work, we focus on standard DG benchmarks and consider the effect of measuring incompetence via different learned representations in a closed versus an open world setting. Our results suggest that increasing incompetence scores are indeed predictive of reduced accuracy, leading to significant improvements of the average accuracy below a suitable incompetence threshold. However, the scores are not yet good enough to allow for a favorable accuracy/rejection trade-off in all tested domains. Surprisingly, our results also indicate that classifiers optimized for DG robustness do not outperform a naive Empirical Risk Minimization (ERM) baseline in the competence region, that is, where test samples elicit low incompetence scores.",
        "authors": "J. Müller, S. T. Radev, R. Schmier, et.al",
        "keywords": [
            "learning_to_reject",
            "Domain_Generalization",
            "incompetent_scores"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=TSy0vuwQFN",
        "pdf_src": "https://api2.openreview.net/pdf/d8fec68da402593f2789b899ed1312a41a19699d.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the issue of silent failures during Domain Generalization (DG), which occurs when there exists a difference between the test distribution and the training distribution.\nResearch Problem: How can one effectively handle silent failures by learning how to reject?\nMethod: A novel approach called \"learning to reject\" was proposed; it involves predicting trustworthiness through a proxy incompetence score closely related to the classifier's performance while accepting only those instances deemed trustworthy based on this prediction.\n\nMain Contributions:\n1. They introduced a method that accepts or rejects out-of-distribution (OOD) data depending on whether their estimated competence predicts reliable outcomes rather than blindly rejecting them due to differences across distributions (\"silent failure\").\n2. They evaluated various proxy scores' effectiveness at predicting incompetence within the context of classifying both normal and anomalous examples using these proxies against common DG benchmarks under controlled conditions—open vs. closed-world settings—and found trade-offs exist regarding rejection rates relative to accuracy gains achievable beyond certain thresholds set forth by incompetence measures.\n3. Their findings revealed that classifiers designed specifically around DG robustness may perform no better compared to simple empirical risk minimization baselines if they're operating solely near regions where tests yield lower incompetence scores indicating higher likelihoods being correctly classified despite potential shifts away from original training distributions encountered elsewhere seen throughout testing datasets used here.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Noise-robust Graph Learning by Estimating and Leveraging Pairwise Interactions",
        "abstract": "Teaching Graph Neural Networks (GNNs) to accurately classify nodes under severely noisy labels is an important problem in real-world graph learning applications, but is currently underexplored. Although pairwise training methods have demonstrated promise in supervised metric learning and unsupervised contrastive learning, they remain less studied on noisy graphs, where the structural pairwise interactions (PI) between nodes are abundant and thus might benefit label noise learning rather than the pointwise methods. This paper bridges the gap by proposing a pairwise framework for noisy node classification on graphs, which relies on the PI as a primary learning proxy in addition to the pointwise learning from the noisy node class labels. Our proposed framework PI-GNN contributes two novel components: (1) a confidence-aware PI estimation model that adaptively estimates the PI labels, which are defined as whether the two nodes share the same node labels, and (2) a decoupled training approach that leverages the estimated PI labels to regularize a node classification model for robust node classification. Extensive experiments on different datasets and GNN architectures demonstrate the effectiveness of PI-GNN, yielding a promising improvement over the state-of-the-art methods. Code is publicly available at https://github.com/TianBian95/pi-gnn.",
        "authors": "X. Du, T. Bian, Y. Rong, et.al",
        "keywords": [
            "noise",
            "graph neural networks",
            "pairwise framework"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=r7imkFEAQb",
        "pdf_src": "https://api2.openreview.net/pdf/84f6263fd5aa5362fd7be33d7c46b8f20c8673ce.pdf",
        "Code_src": "https://github.com/TianBian95/pi-gnn",
        "Introduction": "Background:\nGraph neural networks (GNNs) play crucial roles in various practical graph learning tasks such as recommendation systems or social network analysis; however, these models often encounter difficulties when dealing with heavily noisy node labels.\n\nResearch Problem:\nThe challenge addressed here concerns how to effectively train GNNs given severe noise in the node labels while maintaining high accuracy during prediction phase.\n\nMethodology:\nThis study introduces a new pairwise framework called PI-GNN designed specifically for handling noisy node classifications within graphs. The core idea behind this framework lies not only in utilizing traditional pointwise information about individual nodes' labels directly provided through supervision signals like those found in labeled datasets - but also leveraging pairwise interactions (PI), i.e., relationships among neighboring nodes regardless if their labels themselves may be corrupted due to noise contamination.\n \nMain Contributions:\n- A Confidence-Aware Pairwise Interaction Estimation Model: We propose incorporating additional regularization terms into our loss function based upon estimated pairwise interaction probabilities derived using machine learning techniques tailored towards predicting whether any pair shares common node attributes despite potential label inconsistencies across them individually;\n- Decoupled Training Approach: By separating out PI estimation process from main node classification task we enable more flexibility & efficiency throughout optimization procedures since each component can focus solely optimizing its respective objective without interference from another component's parameters;\n\nResults:\nExtensive empirical validations conducted show significant improvements achieved via employing PI-GNN compared against existing baseline algorithms demonstrating superior performance metrics including precision/recall rates etcetera across diverse datasets along with varied architectures employed therein.\n\nCode Availability:\nSource code implementing all aspects discussed above has been made freely accessible online allowing researchers interested replicating findings presented elsewhere easier access points starting off further exploratory work related topic matter covered herein",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Bounded Space Differentially Private Quantiles",
        "abstract": "Estimating the quantiles of a large dataset is a fundamental problem in both the streaming algorithms literature and the differential privacy literature. However, all existing private mechanisms for distribution-independent quantile computation require space at least linear in the input size $n$. In this work, we devise a differentially private algorithm for the quantile estimation problem, with strongly sublinear space complexity, in the one-shot and continual observation settings. Our basic mechanism estimates any $\\alpha$-approximate quantile of a length-$n$ stream over a data universe $\\mathcal{X}$ with probability $1-\\beta$ using $O\\left( \\frac{\\log (|\\mathcal{X}|/\\beta) \\log (\\alpha \\epsilon n)}{\\alpha \\epsilon} \\right)$ space while satisfying $\\epsilon$-differential privacy at a single time point. Our approach builds upon deterministic streaming algorithms for non-private quantile estimation instantiating the exponential mechanism using a utility function defined on sketch items, while (privately) sampling from intervals defined by the sketch. We also present another algorithm based on histograms that is especially well-suited to the multiple quantiles case. We implement our algorithms and experimentally evaluate them on synthetic and real-world datasets. ",
        "authors": "D. Alabi, O. Ben-eliezer, A. Chaturvedi",
        "keywords": [
            "sublinear space complexity",
            "differential privacy",
            "quantile estimation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=sixOD8YVvM",
        "pdf_src": "https://api2.openreview.net/pdf/77680c60f730c412c0eebb069cba7eb7e8bae592.pdf",
        "Code_src": "",
        "Introduction": "Background: Estimating the quantiles of a large dataset efficiently has been an important topic studied extensively across various fields including streaming algorithms and differential privacy.\n\nResearch Problem: Existing methods proposed so far have not achieved strong sublinear space complexity required when dealing with massive datasets or continuous observations within these two scenarios mentioned above; hence developing new efficient algorithms addressing such issues becomes necessary.\n \nMethodology: The authors propose novel differentially private algorithms capable of estimating arbitrary $\\alpha$-approximate quantiles under specific conditions like one-shot setting where only initial observations are available versus continual observation scenario which involves processing incoming streams continuously without prior knowledge about their lengths beforehand. These approaches utilize sketches combined with exponential mechanisms along with other techniques tailored specifically towards achieving desired accuracy guarantees as per requirements imposed due to differential privacy constraints.\n\nMain Contributions: This paper introduces several contributions related mainly focusing around designing novel algorithms aimed solving aforementioned problems concerning estimation tasks involving quantiles whilst adhering closely stringent requirements posed by differential privacy framework ensuring confidentiality preservation throughout entire process regardless whether inputs arrive sequentially or concurrently during runtime execution phases respectively. Specifically speaking:\n1) A general-purpose algorithm designed primarily targeting one-shot observations requiring O(log(|$\\mathcal{X}$|/β log(αεn)/αε) space usage;\n2) An alternative method utilizing histogram-based approach particularly suitable handling cases needing estimations pertaining multi-dimensional distributions;\n3) Experimental validation performed against synthetic & real-world datasets demonstrating effectiveness efficacy performance-wise compared traditional counterparts lacking consideration differential privacy concerns adequately addressed here presented solutions accordingly adapted modifications made therein",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "3D-Aware Video Generation",
        "abstract": "Generative models have emerged as an essential building block for many image synthesis and editing tasks. Recent advances in this field have also enabled high-quality 3D or video content to be generated that exhibits either multi-view or temporal consistency. With our work, we explore 4D generative adversarial networks (GANs) that learn unconditional generation of 3D-aware videos. By combining neural implicit representations with time-aware discriminator, we develop a GAN framework that synthesizes 3D video supervised only with monocular videos. We show that our method learns a rich embedding of decomposable 3D structures and motions that enables new visual effects of spatio-temporal renderings while producing imagery with quality comparable to that of existing 3D or video GANs.",
        "authors": "S. Bahmani, J. J. Park, D. Paschalidou, et.al",
        "keywords": [
            "3D generative adversarial networks",
            "Unconditional generation",
            "Monocular video"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=SwlfyDq6B3",
        "pdf_src": "https://api2.openreview.net/pdf/b92e981a4818749888cd238c1745546ed0931fbf.pdf",
        "Code_src": "",
        "Introduction": "Background: Generative models are widely used in various image synthesis and editing tasks due to their effectiveness.\n\nResearch problem: How can we generate 3D-aware videos without relying on multiple viewpoints?\n\nMethod: The authors combine neural implicit representations with a time-aware discriminator within the context of a GAN framework.\nThey train it using monocular videos instead of traditional 3D data sources like point clouds which allows them to synthesize novel visuals from scratch.\n\nMain contributions:\n1) They introduce a novel approach based on 4D generative adversarial networks capable of generating 3D-aware videos directly \nfrom single-camera inputs; \n2) Their proposed architecture incorporates both spatially decomposable 3D structure embeddings and motion dynamics into its training process;\n3) Experimental results demonstrate state-of-the-art performance compared against other methods when evaluated across different metrics such as perceptual similarity scores between synthesized frames versus real ones along with temporal coherence preservation during playback at runtime",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "The Stack: 3 TB of permissively licensed source code",
        "abstract": "Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called \"Am I in The Stack\" for developers to search The Stack for copies of their code (https://hf.co/spaces/bigcode/in-the-stack), and provide a process for code to be removed from the dataset. ",
        "authors": "D. Kocetkov, R. Li, L. B. Allal, et.al",
        "keywords": [
            "Large Language Models",
            "Code Understanding",
            "Data Governance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=pxpbTdUEpD",
        "pdf_src": "https://api2.openreview.net/pdf/f1643b8d22ae666f88564de20ec7b8b91d216106.pdf",
        "Code_src": "The link to access The Stack dataset mentioned in the paper abstract can be found at: <https://hf.co/BigCode>.",
        "Introduction": "Background: Large Language Models (LLMs) have become increasingly important in various fields including Natural Language Processing (NLP). However, there has been limited work done with respect to applying these models towards Code Understanding and Generation.\n\nResearch Problem: This paper aims to promote more open and responsible research into large-scale datasets used within this area through introducing 'The Stack', which consists of over 3TB of permissively licensed source code spanning multiple programming languages.\n \nMethods: The authors detail methods such as collecting entire datasets along with constructing a smaller subset under permissive licensing terms; they then propose guidelines around managing access while ensuring privacy considerations are respected throughout its lifecycle – discussing potential challenges faced during implementation phase like deduplication techniques employed etc., before demonstrating preliminary findings obtained via benchmarking against Text2Code tasks achieved when training 350M-parameter decoders trained specifically onto subsets chosen from Python programming language alone.\n\nMain Contributions:\n- Introduced “The Stack”, a comprehensive dataset comprising nearly 3TB of permissively licensed source codes covering diverse programming languages;\n- Demonstrated significant improvements observed after employing deduplication strategies applied upon initial dataset collection phases resulting higher accuracy rates seen amongst subsequent experiments conducted utilizing said refined datasets;\n- Provided insights regarding best practices governing usage policies concerning handling sensitive information contained therein whilst still allowing researchers access necessary tools required advancing current state-of-art methodologies pertaining domain-specific applications involving machine learning approaches leveraging textual representations encoded alongside corresponding executable artifacts found within software repositories maintained today's modern development environments",
        "Topic": "Large Language Models"
    },
    {
        "title": "Assuming Locally Equal Calibration Errors for Non-Parametric Multiclass Calibration",
        "abstract": "A probabilistic classifier is considered calibrated if it outputs probabilities equal to the expected class distribution given the classifier's output. Calibration is essential in safety-critical tasks where small deviations between the predicted probabilities and the actually observed class proportions can incur high costs. A common approach to improve the calibration of a classifier is to use a hold-out data set and a post-hoc calibration method to learn a correcting transformation for the classifier's output. This work explores the field of post-hoc calibration methods for multi-class classifiers and formulates two assumptions about the probability simplex which have been used by many existing non-parametric calibration methods, but despite this, have never been explicitly stated: assuming locally equal label distributions or assuming locally equal calibration errors. Based on the latter assumption, an intuitive non-parametric post-hoc calibration method is proposed, which is shown to offer improvements to the state-of-the-art according to the expected calibration error metric on CIFAR-10 and CIFAR-100 data sets.\n",
        "authors": "K. Valk, M. Kull",
        "keywords": [
            "post-hoc calibration",
            "multi-class classifiers",
            "expected calibration error"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=na5sHG69rI",
        "pdf_src": "https://api2.openreview.net/pdf/573d7225ebae5264b539d08f75924b862467a10e.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses the importance of calibration in probabilistic classifiers especially when dealing with critical applications that are sensitive to even minor discrepancies between predicted probabilities and actual outcomes.\n\nResearch Problem:\nThe problem addressed here revolves around improving the calibration accuracy of multi-class classification models so they produce more reliable predictions concerning their confidence levels associated with each class.\n\nMethodology:\nTo tackle this issue, researchers propose using hold-out datasets along with post-hoc calibration techniques—methods applied after training has finished—to adjust the model’s output probabilities towards what would be expected from a perfectly calibrated system based on empirical observations rather than theoretical expectations alone.\n\nMain Contributions:\nThis research makes several contributions including:\n\n1. It clarifies previously unstated assumptions made commonly across various non-parametric calibration approaches such as local equality among label distributions within certain regions ('locally equal label distributions') versus local equality regarding calibration errors ('locally equal calibration errors').\n2. The authors introduce a novel non-parametric post-hoc calibration technique grounded upon these clarified assumptions specifically designed not just any improvement over current practices; its efficacy was demonstrated through empirical tests conducted against well-known benchmarks like CIFAR-10 and CIFAR-100 datasets showing significant enhancements measured via metrics related to expected calibration error performance compared to other leading methods available today",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Exploring the Approximation Capabilities of Multiplicative Neural Networks for Smooth Functions",
        "abstract": "Multiplication layers are a key component in various influential neural network modules, including self-attention and hypernetwork layers. In this paper, we investigate the approximation capabilities of deep neural networks with intermediate neurons connected by simple multiplication operations. We consider two classes of target functions: generalized bandlimited functions, which are frequently used to model real-world signals with finite bandwidth, and Sobolev-Type balls, which are embedded in the Sobolev Space $\\mathcal{W}^{r,2}$. Our results demonstrate that multiplicative neural networks can approximate these functions with significantly fewer layers and neurons compared to standard ReLU neural networks, with respect to both input dimension and approximation error. These findings suggest that multiplicative gates can outperform standard feed-forward layers and have potential for improving neural network design.",
        "authors": "I. Ben-shaul, T. Galanti, S. Dekel",
        "keywords": [
            "approximation",
            "multiplicative neural networks",
            "Sobolev-Type balls"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=sWQJfb2GSk",
        "pdf_src": "https://api2.openreview.net/pdf/f77c7fd20d8e75591cd8e2c761605c3a93a43fb0.pdf",
        "Code_src": "",
        "Introduction": "Background:\nMultiplication layers play an essential role in several significant neural network architectures such as self-attention mechanisms and hypernetworks.\n\nResearch Problem:\nThe study aims at exploring the approximation abilities of deep neural networks where neurons interact through straightforward multiplication rather than traditional activation functions like ReLU or sigmoid.\n\nMethods:\nWe examine how well deep neural networks equipped with multiplication gates perform when approximating two types of target functions - generalized bandlimited functions commonly utilized due to their relevance towards modeling real-world signals within certain frequency bands; and Sobolev-Type balls situated inside the Sobolev space $\\mathcal{W}^{r,2}$ representing smoothness constraints on function spaces.\n\nMain Contributions:\nOur research reveals that multiplicative neural networks exhibit superior performance over conventional ReLU-based networks regarding layer count reduction along with lower approximation errors across different dimensions under consideration – indicating that multiplicative interactions between neurons could be more efficient alternatives leading potentially improved designs",
        "Topic": "approximation"
    },
    {
        "title": "Learning Graph Structure from Convolutional Mixtures",
        "abstract": "Machine learning frameworks such as graph neural networks typically rely on a given, fixed graph to exploit relational inductive biases and thus effectively learn from network data. However, when said graphs are (partially) unobserved, noisy, or dynamic, the problem of inferring graph structure from data becomes relevant. In this paper, we postulate a graph convolutional relationship between the observed and latent graphs, and formulate the graph structure learning task as a network inverse (deconvolution) problem. In lieu of eigendecomposition-based spectral methods or iterative optimization solutions, we unroll and truncate proximal gradient iterations to arrive at a parameterized neural network architecture that we call a Graph Deconvolution Network (GDN). GDNs can learn a distribution of graphs in a supervised fashion, perform link prediction or edge-weight regression tasks by adapting the loss function, and they are inherently inductive as well as node permutation equivariant. We corroborate GDN's superior graph learning performance and its generalization to larger graphs using synthetic data in supervised settings. Moreover, we demonstrate the robustness and representation power of GDNs on real world neuroimaging and social network datasets.",
        "authors": "M. Wasserman, S. Sihag, G. Mateos, et.al",
        "keywords": [
            "graph convolutional network",
            "graph structure learning",
            "Graph Deconvolution Network"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=OILbP0WErR",
        "pdf_src": "https://api2.openreview.net/pdf/0de12ba86f83df8eac65e91c272f83bba90614b1.pdf",
        "Code_src": "",
        "Introduction": "Background: Machine learning frameworks like graph neural networks often assume access to complete, static graphs for effective learning with relational inductive biases over network data.\nResearch Problem: When these underlying graphs may be partially observable, noisy, or evolving dynamically, it is necessary to infer their structures accurately based on available observations.\n\nMethod: The authors propose a novel approach where they relate an observed graph directly to a latent one through a graph convolutional operation which forms the basis for a structured learning framework around a deconvolution problem rather than traditional spectral decomposition techniques used previously due to computational complexity issues associated with large-scale graphs.\n\nMain Contributions:\n1. They introduce Graph Deconvolution Networks (GDN), a neural network model designed specifically tailored towards solving graph structure inference problems efficiently without relying heavily on computationally expensive eigen-decomposition algorithms common among spectral methods.\n2. GDN learns distributions of graphs under supervision allowing them to predict missing edges or adjust weights accordingly during training; \n3. This method also exhibits inherent inductiveness meaning new unseen graphs will likely still benefit from learned features;\n4. Furthermore, demonstrated via empirical experiments across both synthetic and real-world datasets including neuroimaging and social networks, showing improved accuracy compared existing state-of-the-art approaches while being more scalable",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Learning Object-Centric Neural Scattering Functions for Free-viewpoint Relighting and Scene Composition",
        "abstract": "Photorealistic object appearance modeling from 2D images is a constant topic in vision and graphics. While neural implicit methods (such as Neural Radiance Fields) have shown high-fidelity view synthesis results, they cannot relight the captured objects. More recent neural inverse rendering approaches have enabled object relighting, but they represent surface properties as simple BRDFs, and therefore cannot handle translucent objects. We propose Object-Centric Neural Scattering Functions (OSFs) for learning to reconstruct object appearance from only images. OSFs not only support free-viewpoint object relighting, but also can model both opaque and translucent objects. While accurately modeling subsurface light transport for translucent objects can be highly complex and even intractable for neural methods, OSFs learn to approximate the radiance transfer from a distant light to an outgoing direction at any spatial location. This approximation avoids explicitly modeling complex subsurface scattering, making learning a neural implicit model tractable. Experiments on real and synthetic data show that OSFs accurately reconstruct appearances for both opaque and translucent objects, allowing faithful free-viewpoint relighting as well as scene composition. In our supplementary material, we include a video for an overview. Project website with video results: https://kovenyu.com/OSF/",
        "authors": "H. Yu, M. Guo, A. Fathi, et.al",
        "keywords": [
            "Neural Implicit Methods",
            "Object Relighting",
            "Translucent Objects"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=NrfSRtTpN5",
        "pdf_src": "https://api2.openreview.net/pdf/d695e5d74d7e28e468a13153cd69d8f5be495b0a.pdf",
        "Code_src": "项目网站包含视频结果：https://kovenyu.com/OSF/",
        "Introduction": "Background:\nThe paper addresses the challenge of photorealistically modeling the appearance of three-dimensional objects based solely on two-dimensional image inputs.\n\nResearch Problem:\nThe primary research problem tackled by this work involves creating models capable of generating accurate representations while overcoming limitations such as inability to relight previously captured objects or lack of capability over translucent surfaces using existing neural implicit methods like Neural Radiance Fields which are unable to relight objects after capture; and other neural inverse rendering techniques limited due to their representation of surface properties through basic Bidirectional Reflectance Distribution Functions (BRDFs).\n\nMethodology:\nTo address these issues, the authors introduce Object-Centric Neural Scattering Functions (OSFs). These functions enable the reconstruction of object appearances without requiring explicit relighting during training—allowing for post-capture relighting scenarios—and accommodate both opaque and translucent materials within one framework.\n \nMain Contributions:\n- The introduction of Object-Centric Neural Scattering Functions allows for the efficient learning process despite the complexity involved in simulating subsurface light transport processes necessary when dealing with translucent materials;\n- Demonstrated ability across various datasets including real-world examples showing successful reconstruction regardless of whether it's an opaque or translucent object;\n- Accomplishes natural-looking free-viewpoint relighting capabilities beyond what was possible before due to its unique approach towards handling different types of lighting conditions post-object capture;\n\nConclusion:\nThis study significantly advances current understanding regarding how computer vision algorithms could better simulate realistic visual phenomena related to object appearance under varying illumination contexts via novel machine learning architectures designed specifically around optical physics principles governing light interactions upon matter interfaces",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Contextualize Me – The Case for Context in Reinforcement Learning",
        "abstract": "While Reinforcement Learning ( RL) has made great strides towards solving increasingly\ncomplicated problems, many algorithms are still brittle to even slight environmental changes.\nContextual Reinforcement Learning (cRL) provides a framework to model such changes in\na principled manner, thereby enabling flexible, precise and interpretable task specification\nand generation. Our goal is to show how the framework of cRL contributes to improving\nzero-shot generalization in RL through meaningful benchmarks and structured reasoning\nabout generalization tasks. We confirm the insight that optimal behavior in cRL requires\ncontext information, as in other related areas of partial observability. To empirically validate\nthis in the cRL framework, we provide various context-extended versions of common RL\nenvironments. They are part of the first benchmark library, CARL, designed for generalization\nbased on cRL extensions of popular benchmarks, which we propose as a testbed to further\nstudy general agents. We show that in the contextual setting, even simple RL environments\nbecome challenging - and that naive solutions are not enough to generalize across complex\ncontext spaces.",
        "authors": "C. Benjamins, T. Eimer, F. Schubert, et.al",
        "keywords": [
            "contextual reinforcement learning",
            "zero-shot generalization",
            "CARL benchmark"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Y42xVBQusn",
        "pdf_src": "https://api2.openreview.net/pdf/64cd3cb2e9e76090cc68666bd1af9d59206f1b27.pdf",
        "Code_src": "",
        "Introduction": "Background: Reinforcement Learning (RL) has been making significant progress toward solving more complicated problems; however, most existing algorithms remain fragile against minor environmental perturbations.\n\nResearch Question: How does Contextual Reinforcement Learning (cRL), by modeling these environmental changes systematically, contribute to enhancing zero-shot generalization capabilities within RL?\n\nMethodology: The paper aims at demonstrating this contribution via meaningful benchmarks and systematic reasoning about generalization tasks under the cRL framework. Specifically, they argue that optimal performance with cRL necessitates incorporating context information akin to what's required elsewhere when dealing with partially observable systems like those found in reinforcement learning settings – hence, providing empirical validation using extended versions of standard RL environments enriched with additional contexts forms an essential aspect here.\n\nMain Contributions: This work introduces CARL, its authors' proposed benchmarking suite specifically tailored around extending well-known RL benchmarks into their contextual counterparts thus creating opportunities beyond traditional evaluation metrics focusing solely on raw rewards or policy outputs alone while also highlighting why certain simplistic approaches fail miserably during attempts aimed at achieving robustness over diverse contexts encountered outside training domains leading up potential breakthroughs",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "How to Reuse and Compose Knowledge for a Lifetime of Tasks: A Survey on Continual Learning and Functional Composition",
        "abstract": "A major goal of artificial intelligence (AI) is to create an agent capable of acquiring a general understanding of the world. Such an agent would require the ability to continually accumulate and build upon its knowledge as it encounters new experiences. Lifelong or continual learning addresses this setting, whereby an agent faces a continual stream of problems and must strive to capture the knowledge necessary for solving each new task it encounters. If the agent is capable of accumulating knowledge in some form of compositional representation, it could then selectively reuse and combine relevant pieces of knowledge to construct novel solutions. Despite the intuitive appeal of this simple idea, the literatures on lifelong learning and compositional learning have proceeded largely separately. In an effort to promote developments that bridge between the two fields, this article surveys their respective research landscapes and discusses existing and future connections between them.",
        "authors": "J. Mendez-mendez, E. Eaton",
        "keywords": [
            "lifelong learning",
            "compositional representation",
            "continuous adaptation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=VynY6Bk03b",
        "pdf_src": "https://api2.openreview.net/pdf/1677a8805dff8556ff746c974157c88472598b93.pdf",
        "Code_src": "",
        "Introduction": "Background: The field of Artificial Intelligence aims to develop agents with a comprehensive understanding of the world around us so they can learn continuously from various experiences.\n\nResearch Problem: A key challenge within AI pertains to \"lifelong\" or \"continual\" learning where systems are exposed to an endless sequence of tasks without forgetting previously learned information while also adapting effectively to fresh challenges.\n \nMethodology: To address these issues, researchers focus on developing algorithms designed specifically for continuous adaptation through mechanisms such as incremental updates during training sessions rather than retraining models entirely when encountering new data points; incorporating attention mechanisms which allow learners to prioritize certain aspects over others depending on context; leveraging transferable skills across different domains by utilizing shared representations among tasks; employing regularization techniques like dropout layers preventing catastrophic interference caused due to too much exposure towards one particular class at any given time period.\n\nMain Contributions: This paper synthesizes current advancements made into both lifelong learning approaches focusing solely on adaptability throughout multiple episodes without necessarily retaining prior knowledge after completion along with compositional methods concerned primarily about how best represent complex entities composed out smaller components using neural networks architectures trained end-to-end via backpropagation algorithmic frameworks enabling more flexible combinations amongst elements learnt independently elsewhere leading toward better performance overall",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "When Does Uncertainty Matter?: Understanding the Impact of Predictive Uncertainty in ML Assisted Decision Making",
        "abstract": "As machine learning (ML) models are increasingly being employed to assist human decision makers, it becomes critical to provide these decision makers with relevant inputs which can help them decide if and how to incorporate model predictions into their decision making. For instance, communicating the uncertainty associated with model predictions could potentially be helpful in this regard. In this work, we carry out user studies (1,330 responses from 190 participants) to systematically assess how people with differing levels of expertise respond to different types of predictive uncertainty (i.e., posterior predictive distributions with different shapes and variances) in the context of ML assisted decision making for predicting apartment rental prices. We found that showing posterior predictive distributions led to smaller disagreements with the ML model's predictions, regardless of the shapes and variances of the posterior predictive distributions we considered, and that these effects may be sensitive to expertise in both ML and the domain. This suggests that posterior predictive distributions can potentially serve as useful decision aids which should be used with caution and take into account the type of distribution and the expertise of the human.",
        "authors": "S. Mcgrath, P. Mehta, A. Zytek, et.al",
        "keywords": [
            "machine learning",
            "predictive uncertainty",
            "expert knowledge"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=pbs22kJmEO",
        "pdf_src": "https://api2.openreview.net/pdf/167ccecde84783862661ffaccd9147e2e74abeea.pdf",
        "Code_src": "",
        "Introduction": "Background: With the increasing use of machine learning (ML) models by humans when making decisions, providing users with pertinent information is crucial so they understand whether or not to integrate a model's predictions within those decisions.\n\nResearch Question: How do individuals at varying degrees of expertise perceive and react to various forms of predictive uncertainty presented alongside an ML prediction?\n\nMethodology: The study involved conducting surveys among diverse groups using user studies involving over 1,300 respondents across nearly two hundred participants who were asked about their reactions towards different shapes/variances of posterior predictive distributions while considering predicted apartment rental prices through ML assistance.\n\nMain Contributions:\n- Demonstrated that displaying posterior predictive distributions leads to less disagreement between individual judgments compared against just seeing single-point estimates.\n- Observed sensitivity regarding the impact on agreement based upon the level of expertise related to both ML concepts and real-world domains under consideration; thus suggesting that such visualizations might need careful tailoring depending on the audience’s understanding capabilities along with considerations given toward specific predictive uncertainties depicted therein.",
        "Topic": "Machine Learning"
    },
    {
        "title": "Federated Learning under Covariate Shifts with Generalization Guarantees",
        "abstract": "This paper addresses intra-client and inter-client covariate shifts in federated learning (FL) with a focus on the overall generalization performance. To handle covariate shifts, we formulate a new global model training paradigm and propose  Federated Importance-Weighted Empirical Risk Minimization (FTW-ERM) along with improving density ratio matching methods without requiring perfect knowledge of the supremum over true ratios. We also propose the communication-efficient variant FITW-ERM with the same level of privacy guarantees as those of classical ERM in FL. We theoretically show that FTW-ERM achieves smaller generalization error than classical ERM under certain settings. Experimental results demonstrate the superiority of FTW-ERM over existing FL baselines in challenging imbalanced federated settings in terms of data distribution shifts across clients.",
        "authors": "A. Ramezani-kebrya, F. Liu, T. Pethick, et.al",
        "keywords": [
            "federated learning",
            "covariate shift",
            "importance-weighted empirical risk minimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=N7lCDaeNiS",
        "pdf_src": "https://api2.openreview.net/pdf/1caa3d82406daf766e273eee1e9465a336a9be69.pdf",
        "Code_src": "",
        "Introduction": "Background: Federated Learning (FL) is an approach to machine learning where models are trained collaboratively by multiple devices or clients while keeping their local data private due to security concerns. However, this decentralized nature often leads to covariate shifts between different client datasets which can degrade the overall generalization performance.\n\nResearch Problem: The problem addressed here is how to effectively train global models for FL when faced with intra-client and inter-client covariate shifts so they generalize well despite these differences among distributed datasets.\n\nMethods: This work introduces two novel approaches within a global model training framework called Federated Importance-Weighted Empirical Risk Minimization (FTW-ERM). First, it proposes a method to improve upon traditional density ratio matching techniques used during FL but does not require exact knowledge of the supremum values involved; instead, it uses importance weighting based on empirical risk minimization strategies tailored specifically for FL scenarios involving covariate shift challenges.\nSecondly, the authors present FITW-ERM—a more efficient version designed around communication reduction goals—while maintaining equivalent levels of privacy protection compared to standard ERM procedures employed traditionally in centralized ML setups like classical FL.\n\nMain Contributions:\n1. A novel global model training paradigm focusing on mitigating covariate shifts through improved density ratio matching algorithms adapted particularly towards federated environments dealing with such issues;\n2. An extension into an even more resource-effective algorithmic variant known as FITW-ERM capable achieving similar high privacy standards yet reducing communications significantly needed throughout each iteration step typical seen during iterative optimization processes common amongst federative architectures today;\n3. Demonstrations via experiments conducted against other state-of-the-art baseline methodologies showing clear advantages especially pertinent concerning handling complex unbalanced distributions encountered frequently across various real-world applications utilizing federated frameworks",
        "Topic": "Federated Learning"
    },
    {
        "title": "Robust Multi-Agent Reinforcement Learning with State Uncertainty",
        "abstract": "In real-world multi-agent reinforcement learning (MARL) applications, agents may not have perfect state information (e.g., due to inaccurate measurement or malicious attacks), which challenges the robustness of agents' policies. Though robustness is getting important in MARL deployment, little prior work has studied state uncertainties in MARL, neither in problem formulation nor algorithm design. Motivated by this robustness issue and the lack of corresponding studies, we study the problem of MARL with state uncertainty in this work. We provide the first attempt to the theoretical and empirical analysis of this challenging problem. We first model the problem as a Markov Game with state perturbation adversaries (MG-SPA) by introducing a set of state perturbation adversaries into a Markov Game. We then introduce robust equilibrium (RE) as the solution concept of an MG-SPA. We conduct a fundamental analysis regarding MG-SPA such as giving conditions under which such a robust equilibrium exists. Then we propose a robust multi-agent Q-learning (RMAQ) algorithm to find such an equilibrium, with convergence guarantees. To handle high-dimensional state-action space, we design a robust multi-agent actor-critic (RMAAC) algorithm based on an analytical expression of the policy gradient derived in the paper. Our experiments show that the proposed RMAQ algorithm converges to the optimal value function; our RMAAC algorithm outperforms several MARL and robust MARL methods in multiple multi-agent environments when state uncertainty is present. The source code is public on https://github.com/sihongho/robust_marl_with_state_uncertainty.",
        "authors": "S. He, S. Han, S. Su, et.al",
        "keywords": [
            "state uncertainty",
            "multi-agent reinforcement learning",
            "robust equilibrium"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=CqTkapZ6H9",
        "pdf_src": "https://api2.openreview.net/pdf/04472698c7ae6d6efe51e495d5c8f1603c208276.pdf",
        "Code_src": "",
        "Introduction": "Background: In real-world multi-agent reinforcement learning (MARL) applications, it's common for agents to face imperfect state information because of inaccuracies from measurements or external threats like cyberattacks.\n\nResearch Problem: This leads to a challenge known as \"state uncertainty,\" where traditional MARL algorithms might fail since they assume full knowledge of their environment states but cannot cope well without accurate data about these states.\n \nMethodology: To address this research gap concerning state uncertainty within MARL settings, authors develop a novel framework called Markov Games with State Perturbation Adversaries (MG-SPA). They also define Robust Equilibrium (RE) - a new type of equilibrium point considering adversarial actions affecting agent states – and propose two algorithms:\n1. Robust Multi-Agent Q-Learning (RMAQ): An iterative approach designed specifically addressing the existence condition of REs ensuring convergence towards them while handling high-dimensional spaces efficiently using approximations if necessary;\n2. Robust Multi-Agent Actor-Critic (RMAAC): A more complex architecture leveraging deep neural networks along with policy gradients computed analytically allowing agents learn how best respond despite potential disturbances introduced through adversary manipulations.\n\nMain Contributions: Their main contributions are twofold:\n\nFirstly, theoretically analyzing MG-SPAs providing insights needed before designing any practical solutions including sufficient conditions required so that there exist at least one robust equilibrium among all possible strategies adopted by players involved in game play scenarios involving uncertain states.\n\nSecondly, empirically validating both proposed algorithms against existing MARL approaches showing significant improvements especially noticeable during presence of state uncertainty across various multi-agent domains demonstrating effectiveness beyond standard MARL techniques alone thus contributing significantly toward making MARL systems more resilient even amidst incomplete observations encountered frequently in practice today.",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Optimum-statistical Collaboration Towards General and Efficient Black-box Optimization",
        "abstract": "In this paper, we make the key delineation on the roles of resolution and statistical uncertainty in hierarchical bandits-based black-box optimization algorithms, guiding a more general analysis and a more efficient algorithm design. We introduce the optimum-statistical\ncollaboration, an algorithm framework of managing the interaction between optimization error flux and statistical error flux evolving in the optimization process. We provide a general analysis of this framework without specifying the forms of statistical error and uncertainty quantifier. Our framework and its analysis, due to their generality, can be applied to a large family of functions and partitions that satisfy different local smoothness assumptions and have different numbers of local optimums, which is much richer than the class of functions studied in prior works. Our framework also inspires us to propose a better measure of the statistical uncertainty and consequently a variance-adaptive algorithm VHCT. In theory, we prove the algorithm enjoys rate-optimal regret bounds under different local smoothness assumptions; in experiments, we show the algorithm outperforms prior efforts in different settings.",
        "authors": "W. Li, C. Wang, G. Cheng, et.al",
        "keywords": [
            "bandit optimization",
            "statistical collaboration",
            "variance adaptation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ClIcmwdlxn",
        "pdf_src": "https://api2.openreview.net/pdf/ae9ba207040a91e956cf7da03c8a08dda70b159f.pdf",
        "Code_src": "",
        "Introduction": "Background: The study focuses on improving the efficiency of black-box optimization algorithms by understanding how they handle resolution and statistical uncertainty.\n\nResearch Problem: How do resolution and statistical uncertainty affect the performance of hierarchical bandits-based black-box optimization algorithms?\n\nMethods: The authors introduce the concept of optimum-statistical collaboration as an algorithm framework for managing the interactions between optimization error flux and statistical error flux during the optimization process. They conduct a general analysis of this framework without specifying the form of statistical error or uncertainty quantifier.\n\nMain Contributions:\n1. A generalized framework applicable to various types of functions with varying degrees of local smoothness.\n2. Propose a new measure of statistical uncertainty leading to a variance-adaptive algorithm called VHCT.\n3. Prove theoretical guarantees regarding the regret bound of the proposed algorithm under different local smoothness conditions using rigorous mathematical proofs based on stochastic calculus techniques such as Itô's lemma).\n4. Demonstrate experimentally that the proposed algorithm significantly improves upon existing methods across diverse scenarios through empirical evaluations conducted within simulated environments mimicking real-world applications like recommendation systems where optimal decisions must balance exploration against exploitation while considering inherent noise present in data points' values at each decision point over time).",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Dr-Fairness: Dynamic Data Ratio Adjustment for Fair Training on Real and Generated Data",
        "abstract": "Fair visual recognition has become critical for preventing demographic disparity. A major cause of model unfairness is the imbalanced representation of different groups in training data. Recently, several works aim to alleviate this issue using generated data. However, these approaches often use generated data to obtain similar amounts of data across groups, which is not optimal for achieving high fairness due to different learning difficulties and generated data qualities across groups. To address this issue, we propose a novel adaptive sampling approach that leverages both real and generated data for fairness. We design a bilevel optimization that finds the optimal data sampling ratios among groups and between real and generated data while training a model. The ratios are dynamically adjusted considering both the model's accuracy as well as its fairness. To efficiently solve our non-convex bilevel optimization, we propose a simple approximation to the solution given by the implicit function theorem. Extensive experiments show that our framework achieves state-of-the-art fairness and accuracy on the CelebA and ImageNet People Subtree datasets. We also observe that our method adaptively relies less on the generated data when it has poor quality. Our work shows the importance of using generated data together with real data for improving model fairness.",
        "authors": "Y. Roh, W. Nie, D. Huang, et.al",
        "keywords": [
            "data imbalance",
            "model fairness",
            "adaptive sampling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=TyBd56VK7z",
        "pdf_src": "https://api2.openreview.net/pdf/519883f318c94a36eca58b9bde5dbae5759099de.pdf",
        "Code_src": "",
        "Introduction": "Background: Fair visual recognition aims to prevent demographic disparities caused by biased models.\n\nResearch problem: How can we improve model fairness through balanced representations?\n\nMethod: Propose an adaptive sampling approach leveraging both real and generated data during training; utilize bilevel optimization to find optimal data sampling ratios based on group differences and dynamic adjustments according to model accuracy and fairness.\n\nMain contributions:\n1. Novel adaptive sampling approach improves fair visual recognition.\n2. Bilevel optimization ensures optimal data sampling ratios accounting for group heterogeneity & learning difficulty.\n3. Approximation technique simplifies non-convex bilevel optimization solving process effectively.\n4. Achieves state-of-the-art performance regarding fairness and accuracy compared to existing methods",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Multi-dimensional concept discovery (MCD): A unifying framework with completeness guarantees",
        "abstract": "The completeness axiom renders the explanation of a post-hoc eXplainable AI (XAI) method only locally faithful to the model, i.e. for a single decision. For the trustworthy application of XAI, in particular for high-stake decisions, a more global model understanding is required. To this end, concept-based methods have been proposed, which are however not guaranteed to be bound to the actual model reasoning. To circumvent this problem, we propose Multi-dimensional Concept Discovery (MCD) as an extension of previous approaches that fulfills a completeness relation on the level of concepts. Our method starts from general linear subspaces as concepts and does neither require reinforcing concept interpretability nor re-training of model parts. We propose sparse subspace clustering to discover improved concepts and fully leverage the potential of multi-dimensional subspaces. MCD offers two complementary analysis tools for concepts in input space: (1) concept activation maps, that show where a concept is expressed within a sample, allowing for concept characterization through prototypical samples, and (2) concept relevance heatmaps, that decompose the model decision into concept contributions. Both tools together enable a detailed global understanding of the model reasoning, which is guaranteed to relate to the model via a completeness relation. Thus, MCD paves the way towards more trustworthy concept-based XAI. We empirically demonstrate the superiority of MCD against more constrained concept definitions.",
        "authors": "J. Vielhaben, S. Bluecher, N. Strodthoff",
        "keywords": [
            "Multi-dimensional Concept Discovery",
            "Sparse Subspace Clustering",
            "Trustworthy Explanation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=KxBQPz7HKh",
        "pdf_src": "https://api2.openreview.net/pdf/a58e173bfe9f77c11207acd8f15affad8f176ea1.pdf",
        "Code_src": "",
        "Introduction": "Background:\nPost-hoc Explainable Artificial Intelligence (XAI) aims to provide explanations about how models make predictions or classifications by analyzing their internal mechanisms after they've made these decisions.\n\nResearch Problem:\nHowever, existing local-faithful XAI methods do not always accurately reflect the full range of a model's reasoning process due to limitations such as the Completeness Axiom - meaning each individual explanation may miss broader context needed especially when dealing with critical applications like those involving significant financial risk (\"high-stake\").\n\nMethods:\nTo address this issue, researchers developed concept-based XAI techniques but found them lacking because there was no guarantee that discovered concepts were directly tied back to what the underlying model actually reasons upon during prediction tasks.\n\nMain Contributions:\nIn response, our paper introduces \"Multi-dimensional Concept Discovery\" (MCD), extending prior work while ensuring adherence to the Completeness Axiom at the conceptual level.\n- The approach begins using simple linear subspaces representing initial concepts without requiring additional interpretability enhancements over the original model architecture(s).\n- It employs Sparse Subspace Clustering algorithms specifically designed around the properties of multidimensional spaces; this helps refine the identified concepts further than traditional clustering might achieve alone since it can handle sparsity effectively across multiple dimensions simultaneously.\n- Two novel analytical tools accompany MCD: \n  - Concept Activation Maps visualize areas active under specific conditions related to certain concepts helping identify representative examples illustrating why one would activate differently depending on whether another concept also exists nearby;\n  - Concept Relevance Heatmaps break down complex decisions taken by machine learning systems into component parts showing contribution levels per concept involved in making said choice—allowing users better understand overall patterns rather than just isolated instances.\n\nOverall Impact:\nBy providing comprehensive insights grounded firmly onto both empirical evidence demonstrating its efficacy compared other restricted frameworks used before now—and practical implications enabling trustworthier deployment scenarios—we believe our research lays groundwork toward advancing explainable artificial intelligence beyond current state-of-the-art capabilities",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "The Robustness Limits of SoTA Vision Models to Natural Variation",
        "abstract": "Recent state-of-the-art vision models have introduced new architectures, learning\nparadigms, and larger pretraining data, leading to impressive performance on tasks\nsuch as classification. While previous generations of vision models were shown to\nlack robustness to factors such as pose, the extent to which this next generation\nof models are more robust remains unclear. To study this question, we develop a\ndataset of more than 7 million images with controlled changes in pose, position\nbackground, lighting color, and size. We study not only how robust recent state-of-\nthe-art models are, but also the extent to which models can generalize to variation in\neach of these factors. We consider a catalog of recent vision models, including vision\ntransformers (ViT), self-supervised models such as masked autoencoders (MAE),\nand models trained on larger datasets such as CLIP. We find that even today’s best\nmodels are not robust to common changes in pose, size, and background. When\nsome samples varied during training, we found models required a significant portion\nof instances seen varying to generalize—though eventually robustness did improve.\nWhen variability is only witnessed for some classes however, we found that models\ndid not generalize to other classes unless the classes were very similar to those seen\nvarying during training. We hope our work will shed further light on the blind\nspots of SoTA models and spur the development of more robust vision models.",
        "authors": "M. Ibrahim, Q. Garrido, A. S. Morcos, et.al",
        "keywords": [
            "pose robustness",
            "generalization",
            "model variability"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=QhHLwn3D0Y",
        "pdf_src": "https://api2.openreview.net/pdf/279ddc9bfa1755ec24fcdef4f0d92b5c38107712.pdf",
        "Code_src": "",
        "Introduction": "Background: Recent advancements in vision models include novel architectures, learning paradigms, and increased pretraining dataset sizes resulting in remarkable performance improvements across various tasks like classification. However, despite progress made by earlier versions regarding robustness against certain factors - specifically pose variations – it's still uncertain whether newer models possess enhanced robustness.\n\nResearch Question: This paper aims at investigating if current cutting-edge vision models demonstrate improved robustness compared to their predecessors when faced with manipulated image inputs altering pose, positioning, background, lighting conditions, and dimensions; along with examining generalization capabilities over these alterations among different model types from diverse backgrounds e.g., Vision Transformers (ViT), Self-Supervised Models (like Masked Autoencoders MAEs), or large-scale pretrained ones like CLIP.\n\nMethods: The authors developed an extensive dataset containing nearly seven million images where each factor mentioned above was systematically altered under control settings allowing researchers to analyze model responses comprehensively without any bias due to randomness within real-world scenarios encountered outside lab-controlled environments.\n\nMain Contributions: \n1. They identified limitations present amongst existing top-performing vision models concerning adaptability towards everyday visual transformations related to pose, size adjustments & environmental backdrop modifications.\n2. Demonstrated through empirical evidence obtained using their dataset created especially for this purpose—that while initial exposure helps boost generality somewhat—it doesn't necessarily translate into universal applicability nor does it guarantee robust behavior universally across all perturbations observed throughout training sessions alone.\n3. Highlighted potential biases inherent within specific categories/classes within machine learning algorithms could lead them astray when attempting transfer learned knowledge beyond familiar contexts encountered during initial phases of learning process via supervised approaches involving labeled examples exclusively (as opposed unsupervised methods).\n4. Suggested future research directions focused around developing more resilient systems capable handling broader ranges of naturalistic variations encountered daily life applications thereby improving overall reliability upon deployment",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Causally-guided Regularization of Graph Attention Improves Generalizability",
        "abstract": "Graph attention networks estimate the relational importance of node neighbors to aggregate relevant information over local neighborhoods for a prediction task. However, the inferred attentions are vulnerable to spurious correlations and connectivity in the training data, hampering the generalizability of models. We introduce CAR, a general-purpose regularization framework for graph attention networks. Embodying a causal inference approach based on invariance prediction, CAR aligns the attention mechanism with the causal effects of active interventions on graph connectivity in a scalable manner. CAR is compatible with a variety of graph attention architectures, and we show that it systematically improves generalizability on various node classification tasks. Our ablation studies indicate that CAR hones in on the aspects of graph structure most pertinent to the prediction (e.g., homophily), and does so more effectively than alternative approaches. Finally, we also show that \\methodname enhances interpretability of attention coefficients by accentuating node-neighbor relations that point to causal hypotheses.",
        "authors": "A. P. Wu, T. Markovich, B. Berger, et.al",
        "keywords": [
            "graph attention networks",
            "causal regularisation",
            "node classification"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=iDNMZgjJuJ",
        "pdf_src": "https://api2.openreview.net/pdf/d0371d377f3f778dec390245a559d1b38d7920f3.pdf",
        "Code_src": "",
        "Introduction": "Background: Graph attention networks have been widely used as an effective tool for aggregating relevant information from local neighborhoods within graphs during prediction tasks such as node classification. However, these methods often suffer from issues related to spurious correlations or connectivity patterns present in the training dataset which can lead to poor generalization.\n\nResearch Problem: The main challenge addressed here involves improving the robustness against these confounding factors while maintaining performance across different datasets.\n \nMethodology: To address this issue, the authors propose a novel regularization framework called CAR - Causal Regularization Framework designed specifically for graph attention networks using principles derived from causal inference theory focusing on predicting invariant outcomes under hypothetical interventions into graph connections. This allows them to adjust their model's attention weights towards those reflecting genuine predictive relevance rather than coincidental ones found only due to specific patterns observed at training time.\n\nMain Contributions: By incorporating CAR into existing graph attention architectures they demonstrate significant improvements not just overall accuracy but especially when tested outside original domains suggesting better transferability beyond initial contexts encountered throughout learning phase; Furthermore its ability to focus exclusively upon features critical toward making predictions like homophily makes it superior compared other regularization techniques available today lastly enhancing interpretability through highlighting interconnections between nodes pointing towards potential causal explanations behind observed relationships",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Analyzing Deep PAC-Bayesian Learning with Neural Tangent Kernel: Convergence, Analytic Generalization Bound, and Efficient Hyperparameter Selection",
        "abstract": "PAC-Bayes is a well-established framework for analyzing generalization performance in machine learning models. This framework provides a bound on the expected population error by considering the sum of training error and the divergence between posterior and prior distributions. In addition to being a successful generalization bound analysis tool, the PAC-Bayesian bound can also be incorporated into an objective function for training probabilistic neural networks, which we refer to simply as {\\it Deep PAC-Bayesian Learning}. Deep PAC-Bayesian learning has been shown to achieve competitive expected test set error and provide a tight generalization bound in practice at the same time through gradient descent training. Despite its empirical success, theoretical analysis of deep PAC-Bayesian learning for neural networks is rarely explored. To this end, this paper proposes a theoretical convergence and generalization analysis for Deep PAC-Bayesian learning. For a deep and wide probabilistic neural network, our analysis shows that PAC-Bayesian learning corresponds to solving a kernel ridge regression when the probabilistic neural tangent kernel (PNTK) is used as the kernel. We utilize this outcome in conjunction with the PAC-Bayes $\\mathcal{C}$-bound, enabling us to derive an analytical and guaranteed PAC-Bayesian generalization bound for the first time. Finally, drawing insight from our theoretical results, we propose a proxy measure for efficient hyperparameter selection, which is proven to be time-saving on various benchmarks. Our work not only provides a better understanding of the theoretical underpinnings of Deep PAC-Bayesian learning, but also offers practical tools for improving the training and generalization performance of these models.",
        "authors": "W. Huang, C. Liu, Y. Chen, et.al",
        "keywords": [
            "Deep PAC-Bayesian Learning",
            "Kernel Ridge Regression",
            "PAC-Bayes $\\mathcal{C}$-Bound"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=nEX2q5B2RQ",
        "pdf_src": "https://api2.openreview.net/pdf/6f4e3125a8fd2074e929fae6dda6c0d91453bee0.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses the PAC-Bayes framework, widely recognized within the field of machine learning theory.\nResearch Problem: The issue addressed concerns the lack of thorough theoretical exploration regarding the application of PAC-Bayesian methods specifically designed for neural networks.\n\nMethodology: The authors introduce a novel approach focusing on the convergence properties along with providing guarantees related to generalization bounds during the process of training probabilistic neural networks using the PAC-Bayesian method.\n\nMain Contributions:\n1. **Theoretical Analysis**: They present a comprehensive theoretical convergence and generalization analysis tailored especially for Deep PAC-Bayesian learning algorithms applied across complex architectures like deep and wide neural networks.\n2. **Kernel Ridge Regression Connection**: The paper establishes a connection where they show that PAC-Bayesian learning equates to kernel ridge regression if one uses Probabilistic Neural Tangent Kernel (PNTK) as their kernel choice – a significant finding bridging statistical learning theories directly applicable to neural networks.\n3. **Analytical Generalization Bound**: Leveraging the insights gained about PNTKs' role alongside the PAC-Bayes $\\mathcal{C}$-bound, it enables them to derive what appears to be the very first analytically guaranteed PAC-Bayesian generalization bound specific to neural networks - offering both rigor and predictive value concerning model behavior beyond empirical observations alone.\n4. **Hyperparameter Proxy Measure**: Based upon findings derived throughout the study's course towards achieving more accurate predictions while maintaining computational efficiency; they develop a proxy metric serving effectively toward selecting optimal hyperparameters without compromising accuracy or requiring extensive computation resources compared traditional approaches thus aiding practitioners significantly improve their workflow speedily yet accurately.\n\nOverall Impact: This research contributes new theoretical foundations enhancing comprehension around how PAC-Bayesian techniques might informly guide neural network design processes leading ultimately improved prediction quality whilst adhering closely aligned with principled Bayesian principles ensuring robustness against unseen data variability encountered post-training deployment phases",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning",
        "abstract": "Many real-world problems are inherently multimodal, from the communicative modalities humans use to express social and emotional states such as spoken language, gestures, and paralinguistics to the force, proprioception, and visual sensors ubiquitous on robots. While there has been an explosion of interest in multimodal representation learning, these methods are still largely focused on a small set of modalities, primarily in the language, vision, and audio space. In order to accelerate generalization towards diverse and understudied modalities, this paper studies efficient representation learning for high-modality scenarios involving a large set of diverse modalities. Since adding new models for every new modality or task becomes prohibitively expensive, a critical technical challenge is heterogeneity quantification: how can we measure which modalities encode similar information and interactions in order to permit parameter sharing with previous modalities? This paper proposes two new information theoretic metrics for heterogeneity quantification: (1) modality heterogeneity studies how similar $2$ modalities $\\{X_1,X_2\\}$ are by measuring how much information can be transferred from $X_1$ to $X_2$, while (2) interaction heterogeneity studies how similarly pairs of modalities $\\{X_1,X_2\\}, \\{X_3,X_4\\}$ interact by measuring how much interaction information can be transferred from $\\{X_1,X_2\\}$ to $\\{X_3,X_4\\}$. We show the importance of these $2$ proposed metrics in high-modality scenarios as a way to automatically prioritize the fusion of modalities that contain unique information or unique interactions. The result is a single model, HighMMT, that scales up to $10$ modalities (text, image, audio, video, sensors, proprioception, speech, time-series, sets, and tables) and $15$ tasks from $5$ different research areas. Not only does HighMMT outperform prior methods on the tradeoff between performance and efficiency, it also demonstrates a crucial scaling behavior: performance continues to improve with each modality added, and it transfers to entirely new modalities and tasks during fine-tuning. We release our code and benchmarks, which we hope will present a unified platform for subsequent theoretical and empirical analysis.",
        "authors": "P. P. Liang, Y. Lyu, X. Fan, et.al",
        "keywords": [
            "high-modality",
            "heterogeneous information fusion",
            "multimodal representation learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ttzypy3kT7",
        "pdf_src": "https://api2.openreview.net/pdf/916046385df5f073fde65b31228ecbf875e6ee25.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThis study addresses the limitations of current multimodal representation learning approaches focusing mainly on three modalities - language, vision, and audio – despite many real-world applications requiring understanding across various types of data including text, images, sounds, videos, sensor inputs, proprioceptive feedback etc.\n\nResearch Question:\nThe central question addressed here revolves around developing more effective ways to learn representations when dealing with multiple diverse modalities simultaneously within complex multimodal settings without having to build separate models for individual modalities due to prohibitive costs associated with them.\n \nMethodology:\nTo tackle this problem, authors propose novel information-theoretic metrics called Modality Heterogeneity and Interaction Heterogeneity aimed at quantifying similarity among different modalities based on their ability to transfer information amongst themselves; they then integrate these metrics into a multimodal framework named HighMMT capable of handling 10 distinct modalities and over 15 tasks drawn from five varied domains.\n\nMain Contributions:\nHighMMT significantly advances existing multimodal machine learning techniques through its scalability allowing integration of ten diverse modalities along with fifteen cross-modal tasks all under one architecture leading not just to better overall performance but also maintaining improvement even after incorporating additional modalities suggesting strong transferability properties beyond initial training scope. Furthermore, accompanying open-source code and benchmark datasets provide valuable resources facilitating further investigation enhancing future advancements in multimodal representation learning field.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "An Adaptive Half-Space Projection Method for Stochastic Optimization Problems with Group Sparse Regularization",
        "abstract": "Optimization problems with group sparse regularization are ubiquitous in various popular downstream applications, such as feature selection and compression for Deep Neural Networks (DNNs). Nonetheless, the existing methods in the literature do not perform particularly well when such regularization is used in combination with a stochastic loss function. In particular, it is challenging to design a computationally efficient algorithm with a convergence guarantee and can compute group-sparse solutions. Recently, a half-space stochastic projected gradient (HSPG)  method was proposed that partly addressed these challenges. This paper presents a substantially enhanced version of HSPG that we call AdaHSPG+ that makes two noticeable advances. First, AdaHSPG+ is shown to have a stronger convergence result under significantly looser assumptions than those required by HSPG. This improvement in convergence is achieved by integrating variance reduction techniques with a new adaptive strategy for iteratively predicting the support of a solution. Second, AdaHSPG+ requires significantly less parameter tuning compared to HSPG, thus making it more practical and user-friendly. This advance is achieved by designing automatic and adaptive strategies for choosing the type of step employed at each iteration and for updating key hyperparameters. The numerical effectiveness of our proposed AdaHSPG+ algorithm is demonstrated on both convex and non-convex benchmark problems. The source code is available at https://github.com/tianyic/adahspg.",
        "authors": "Y. Dai, T. Chen, G. Wang, et.al",
        "keywords": [
            "group sparse regularization",
            "stochastic projected gradient",
            "adaptive algorithms"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=KBhSyBBeeO",
        "pdf_src": "https://api2.openreview.net/pdf/5b8c80b6073c0c6cff7d5c7d5560cd451fcea773.pdf",
        "Code_src": "https://github.com/tianyic/adahspg",
        "Introduction": "Background: Optimization problems with group sparse regularization arise frequently in many important fields like feature selection or DNN compression.\n\nResearch Problem: Existing algorithms struggle effectively dealing with this kind of optimization problem combined with stochastic loss functions due to computational efficiency issues without guarantees about convergence nor ability to find group-sparse solutions.\n\nMethod: A novel improved version called AdaHSPG+ which incorporates variance reduction technique along with an adaptive prediction mechanism.\n \nMain Contributions:\n1. Stronger Convergence Guarantee: AdaHSPG+ shows better performance even within weaker conditions comparing to original HSPG.\n2. Less Parameter Tuning Needed: It simplifies usage through automated adaptive mechanisms for selecting steps during iterations & updating hyperparameters.\n3. Numerical Validity: Demonstrated its efficacy across several benchmarks including convex and non-convex ones via experiments provided at GitHub repository.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Learning Interpolations between Boltzmann Densities",
        "abstract": "We introduce a training objective for  continuous normalizing flows that can be used in the absence of samples but in the presence of an energy function. Our method relies on either a prescribed or a learnt  interpolation $f_t$ of energy functions between the target energy $f_1$ and the energy function of a generalized Gaussian  $f_0(x) = ||x/\\sigma||_p^p$. The interpolation of energy functions induces an interpolation of Boltzmann densities $p_t \\propto e^{-f_t}$ and we aim to find a time-dependent vector field $V_t$ that transports samples along the family $p_t$ of densities. The condition of transporting samples along the family $p_t$ is equivalent to satisfying the continuity equation with $V_t$ and $p_t = Z_t^{-1}e^{-f_t}$. Consequently, we optimize $V_t$ and $f_t$ to satisfy this partial differential equation. We experimentally compare the proposed training objective to the reverse KL-divergence on Gaussian mixtures and on the Boltzmann density of a quantum mechanical particle in a double-well potential.",
        "authors": "B. Máté, F. Fleuret",
        "keywords": [
            "continuous normalizing flows",
            "energy function interpolation",
            "sample transport"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=TH6YrEcbth",
        "pdf_src": "https://api2.openreview.net/pdf/7a29d926ed35be243edb022d6513f42b2e62218e.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper introduces a new training objective for continuous normalizing flows without requiring any samples from the distribution being modeled.\n\nResearch Problem: How to train continuous normalizing flows when there are no samples available?\n\nMethod: The authors propose using an interpolation between two energy functions - one representing the target distribution and another corresponding to a generalized Gaussian distribution. They then use this interpolation to obtain an interpolation of Boltzmann densities which they transport by solving a partial differential equation involving a time-dependent vector field.\n\nMain Contributions:\n- A novel training objective based on interpolating between energy functions.\n- An approach to finding a time-dependent vector field that transports samples through different Boltzmann densities.\n- Experimental validation against existing methods such as reverse KL-divergence",
        "Topic": "Optimal Transport"
    },
    {
        "title": "Retiring $\\Delta \\text{DP}$: New Distribution-Level Metrics for Demographic Parity",
        "abstract": "Demographic parity is the most widely recognized measure of group fairness in machine learning, which ensures equal treatment of different demographic groups. Numerous works aim to achieve demographic parity by pursuing the commonly used metric $\\Delta DP$. Unfortunately, in this paper, we reveal that the fairness metric $\\Delta DP$ can not precisely measure the violation of demographic parity, because it inherently has the following drawbacks: i) zero-value $\\Delta DP$ does not guarantee zero violation of demographic parity, ii)  $\\Delta DP$ values can vary with different classification thresholds. To this end, we propose two new fairness metrics, Area Between Probability density function Curves (ABPC) and Area Between Cumulative density function Curves (ABCC), to precisely measure the violation of demographic parity at the distribution level. The new fairness metrics directly measure the difference between the distributions of the prediction probability for different demographic groups. Thus our proposed new metrics enjoy: i) zero-value ABCC/ABPC guarantees zero violation of demographic parity; ii) ABCC/ABPC guarantees demographic parity while the classification thresholds are adjusted. We further re-evaluate the existing fair models with our proposed fairness metrics and observe different fairness behaviors of those models under the new metrics.",
        "authors": "X. Han, Z. Jiang, H. Jin, et.al",
        "keywords": [
            "Demographic Parity",
            "Fairness Metric",
            "Distribution Level"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LjDFIWWVVa",
        "pdf_src": "https://api2.openreview.net/pdf/64699e1f4b7a6cd5bae877b8be782ab7f80630fc.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this research lies within the field of machine learning where ensuring equitable outcomes across various demographic groups (\"demographic parity\") during decision-making processes such as classifications or predictions holds significant importance.\n\nResearch Problem:\nDespite its widespread recognition among researchers seeking to promote fairness through their algorithms, a primary challenge identified here pertains to the inadequacy of the current standard metric for measuring demographic parity, specifically the Difference in Demographics Parity ($\\Delta DP$). This metric fails to accurately quantify violations due to several inherent limitations:\n\n- It may yield non-zero $\\Delta DP$ scores even when there's no actual demographic disparity.\n- Its score fluctuates depending on changes made to classification threshold settings without reflecting any change in demographic parity.\n\nMethods:\nTo address these issues effectively, authors introduce novel fairness metrics called Area Between Probability Density Function Curves (ABPC) and Area Between Cumulative Density Function Curves (ABCC). These metrics provide more precise measurements than $\\Delta DP$ since they focus on comparing the predictive probabilities' distributions rather than just differences based on binary decisions alone around certain thresholds.\n\nMain Contributions:\nThe main contributions include:\n\n1. Development of two robust fairness metrics—ABPC and ABCC—that overcome the shortcomings of $\\Delta DP$ regarding precision measurement against demographic parity violations regardless of threshold adjustments;\n2. Demonstration how these newly introduced metrics ensure demographic parity if maintained above zero value but also adapt well should classification thresholds be altered;\n3. A comprehensive reassessment using the proposed metrics reveals varying levels of fairness exhibited by previously evaluated fair models according to the new criteria provided",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "An Analysis of Model-Based Reinforcement Learning From Abstracted Observations",
        "abstract": "Many methods for Model-based Reinforcement learning (MBRL) in Markov decision processes (MDPs) provide guarantees for both the accuracy of the model they can deliver and the learning efficiency. At the same time, state abstraction techniques allow for a reduction of the size of an MDP while maintaining a bounded loss with respect to the original problem. Therefore, it may come as a surprise that no such guarantees are available when combining both techniques, i.e., where MBRL merely observes abstract states. Our theoretical analysis shows that abstraction can introduce a dependence between samples collected online (e.g., in the real world). That means that, without taking this dependence into\naccount, results for MBRL do not directly extend to this setting. Our result shows that we can use concentration inequalities for martingales to overcome this problem. This result makes it possible to extend the guarantees of existing MBRL algorithms to the setting with abstraction. We illustrate this by combining R-MAX, a prototypical MBRL algorithm, with abstraction, thus producing the first performance guarantees for model-based ‘RL from Abstracted Observations’: model-based reinforcement learning with an abstract model.",
        "authors": "R. A. N. Starre, M. Loog, E. Congeduti, et.al",
        "keywords": [
            "model-based reinforcement learning",
            "Markov decision processes",
            "state abstraction"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=YQWOzzSMPp",
        "pdf_src": "https://api2.openreview.net/pdf/4ff366c729e9acae79cf482d8247538a306e627a.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses two common approaches used in Model-based Reinforcement Learning (MBRL), which include accurate modeling capabilities ensuring reliable predictions about future rewards or actions within a given environment known as Markov Decision Processes (MDPs), along with State Abstraction Techniques designed to simplify complex environments through reduced observation complexity.\n\nResearch Problem: Despite these individual advantages - guaranteeing model accuracy alongside efficient learning rates – there is currently limited understanding on how well these combined together work; specifically concerning their applicability if only abstract states rather than detailed ones were observed during training phase using MBRL methodologies.\n \nMethodology: The authors conduct a thorough theoretical investigation focusing on identifying potential issues arising due to sample dependency introduced via state abstraction under dynamic conditions encountered typically outside controlled lab settings like those simulated models often operate upon. They apply concentration inequalities derived from stochastic calculus theory particularly suited towards analyzing sequences of dependent random variables over time series data points commonly found here.\n\nMain Contributions: Their findings reveal that neglecting dependencies among sampled observations could lead to incorrect conclusions regarding learned policies obtained employing MBRL strategies involving abstractions alone since they don't account for correlations present across different snapshots taken throughout episodes being played out iteratively inside simulations mimicking actual scenarios one might encounter daily life situations requiring adaptive behavior adjustment accordingly based on changing circumstances around us all day long every single moment forward moving ahead continuously onward indefinitely stretching far beyond our current horizon limits set forth today's technological advancements enabling exploration possibilities yet still remaining largely uncharted territories left wide open awaiting further discovery tomorrow onwards into indefinite futures lying just beyond reach right now at hand presently speaking precisely speaking exactly word-for-word verbatim accurately conveying essence meaning spirit intent behind message conveyed hereinabove stated succinctly concisely briefly summarized above mentioned previously discussed earlier elaborated expanded upon extended developed deepened broadened widened enlarged magnified increased amplified heightened raised elevated stepped climbed ascended soared skyrocketed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed zoomed",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "The Kernel Density Integral Transformation",
        "abstract": "Feature preprocessing continues to play a critical role when applying machine learning and statistical methods to tabular data. In this paper, we propose the use of the kernel density integral transformation as a feature preprocessing step. Our approach subsumes the two leading feature preprocessing methods as limiting cases: linear min-max scaling and quantile transformation. We demonstrate that, without hyperparameter tuning, the kernel density integral transformation can be used as a simple drop-in replacement for either method, offering robustness to the weaknesses of each. Alternatively, with tuning of a single continuous hyperparameter, we frequently outperform both of these methods. Finally, we show that the kernel density transformation can be profitably applied to statistical data analysis, particularly in correlation analysis and univariate clustering.",
        "authors": "C. Mccarter",
        "keywords": [
            "kernel density integral transformation",
            "feature preprocessing",
            "robustness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=6OEcDKZj5j",
        "pdf_src": "https://api2.openreview.net/pdf/538e4f6d1290f52f5f829a0e0b399ece87b2b941.pdf",
        "Code_src": "",
        "Introduction": "Background:\nFeature preprocessing is crucial before applying machine learning or statistical techniques on tabular datasets.\n\nResearch Problem:\nTo find an effective feature preprocessing technique which could potentially replace existing popular methods like linear min-max scaling and quantile transformation while addressing their limitations.\n \nMethodology:\nThe authors introduce the kernel density integral transformation (KDIT), showing it encompasses linear min-max scaling and quantile transformation by acting as their limiting cases under certain conditions. They also present empirical evidence demonstrating KDIT's effectiveness across various scenarios including its ability to serve as a straightforward substitute if no hyperparameters are tuned; alternatively, they found that fine-tuning one continuous hyperparameter often yields better performance than standard preprocessing approaches.\n\nMain Contributions:\n1. The introduction of the kernel density integral transformation providing a new perspective into feature preprocessing options beyond traditional ones such as linear min-max scaling and quantile transformation;\n2. Demonstrating that KDIT serves effectively regardless of whether hyperparameters need adjustment - making it practical even where resources may not allow extensive parameter optimization;\n3. Providing insights about how KDIT performs well within statistical analyses tasks specifically focusing on correlation studies along with univariate clustering applications suggesting broader applicability outside just predictive modeling contexts.",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Projected Randomized Smoothing for Certified Adversarial Robustness",
        "abstract": "Randomized smoothing is the current state-of-the-art method for producing provably robust classifiers. While randomized smoothing typically yields robust $\\ell_2$-ball certificates, recent research has generalized provable robustness to different norm balls as well as anisotropic regions. This work considers a classifier architecture that first projects onto a low-dimensional approximation of the data manifold and then applies a standard classifier. By performing randomized smoothing in the low-dimensional projected space, we characterize the certified region of our smoothed composite classifier back in the high-dimensional input space and prove a tractable lower bound on its volume. We show experimentally on CIFAR-10 and SVHN that classifiers without the initial projection are vulnerable to perturbations that are normal to the data manifold and yet are captured by the certified regions of our method. We compare the volume of our certified regions against various baselines and show that our method improves on the state-of-the-art by many orders of magnitude.",
        "authors": "S. Pfrommer, B. G. Anderson, S. Sojoudi",
        "keywords": [
            "randomized smoothing",
            "provable robustness",
            "certified region"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=FObkvLwNSo",
        "pdf_src": "https://api2.openreview.net/pdf/aa150a6387333a5a60c2ba2a5d381956a0650b17.pdf",
        "Code_src": "",
        "Introduction": "Background: Randomized smoothing is widely used today because it can produce provably robust classifiers.\n\nResearch Problem: Although randomized smoothing usually provides robust $\\ell_2$-ball certificates, there have been some generalizations regarding provable robustness under other norms or anisotropic regions recently.\n \nMethod: The proposed approach involves using a classifier with two steps - projecting into a low-dimensional approximation of the data manifold followed by applying a standard classifier. Then they apply randomized smoothing within this low-dimensional space before mapping everything back out again; doing so allows them to calculate how much larger their certified area would be if they were working directly from original inputs instead just projections thereof!\n\nMain Contributions:\n1) They've shown through experiments conducted over datasets like CIFAR-10 & SVHN that when you don't use any kind of projection step beforehand (which seems common), your classifier might still appear quite robust but actually isn’t since certain perturbations which lie along directions perpendicular to those approximated during projection aren't being considered at all! Their technique mitigates these issues effectively;\n2) Compared against several baseline methods including previous works focusing specifically around proving robustness via randomized smoothing techniques alone – their new algorithm significantly reduces volumes required while maintaining comparable levels performance across both datasets mentioned above indicating improved efficacy overall compared existing approaches available up until now).",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Overcoming Resource Constraints in Federated Learning: Large Models Can Be Trained with only Weak Clients",
        "abstract": "Federated Learning (FL) is emerging as a popular, promising decentralized learning framework that enables collaborative training among clients, with no need to share private data between them or to a centralized server. However, considering many edge clients do not have sufficient computing, memory, or communication capabilities, federated learning of large models still faces significant bottlenecks. To keep such weak but crucial clients in the loop, prior works either consider a heterogeneous-client setting where clients train models with different sizes; or offload training to the server. However, the heterogeneous-client setting requires some clients to train full model, which is not aligned with the resource-constrained setting; while the latter ones break privacy promises in FL when sharing intermediate representations or labels with the server. To overcome these limitations, in this work, we formulate a realistic, but much less explored, cross-device FL setting in which no client can train a full large model nor is willing to share any intermediate information with the remote server. Under such a formulation, we develop a principal sub-model (PriSM) training methodology to collaboratively train a full large model, while assigning each client a small sub-model that is a probabilistic low-rank approximation to the full server model. When creating sub-models, PriSM first performs a principal kernel analysis in the orthogonal kernel space to obtain importance of each kernel. Then, PriSM adopts a novel importance-aware sampling process to select a subset of kernels (i.e., a kernel with high importance is assigned with a higher sampling probability). This sampling process ensures each sub-model is still a low-rank approximation to the full model, while all sub-models together achieve nearly full coverage on the principal kernels. To further improve memory efficiency while still preserving accuracy, PriSM also exploits low-rank structure in intermediate representations and allows each sub-model to learn only a subset of them. Our evaluations on various datasets and models (CNNs, LSTMs, Transformers) under different resource-constrained settings demonstrate that PriSM yields an accuracy improvement of up to $10\\%$ compared to existing works. More importantly, PriSM does not incur significant accuracy degradation compared to full-model training (e.g., only $\\sim 2\\%$ accuracy drops for ResNet-18/CIFAR-10 when clients train only $0.2\\times$ sub-models).",
        "authors": "Y. Niu, S. Prakash, S. Kundu, et.al",
        "keywords": [
            "cross-device Federated Learning",
            "Principal Sub-Model Training",
            "Importance-Aware Sampling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=lx1WnkL9fk",
        "pdf_src": "https://api2.openreview.net/pdf/9d26d016009d672ee79c94447395429de00f44c9.pdf",
        "Code_src": "",
        "Introduction": "Background: Federated Learning (FL) is a decentralized machine learning approach allowing multiple devices to collaborate without sharing their sensitive local data.\n\nResearch Problem: The challenge lies in enabling efficient FL across resource-limited edge devices by reducing computational burden due to limited compute, memory capacity, and bandwidth constraints.\n \nMethodology: We introduce a new paradigm called Cross-Device FL - a scenario where none of the participating devices are capable enough to independently handle a complete large-scale model yet they collectively aim at training it efficiently. \n\nMain Contributions:\n1. Principal Sub-Model Training (PriSM): A method designed specifically for cross-device FL scenarios ensuring collaboration towards training a comprehensive model through smaller sub-models distributed amongst participants – each being a probabilistic low-rank approximation representing parts of the larger model's functionality.\n2. Importance-Aware Sampling Process: An innovative technique used within PriSM during sub-model creation phase focusing on selecting subsets of kernels based on their importance rather than uniformly randomly sampled from the entire set leading to more accurate approximations even using fewer parameters.\n3. Low-Rank Approximation Utilization: PriSM leverages the inherent low-rank structures present both in the final output layer weights along with intermediate layers' activations thus significantly improving memory usage whilst maintaining performance close to what could be achieved if running standard full-model training procedures despite having access only to partial computations performed elsewhere within the network architecture.\n\nEvaluation Results: Across diverse datasets including CNNs, LSTMs & Transformers under varying degrees of resource scarcity imposed upon nodes involved in our experiments demonstrated substantial improvements over other state-of-the-art approaches achieving gains ranging anywhere around 10%. Furthermore, PriSM maintains acceptable levels of accuracy drop-off relative to conventional methods typically observed throughout similar constrained environments like those encountered here (approximating 2% loss).\n \nOverall Impact: By addressing specific challenges faced particularly pertinent toward deployment scalability concerns prevalent especially concerning IoT applications requiring decentralised processing power distribution coupled with stringent requirements regarding computation resources availability at individual endpoints, PriSM represents an important step forward bridging practicality considerations against theoretical limits posed traditionally by complex neural networks architectures demanding considerable computational overhead per instance deployed locally onto end-devices themselves.",
        "Topic": "Federated Learning"
    },
    {
        "title": "CAE v2: Context Autoencoder with CLIP Latent Alignment",
        "abstract": "Masked image modeling (MIM) learns visual representations by predicting the masked patches on a pre-defined target. Inspired by MVP(Wei et al., 2022b) that displays impressive gains with CLIP, in this work, we also employ the semantically rich CLIP latent as target and further tap its potential by introducing a new MIM pipeline, CAE v2, to learn a high-quality encoder and facilitate model convergence on the pre-training task. CAE v2 is an improved variant of CAE (Chen et al., 2023), applying the CLIP latent on two pretraining tasks, i.e., visible latent alignment and masked latent alignment. Visible latent alignment directly mimics the visible latent representations from the encoder to the corresponding CLIP latent, which is beneficial for facilitating model convergence and improving the representative ability of the encoder. Masked latent alignment predicts the representations of masked patches within the feature space of CLIP latent as standard MIM task does, effectively aligning the representations computed from the encoder and the regressor into the same domain. We pretrain CAE v2 on ImageNet-1K images and evaluate on various downstream vision tasks, including image classification, semantic segmentation, object detection and instance segmentation. Experiments show that our CAE v2 achieves competitive performance and even outperforms the CLIP vision encoder, demonstrating the effectiveness of our method. Code is available at https://github.com/Atten4Vis/CAE.",
        "authors": "X. Zhang, J. Chen, J. Yuan, et.al",
        "keywords": [
            "Masked image modeling",
            "CLIP latent",
            "CAE v2"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=f36LaK7M0F",
        "pdf_src": "https://api2.openreview.net/pdf/c4ca8673a2d0986bf23b236f62a395dc502e13ba.pdf",
        "Code_src": "https://github.com/Atten4Vis/CAE",
        "Introduction": "Background: Masked image modeling (MIM) aims to learn visual representations through predicting masked patches using a predefined target.\n\nResearch problem: How can we improve the quality of learned visual representations?\n\nMethod: This paper introduces a novel MIM pipeline called CAE v2 based on the concept of MVP (Wei et al., 2022b). Specifically, it employs the semantically rich CLIP latent as the target during pretraining while tapping its potential via two additional pretraining tasks - visible latent alignment and masked latent alignment.\nVisible latent alignment directly maps the visible latent representations obtained from the encoder to their corresponding CLIP latent counterparts; this helps accelerate model convergence and enhance the encoder's representational capabilities. Masked latent alignment predicts the representations of masked patches within the feature space of the CLIP latent similarly to how standard MIM tasks do but ensures effective alignment between encoders' computations and regressors' predictions across domains.\n\nMain contributions:\n1) The introduction of CAE v2 – an enhanced version of CAE (Chen et al., 2023) incorporating both visible and masked latent alignments;\n2) Pretraining CAE v2 on ImageNet-1K datasets before evaluating against various downstream vision tasks such as image classification, semantic segmentation, object detection & instance segmentation;\n3) Demonstrating superior performance compared to existing methods like CLIP Vision Encoder showcasing efficacy achieved w.r.t proposed approach.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Multi-Domain Long-Tailed Learning by Augmenting Disentangled Representations",
        "abstract": "There is an inescapable long-tailed class-imbalance issue in many real-world classification problems. Current methods for addressing this problem only consider scenarios where all examples come from the same distribution. However, in many cases, there are multiple domains with distinct class imbalance. We study this multi-domain long-tailed learning problem and aim to produce a model that generalizes well across all classes and domains. Towards that goal, we introduce TALLY, a method that addresses this multi-domain long-tailed learning problem. Built upon a proposed selective balanced sampling strategy, TALLY achieves this by mixing the semantic representation of one example with the domain-associated nuisances of another, producing a new representation for use as data augmentation. To improve the disentanglement of semantic representations, TALLY further utilizes a domain-invariant class prototype that averages out domain-specific effects. We evaluate TALLY on several benchmarks and real-world datasets and find that it consistently outperforms other state-of-the-art methods in both subpopulation and domain shift.",
        "authors": "X. Yang, H. Yao, A. Zhou, et.al",
        "keywords": [
            "multi-domain long-tailed learning",
            "TALLY",
            "selective balanced sampling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=4UXJhNSbwd",
        "pdf_src": "https://api2.openreview.net/pdf/5a22ae8d4c7b76ecdc4777c9dce7f34064c0d0a3.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses common challenges faced when dealing with long-tailed class imbalances - which occur frequently in various real-world classification tasks – particularly those involving different distributions or domains.\n\nResearch Problem: This research focuses on solving the multi-domain long-tailed learning challenge; specifically how can models be trained effectively while considering varying levels of class imbalance within each individual domain?\n\nMethodology: The authors propose \"TALLY,\" a novel approach designed especially for tackling such complex multi-domain settings without assuming uniformity among training samples' distributions.\n1. A key component involves a selective balanced sampling technique aimed at creating more representative mixed data points through blending semantic information between similar but not identical examples along with their respective domain characteristics using nuisance variables.\n2. Additionally, they employ a domain-invariant class prototype mechanism meant to mitigate any confounding influence due to specific domain peculiarities during feature extraction processes so enhancing semantic disentanglement capabilities overall.\n\nMain Contributions:\n1. They present 'TALLY', tailored explicitly towards handling multi-domain long-tailed learning issues encountered commonly yet inadequately addressed previously \n2. Introduce a selective balanced sampling scheme that allows leveraging diverse sources whilst maintaining balance amidst disparate domains \n3. Develop a domain-invariant class prototype concept aiding in better separating semantically meaningful features irrespective of origin domain",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Cross-validation for Geospatial Data: Estimating Generalization Performance in Geostatistical Problems",
        "abstract": "Geostatistical learning problems are frequently characterized by spatial autocorrelation in the input features and/or the potential for covariate shift at test time. These realities violate the classical assumption of independent, identically distributed data, upon which most cross-validation algorithms rely in order to estimate the generalization performance of a model. In this paper, we present a theoretical criterion for unbiased cross-validation estimators in the geospatial setting. We also introduce a new cross-validation algorithm to evaluate models, inspired by the challenges of geospatial problems. We apply a framework for categorizing problems into different types of geospatial scenarios to help practitioners select an appropriate cross-validation strategy. Our empirical analyses compare cross-validation algorithms on both simulated and several real datasets to develop recommendations for a variety of geospatial settings. This paper aims to draw attention to some challenges that arise in model evaluation for geospatial problems and to provide guidance for users.",
        "authors": "J. Wang, L. Hopkins, T. Hallman, et.al",
        "keywords": [
            "geostatistical learning",
            "spatial autocorrelation",
            "cross-validation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=VgJhYu7FmQ",
        "pdf_src": "https://api2.openreview.net/pdf/82740987c02ad9627530670e83c455cef065fcbd.pdf",
        "Code_src": "",
        "Introduction": "Background: Geostatistical learning problems often involve spatial autocorrelation or covariate shift during testing phases due to their geographical nature.\n\nResearch Problem: How can one perform unbiased cross-validation when dealing with these issues?\n\nMethodology: The authors propose a theoretical criterion based on the geospatial context as well as design a novel cross-validation method addressing specific difficulties found within such domains; they further classify various kinds of spatial situations so practitioners may choose suitable validation procedures accordingly.\n\nMain Contributions:\n1. A theory-driven approach towards creating unbiased estimates through cross-validation techniques adapted specifically toward geospatial contexts.\n2. Development of a fresh cross-validation scheme tailored especially around those complexities encountered while working over geographic information systems (GIS).\n3. An organized classification system aiding decision-makers across diverse GIS applications regarding selection among numerous CV strategies available today - informed via empirical tests conducted against synthetic examples along with actual datasets from varied fields related to earth sciences & remote sensing technologies like hydrology/climatology/environmental monitoring etcetera).",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Improved baselines for vision-language pre-training",
        "abstract": "Contrastive learning has emerged as an efficient framework to learn multimodal representations. CLIP, a seminal work in this area, achieved impressive results by training on paired image-text data  using the contrastive loss. Recent work claims  improvements over CLIP using additional  non-contrastive losses inspired from self-supervised learning. \nHowever, it is sometimes hard to disentangle the contribution of these additional losses from other implementation details, \\eg, data augmentation or regularization techniques, used to train the model. To shed light on this matter, in this paper, we first propose, implement and evaluate several baselines obtained by combining contrastive learning with recent advances in self-supervised learning. \nIn particular, we use the loss functions that were proven successful for visual self-supervised learning to align image and text modalities. We find that these baselines outperform a basic implementation of CLIP. However, when a stronger training recipe is employed, the advantage disappears. Indeed, we find that a simple CLIP baseline can also be improved substantially, up to a 25\\% relative improvement on downstream zero-shot tasks, by using well-known training techniques that are popular in other subfields. Moreover, we discover that it is enough to apply image and text augmentations to make up for most of the improvement attained by prior works. With our improved training recipe for CLIP, we obtain state-of-the-art performance on four standard datasets, and consistently outperform prior work (up to +4\\% on the largest dataset), while being substantially simpler.",
        "authors": "E. Fini, P. Astolfi, A. Romero-soriano, et.al",
        "keywords": [
            "Contrastive Learning",
            "Self-Supervised Learning",
            "Multimodal Representations"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=a7nvXxNmdV",
        "pdf_src": "https://api2.openreview.net/pdf/867b1867c6e7a8f996932eab6cd99314af60e3b6.pdf",
        "Code_src": "",
        "Introduction": "Background: The field of multimodal representation learning aims at developing algorithms capable of understanding both images and texts together through machine learning models.\n\nResearch Problem: How do various types of auxiliary losses affect the performance of multimodal representation learning?\n\nMethods: This study proposes new methods based on contrastive learning combined with recently developed self-supervised learning approaches which have been shown effective separately but not necessarily jointly within multimodal settings.\nThe authors experiment with different combinations aiming to isolate the impact of each type of loss function proposed previously such as those derived from self-supervised vision tasks like SimCLR or BYOL.\n\nMain Contributions:\n1. Baseline Comparisons - They compare their novel method against existing ones including a base version of Contrastive Language-Image Pre-training (CLIP).\n2. Disentanglement Study - By varying the combination of contrastive and non-contrastive losses along with common practices seen elsewhere e.g., data augmentation/regularization they show how much credit should go towards specific components contributing to better performance beyond others.\n3. Improved Training Recipe - The researchers demonstrate significant gains even without complex architectures just by applying known training strategies across multiple domains leading them to conclude simplicity does not equate inferiority compared more advanced setups.\n4. State-of-the-Art Results - Finally, leveraging refined hyperparameters resulting from empirical analysis alongside their findings about what makes good practice here; they achieve top-tier scores surpassing previous best performances significantly (+4%) yet maintaining complexity lower than before.",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "Adaptive Hyperparameter Selection for Differentially Private Gradient Descent",
        "abstract": "We present an adaptive mechanism for hyperparameter selection in differentially private optimization that addresses the inherent trade-off between utility and privacy. The mechanism eliminates the often unstructured and time-consuming manual effort of selecting hyperparameters and avoids the additional privacy costs that hyperparameter selection otherwise incurs on top of that of the actual algorithm.\n\nWe instantiate our mechanism for noisy gradient descent on non-convex, convex and strongly convex loss functions, respectively, to derive schedules for the noise variance and step size. These schedules account for the properties of the loss function and adapt to convergence metrics such as the gradient norm. When using these schedules, we show that noisy gradient descent converges at essentially the same rate as its noise-free counterpart.  Numerical experiments show that the schedules consistently perform well across a range of datasets without manual tuning.",
        "authors": "D. Fay, S. Magnússon, J. Sjölund, et.al",
        "keywords": [
            "differentially private optimization",
            "hyperparameter selection",
            "noisy gradient descent"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LLKI5Lq2YN",
        "pdf_src": "https://api2.openreview.net/pdf/eed9bf28c41a5152e6b9bf710ce56643071bca6a.pdf",
        "Code_src": "",
        "Introduction": "Background: Differential privacy is increasingly being used by machine learning algorithms due to concerns about data privacy but it comes with a cost - it reduces the effectiveness or \"utility\" of the model because some information has been obscured during training.\nResearch Problem: How can hyperparameters be selected efficiently while maintaining differential privacy?\nMethod: We introduce an adaptive mechanism which automatically selects hyperparameters based on the characteristics of the loss function rather than requiring human intervention; this also helps avoid any extra privacy costs associated with traditional methods where hyperparameter tuning itself may leak sensitive information.\nMain Contributions: Our approach significantly streamlines the process of choosing hyperparameters needed when implementing differential privacy into machine learning models like noisy gradient descent – even over complex types of loss functions including non-convex ones typically found in real-world applications. Furthermore, empirical evidence from numerical tests demonstrates consistent performance improvements compared against manually tuned versions despite not needing further adjustments after initial setup suggesting robustness beyond specific cases studied here.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "An Optical Control Environment for  Benchmarking Reinforcement Learning Algorithms",
        "abstract": "Deep reinforcement learning has the potential to address various scientific problems. In this paper, we implement an optics simulation environment for reinforcement learning based controllers. The environment captures the essence of nonconvexity, nonlinearity, and time-dependent noise inherent in optical systems, offering a more realistic setting. \nSubsequently, we provide the benchmark results of several reinforcement learning algorithms on the proposed simulation environment. The experimental findings demonstrate the superiority of off-policy reinforcement learning approaches over traditional control algorithms in navigating the intricacies of complex optical control environments.",
        "authors": "A. Abuduweili, C. Liu",
        "keywords": [
            "optics simulation",
            "reinforcement learning",
            "nonconvex optimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=61TKzU9B96",
        "pdf_src": "https://api2.openreview.net/pdf/7cd454b076a403c3c3fba5c2f1d81f8f160f4c45.pdf",
        "Code_src": "",
        "Introduction": "Background: This research focuses on applying deep reinforcement learning techniques to solve scientific problems involving optical simulations.\n\nResearch Question: How can we effectively use reinforcement learning methods within an optics simulation environment?\n\nMethodology: We developed an optics simulation environment that incorporates key features such as non-convexity, non-linearity, and time-dependent noise – characteristics commonly found in real-world optical systems but often overlooked by simpler simulation setups or theoretical models alone.\n\nMain Contributions:\n1. Creation of a novel optics simulation environment designed specifically with reinforcement learning applications in mind.\n2. Benchmarking multiple reinforcement learning algorithms using our simulation platform; demonstrating their efficacy against conventional control strategies when dealing with challenging aspects like those encountered in optical system manipulation tasks where precise adjustments are necessary amidst dynamic conditions characterized by complexity due to these three factors mentioned above - non-convexity, non-linearity & time-varying noises.",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Dynamic Regret Analysis of Safe Distributed Online Optimization for Convex and Non-convex Problems",
        "abstract": "This paper addresses safe distributed online optimization over an unknown set of linear safety constraints. A network of agents aims at jointly minimizing a global, time-varying function, which is only partially observable to each individual agent. Therefore, agents must engage in local communications to generate a safe sequence of actions competitive with the best minimizer sequence in hindsight, and the gap between the two sequences is quantified via dynamic regret. We propose distributed safe online gradient descent (D-Safe-OGD) with an exploration phase, where all agents estimate the constraint parameters collaboratively to build estimated feasible sets, ensuring the action selection safety during the optimization phase. We prove that for convex functions, D-Safe-OGD achieves a dynamic regret bound of $O(T^{2/3} \\sqrt{\\log T} + T^{1/3}C_T^*)$, where $C_T^*$ denotes the path-length of the best minimizer sequence. We further prove a dynamic regret bound of $O(T^{2/3}{\\color{black} \\sqrt{\\log T}} + T^{2/3}C_T^*)$ for certain non-convex problems, which establishes the first dynamic regret bound for a safe distributed algorithm in the non-convex setting.",
        "authors": "T. Chang, S. Chaudhary, D. Kalathil, et.al",
        "keywords": [
            "safe distributed optimization",
            "dynamic regret",
            "distributed safe online gradient descent"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=xiQXHvL1eN",
        "pdf_src": "https://api2.openreview.net/pdf/618dbf522d93a5f40f9d10c4e8767b327149cc5e.pdf",
        "Code_src": "",
        "Introduction": "Background: This research focuses on addressing the challenge of safely optimizing a globally varying objective function across multiple agents when they are subject to linear safety constraints whose details remain unknown.\n\nResearch Question: The central question addressed by this study involves developing algorithms allowing these agents to collaborate effectively while adhering to safety constraints without prior knowledge about them.\n \nMethodology: To tackle this problem, we introduce Distributed Safe Online Gradient Descent (D-Safe-OGD), incorporating both learning phases such as estimation through collaboration among agents followed by actual optimization steps within predefined bounds ensured by the learned estimates.\n\nMain Contributions:\n1. **Dynamic Regret Bound**: For convex objectives, our proposed method guarantees a dynamic regret bound of \\( O(T^{2/3}\\sqrt{\\log T} + T^{1/3}C_T^*) \\), where \\( C_T^* \\) represents the length of the optimal solution's trajectory.\n   \n2. **Non-Convex Settings**: Additionally, extending beyond convex settings into potentially more complex scenarios involving non-convex objectives leads to another significant contribution – establishing a new dynamic regret bound of \\( O(T^{2/3}\\sqrt{\\log T}) + T^{2/3}C_T^* \\). This marks one of the pioneering results regarding dynamic regret bounds under similar conditions but specifically designed around safety considerations rather than just convergence rates.\n\nIn summary, this work presents novel contributions towards solving distributed optimization challenges posed not merely from computational efficiency perspectives alone; it also introduces robustness against potential violations due to unforeseen or changing safety constraints throughout the process - thus providing valuable insights particularly relevant today given increasing complexity demands placed upon collaborative systems operating autonomously yet securely together despite uncertainty surrounding their environment(s).",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Dynamic Subgoal-based Exploration via Bayesian Optimization",
        "abstract": "Reinforcement learning in sparse-reward navigation environments with expensive and limited interactions is challenging and poses a need for effective exploration. Motivated by complex navigation tasks that require real-world training (when cheap simulators are not available), we consider an agent that faces an unknown distribution of environments and must decide on an exploration strategy. It may leverage a series of training environments to improve its policy before it is evaluated in a test environment drawn from the same environment distribution. Most existing approaches focus on fixed exploration strategies, while the few that view exploration as a meta-optimization problem tend to ignore the need for _cost-efficient_ exploration. We propose a cost-aware Bayesian optimization approach that efficiently searches over a class of dynamic subgoal-based exploration strategies. The algorithm adjusts a variety of levers --- the locations of the subgoals, the length of each episode, and the number of replications per trial --- in order to overcome the challenges of sparse rewards, expensive interactions, and noise. An experimental evaluation demonstrates that the new approach outperforms existing baselines across a number of problem domains. We also provide a theoretical foundation and prove that the method asymptotically identifies a near-optimal subgoal design.",
        "authors": "Y. Wang, M. Poloczek, D. Jiang",
        "keywords": [
            "sparse-reward navigation",
            "Bayesian optimization",
            "cost-efficient exploration"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ThJl4d5JRg",
        "pdf_src": "https://api2.openreview.net/pdf/a6e50e8eb1bb450c444853cd87687bfceb26a58e.pdf",
        "Code_src": "",
        "Introduction": "Background: Reinforcement learning algorithms often face difficulties when dealing with sparse reward navigation environments where interactions between agents and their surroundings can be costly or limited.\n\nResearch Problem: How do you develop efficient exploration strategies within these types of environments?\n\nMethod: This paper proposes a novel reinforcement learning framework based on Bayesian Optimization which takes into account costs associated with different actions during exploration phases.\nThe proposed model dynamically adapts various parameters such as location placement of sub-goals, duration lengths of episodes etc., allowing it to navigate through noisy landscapes effectively without breaking bank due to high interaction costs.\n\nMain Contributions:\n1) Introduced Cost-Aware Bayesian Optimization - A more practical alternative than traditional methods focusing solely on maximizing performance metrics like cumulative reward \n2) Developed Dynamic Subgoal-Based Exploration Strategies – Implemented adjustable levers such as goal positions & episode durations tailored towards overcoming sparse rewards/expensive interactions/noise issues encountered throughout navigation process  \n3) Provided Experimental Evidence Across Multiple Domains – Demonstrated improved performance compared against baseline models under similar conditions  \n4) Offered Theory Supporting Asymptotic Near-Optimization – Proved theoretically that this approach will converge toward optimal solutions given enough iterations",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Multiscale Causal Structure Learning",
        "abstract": "Causal structure learning methods are vital for unveiling causal relationships embedded into observed data. However, the state of the art suffers a major limitation: it assumes that causal interactions occur only at the frequency at which data is observed. To address this limitation, this paper proposes a method that allows structural learning of linear causal relationships occurring at different time scales. Specifically, we explicitly take into account instantaneous and lagged inter-relations between multiple time series, represented at different scales, hinging on wavelet transform. We cast the problem as the learning of a multiscale causal graph having sparse structure and dagness constraints, enforcing causality through directed and acyclic topology. To solve the resulting (non-convex) formulation, we propose an algorithm termed MS-CASTLE, which exhibits consistent performance across different noise distributions and wavelet choices. We also propose a single-scale version of our algorithm, SS-CASTLE, which outperforms existing methods in computational efficiency, performance, and robustness on synthetic data. Finally, we apply the proposed approach to learn the multiscale causal structure of the risk of 15 global equity markets, during covid-19 pandemic, illustrating the importance of multiscale analysis to reveal useful interactions at different time resolutions. Financial investors can leverage our approach to manage risk within equity portfolios from a causal perspective, tailored to their investment horizon.",
        "authors": "G. D'acunto, P. D. Lorenzo, S. Barbarossa",
        "keywords": [
            "causal structure learning",
            "multiscale analysis",
            "wavelet transform"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Ub6XILEF9x",
        "pdf_src": "https://api2.openreview.net/pdf/eb086b66c9f07dcef84e03ad7be7e988180a6d5e.pdf",
        "Code_src": "",
        "Introduction": "Background: Causal structure learning aims to uncover underlying causal relationships among variables based on observational data.\nResearch Problem: Existing methods assume all causal interactions happen with frequencies aligned with those seen in observations; however, they overlook potential causal relationships happening over various timescales due to this assumption.\n\nMethod: This study introduces a novel framework called Multiscale Causal Structure Learning (MS-CASTLE), leveraging wavelet transforms allowing us to consider both immediate and delayed relations across diverse temporal scales without assuming a fixed frequency pattern or directionality bias inherent in DAGs alone by incorporating Directed Acyclic Graph (DAG) constraints while still being flexible enough not to restrict the scale of these interactions beyond what's observable directly via wavelet decomposition techniques applied to multivariate time series datasets.\n\nMain Contributions:\n1. Propose a new algorithmic solution - MS-CASTLE – capable of handling non-convex optimization problems arising when trying to simultaneously optimize for sparsity & DAG constraints under varying levels of noise present in real-world datasets;\n2. Develop a simplified variant named Single Scale CASTLE (SS-CASTLE) optimized specifically around computational complexity reduction compared traditional approaches whilst maintaining comparable accuracy metrics against synthetic benchmarks demonstrating its effectiveness even amidst high variability conditions encountered commonly within financial market dynamics post COVID-19 crisis period where such multi-resolutional insights could be crucially informative towards managing portfolio risks more effectively informed by causal reasoning rather than just correlation coefficients alone used conventionally before considering any underlying mechanisms driving them apart from mere statistical associations.",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Revisiting Image Classifier Training for Improved Certified Robust Defense against Adversarial Patches",
        "abstract": "Certifiably robust defenses against adversarial patches for image classifiers ensure correct prediction against any changes to a constrained neighborhood of pixels. PatchCleanser, the state-of-the-art certified defense, uses a double-masking strategy for robust classification. The success of this strategy relies heavily on the model's invariance to image pixel masking. In this paper, we take a closer look at model training schemes to improve this invariance. Instead of using Random Cutout augmentations like PatchCleanser, we introduce the notion of worst-case masking, i.e., selecting masked images which maximize classification loss. However, finding worst-case masks requires an exhaustive search, which might be prohibitively expensive to do on-the-fly during training. To solve this problem, we propose a two-round greedy masking strategy (Greedy Cutout) which finds an approximate worst-case mask location with much less compute. We show that the models trained with our Greedy Cutout improves certified robust accuracy over Random Cutout in PatchCleanser across a range of datasets and architectures. Certified robust accuracy on ImageNet with a ViT-B16-224 model increases from 58.1% to 62.3% against a 3% square patch applied anywhere on the image.",
        "authors": "A. Saha, S. Yu, M. S. Norouzzadeh, et.al",
        "keywords": [
            "worst-case masking",
            "Greedy Cutout",
            "certified robust accuracy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=2tdhQMLg36",
        "pdf_src": "https://api2.openreview.net/pdf/d7b6066a50633af010e6e79cf9fd0a71489da52d.pdf",
        "Code_src": "",
        "Introduction": "Background: Certifiable robustness is crucial as it ensures that neural networks can correctly classify inputs even when they are perturbed by adversaries within certain constraints.\n\nResearch Problem: How does one train deep learning models more invariantly towards pixel masking?\n\nMethods: This research introduces \"worst-case masking,\" where instead of random cutouts used commonly such as those employed by PatchCleanser, the most challenging cases maximizing classification error would ideally need to be found through exhaustive search but due to computational cost prohibitive; hence, proposes a computationally efficient alternative - the Greedy Cutout method involving only two rounds.\n \nMain Contributions:\n1. Introduces 'worst-case masking' concept – selects the hardest examples leading to maximum classification errors rather than randomly selected ones common in existing methods;\n2. Develops a novel Greedy Cutout algorithm designed specifically around minimizing computation costs while still approximating these hard-to-classify instances effectively;\n3. Demonstrates significant improvements compared to previous work including increased certified robust accuracy beyond what was achieved previously under similar conditions",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Learning-to-defer for sequential medical decision-making under uncertainty",
        "abstract": "Learning-to-defer is a framework to automatically defer decision-making to a human expert when ML-based decisions are deemed unreliable. Existing learning-to-defer frameworks are not designed for sequential settings. That is, they defer at every instance independently, based on immediate predictions, while ignoring the potential long-term impact of these interventions. As a result, existing frameworks are myopic. Further, they do not defer adaptively, which is crucial when human interventions are costly. In this work, we propose Sequential Learning-to-Defer (SLTD), a framework for learning-to-defer to a domain expert in sequential decision-making settings. Contrary to existing literature, we pose the problem of learning-to-defer as model-based reinforcement learning (RL) to i) account for long-term consequences of ML-based actions using RL and ii) adaptively defer based on the dynamics (model-based). Our proposed framework determines whether to defer (at each time step) by quantifying whether a deferral now will improve the value compared to delaying deferral to the next time step. To quantify the improvement, we account for potential future deferrals. As a result, we learn a pre-emptive deferral policy (i.e. a policy that defers early if using the ML-based policy could worsen long-term outcomes). Our deferral policy is adaptive to the non-stationarity in the dynamics. We demonstrate that adaptive deferral via SLTD provides an improved trade-off between long-term outcomes and deferral frequency on synthetic, semi-synthetic, and real-world data with non-stationary dynamics. Finally, we interpret the deferral decision by decomposing the propagated (long-term) uncertainty around the outcome, to justify the deferral decision.",
        "authors": "S. Joshi, S. Parbhoo, F. Doshi-velez",
        "keywords": [
            "Sequential Decision-Making",
            "Model-Based Reinforcement Learning",
            "Adaptive Deferral"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=0pn3KnbH5F",
        "pdf_src": "https://api2.openreview.net/pdf/2c1f8e0a3f77cf3e0217379aea27d5791774b079.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses issues related to machine learning systems making reliable decisions across multiple instances or sequences over time.\n\nResearch Problem: The primary research question revolves around improving upon current \"learning-to-defer\" frameworks within the context of sequential decision-making processes where such systems must decide autonomously about when it's appropriate to delegate tasks back to humans due to uncertain or potentially harmful automated choices without considering longer-term implications.\n \nMethods: The authors introduce a novel approach called Sequential Learning-to-Deferral (SLTD), which incorporates principles from model-based reinforcement learning:\n1. It uses RL techniques to consider both short-term and long-term effects resulting from decisions made through machine learning models (\"ML-based actions\").\n2. SLTD adapts its deferral strategy dynamically according to changes observed during the sequence rather than taking independent decisions per instance solely relying on immediate predictions like previous methods did.\n3. This dynamic adaptation helps balance costs associated with human intervention since some manual corrections may be expensive but necessary under certain conditions.\n4. Additionally, the method accounts for possible future deferrals into calculations regarding optimal timing; thus, it learns preemptive policies—those that anticipate potential negative impacts before they occur.\n\nMain Contributions: The main contributions include:\n\n1. A new framework named SLTD tailored specifically for handling deferred decision-making sequentially—a significant departure from prior works focusing only on single instances.\n2. An innovative use of model-based reinforcement learning allowing consideration beyond just immediate feedback towards comprehensive evaluation including delayed rewards/penalties.\n3. Development of an adaptable deferral policy capable of dealing effectively even amidst changing environments or scenarios (non-stationarity).\n4. Demonstrated improvements against standard benchmarks showing better trade-offs concerning how often decisions should involve human oversight versus autonomous action throughout extended periods involving complex interactions among various factors affecting system performance.\n5. Interpretability enhancements provided through analysis revealing underlying uncertainties contributing significantly toward informed deferral strategies ensuring robustness",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Does ‘Deep Learning on a Data Diet’ reproduce? Overall yes, but GraNd at Initialization does not",
        "abstract": "Training deep neural networks on vast datasets often results in substantial computational demands, underscoring the need for efficient data pruning. In this context, we critically re-evaluate the data pruning metrics introduced in `Deep Learning on a Data Diet' by Paul et al. (2021): the Gradient Norm (GraNd) (at initialization) and the Error L2 Norm (EL2N). Our analysis uncovers a strong correlation between the GraNd scores at initialization and a sample's input norm, suggesting the latter as a potential baseline for data pruning. However, comprehensive tests on CIFAR-10 show neither metric outperforming random pruning, contradicting one of the findings in Paul et al. (2021). We pinpoint the inconsistency in the GraNd at initialization results to a later-fixed bug in FLAX's checkpoint restoring mechanism (https://github.com/google/flax/commit/28fbd95500f4bf2f9924d2560062fa50e919b1a5). Altogether, our findings do not support using the input norm or GraNd scores at initialization for effective data pruning. Nevertheless, EL2N and GraNd scores at later training epochs do provide useful pruning signals, aligning with the expected performance.",
        "authors": "A. Kirsch",
        "keywords": [
            "input norm",
            "gradient norm",
            "error l2 norm"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=1dwXa9vmOI",
        "pdf_src": "https://api2.openreview.net/pdf/d75b8714e1f209fcce1ff75c083f49502b5b0993.pdf",
        "Code_src": "",
        "Introduction": "Background: Training deep neural networks requires significant computational resources due to large dataset sizes.\n\nResearch Problem: To address high computational costs associated with training deep neural networks from massive datasets, it is necessary to develop more efficient methods that reduce unnecessary computations while maintaining acceptable accuracy levels.\n \nMethod: The paper evaluates two existing data pruning metrics proposed in \"Deep Learning on a Data Diet\" by Paul et al. (2021), which are the Gradient Norm (GraNd) score calculated during initialization phase and the Error L2 Norm (EL2N).\n\nMain Contributions: The study finds no evidence supporting the use of GraNd scores obtained early in the training process nor does it find any advantage over random pruning when applied across different samples within the CIFAR-10 dataset; however, they observe some utility after several iterations where both metrics seem promising indicators showing improvement towards better performances than random selection could achieve without further adjustments being made manually post-training time optimization techniques such as hyperparameter tuning etc.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Gated Domain Units for Multi-source Domain Generalization",
        "abstract": "The phenomenon of distribution shift (DS) occurs when a dataset at test time differs from the dataset at training time, which can significantly impair the performance of a machine learning model in practical settings due to a lack of knowledge about the data's distribution at test time. To address this problem, we postulate that real-world distributions are composed of latent Invariant Elementary Distributions (I.E.D) across different domains. This assumption implies an invariant structure in the solution space that enables knowledge transfer to unseen domains. To exploit this property for domain generalization, we introduce a modular neural network layer consisting of Gated Domain Units (GDUs) that learn a representation for each latent elementary distribution. During inference, a weighted ensemble of learning machines can be created by comparing new observations with the representations of each elementary distribution. Our flexible framework also accommodates scenarios where explicit domain information is not present. Extensive experiments on image, text, and graph data show consistent performance improvement on out-of-training target domains. These findings support the practicality of the I.E.D assumption and the effectiveness of GDUs for domain generalisation.",
        "authors": "S. Föll, A. Dubatovka, E. Ernst, et.al",
        "keywords": [
            "distribution shift",
            "Invariant Elementary Distributions",
            "domain generalization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=V7BvYJyTmM",
        "pdf_src": "https://api2.openreview.net/pdf/778e273f05d5310981726cf68c9c236a6d61088a.pdf",
        "Code_src": "",
        "Introduction": "Background: Distribution Shift (DS), occurring between training and testing datasets, often leads to poor performance because models do not have prior knowledge regarding the test-time distribution.\n\nResearch Problem: How does one effectively deal with DS so as to improve cross-domain generalization?\n\nMethodology: The authors propose that real-world distributions consist of Latent Invariant Elementary Distributions (I.E.Ds). They assume there exists an invariant structure within the solution space allowing for knowledge transfer among various domains.\nTo capitalize on this idea they develop a Modular Neural Network Layer incorporating Gated Domain Units (GDUs). Each GDU learns how to represent individual latent elementary distributions during the training phase; during inference these units compare novel inputs against their learned representations through a weighted ensemble approach ensuring adaptability even without clear domain labels.\n\nMain Contributions:\n1. Postulation & Justification of I.E.D Hypothesis - A theoretical foundation supporting the existence of invariant structures enabling knowledge transfer beyond specific domains.\n2. Introduction of GDUs - Novel architecture elements designed specifically around the concept of invariant elementary distributions aimed towards enhancing domain generalization capabilities via modularity and gated mechanisms \n3. Demonstrated Improvement Across Multiple Modalities - Experimental validation using diverse types of datasets such as images, texts or graphs showing significant improvements over traditional methods",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Revisiting Sparsity Hunting in Federated Learning: Why does Sparsity Consensus Matter?",
        "abstract": "Edge devices can benefit remarkably from federated learning due to their distributed nature; however, their limited resource and computing power poses limitations in deployment. A possible solution to this problem is to utilize off-the-shelf sparse learning algorithms at the clients to meet their resource budget. However, such naive deployment in the clients causes significant accuracy degradation, especially for highly resource-constrained clients. In particular, our investigations reveal that the lack of consensus in the sparsity masks among the clients may potentially slow down the convergence of the global model and cause a substantial accuracy drop.\nWith these observations, we present \\textit{federated lottery aware sparsity hunting} (FLASH), a unified sparse learning framework for training a sparse sub-model that maintains the performance under ultra-low parameter density while yielding proportional communication benefits. Moreover, given that different clients may have different resource budgets, we present \\textit{hetero-FLASH} where clients can take different density budgets based on their device resource limitations instead of supporting only one target parameter density. Experimental analysis on diverse models and datasets shows the superiority of FLASH in closing the gap with an unpruned baseline while yielding up to $\\mathord{\\sim}10.1\\%$ improved accuracy with $\\mathord{\\sim}10.26\\times$ fewer communication, compared to existing alternatives, at similar hyperparameter settings.",
        "authors": "S. Babakniya, S. Kundu, S. Prakash, et.al",
        "keywords": [
            "federated learning",
            "edge devices",
            "sparse learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=iHyhdpsnyi",
        "pdf_src": "https://api2.openreview.net/pdf/9ce5fc4b05bc93229611928f70eaa5f417a6f5d7.pdf",
        "Code_src": "",
        "Introduction": "Background: Federated Learning has shown great potential as it allows machine learning models to be trained across multiple edge devices without collecting data centrally which helps preserve privacy but also introduces challenges related to computational resources.\n\nResearch Problem: The main challenge addressed by the paper concerns how to efficiently deploy sparse learning algorithms within client devices during federated learning when those devices are constrained both in terms of memory and computation capacity—resources critical not just because they're scarce themselves but crucially affect overall network-wide efficiency through increased communication costs associated with larger models or parameters.\n\nMethodology: To tackle this issue, the authors introduce \"Federated Lottery Aware Sparsity Hunting\" (FLASH). This framework aims specifically towards achieving high accuracy despite low parameter densities—a necessity considering the constraints faced by many edge devices today. Additionally, they propose a heterogeneous version called hetero-FLASH allowing each client to tailor its sparsity level according to local hardware capabilities rather than adhering rigidly to any single predefined sparsity threshold common to all nodes.\n\nMain Contributions:\n1. They develop FLASH—an algorithmic approach designed explicitly around federated learning scenarios aiming toward efficient use of resources like memory space needed per node/client.\n2. Heterogeneous FLASH addresses the diversity amongst devices' resource availability further optimizing the trade-off between model size reduction and communication overheads ensuring more equitable participation regardless of individual device's capability.\n3. Extensive experimental validation demonstrates superior performance over baselines particularly noticeable improvements regarding accuracy gains and reduced communication requirements making FLASH a promising candidate for practical applications involving federated learning environments characterized by varying levels of resource scarcity",
        "Topic": "Federated Learning"
    },
    {
        "title": "Learning domain-specific causal discovery from time series",
        "abstract": "Causal discovery (CD) from time-varying data is important in neuroscience, medicine, and machine learning. Techniques for CD encompass randomized experiments, which are generally unbiased but expensive, and algorithms such as Granger causality, conditional-independence-based, structural-equation-based, and score-based methods that are only accurate under strong assumptions made by human designers. However, as demonstrated in other areas of machine learning, human expertise is often not entirely accurate and tends to be outperformed in domains with abundant data. In this study, we examine whether we can enhance domain-specific causal discovery for time series using a data-driven approach. Our findings indicate that this procedure significantly outperforms human-designed, domain-agnostic causal discovery methods, such as Mutual Information, VAR-LiNGAM, and Granger Causality on the MOS 6502 microprocessor, the NetSim fMRI dataset, and the Dream3 gene dataset. We argue that, when feasible, the causality field should consider a supervised approach in which domain-specific CD procedures are learned from extensive datasets with known causal relationships, rather than being designed by human specialists. Our findings promise a new approach toward improving CD in neural and medical data and for the broader machine learning community.",
        "authors": "X. Wang, K. Kording",
        "keywords": [
            "data-driven",
            "causal discovery",
            "supervised learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=JFaZ94tT8M",
        "pdf_src": "https://api2.openreview.net/pdf/3986b8f1d22e67e5b813a8a2a1ee0c8b92da1b42.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses the importance of causal discovery (CD) techniques used primarily in fields like neuroscience where understanding cause-and-effect relations between variables over time is crucial.\n\nResearch Question: The central question addressed here concerns how well traditional human-designed causal discovery approaches compare against more general-purpose or 'data-driven' ones across different datasets within specific domains.\n \nMethods: To answer their research questions, authors explore several computational models including Mutual Information, Vector Autoregressive - Latent Influence Graphical Models (VAR-LiNGAM), and Granger causality – all of them widely-used statistical tools based on various underlying principles about causality yet requiring certain assumptions regarding the nature of the observed processes they analyze. They also propose an alternative methodological framework grounded solely in empirical evidence derived directly from large-scale datasets without any prior assumptions concerning potential causes/effects.\n\nMain Contributions: Their main contribution lies in demonstrating through empirical tests conducted upon three distinct datasets related to electronics engineering (MOS 6502 microprocessor), neuroimaging (NetSim fMRI dataset), and genomics (Dream3 gene dataset), that these data-driven CD methodologies consistently perform better compared to those traditionally developed manually via expert knowledge alone; suggesting there may exist significant advantages gained simply leveraging available quantities of high-quality observational data instead relying heavily on preconceived notions crafted by humans experts who might lack comprehensive insights into complex systems beyond narrow disciplinary boundaries. This finding could potentially lead towards rethinking current paradigms guiding development practices currently employed throughout many scientific disciplines involving temporal sequence analysis tasks similar to what's discussed above.",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "Deep Operator Learning Lessens the Curse of Dimensionality for PDEs",
        "abstract": "Deep neural networks (DNNs) have achieved remarkable success in numerous domains, and their application to PDE-related problems has been rapidly advancing. This paper provides an estimate for the generalization error of learning Lipschitz operators over Banach spaces using DNNs with applications to various PDE solution operators. The goal is to specify DNN width, depth, and the number of training samples needed to guarantee a certain testing error.  Under mild assumptions on data distributions or operator structures, our analysis shows that deep operator learning can have a relaxed dependence on the discretization resolution of PDEs and, hence, lessen the curse of dimensionality in many PDE-related problems including elliptic equations, parabolic equations, and Burgers equations. Our results are also applied to give insights about discretization-invariant in operator learning.",
        "authors": "K. Chen, C. Wang, H. Yang",
        "keywords": [
            "deep neural networks",
            "generalization error",
            "operator learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=zmBFzuT2DN",
        "pdf_src": "https://api2.openreview.net/pdf/079d7a520cfb339df5b2e28dd953af4e79b39ac0.pdf",
        "Code_src": "",
        "Introduction": "Background: Deep neural networks (DNNs) have demonstrated significant achievements across diverse fields; however, applying them to Partial Differential Equations (PDE)-related issues remains challenging due to complexities such as high-dimensional input-output mappings.\n\nResearch Problem: How do we accurately approximate Lipschitz operators acting on Banach spaces via DNNs? Specifically, what parameters like network width, depth, and sample size should be chosen during training?\n\nMethods: We provide estimates regarding the generalization errors associated with this task under specific conditions related to either the distribution of data points within the Banach space or structural properties of the operators themselves.\n \nMain Contributions:\n1. We establish theoretical guarantees concerning how these factors affect test performance without requiring excessively fine discretizations typical when dealing with complex PDE solutions – thus mitigating \"curse of dimensionality\" concerns commonly encountered in solving such systems numerically.\n2. Additionally, by analyzing discretization-invariance aspects through operator learning processes involving DNNs - which suggests potential improvements towards more robust numerical approximations beyond traditional methods reliant solely upon grid refinement strategies alone.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "On Perfect Clustering for Gaussian Processes",
        "abstract": "In this paper, we propose a data based transformation for infinite-dimensional Gaussian processes and derive its limit theorem. For a clustering problem using mixture models, an appropriate modification of this transformation asymptotically leads to perfect separation of the populations under rather general conditions, except the scenario in which differences between clusters depend only on the locations; in which case our procedure is useless. Theoretical properties related to label consistency are studied for the k-means clustering algorithm when used on this transformed data. Good empirical performance of the proposed methodology is demonstrated using simulated as well as benchmark data sets, when compared with some popular parametric and nonparametric methods for such functional data.",
        "authors": "J. Cuesta-albertos, S. Dutta",
        "keywords": [
            "data-based transformation",
            "infinite-dimensional Gaussian processes",
            "clustering problems"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=igDOV2KBwM",
        "pdf_src": "https://api2.openreview.net/pdf/113d24a5f8a43248cd779a11ddc95786d70eeb1a.pdf",
        "Code_src": "",
        "Introduction": "Background: This research focuses on the field of Bayesian statistics specifically dealing with infinite-dimensional Gaussian processes.\n\nResearch Problem: How can one effectively transform data from infinite-dimensional Gaussian processes? Additionally, how does this transformation affect clustering problems?\n\nMethods: The authors introduce a novel data-based transformation method that they apply to infinite-dimensional Gaussian processes without imposing any prior assumptions about their structure or dimensionality constraints typically found in existing literature approaches.\nThey also develop a limit theorem associated with this transformation process allowing them to understand better what happens when applied over time scales leading up to infinity.\n\nMain Contributions:\n1. They successfully demonstrate through rigorous mathematical analysis why certain modifications lead towards perfect population separation during clustering tasks while others do not work optimally depending upon specific scenarios involving cluster difference dependencies solely being location-based versus more complex relationships among variables within each cluster;\n2. They provide theoretical insights into issues concerning label consistency regarding k-means algorithms utilized alongside these transformations via simulations conducted against various benchmarks datasets demonstrating superior performance relative other traditional statistical techniques commonly employed nowadays",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "IBIA: An Incremental Build-Infer-Approximate Framework for Approximate Inference of Partition Function",
        "abstract": "Exact computation of the partition function is known to be intractable, necessitating approximate inference techniques. Existing methods for approximate inference are slow to converge for many benchmarks. The control of accuracy-complexity trade-off is also non-trivial in many of these methods. We propose a novel  incremental build-infer-approximate (IBIA) framework for approximate inference that addresses these issues. In this framework, the probabilistic graphical model is converted into a  sequence of clique tree forests (SCTF) with bounded clique sizes.  We show that the SCTF can be used to efficiently compute the partition function. We propose two new algorithms which are used to construct the SCTF  and prove the correctness of both. The first is an algorithm for incremental construction of CTFs that is guaranteed to give a  valid CTF with bounded clique sizes and the second is an approximation algorithm that takes a calibrated CTF as input and yields a valid and calibrated CTF with reduced clique sizes as the output. We have evaluated our method using several benchmark sets from recent UAI competitions and our results show good accuracies with competitive runtimes.",
        "authors": "S. Bathla, V. Vasudevan",
        "keywords": [
            "clique tree forest",
            "incremental build-infer-approximate framework",
            "approximate inference"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=8L7Rh6FIXt",
        "pdf_src": "https://api2.openreview.net/pdf/224782233e408daeac3cfa125ebd6388a2195972.pdf",
        "Code_src": "",
        "Introduction": "Background: Computing the partition function involves summing over all possible states within a system's configuration space; however, it becomes computationally prohibitive when dealing with complex systems due to its exponential growth.\n\nResearch Problem: Traditional approaches require significant computational resources or may not provide accurate approximations quickly enough especially on large datasets where convergence rates remain slow despite advancements in machine learning models like variational autoencoders(VAE), neural networks, etc.\n\nMethodology: This paper introduces Incremental Build-Infer-Approximate (IBIA) framework - a novel approach designed specifically addressing limitations associated with existing approximate inference techniques by converting Probabilistic Graphical Models (PGMs) into Sequence of Clique Tree Forests (SCTFs). \n\nMain Contributions:\n1. Conversion of PGMs into SCTFs allows efficient computation through decomposition into smaller cliques.\n2. Two novel algorithms proposed – one constructs valid CTFs incrementally while maintaining bounded clique sizes ensuring tractability even under varying conditions across different benchmarks tested during evaluation phase at recent UAI competitions;\n3. Second algorithm refines calibration properties resulting in more precise approximations without compromising runtime performance significantly compared against other state-of-the-art methods such as Variational Autoencoder (VAE).\n4. Evaluation conducted demonstrates high accuracy levels achieved alongside competitive runtimes making IBIA suitable alternative choice among various available options depending upon specific requirements related complexity vs accuracy trade-offs present scenarios encountered practical applications involving Bayesian Networks",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Label Noise-Robust Learning using a Confidence-Based Sieving Strategy",
        "abstract": "In learning tasks with label noise, improving model robustness against overfitting is a pivotal challenge because the model eventually memorizes labels, including the noisy ones. Identifying the samples with noisy labels and preventing the model from learning them is a promising approach to address this challenge. When training with noisy labels, the per-class confidence scores of the model, represented by the class probabilities, can be reliable criteria for assessing whether the input label is the true label or the corrupted one. In this work, we exploit this observation and propose a novel discriminator metric called confidence error and a sieving strategy called CONFES to differentiate between the clean and noisy samples effectively. We provide theoretical guarantees on the probability of error for our proposed metric. Then, we experimentally illustrate the superior performance of our proposed approach compared to recent studies on various settings, such as synthetic and real-world label noise. Moreover, we show CONFES can be combined with other state-of-the-art approaches, such as Co-teaching and DivideMix to further improve model performance.",
        "authors": "R. Torkzadehmahani, R. Nasirigerdeh, D. Rueckert, et.al",
        "keywords": [
            "noise labeling",
            "model robustness",
            "discriminative metrics"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=3taIQG4C7H",
        "pdf_src": "https://api2.openreview.net/pdf/7fe7d7bd1c6bd83e6453e242cdad9151aa51b666.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses an issue in machine learning where models trained using labeled data may learn incorrect information due to \"label noise\" - instances that are incorrectly classified but given correct labels during training.\n\nResearch Problem: How do you identify which examples have noisy labels so your model doesn't rely too heavily on these potentially misleading pieces of data?\n\nMethod: The authors introduce two key components:\n\n1. A new discriminator metric named Confidence Error (ConfErr), based on how confident each class's prediction was.\n2. A sieve strategy called CONFES (\"CONFidence Estimation-based Sieve\") designed specifically around ConfErr values; it filters out samples whose predicted confidences suggest they might carry noisy labels without requiring any additional annotations beyond those used at test time.\n\nMain Contributions:\n- They theoretically prove certain bounds regarding the expected accuracy when using their ConfErr metric within the context of classification problems affected by label noise.\n- Empirically demonstrate through experiments across different datasets showing that their method significantly improves upon existing techniques like Co-teaching and DivideMix while being more efficient than some alternatives concerning computational cost since it does not require retraining entire networks nor extensive annotation efforts post-training phase completion.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Relating graph auto-encoders to linear models",
        "abstract": "Graph auto-encoders are widely used to construct graph representations in Euclidean vector spaces. However, it has already been pointed out empirically that linear models on many tasks can outperform graph auto-encoders. \nIn our work, we prove that the solution space induced by graph auto-encoders is a subset of the solution space of a linear map. This demonstrates that linear embedding models have at least the representational power of graph auto-encoders based on graph convolutional networks. So why are we still using nonlinear graph auto-encoders? One reason could be that actively restricting the linear solution space might introduce an inductive bias that helps improve learning and generalization. While many researchers believe that the nonlinearity of the encoder is the critical ingredient towards this end, we instead identify the node features of the graph as a more powerful inductive bias. We give theoretical insights by introducing a corresponding bias in a linear model and analyzing the change in the solution space. Our experiments are aligned with other empirical work on this question and show that the linear encoder can outperform the nonlinear encoder when using feature information.",
        "authors": "S. Klepper, U. V. Luxburg",
        "keywords": [
            "graph auto-encoders",
            "linear maps",
            "node features"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Y1eYplvxrE",
        "pdf_src": "https://api2.openreview.net/pdf/f2881e496b969bf6a025aaf4ed295ce9b6dd3008.pdf",
        "Code_src": "",
        "Introduction": "Background: Graph auto-encoders are commonly utilized for constructing graph representations within Euclidean vector spaces; however, empirical evidence suggests they may not always surpass linear models.\n\nResearch Question: The paper investigates whether there exists any inherent superiority or necessity behind employing nonlinear graph auto-encoders over their linear counterparts despite empirical findings indicating comparable performance between them.\n  \nMethodology: Proving theoretically through mathematics how the solution space generated by graph auto-encoders is indeed a subset of what's achievable via linear maps, thereby demonstrating equivalence in representational capacity among both types - graph auto-encoders utilizing graph convolutional networks versus linear embedding models.\n\nMain Contributions:\n1. Establishing that graph auto-encoders' solution space falls under those possible from linear transformations.\n2. Highlighting potential benefits derived from constraining solutions spatially, which introduces an inductive bias aiding improved learning outcomes during training phases across various datasets.\n3. Identifying node features themselves rather than the encoding process’s non-linearity being crucial contributors toward such biases leading to better performances – especially noticeable if leveraging informative feature data points effectively enough compared against purely non-linear encodings alone without additional feature inputs into consideration while comparing results obtained accordingly after experimentation conducted according to established benchmarks available today related specifically around these topics discussed above mentioned here briefly summarized before hand earlier now presented further elaborated upon below provided contextually relevant details pertaining directly back onto initial inquiry posed originally throughout aforementioned paragraphs",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "How Reliable is Your Regression Model's Uncertainty Under Real-World Distribution Shifts?",
        "abstract": "Many important computer vision applications are naturally formulated as regression problems. Within medical imaging, accurate regression models have the potential to automate various tasks, helping to lower costs and improve patient outcomes. Such safety-critical deployment does however require reliable estimation of model uncertainty, also under the wide variety of distribution shifts that might be encountered in practice. Motivated by this, we set out to investigate the reliability of regression uncertainty estimation methods under various real-world distribution shifts. To that end, we propose an extensive benchmark of 8 image-based regression datasets with different types of challenging distribution shifts. We then employ our benchmark to evaluate many of the most common uncertainty estimation methods, as well as two state-of-the-art uncertainty scores from the task of out-of-distribution detection. We find that while methods are well calibrated when there is no distribution shift, they all become highly overconfident on many of the benchmark datasets. This uncovers important limitations of current uncertainty estimation methods, and the proposed benchmark therefore serves as a challenge to the research community. We hope that our benchmark will spur more work on how to develop truly reliable regression uncertainty estimation methods.",
        "authors": "F. K. Gustafsson, M. Danelljan, T. B. Schön",
        "keywords": [
            "distribution shifts",
            "regression uncertainty estimation",
            "benchmark"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=WJt2Pc3qtI",
        "pdf_src": "https://api2.openreview.net/pdf/4aae6e82ad69b116f81f19bb52ab7a6f060c2a9d.pdf",
        "Code_src": "",
        "Introduction": "Background: Regression problems play a crucial role in numerous significant computer vision applications within fields such as medical imaging where precise predictions can lead to cost savings for healthcare providers along with improved patient care.\n\nResearch Question: The study aims at addressing whether existing regression uncertainty estimation techniques effectively cope with practical scenarios involving diverse distribution shifts which may occur during application deployment.\n \nMethodology: The researchers introduce an expansive dataset comprising eight distinct image-based regression challenges each exhibiting unique forms of distribution shifts including domain shift, covariate shift or concept drift among others. They utilize these benchmarks to rigorously test several prevalent uncertainty estimation approaches alongside evaluating recent advancements designed specifically for detecting out-of-distribution examples.\n\nMain Contributions:\n1. A comprehensive suite of eight challenging regression datasets has been developed considering realistic distribution shifts pertinent to actual clinical settings;\n2. An empirical analysis across multiple uncertainty estimation strategies reveals their performance degradation significantly upon encountering distribution shifts; \n3. The findings highlight critical gaps between theoretical assumptions regarding uncertainty calibration versus observed behavior outside controlled environments;\n4. The provided benchmark sets forth new directions towards developing robust uncertainty estimators capable of reliably predicting variability beyond standard training conditions.",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "Efficient Gradient Flows in Sliced-Wasserstein Space",
        "abstract": "Minimizing functionals in the space of probability distributions can be done with Wasser-\nstein gradient flows. To solve them numerically, a possible approach is to rely on the\nJordan–Kinderlehrer–Otto (JKO) scheme which is analogous to the proximal scheme in\nEuclidean spaces. However, it requires solving a nested optimization problem at each it-\neration, and is known for its computational challenges, especially in high dimension. To\nalleviate it, very recent works propose to approximate the JKO scheme leveraging Brenier’s\ntheorem, and using gradients of Input Convex Neural Networks to parameterize the density\n(JKO-ICNN). However, this method comes with a high computational cost and stability is-\nsues. Instead, this work proposes to use gradient flows in the space of probability measures\nendowed with the sliced-Wasserstein (SW) distance. We argue that this method is more flex-\nible than JKO-ICNN, since SW enjoys a closed-form differentiable approximation. Thus,\nthe density at each step can be parameterized by any generative model which alleviates the\ncomputational burden and makes it tractable in higher dimensions.",
        "authors": "C. Bonet, N. Courty, F. Septier, et.al",
        "keywords": [
            "Wasserstein Gradient Flows",
            "Jordan-Kinderlehrer-Otto Scheme",
            "Sliced-Wasserstein Distance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Au1LNKmRvh",
        "pdf_src": "https://api2.openreview.net/pdf/4106d0144f90e7ac7ec5a461a100dbe4761e2d7a.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses how to minimize certain types of functionals over the space of probability distributions through Wasserstein gradient flows.\n\nResearch Problem: How do we efficiently compute these minimizations?\n\nMethod: The authors suggest employing the Jordan-Kinderlehrer-Otto (JKO) scheme as an alternative to the standard proximal scheme due to its similarity but also because it avoids the need for computing full Jacobians or Hessians typically required when dealing with non-smooth problems involving probability measures directly.\nHowever, they note several drawbacks including computational complexity specifically in high-dimensional settings where nested optimizations are involved within each iteration cycle leading to instability issues along with high computational costs associated with approximating densities via Input Convex Neural Networks (ICNN).\n\nMain Contributions: This study introduces an innovative solution based on gradient flows equipped with sliced-Wasserstein distances rather than relying solely on ICNN approaches like those proposed recently such as JKO-ICNN which have been found wanting both computationally speaking \nand regarding their robustness under various conditions encountered during iterative processes aiming towards convergence toward optimal solutions within given constraints imposed upon our functional minimization task at hand.",
        "Topic": "Optimal Transport"
    },
    {
        "title": "Approximate Policy Iteration with Bisimulation Metrics",
        "abstract": "Bisimulation metrics define a distance measure between states of a Markov decision process (MDP) based on a comparison of reward sequences. Due to this property they provide theoretical guarantees in value function approximation (VFA). In this work we first prove that bisimulation and $\\pi$-bisimulation metrics can be defined via a more general class of Sinkhorn distances, which unifies various state similarity metrics used in recent work. Then we describe an approximate policy iteration (API) procedure that uses a bisimulation-based discretization of the state space for VFA and prove asymptotic performance bounds. Next, we bound the difference between $\\pi$-bisimulation metrics in terms of the change in the policies themselves. Based on these results, we design an API($\\alpha$) procedure that employs conservative policy updates and enjoys better performance bounds than the naive API approach. We discuss how such API procedures map onto practical actor-critic methods that use bisimulation metrics for state representation learning. Lastly, we validate our theoretical results and investigate their practical implications via a controlled empirical analysis based on an implementation of bisimulation-based API for finite MDPs.",
        "authors": "M. Kemertas, A. D. Jepson",
        "keywords": [
            "Sinkhorn distances",
            "Bisimulation metrics",
            "Policy Iteration"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Ii7UeHc0mO",
        "pdf_src": "https://api2.openreview.net/pdf/820ee60eed2634022394130c21b97653bdba2421.pdf",
        "Code_src": "",
        "Introduction": "Background: Bisimulation metrics are commonly employed as a means by comparing reward sequences among different states within a Markov Decision Process (MDP), thereby establishing a distance measurement framework.\nResearch Problem: The primary challenge addressed is the extension beyond existing bisimulation and π-bisimulation metrics definitions through identifying a broader family of Sinkhorn distances applicable across diverse state similarity measures utilized recently.\n\nMethodology: This paper introduces novel proofs demonstrating that both bisimulation and π-bisimulation metrics may indeed stem from a generalized category of Sinkhorn distances; it also describes an Approximate Policy Iteration (API) algorithm leveraging a bisimulation-based discretization strategy during Value Function Approximation (VFA).\nMain Contributions:\n1. A unified theory linking bisimulation and π-bisimulation metrics with a wider range of Sinkhorn distances underpins new insights into metric properties;\n2. An API algorithm utilizing bisimulation discretization has been developed along with rigorous convergence proofing against asymptotic performance benchmarks;\n3. Bounds have been established quantifying discrepancies arising due to changes in policies when employing π-bisimulation metrics;\n4. An improved API($α$) variant incorporating conservative policy update strategies offers enhanced performance over standard approaches;\n5. Practical relevance was demonstrated using empirical validation experiments conducted upon a real-world application implementing the proposed bisimulation-based API for finite MDPs scenarios.\n\n\nConclusion: By synthesizing advanced mathematical frameworks like Sinkhorn distances alongside innovative algorithms tailored specifically towards discrete-state spaces found in many reinforcement learning problems, researchers gain deeper understanding while practitioners benefit significantly",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Mitigating Catastrophic Forgetting in Spiking Neural Networks through Threshold Modulation",
        "abstract": "Artificial Neural Networks (ANNs) trained with Backpropagation and Stochastic Gradient Descent (SGD) suffer from the problem of Catastrophic Forgetting; when learning tasks sequentially, the ANN tends to abruptly forget previous knowledge upon being trained on a new task. On the other hand, biological neural networks do not suffer from this problem. Spiking Neural Networks (SNNs) are a class of Neural Networks that are closer to biological networks than ANNs and their intrinsic properties inspired from biology could alleviate the problem of Catastrophic Forgetting. In this paper, we investigate if the firing threshold mechanism of SNNs can be used to gate the activity of the network in order to reduce catastrophic forgetting. To this end, we evolve a Neuromodulatory Network that adapts the thresholds of an SNN depending on the spiking activity of the previous layer. Our experiments on different datasets show that the neurmodulated SNN can mitigate forgetting significantly with respect to a fixed threshold SNN. We also show that the evolved Neuromodulatory Network can generalize to multiple new scenarios and analyze its behavior.",
        "authors": "I. Hammouamri, T. Masquelier, D. G. Wilson",
        "keywords": [
            "spiking neural networks",
            "catastrophic forgetting",
            "neuromodulatory network"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=15SoThZmtU",
        "pdf_src": "https://api2.openreview.net/pdf/07980a9e19ead34ebbb2f23281d282a3e62e9f76.pdf",
        "Code_src": "",
        "Introduction": "Background: The traditional Artificial Neural Networks (ANNs), such as those trained using Backpropagation and Stochastic Gradient Descent (SGD), have been widely applied due to their effectiveness for various complex problems solving capabilities. However, they face issues like catastrophic forgetting during sequential learning where previously learned information is forgotten after acquiring newer data.\n\nResearch Question: This research aims at addressing the issue of catastrophic forgetting by exploring whether the firing threshold mechanism present in Spiking Neural Networks (SNNs) - which more closely mimic biological neural networks – can help retain prior knowledge while adapting to novel tasks without erasing old ones.\n\nMethodology: A novel approach involves evolving a neuromodulatory network whose role would be to dynamically adjust the firing thresholds within each neuron based on the spiking activities observed across preceding layers or neurons' outputs rather than having static thresholds throughout training sessions leading up to subsequent epochs.\n\nMain Contributions:\n1. The introduction into SNN architecture through evolution-based optimization techniques has led us towards developing adaptive thresholding mechanisms.\n2. Experimental validation conducted over diverse datasets demonstrates significant improvement against catastrophic forgetting compared to conventional fixed-threshold SNN models suggesting better retention capability under continuous learning conditions.\n3. Furthermore, our findings indicate that these neuromodulated networks exhibit generalization ability beyond single domains allowing them adapt flexibly even outside pre-trained contexts thus providing insights about how biological systems might cope similarly",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Exposing Outlier Exposure: What Can Be Learned From Few, One, and Zero Outlier Images",
        "abstract": "Due to the intractability of characterizing everything that looks unlike the normal data, anomaly detection (AD) is traditionally treated as an unsupervised problem utilizing only normal samples. However, it has recently been found that unsupervised image AD can be\ndrastically improved through the utilization of huge corpora of random images to represent anomalousness; a technique which is known as Outlier Exposure. In this paper we show that specialized AD learning methods seem unnecessary for state-of-the-art performance, and furthermore one can achieve strong performance with just a small collection of Outlier Exposure data, contradicting common assumptions in the field of AD. We find that standard classifiers and semi-supervised one-class methods trained to discern between normal samples and relatively few random natural images are able to outperform the current state of the art on an established AD benchmark with ImageNet. Further experiments reveal that even one well-chosen outlier sample is sufficient to achieve decent performance on this benchmark (79.3% AUC). We investigate this phenomenon and find that one-class methods are more robust to the choice of training outliers, indicating that there are scenarios where these are still more useful than standard classifiers. Additionally, we include experiments that delineate the scenarios where our results hold. Lastly, no training samples are necessary when one uses the representations learned by CLIP, a recent foundation model, which achieves state-of-the-art AD results on CIFAR-10 and ImageNet in a zero-shot setting.",
        "authors": "P. Liznerski, L. Ruff, R. A. Vandermeulen, et.al",
        "keywords": [
            "Outlier Exposure",
            "Anomaly Detection",
            "One-Class Methods"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=3v78awEzyB",
        "pdf_src": "https://api2.openreview.net/pdf/5c5d369aed67bf3c79e02d0cb126089668f3744d.pdf",
        "Code_src": "",
        "Introduction": "Background: Anomaly detection (AD), typically considered an unsupervised task using solely normal samples due to its complexity involving identifying all anomalies not resembling regular patterns.\n\nResearch Problem: Investigating whether leveraging large collections of random images (\"Outlier Exposure\") improves unsupervised AD beyond what's achieved without them while questioning if advanced AD algorithms like specialized models or extensive datasets truly enhance performance compared to simpler approaches such as standard classifiers.\n \nMethods: The authors demonstrate how standard classifiers combined with semi-supervised techniques capable of distinguishing among normal samples along with a limited number of randomly chosen natural images significantly surpass existing benchmarks despite challenging conventional wisdom about the necessity of complex methodologies within the AD community.\n \nMain Contributions:\n1. Contradicting prevalent beliefs regarding the need for sophisticated AD systems based on specialized learning methods versus simple yet effective strategies employing standard classifiers and semi-supervised ones.\n2. Demonstrating that modest amounts of \"Outlier Exposure\" data alone could yield substantial improvements over traditional unsupervised AD practices – showing efficacy from merely 1 carefully selected outlier example achieving nearly 80% Area Under Curve (AUC).\n3. Revealing increased robustness towards outlier selection variability amongst one-class classification methods suggesting their potential superiority under certain conditions against standard classifiers used widely today across various domains including computer vision tasks related to AD.\n4. Extending findings into practical applications via further experimental validations highlighting specific contexts where proposed approach yields superior outcomes relative to prior work thus providing actionable insights moving forward toward better understanding & implementation of AD solutions",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "ZerO Initialization: Initializing Neural Networks with only Zeros and Ones",
        "abstract": "Deep neural networks are usually initialized with random weights, with adequately selected initial variance to ensure stable signal propagation during training. However, selecting the appropriate variance becomes challenging especially as the number of layers grows. In this work, we replace random weight initialization with a fully deterministic initialization scheme, viz., ZerO, which initializes the weights of networks with only zeros and ones (up to a normalization factor), based on identity and Hadamard transforms. Through both theoretical and empirical studies, we demonstrate that ZerO is able to train networks without damaging their expressivity. Applying ZerO on ResNet achieves state-of-the-art performance on various datasets, including ImageNet, which suggests random weights may be unnecessary for network initialization. In addition, ZerO has many benefits, such as training ultra deep networks (without batch-normalization), exhibiting low-rank learning trajectories that result in low-rank and sparse solutions, and improving training reproducibility.",
        "authors": "J. Zhao, F. T. Schaefer, A. Anandkumar",
        "keywords": [
            "ZerO Initialization",
            "Deterministic Weight Initialization",
            "Network Expressivity"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=1AxQpKmiTc",
        "pdf_src": "https://api2.openreview.net/pdf/eaede848ff9bbe538b3cb6b118f9e75dad4c798e.pdf",
        "Code_src": "",
        "Introduction": "Background: The initialization of weights plays an important role in the stability and convergence of deep neural networks; however, it can become increasingly difficult when dealing with large numbers of layers.\n\nResearch Question: How does replacing random weight initialization with a fully deterministic initialization method affect the performance and stability of deep neural networks?\n\nMethod: We introduce a new deterministic initialization scheme called ZerO, which initializes the weights using only zeros and ones after applying identity and Hadamard transforms up to a normalization factor. \n\nMain Contributions: Our experiments show that ZerO can successfully train networks while preserving their expressivity across multiple datasets like ImageNet compared to standard random weight initialization methods. Additionally, our approach allows us to train much deeper networks than previously possible due to its lack of dependency on batch normalization techniques or other regularization strategies commonly used today - leading towards potentially more accurate models given enough computational resources available at runtime time points throughout inference phases within these architectures themselves! Furthermore ,we observe lower rank learning trajectories resulting from usage thereof ; henceforth enabling faster convergence rates along optimization paths taken by algorithms employed here too ! Finally ,our findings suggest improved reproducibility amongst different runs thanks largely owing partly because there exists less variance introduced into parameters involved during each iteration step forward passed through respective computations taking place therein respectively .",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "A Generalist Agent",
        "abstract": "Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.",
        "authors": "S. Reed, K. Zolna, E. Parisotto, et.al",
        "keywords": [
            "multi-modal",
            "multi-task",
            "multi-embodiment"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=1ikK0kHjvj",
        "pdf_src": "https://api2.openreview.net/pdf/a11e6601c3e6ea4fddfa784caac13c16d3344ed2.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper is inspired by advancements in large-scale language modeling techniques that have led to significant improvements in natural language processing tasks such as translation, summarization, question answering etc. However, these models are primarily designed for text-based inputs and outputs.\n\nResearch Problem: The research problem addressed here involves extending these advances from purely text-based systems into multimodal domains where agents need to interact not only through language but also via vision, touch, sound among others.\n \nMethodology: To tackle this challenge, researchers developed an agent called Gato (\"Generalist Agent Towards Open-ended Tasks\") using a novel architecture known as \"multi-embodiment\" neural networks. These networks allow different types of sensory input (e.g., visual, auditory, tactile) to be processed within one shared framework while maintaining distinct embeddings specific to each modality. Additionally, they used transfer learning strategies leveraging pre-trained models across various modalities like image recognition, speech synthesis/text-to-speech conversion before fine-tuning them further according to their application requirements.\n\nMain Contributions:\n1. They introduced Gato - a versatile multimodal agent capable performing diverse tasks including playing classic video games (Atari), generating descriptions for given images, engaging in conversational dialogue/chatbots, controlling robotic arms to stack objects precisely without prior training on those particular tasks themselves; all underpinned by a unified neural network structure trained jointly over multiple datasets representing these disparate domains.\n2. Demonstrated how combining deep reinforcement learning algorithms along with backpropagation allows endowing agents with skills learned during supervised training phases when interacting with external environments (like robots).",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Convergence of denoising diffusion models under the manifold hypothesis",
        "abstract": "  Denoising diffusion models are a recent class of generative models exhibiting state-of-the-art performance in image and audio synthesis. Such models approximate the time-reversal of a forward noising process from a target distribution to a reference measure, which is usually Gaussian. Despite their strong empirical results, the theoretical analysis of such models remains limited. In particular, all current approaches crucially assume that the target density admits a density w.r.t. the Lebesgue measure. This does not cover settings where the target distribution is supported on a lower-dimensional manifold or is given by some empirical distribution. In this paper, we bridge this gap by providing the first convergence results for diffusion models in this setting. In particular, we provide quantitative bounds on the Wasserstein distance of order one between the target data distribution and the generative distribution of the diffusion model.\n",
        "authors": "V. D. Bortoli",
        "keywords": [
            "diffusion models",
            "generative models",
            "convergence results"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=MhK5aXo3gB",
        "pdf_src": "https://api2.openreview.net/pdf/4918afeecd24bed01f0eedf6df222ecbb77e826a.pdf",
        "Code_src": "",
        "Introduction": "Background: Denoising Diffusion Models have recently emerged as powerful generative models capable of producing high-quality images and audio through an iterative denoising process starting with noise and ending up at the desired signal.\n\nResearch Question: The main challenge addressed in this research concerns the theoretical understanding behind these models under certain assumptions about the target distributions they aim to generate; specifically, whether those targets can be described using densities with respect to standard measures like the Lebesgue measure when dealing with more complex scenarios involving low-dimensional manifolds or empirical distributions rather than simple Gaussians.\n\nMethodology: To address this question, researchers introduce new convergence guarantees focusing on diffusion processes operating within non-standard environments beyond the classical Gaussian case typically considered previously due to computational complexity reasons related to higher dimensions involved therein.\n\nMain Contributions:\n1. They present novel convergence results demonstrating how diffusion models generalize successfully even if the underlying target distribution doesn't fit neatly into traditional frameworks requiring densities over Lebesgue spaces exclusively - thus expanding upon existing theoretical foundations significantly;\n2. Furthermore, they quantify precisely how close generated samples come towards approximating actual target distributions quantitatively via Wasserstein distances measuring discrepancy across probability measures explicitly defined hereunder;\n\nOverall Impact: These findings contribute substantially toward deepening our comprehension regarding diffusion models' robustness outside typical Gaussian contexts while also offering practical implications concerning improved sample quality control during training procedures ensuring better alignment closer towards intended outputs regardless complexities encountered along way!",
        "Topic": "Generative Models"
    },
    {
        "title": "Fail-Safe Adversarial Generative Imitation Learning",
        "abstract": "For flexible yet safe imitation learning (IL), we propose theory and a modular method, with a safety layer that enables a closed-form probability density/gradient of the safe generative continuous policy, end-to-end generative adversarial training, and worst-case safety guarantees. The safety layer maps all actions into a set of safe actions, and uses the change-of-variables formula plus additivity of measures for the density. The set of safe actions is inferred by first checking safety of a finite sample of actions via adversarial reachability analysis of fallback maneuvers, and then concluding on the safety of these actions' neighborhoods using, e.g., Lipschitz continuity. We provide theoretical analysis showing the robustness advantage of using the safety layer already during training (imitation error linear in the horizon) compared to only using it at test time (up to quadratic error). In an experiment on real-world driver interaction data, we empirically demonstrate tractability, safety and imitation performance of our approach.",
        "authors": "P. Geiger, C. Straehle",
        "keywords": [
            "flexible imitation learning",
            "safety layer",
            "generative adversarial training"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=e4Bb0b3QgJ",
        "pdf_src": "https://api2.openreview.net/pdf/106b79d981147ab7fb5f7ff62bdb8bccb54887b4.pdf",
        "Code_src": "",
        "Introduction": "Background: Imitation learning aims to train agents through demonstrations from human experts or other agents rather than relying solely on rewards as in reinforcement learning.\n\nResearch Problem: However, existing IL methods may not be both flexible enough nor completely safe due to potential hazards when generating new behaviors based on learned policies.\n\nMethod: To address this issue, they introduce a novel modular framework incorporating a \"safety layer\" which ensures that generated actions are within certain predefined boundaries ensuring their safety while still allowing flexibility.\n1. Safety Layer: Maps any action space into a subset of 'safe actions'. This mapping leverages the change of variables formula along with measure additivity properties concerning probabilities.\n2. Training Process: Uses Generative Adversarial Networks (GANs) but extends them beyond just image generation tasks here; instead focusing on generating safe trajectories over time.\n3. Worst-case Safety Guarantees: Provides mathematical proofs indicating how even if there exists some unforeseen hazard outside current knowledge bounds, those would likely lead to no more than linearly increasing errors under their proposed setup versus quadratic increases without such precautions.\n\nMain Contributions:\n1. A theoretically grounded modular architecture integrating safety considerations directly throughout the imitation process - not merely post-processing.\n2. Demonstrated empirical improvements regarding both computational feasibility & practical safety metrics alongside successful imitation capabilities across actual driving scenarios involving complex interactions between vehicles,\n3. Provided rigorous theoretical analyses highlighting benefits gained early-on during training phase vs later stages alone",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
        "abstract": "We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements.",
        "authors": "J. Yu, Y. Xu, J. Y. Koh, et.al",
        "keywords": [
            "text-to-image generation",
            "Transformer-based model",
            "high-fidelity photorealistic images"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=AFDcYJKhND",
        "pdf_src": "https://api2.openreview.net/pdf/69cd7065924e1931004a9c0b19875e05b272b900.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this paper lies in the field of computer vision and natural language processing where there has been an increasing interest in generating high-quality images from textual descriptions or prompts.\n\nResearch Problem:\nThe research question addressed here revolves around improving upon existing methods that generate images based on text inputs (\"text-to-image\" generation). These systems typically struggle when faced with complex compositions requiring understanding of spatial relationships between objects within the scene; they often lack the ability to incorporate real-world knowledge effectively leading to unrealistic results despite being trained extensively using vast datasets like ImageNet or COCO.\n\nMethodology:\nTo address these issues, the authors introduce \"Pathways Autoregressive Text-to-Image\" (Parti) - a novel architecture designed specifically tailored towards solving such challenges posed above while leveraging advancements made possible due to recent breakthroughs achieved via scaling both dataset size & complexity along with neural network architectures themselves (e.g., Transformers).\n\nMain Contributions:\n1) The Parti framework adopts a sequence-to-sequence approach similar to machine translation tasks but replaces linguistic units (words) with visual ones (tokens); \n2) It utilizes Vision Transformer (ViT)-based Vector Quantization Generative Adversarial Network (VQGAN) encoding mechanism allowing it to process raw input images directly without preprocessing steps involved before training;\n3) By employing scaled versions of Encoder-Decoder Transformer architectures up till 20 billion parameters range resulting in significant improvements observed regarding fidelity scores measured against standard benchmarks including MS-COCO dataset yielding competitive performances compared current state-of-the-art approaches available today;\n4) Furthermore, extensive experiments conducted show Parti's capability not only producing visually appealing realistic imagery under various conditions but also demonstrating its potential usefulness beyond basic functionalities provided by traditional methods e.g., handling more sophisticated compositional structures incorporating contextual information about surrounding environment etcetera;\n\nLimitations Identified:\nLastly although promising progress was reported throughout aforementioned contributions yet certain drawbacks were identified pointing out directions toward future enhancements required ensuring better integration among different modalities concerned namely vision/textual representations facilitating seamless interaction between them thereby enabling richer interactions amongst users interacting with generated visuals alongside their corresponding narratives presented verbally or otherwise.",
        "Topic": "Vision Transformer"
    },
    {
        "title": "A Note on \"Assessing Generalization of SGD via Disagreement\"",
        "abstract": "Several recent works find empirically that the average test error of deep neural networks can be estimated via the prediction disagreement of models, which does not require labels. In particular, Jiang et al. (2022) show for the disagreement between two separately trained networks that this `Generalization Disagreement Equality' follows from the well-calibrated nature of deep ensembles under the notion of a proposed `class-aggregated calibration.' In this reproduction, we show that the suggested theory might be impractical because a deep ensemble's calibration can deteriorate as prediction disagreement increases, which is precisely when the coupling of test error and disagreement is of interest, while labels are needed to estimate the calibration on new datasets. Further, we simplify the theoretical statements and proofs, showing them to be straightforward within a probabilistic context, unlike the original hypothesis space view employed by Jiang et al. (2022).",
        "authors": "A. Kirsch, Y. Gal",
        "keywords": [
            "prediction disagreement",
            "generalization disagreement equality",
            "class-aggregated calibration"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=oRP8urZ8Fx",
        "pdf_src": "https://api2.openreview.net/pdf/4bf0bfb01eb3fecd8e94bbb3623d50860ea1a9bf.pdf",
        "Code_src": "",
        "Introduction": "Background: Recent studies have shown that the generalization performance of deep neural networks may be inferred through their prediction disagreements without requiring labeled data.\n\nResearch Question: This paper investigates whether the empirical finding holds true in practice; specifically, it examines if the 'Generalization Disagreement Equality', observed previously among independently trained networks with high agreement rates but potentially deteriorating calibration upon increased disagreement, still applies.\n \nMethodology: The authors replicate the findings presented by Jiang et al. 2022 regarding class-aggregated calibration using a deep ensemble model where they observe an inconsistency – the calibration worsens despite increasing disagreement at higher levels suggesting that predictions become less reliable even though there seems to be more consensus among different network outputs.\n\nMain Contributions:\n1. Practical Limitation Identification: They identify practical limitations related to the use of prediction disagreement alone due to potential deterioration in calibration quality amidst growing disagreement - highlighting its inadequacy compared to traditional label-based methods during estimation tasks involving unseen data.\n2. Simplification & Probabilistic Framework: By simplifying the theoretical framework into a probabilistic perspective rather than focusing solely on the hypothesis space used earlier [Jiang et al., 2022], these researchers provide clarity about why such discrepancies arise based on how probabilities interact differently depending on whether or not one considers all classes equally important across different samples within each dataset being considered",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "DHA: End-to-End Joint Optimization of Data Augmentation Policy, Hyper-parameter and Architecture",
        "abstract": "Automated machine learning (AutoML) usually involves several crucial components, such as Data Augmentation (DA) policy, Hyper-Parameter Optimization (HPO), and Neural Architecture Search (NAS). Although many strategies have been developed for automating these components in separation, joint optimization of these components remains challenging due to the largely increased search dimension and the variant input types of each component. In parallel to this, the common practice of searching for the optimal architecture first and then retraining it before deployment in NAS often suffers from the low-performance correlation between the searching and retraining stages. An end-to-end solution that integrates the AutoML components and returns a ready-to-use model at the end of the search is desirable. In view of these, we propose DHA, which achieves joint optimization of Data augmentation policy, Hyper-parameter, and Architecture. Specifically, end-to-end NAS is achieved in a differentiable manner by optimizing a compressed lower-dimensional feature space, while DA policy and HPO are regarded as dynamic schedulers, which adapt themselves to the update of network parameters and network architecture at the same time. Experiments show that DHA achieves state-of-the-art (SOTA) results on various datasets and search spaces. To the best of our knowledge, we are the first to efficiently and jointly optimize DA policy, NAS, and HPO in an end-to-end manner without retraining.",
        "authors": "K. Zhou, L. Hong, S. Hu, et.al",
        "keywords": [
            "Data Augmentation",
            "Hyper-Parameter Optimization",
            "Neural Architecture Search"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=MHOAEiTlen",
        "pdf_src": "https://api2.openreview.net/pdf/2c7a2c9618835c0d59541343a58dc71730f32873.pdf",
        "Code_src": "",
        "Introduction": "Background: Automated Machine Learning (AutoML) aims to automate the process of building predictive models with minimal human intervention through techniques like Data Augmentation (DA), Hyper-Parameter Optimization (HPO), and Neural Architecture Search (NAS).\n\nResearch Problem: Jointly optimizing all three components effectively has proven difficult because they require significantly higher dimensional searches than when optimized separately.\n\nMethods: We introduce Differential Heuristic Attention (DHA), where we achieve joint optimization using a differentiable approach based on a compressed lower-dimensional feature space rather than directly optimizing high-dimensional architectures or hyperparameters individually. Additionally, we treat DA policies and HPO as dynamic schedulers adapting continuously during training sessions involving both data augmentation and hyperparameter tuning processes respectively.\n\nMain Contributions: Our work presents one of the most advanced solutions integrating DA, NAS & HPO into an end-to-end framework eliminating the need for subsequent retraining phases after NAS completion; achieving significant improvements over existing benchmarks across multiple datasets and search spaces thus far unseen within literature related to automated machine learning tasks up until now according to current understanding available here",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Data Leakage in Federated Averaging",
        "abstract": "Recent attacks have shown that user data can be recovered from FedSGD updates, thus breaking privacy. However, these attacks are of limited practical relevance as federated learning typically uses the FedAvg algorithm. Compared to FedSGD, recovering data from FedAvg updates is much harder as: (i) the updates are computed at unobserved intermediate network weights, (ii) a large number of batches are used, and (iii) labels and network weights vary simultaneously across client steps. In this work, we propose a new optimization-based attack which successfully attacks FedAvg by addressing the above challenges. First, we solve the optimization problem using automatic differentiation that forces a simulation of the client's update that generates the unobserved parameters for the recovered labels and inputs to match the received client update. Second, we address the large number of batches by relating images from different epochs with a permutation invariant prior. Third, we recover the labels by estimating the parameters of existing FedSGD attacks at every FedAvg step. On the popular FEMNIST dataset, we demonstrate that on average we successfully recover >45% of the client's images from realistic FedAvg updates computed on 10 local epochs of 10 batches each with 5 images, compared to only <10% using the baseline. Our findings show many real-world federated learning implementations based on FedAvg are vulnerable.",
        "authors": "D. I. Dimitrov, M. Balunovic, N. Konstantinov, et.al",
        "keywords": [
            "optimization-based attack",
            "Federated Learning",
            "privacy breach"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=e7A0B99zJf",
        "pdf_src": "https://api2.openreview.net/pdf/c5451b738af2e6f8ffacb6f7401dff8b854279dd.pdf",
        "Code_src": "",
        "Introduction": "Background:\nFederated learning aims to train machine learning models over decentralized datasets without exposing sensitive information about individual users or devices.\n\nResearch Problem:\nDespite its promise in preserving privacy through decentralization, recent studies revealed vulnerabilities where adversaries could infer private user data even when employing federated learning algorithms like FedAvg instead of FedSGD due to certain characteristics such as computation at unseen intermediate weights during model updates; usage of multiple batches per epoch leading to correlation between them despite label changes among clients' steps; simultaneous variation both labels and network weights within client iterations.\n\nMethodology:\nIn response to overcoming these challenges posed specifically against FedAvg, our approach involves an optimization-based attack strategy:\n\n1. We tackle the issue related to unseen intermediate weights via solving the optimization problem automatically differentiated so it simulates what would happen if one were able to observe those intermediary updates.\n2. To deal with numerous batches being utilized throughout training phases while still maintaining some degree of batch correlation amidst changing labels amongst participants’ contributions - we introduce a permutation-invariant prior that relates images taken under various epochs together into clusters regardless of their order.\n3. Finally concerning recovery efforts themselves – rather than starting anew after each round iteration involving FedAvg’s communication protocol – we leverage pre-existing knowledge gained previously utilizing FedSGD attacks iteratively refine estimates towards identifying corresponding recovered labels associated with incoming client updates.\n\nMain Contributions:\nThis paper introduces novel techniques designed especially around attacking Federated Learning systems running on top of FedAvg protocols effectively circumventing previous limitations imposed upon attackers attempting similar endeavors before us mainly because they focused exclusively onto FedSGD variants alone ignoring potential weaknesses present elsewhere including those found here pertinent particularly relevant today given widespread adoption rates observed worldwide nowadays making up majority deployments currently available publicly accessible repositories showcasing examples demonstrating how organizations might deploy solutions leveraging distributed computing architectures powered by artificial intelligence technologies",
        "Topic": "Federated Learning"
    },
    {
        "title": "A Rigorous Study Of The Deep Taylor Decomposition",
        "abstract": "Saliency methods attempt to explain deep neural networks by highlighting the most salient features of a sample. Some widely used methods are based on a theoretical framework called Deep Taylor Decomposition (DTD), which formalizes the recursive application of the Taylor Theorem to the network's layers. However, recent work has found these methods to be independent of the network's deeper layers and appear to respond only to lower-level image structure. Here, we investigate DTD theory to better understand this perplexing behavior and found that the Deep Taylor Decomposition is equivalent to the basic gradient$\\times$input method when the Taylor root points (an important parameter of the algorithm chosen by the user) are locally constant. If the root points are locally input-dependent, then one can justify any explanation. In this case, the theory is under-constrained. In an empirical evaluation, we find that DTD roots do not lie the same linear regions as the input -- contrary to a fundamental assumption of the Taylor Theorem. The theoretical foundations of DTD were cited as a source of reliability for the explanations. However, our findings urge caution in making such claims.",
        "authors": "L. Sixt, T. Landgraf",
        "keywords": [
            "DTD",
            "saliency maps",
            "gradient"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Y4mgmw9OgV",
        "pdf_src": "https://api2.openreview.net/pdf/edc023c04f413b5b3613b94ddf2b2994ece5d687.pdf",
        "Code_src": "",
        "Introduction": "Background: Saliency methods aim to interpret how images influence predictions made by convolutional neural networks through visualizing their response patterns.\n\nResearch Question: This paper investigates why existing saliency methods grounded in the Deep Taylor Decomposition (DTD) often fail to capture higher-level semantic information within images despite being theoretically capable of doing so due to its recursive nature applied across multiple network layers.\n\nMethodology: By analyzing the mathematical properties of DTD, authors demonstrate it reduces to a simple gradient $\\times$ input operation if the selected Taylor expansion root points remain constant over different inputs; otherwise, interpretations become arbitrary without clear constraints from the theory itself leading to potential misinterpretations or irrelevance with respect to high-level semantics.\n\nMain Contributions:\n1. They uncover inconsistencies between the theoretical expectations set forth by DTD regarding capturing hierarchical feature representations versus observed practical limitations.\n2. Highlight issues related to the selection process where root point choice significantly affects the resulting saliency maps but lacks theoretical guidance beyond local constancy assumptions about those choices implying significant flexibility rather than rigidity expected given the decomposition’s recursive approach at each layer level throughout the network architecture depth-wise.\n3. Offer empirical evidence showing that DTD root locations diverge substantially from corresponding input gradients’ linear spaces challenging foundational beliefs underlying previous justifications relying heavily upon this methodology providing reliable insights into model decision-making processes during prediction tasks involving complex datasets like natural imagery recognition scenarios encountered commonly nowadays especially concerning computer vision applications requiring interpretable models aiding human understanding alongside automated ones.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Behind the Machine’s Gaze: Neural Networks with Biologically-inspired Constraints Exhibit Human-like Visual Attention",
        "abstract": "By and large, existing computational models of visual attention tacitly assume perfect vision and full access to the stimulus and thereby deviate from foveated biological vision. Moreover, modeling top-down attention is generally reduced to the integration of semantic features without incorporating the signal of a high-level visual tasks that have been shown to partially guide human attention.\nWe propose the Neural Visual Attention (NeVA) algorithm to generate visual scanpaths in a top-down manner. With our method, we explore the ability of neural networks on which we impose a biologically-inspired foveated vision constraint to generate human-like scanpaths without directly training for this objective. The loss of a neural network performing a downstream visual task (i.e., classification or reconstruction) flexibly provides top-down guidance to the scanpath.\nExtensive experiments show that our method outperforms state-of-the-art unsupervised human attention models in terms of similarity to human scanpaths. Additionally, the flexibility of the framework allows to quantitatively investigate the role of different tasks in the generated visual behaviors. Finally, we demonstrate the superiority of the approach in a novel experiment that investigates the utility of scanpaths in real-world applications, where imperfect viewing conditions are given.",
        "authors": "L. Schwinn, D. Precup, B. Eskofier, et.al",
        "keywords": [
            "biologically-inspired",
            "foveated vision",
            "top-down attention"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=7iSYW1FRWA",
        "pdf_src": "https://api2.openreview.net/pdf/83405c024fa3278646c43286cbe8b93fdf11d2bb.pdf",
        "Code_src": "",
        "Introduction": "Background: Existing computational models of visual attention often make assumptions about perfect vision with complete information availability but do not align well with the biology of how humans focus their attention through a process called \"foveation.\" This means they don't account for the fact that people typically look at certain areas more closely than others.\n\nResearch Question: How can one create an artificial model inspired by human visual attention mechanisms within the constraints of limited field of view?\n\nMethod: We introduce the Neural Visual Attention (NeVA) algorithm as a way to simulate top-down directed scanning paths using neural networks while imposing a foveated vision constraint - mimicking natural human vision's tendency to concentrate on specific parts first before moving outward when necessary based on context clues provided by higher level cognitive processes (\"top-down\" signals).\n\nMain Contributions:\n1. NeVA generates visually plausible scanpaths guided both by bottom-up sensory input like image content details yet also influenced by top-down contextual cues such as those derived from prior knowledge during learning tasks related to object recognition or scene understanding.\n2. Unlike previous works focusing solely on supervised methods requiring labeled data sets specifically designed around generating scanpaths, ours uses only end-to-end trained neural networks doing unrelated downstream tasks; it leverages these networks' inherent biases towards efficient processing patterns akin to what might be observed in humans under similar circumstances rather than explicitly programming them toward scanpath generation itself thus avoiding overfitting issues common among specialized scanpath algorithms developed previously \n3. Extensive experimental validation shows superior performance compared against other leading unsupervised human attention models across various metrics including similarity scores between predicted scans and actual human gaze behavior patterns along with adaptability demonstrated even amidst challenging real-world scenarios featuring less-than-ideal viewing conditions",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "On the Adversarial Robustness of Vision Transformers",
        "abstract": "Following the success in advancing natural language processing and understanding, transformers are expected to bring revolutionary changes to computer vision. This work provides a comprehensive study on the robustness of vision transformers (ViTs) against adversarial perturbations. Tested on various white-box and transfer attack settings, we find that ViTs possess better adversarial robustness when compared with MLP-Mixer and convolutional neural networks (CNNs) including ConvNeXt, and this observation also holds for certified robustness. Through frequency analysis and feature visualization, we summarize the following main observations contributing to the improved robustness of ViTs: 1) Features learned by ViTs contain less high-frequency patterns that have spurious correlation,  which helps explain why ViTs are less sensitive to high-frequency perturbations than CNNs and MLP-Mixer, and there is a high correlation between how much the model learns high-frequency features and its robustness against different frequency-based perturbations. 2) Introducing convolutional or tokens-to-token blocks for learning high-frequency features in ViTs can improve classification accuracy but at the cost of adversarial robustness. 3) Modern CNN designs that borrow techniques from ViTs including activation function, layer norm, larger kernel size to imitate the global attention, and patchify the images as inputs, etc., could help bridge the performance gap between ViTs and CNNs not only in terms of performance, but also certified and empirical adversarial robustness. Moreover, we show adversarial training is also applicable to ViT for training robust models, and sharpness-aware minimization can also help improve robustness, while pre-training with clean images on larger datasets does not significantly improve adversarial robustness. ",
        "authors": "R. Shao, Z. Shi, J. Yi, et.al",
        "keywords": [
            "vision transformers",
            "adversarial robustness",
            "feature learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=lE7K4n1Esk",
        "pdf_src": "https://api2.openreview.net/pdf/3cb48c3298378819ec961818c9cab01c69cbba68.pdf",
        "Code_src": "",
        "Introduction": "Background:\nWith the advancements in Natural Language Processing(NLP), Transformers have been anticipated to revolutionize Computer Vision(CV). However, despite their effectiveness across CV tasks, it remains unclear whether they would be similarly robust under adversarial attacks.\n\nResearch Question:\nThis paper aims to comprehensively investigate the adversarial robustness of Vision Transformers(ViTs) vis-à-vis traditional architectures like Multi-Layer Perceptrons(MLP)-Mixer and Convolutional Neural Networks(CNNs).\n\nMethodology:\nThe authors conducted experiments using both white-box and transfer attack settings over several benchmarks. They employed frequency analysis and feature visualization tools to understand the underlying reasons behind the observed robustness differences among these architectures.\n\nMain Contributions:\n1) The findings indicate that ViTs learn fewer high-frequency patterns with spurious correlations leading them to exhibit lower sensitivity towards such perturbations.\n2) Although incorporating convolutional or tokens-to-token blocks within ViTs may enhance classification accuracy, doing so comes at an expense compromising adversarial robustness.\n3) By adopting certain Transformer-inspired modifications into CNNs, one might narrow down the performance gap existing between ViTs and CNNs – not just in standard metrics - but even regarding certified and empirical adversarial robustness levels too.\n4) Adversarial training was found effective specifically tailored toward enhancing the robustness of ViTs; further, sharpness-aware minimization strategies were shown beneficial along those lines without necessitating extensive pre-training efforts solely based on large-scale datasets alone.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Structured Uncertainty in the Observation Space of Variational Autoencoders",
        "abstract": "Variational autoencoders (VAEs) are a popular class of deep generative models with many variants and a wide range of applications. Improvements upon the standard VAE mostly focus on the modelling of the posterior distribution over the latent space and the properties of the neural network decoder. In contrast, improving the model for the observational distribution is rarely considered and typically defaults to a pixel-wise independent categorical or normal distribution. In image synthesis, sampling from such distributions produces spatially-incoherent results with uncorrelated pixel noise, resulting in only the sample mean being somewhat useful as an output prediction. In this paper, we aim to stay true to VAE theory by improving the samples from the observational distribution. We propose SOS-VAE, an alternative model for the observation space, encoding spatial dependencies via a low-rank parameterisation. We demonstrate that this new observational distribution has the ability to capture relevant covariance between pixels, resulting in spatially-coherent samples. In contrast to pixel-wise independent distributions, our samples seem to contain semantically-meaningful variations from the mean allowing the prediction of multiple plausible outputs with a single forward pass.",
        "authors": "J. Langley, M. Monteiro, C. Jones, et.al",
        "keywords": [
            "SOS-VAE",
            "Variational Autoencoder",
            "Spatial Coherence"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=cxp7n9q5c4",
        "pdf_src": "https://api2.openreview.net/pdf/bf534300cfe78edf5a772a16d19889a647c5a805.pdf",
        "Code_src": "",
        "Introduction": "Background: Variational autoencoders (VAEs) are widely used deep generative models but improvements mainly target the posterior distribution modeling and decoder architecture.\n\nResearch problem: How can one improve the observational distribution within VAEs?\n\nMethod: The authors introduce SOS-VAE which uses a low-rank parameterization to encode spatial dependencies into the observational distribution instead of using independent categorical or normal distributions.\n\nMain contributions: By capturing pixel covariances, SOS-VAE generates spatially coherent samples containing meaningful variations around the mean leading to predictions beyond just the sample mean per forward pass.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Distributed Stochastic Algorithms for High-rate Streaming Principal Component Analysis",
        "abstract": "This paper considers the problem of estimating the principal eigenvector of a covariance matrix from independent and identically distributed data samples in streaming settings. The streaming rate of data in many contemporary applications can be high enough that a single processor cannot finish an iteration of existing methods for eigenvector estimation before a new sample arrives. This paper formulates and analyzes a distributed variant of the classical Krasulina's method (D-Krasulina) that can keep up with the high streaming rate of data by distributing the computational load across multiple processing nodes. The analysis improves upon the one in (Balsubramani et al., 2013) for the original Krasulina's method and shows that---under appropriate conditions---D-Krasulina converges to the principal eigenvector in an order-wise optimal manner; i.e., after receiving $M$ samples across all nodes, its estimation error can be $O(1/M)$. In order to reduce the network communication overhead, the paper also develops and analyzes a mini-batch extension of D-Krasulina, which is termed DM-Krasulina. The analysis of DM-Krasulina shows that it can also achieve order-optimal estimation error rates under appropriate conditions, even when some samples have to be discarded within the network due to communication latency. Finally, experiments are performed over synthetic and real-world data to validate the convergence behaviors of D-Krasulina and DM-Krasulina in high-rate streaming settings.",
        "authors": "H. Raja, W. U. Bajwa",
        "keywords": [
            "distributed computing",
            "eigenvalue estimation",
            "high-speed streaming"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=CExeD0jpB6",
        "pdf_src": "https://api2.openreview.net/pdf/b00e244fffa49e0d5f6fc5b06794497a3441ed9d.pdf",
        "Code_src": "",
        "Introduction": "Background: Estimating the principal eigenvector of a covariance matrix efficiently has been widely studied as it plays crucial roles in various fields such as machine learning algorithms like PCA or spectral clustering.\n\nResearch Problem: However, this task becomes challenging especially if we consider large-scale datasets where the incoming stream of data is too fast compared to traditional batch-based approaches allowing only limited iterations per unit time.\n \nMethod: To tackle these issues, our work introduces Distributed Krasulina (D-Krasulina), a novel algorithm designed specifically for high-speed streaming scenarios involving parallel computation on multiple processors. We further propose Mini-Batch Distributed Krasulina (DM-Krasulina) aiming at reducing networking costs while maintaining similar performance guarantees.\n\nMain Contributions:\n- Formulate & analyze D-Krasulina - A distributed version of Krasulina’s method capable of handling rapid streams without sacrificing accuracy significantly improving previous bounds;\n- Prove convergence behavior – Under certain assumptions about the distribution properties along with sufficient number of samples received globally across all nodes, D-Krasulina yields estimation errors scaling linearly inversely proportional to \\( M \\);\n- Develop DM-Krasulina - An optimized version leveraging mini-batches minimizing communication cost yet still achieving near-linear scale-up benefits;\n- Validate proposed algorithms via empirical studies conducted against both simulated datasets demonstrating their efficacy particularly suited towards high-throughput environments commonly encountered nowadays",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "On the Paradox of Certified Training",
        "abstract": "Certified defenses based on convex relaxations are an established technique for training provably robust models. The key component is the choice of relaxation, varying from simple intervals to tight polyhedra. Counterintuitively, loose interval-based training often leads to higher certified robustness than what can be achieved with tighter relaxations, which is a well-known but poorly understood paradox. While recent works introduced various improvements aiming to circumvent this issue in practice, the fundamental problem of training models with high certified robustness remains unsolved. In this work, we investigate the underlying reasons behind the paradox and identify two key properties of relaxations, beyond tightness, that impact certified training dynamics: continuity and sensitivity. Our extensive experimental evaluation with a number of popular convex relaxations provides strong evidence that these factors can explain the drop in certified robustness observed for tighter relaxations. We also systematically explore modifications of existing relaxations and discover that improving unfavorable properties is challenging, as such attempts often harm other properties, revealing a complex tradeoff. Our findings represent an important first step towards understanding the intricate optimization challenges involved in certified training.",
        "authors": "N. Jovanović, M. Balunovic, M. Baader, et.al",
        "keywords": [
            "convex relaxations",
            "certified robustness",
            "optimization challenges"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=atJHLVyBi8",
        "pdf_src": "https://api2.openreview.net/pdf/890b0c6fbb15ff33767a3d75c7a63c6b53d753e4.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses the use of convex relaxations techniques within Certified Defenses framework used during model training process ensuring their robustness against adversarial perturbations.\n\nResearch Problem:\nDespite the widespread adoption of convex relaxations due to their simplicity compared to non-convex relaxations or neural network approximations while still providing guarantees about robustness, there exists a counterintuitive observation where looser interval-based relaxations lead to more robust certification results rather than stricter ones despite theoretically weaker guarantees.\n\nMethods:\nThe authors delve into investigating why this phenomenon occurs by analyzing additional characteristics besides just the tightness level - namely, continuity and sensitivity – of the chosen relaxations when applied toward certifying robustness under attacks.\n \nMain Contributions:\nThis research identifies two critical aspects related to convex relaxations apart from their strictness levels influencing certified training outcomes; they found that both continuity and sensitivity play significant roles affecting how effectively relaxations capture true decision boundaries leading to different robustness certifications. Furthermore, through comprehensive experiments using several common convex relaxations across multiple datasets/models, empirical validation supports those observations indicating complexity around optimizing certain desirable traits without negatively impacting others highlighting potential trade-offs between them suggesting further exploration needed before fully addressing issues surrounding optimal convex relaxation choices",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Time Series Alignment with Global Invariances",
        "abstract": "Multivariate time series are ubiquitous objects in signal processing. Measuring a distance or similarity between two such objects is of prime interest in a variety of applications, including machine learning, but can be very difficult as soon as the temporal dynamics and the representation of the time series, i.e. the nature of the observed quantities, differ from one another. In this work, we propose a novel distance accounting both feature space and temporal variabilities by learning a latent global transformation of the feature space together with a temporal alignment, cast as a joint optimization problem. The versatility of our framework allows for several variants depending on the invariance class at stake. Among other contributions, we define a differentiable loss for time series and present two algorithms for the computation of time series barycenters under this new geometry. We illustrate the interest of our approach on both simulated and real world data and show the robustness of our approach compared to state-of-the-art methods.\n",
        "authors": "T. Vayer, R. Tavenard, L. Chapel, et.al",
        "keywords": [
            "time series analysis",
            "feature space",
            "temporal variability"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=JXCH5N4Ujy",
        "pdf_src": "https://api2.openreview.net/pdf/9a1d423bcb1b748a049901b02d4cb4efc7476b67.pdf",
        "Code_src": "",
        "Introduction": "Background: Multivariate time series play an important role in various fields like signal processing due to their ubiquity.\n\nResearch Problem: How do you measure distances or similarities accurately among multivariate time series when they have different temporal dynamics?\n\nMethod: This paper proposes a novel method that learns a latent global transformation of the feature space along with a temporal alignment through joint optimization problems considering both feature space and temporal variabilities.\n\nMain Contributions:\n1. A versatile framework allowing for multiple variants based on the invariance classes involved;\n2. Definition of a differentiable loss function specifically designed for time series analysis;\n3. Introduction of two algorithms aimed at computing time series barycenters within the proposed geometric framework; \n4. Illustration using simulations and real-world datasets demonstrating the effectiveness against existing state-of-the-art approaches while highlighting its robustness properties.",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Benchmarking Progress to Infant-Level Physical Reasoning in AI",
        "abstract": "To what extent do modern AI systems comprehend the physical world? We introduce the open-access Infant-Level Physical Reasoning Benchmark (InfLevel) to gain insight into this question. We evaluate ten neural-network architectures developed for video understanding on tasks designed to test these models' ability to reason about three essential physical principles which researchers have shown to guide human infants' physical understanding. We explore the sensitivity of each AI system to the continuity of objects as they travel through space and time, to the solidity of objects, and to gravity. We find strikingly consistent results across 60 experiments with multiple systems, training regimes, and evaluation metrics: current popular visual-understanding systems are at or near chance on all three principles of physical reasoning. We close by suggesting some potential ways forward.",
        "authors": "L. Weihs, A. Yuile, R. Baillargeon, et.al",
        "keywords": [
            "Infant-level physical reasoning benchmark",
            "Neural network architectures",
            "Physical principles"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=9NjqD9i48M",
        "pdf_src": "https://api2.openreview.net/pdf/a388498ddf8e5e9f1b744459576c8da685c4498e.pdf",
        "Code_src": "",
        "Introduction": "Background:\nModern artificial intelligence (AI) systems such as neural networks trained in image recognition can process images effectively but their comprehension of the underlying physics remains unclear.\n\nResearch Question:\nThis paper aims to investigate how well contemporary AI systems understand fundamental physical concepts that humans learn from infancy.\n \nMethodology:\nThe authors introduced an open-access benchmark called \"Infant-Level Physical Reasoning Benchmark\" (InfLevel), a set of tasks specifically designed to assess whether AIs could demonstrate similar abilities related to object perception based on three key physical principles – continuity over space-time, object solidity, and gravitational effects.\n\nMain Contributions:\nThey evaluated several neural network architectures commonly used for video understanding against InfLevel benchmarks using various experimental setups including different datasets, training procedures, and performance measures. The study found that none of the tested systems performed significantly better than random guessing when it comes to inferring those basic physical properties just like young children would be expected to achieve without any prior knowledge beyond everyday experience demonstrating limitations even after extensive training data exposure. This highlights significant gaps between existing machine learning capabilities compared traditional human cognition especially regarding intuitive graspes foundational aspects reality around us daily life scenarios where common sense plays crucial role decision-making processes",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "INR-V: A Continuous Representation Space for Video-based Generative Tasks",
        "abstract": "Generating videos is a complex task that is accomplished by generating a set of temporally coherent images frame-by-frame. This limits the expressivity of videos to only image-based operations on the individual video frames needing network designs to obtain temporally coherent trajectories in the underlying image space. We propose INR-V, a video representation network that learns a continuous space for video-based generative tasks. INR-V parameterizes videos using implicit neural representations (INRs), a multi-layered perceptron that predicts an RGB value for each input pixel location of the video. The INR is predicted using a meta-network which is a hypernetwork trained on neural representations of multiple video instances. Later, the meta-network can be sampled to generate diverse novel videos enabling many downstream video-based generative tasks. Interestingly, we find that conditional regularization and progressive weight initialization play a crucial role in obtaining INR-V. The representation space learned by INR-V is more expressive than an image space showcasing many interesting properties not possible with the existing works. For instance, INR-V can smoothly interpolate intermediate videos between known video instances (such as intermediate identities, expressions, and poses in face videos). It can also in-paint missing portions in videos to recover temporally coherent full videos. In this work, we evaluate the space learned by INR-V on diverse generative tasks such as video interpolation, novel video generation, video inversion, and video inpainting against the existing baselines. INR-V significantly outperforms the baselines on several of these demonstrated tasks, clearly showing the potential of the proposed representation space.",
        "authors": "B. Sen, A. Agarwal, V. P. Namboodiri, et.al",
        "keywords": [
            "video generation",
            "implicit neural representations",
            "temporal coherence"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=aIoEkwc2oB",
        "pdf_src": "https://api2.openreview.net/pdf/ab80702909005df471eaaa31f652c041c9acb040.pdf",
        "Code_src": "",
        "Introduction": "Background: Generating videos involves creating a sequence of temporally coherent images one after another; however, current methods are limited because they treat each video frame independently without considering temporal coherence.\n\nResearch Problem: How do you design a model capable of representing entire videos rather than just single frames?\n\nMethod: The authors introduce INR-V, a video representation network based on implicit neural representations (INRs). These INRs predict RGB values at every pixel position within a video's frames through a multi-layer perceptron called a meta-network - itself a hypernetwork trained from various video examples' neural representations.\n \nMain Contributions:\n1. They develop a new approach where the entire video content isn't explicitly stored but instead represented implicitly via INRs allowing for greater flexibility over traditional image-based models due to its continuous nature.\n2. Conditional regularization during training helps stabilize learning while progressive weight initialization aids in better convergence towards optimal solutions leading to improved performance across different generative tasks like video interpolation or inpainting compared to baseline approaches used previously demonstrating significant improvements overall suggesting promise toward future advancements in video generation technologies utilizing similar frameworks.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "A Simple Convergence Proof of Adam and Adagrad",
        "abstract": "We provide a simple proof of convergence covering both the Adam and Adagrad adaptive optimization algorithms when applied to smooth (possibly non-convex) objective functions with bounded gradients. We show that in expectation, the squared norm of the objective gradient averaged over the trajectory has an upper-bound which is explicit in the constants of the problem, parameters of the optimizer, the dimension $d$, and the total number of iterations $N$. \nThis bound can be made arbitrarily small, and with the right hyper-parameters, Adam can be shown to converge with the same rate of convergence $O(d\\ln(N)/\\sqrt{N})$. When used with the default parameters, Adam doesn't converge, however, and just like constant step-size SGD, it moves away from the initialization point faster than\nAdagrad, which might explain its practical success.\nFinally, we obtain the tightest dependency on the heavy ball momentum decay rate $\\beta_1$ among all previous convergence bounds for non-convex Adam and Adagrad,\nimproving from $O((1-\\beta_1)^{-3})$ to $O((1-\\beta_1)^{-1})$. ",
        "authors": "A. Défossez, L. Bottou, F. Bach, et.al",
        "keywords": [
            "convergence",
            "Adam",
            "Adagrad"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ZPQhzTSWA7",
        "pdf_src": "https://api2.openreview.net/pdf/d1743b5d08ae3891ea8a696e03081e6c9ebdbc4d.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper focuses on analyzing the convergence properties of two popular adaptive optimization algorithms - Adam and Adagrad – under certain conditions.\n\nResearch Question: The main research question addressed by this study revolves around understanding how these algorithms behave as they optimize objectives with possibly non-convex nature but with bounded gradients during iterative processes such as machine learning training.\n\nMethods: The authors present a novel approach involving expected value analysis along trajectories taken through parameter space while optimizing the given function using either algorithm or their variants including those with momentum terms represented by $\\beta_1$. They derive an upper bound on the average squared norm of the gradient within each iteration's trajectory based upon several factors inclusive of dimensions ($d$), iterations ($N$), optimizer parameters, and specific constants related to the problem at hand.\n\nMain Contributions:\n1. Convergence Bound: A new convergence bound was established showing that if expectations are considered across trajectories throughout optimization steps, then there exists an upper limit on the square of the gradient norms depending explicitly on various elements mentioned above; this implies that the algorithms have potential to stabilize after some iterations leading towards convergence despite potentially encountering non-convexity issues commonly found in real-world problems.\n2. Optimizer Performance Analysis: It’s demonstrated mathematically why standard configurations don’t lead directly into convergence—Adam diverges more rapidly compared to Adagrad due to its design choices concerning updates per parameter independently rather than jointly considering them via stochastic gradient descent (SGD). However, with carefully chosen hyperparameters, Adam could theoretically achieve similar convergence rates as SGD does, albeit not without additional computational effort required beyond what would typically occur naturally following initial setup.\n3. Momentum Decay Rate Dependency Improvement: Previous works had tighter bounds dependent on $\\beta_1$ ranging up to $(1-\\beta_1)^{-3}$ whereas here presented improvements reduce this dependence significantly down to $(1-\\beta_1)^{-1}$.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Explicit Group Sparse Projection with Applications to Deep Learning and NMF",
        "abstract": "We design a new sparse projection method for a set of vectors that guarantees a desired average sparsity level measured leveraging the popular Hoyer measure (an affine function of the ratio of the $\\ell_1$ and $\\ell_2$ norms).\nExisting approaches either project each vector individually or require the use of a regularization parameter which implicitly maps to the average $\\ell_0$-measure of sparsity. Instead, in our approach we set the sparsity level for the whole set explicitly and simultaneously project a group of vectors with the sparsity level of each vector tuned automatically.\nWe show that the computational complexity of our projection operator is linear in the size of the problem.\nAdditionally, we propose a generalization of this projection by replacing the $\\ell_1$ norm by its weighted version.\nWe showcase the efficacy of our approach in both supervised and unsupervised learning tasks on image datasets including CIFAR10 and ImageNet. In deep neural network pruning, the sparse models produced by our method on ResNet50 have significantly higher accuracies at corresponding sparsity values compared to existing competitors. In nonnegative matrix factorization, our approach yields competitive reconstruction errors against state-of-the-art algorithms.",
        "authors": "R. Ohib, N. Gillis, N. Dalmasso, et.al",
        "keywords": [
            "sparse projection",
            "average sparsity level",
            "linear computational complexity"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=jIrOeWjdpc",
        "pdf_src": "https://api2.openreview.net/pdf/5f02988c7721038cbd7d1d83db432113f95ff96a.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses an optimization challenge related to sparse projections where one aims to find a subset of vectors from a given dataset while maintaining a certain desired average sparsity rate.\n\nResearch Problem: How can we efficiently project a set of vectors towards achieving a specified average sparsity without using individual projections per vector?\n\nMethod: The authors introduce a novel sparse projection technique based on the Hoyer measure - a metric used previously as it correlates well with the true $\\ell_0$-norm measure but is computationally more tractable due to being an affine function of $\\ell_1$ and $\\ell_2$ norms ratios within vectors. Their algorithm projects multiple vectors together rather than independently; thus, they do not need to rely on regularization parameters associated with $\\ell_0$ measures directly since these are implicit when using such parameters indirectly through $\\ell_1$ norms. They also present a generalized form of their projection operation allowing for weighted $\\ell_1$ norms instead of unweighted ones if needed.\n\n\nMain Contributions:\n1. A linear-time sparse projection operator designed specifically around the Hoyer measure ensuring efficient computation relative to other methods requiring independent projections over all vectors;\n2. An explicit setting mechanism enabling control over overall sparsity levels across entire sets of projected vectors concurrently during processing;\n3. Extension into handling weighted $\\ell_1$ norms providing flexibility beyond standard implementations focusing solely on unweighted versions;\n4. Demonstrated effectiveness via empirical results showing improved performance metrics like accuracy rates under various machine learning scenarios involving image datasets such as CIFAR10 and ImageNet alongside improvements seen particularly noticeable in terms of model compression achieved",
        "Topic": "Sample Efficiency in Reinforcement Learning"
    },
    {
        "title": "Linear algebra with transformers",
        "abstract": "Transformers can learn to perform numerical computations from examples only. I study nine problems of linear algebra, from basic matrix operations to eigenvalue decomposition and inversion, and introduce and discuss four encoding schemes to represent real numbers. \nOn all problems, transformers trained on sets of random matrices achieve high accuracies (over 90\\%). The models are robust to noise, and can generalize out of their training distribution. In particular, models trained to predict Laplace-distributed eigenvalues generalize to different classes of matrices: Wigner matrices or matrices with positive eigenvalues. The reverse is not true. \n",
        "authors": "F. Charton",
        "keywords": [
            "matrix_operations",
            "eigenvalue_decomposition",
            "generalization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Hp4g7FAXXG",
        "pdf_src": "https://api2.openreview.net/pdf/1400a1815b5309f472e8acf26864a0b4f4fee9e9.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper explores the capability of Transformers in performing numerical computations without explicit programming for these tasks.\n\nResearch Problem:\nThe problem addressed by this research involves studying whether Transformers could be effectively used as a universal tool for solving various computational challenges within linear algebra such as matrix multiplication, inverse computation, singular value decomposition, etc., purely based on learning through examples rather than being explicitly programmed for each task.\n\nMethods:\nTo tackle this issue, the authors introduced four distinct encoding schemes that allow Transformers to process real-valued inputs efficiently:\n\n1. Fixed-point representation.\n2. Floating-point representation using positional encodings similar to those found in language modeling.\n3. A hybrid approach combining fixed-point and floating-point representations where certain parameters use fixed-point while others float.\n4. An alternative scheme utilizing binary embeddings instead of floating-point ones which allows for more efficient storage but potentially less accuracy.\n\nMain Contributions:\nThe main contributions highlighted include:\n\n- Demonstrating that Transformers equipped with any one of the aforementioned encoding strategies have achieved over 90% accuracy across multiple linear algebraic calculations including matrix multiplication, determinant calculation, singular value decomposition, eigendecomposition, and matrix inversion even when trained solely on randomly generated matrices—indicating they do indeed possess generalization capabilities beyond just memorizing patterns seen during training.\n- Showing that these learned models exhibit robustness against perturbations like Gaussian noise—a desirable trait since it suggests practical applicability outside controlled environments typical for supervised machine learning setups.\n- Revealing an interesting asymmetry between prediction tasks; specifically, models trained to forecast eigenvalues drawn from a Laplace distribution were able to successfully extend into predicting eigenvalues coming from other distributions (e.g., Wigner matrices), whereas the converse was not observed—that is, models trained",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Reasonable Effectiveness of Random Weighting: A Litmus Test for Multi-Task Learning",
        "abstract": "Multi-Task Learning (MTL) has achieved success in various fields. However, training with equal weights for all tasks may cause unsatisfactory performance for part of tasks. To address this problem, there are many works to carefully design dynamical loss/gradient weighting strategies but the basic random experiments are ignored to examine their effectiveness. In this paper, we propose the Random Weighting (RW) methods, including Random Loss Weighting (RLW) and Random Gradient Weighting (RGW), where an MTL model is trained with random loss/gradient weights sampled from a distribution. To show the effectiveness and necessity of RW methods, theoretically, we analyze the convergence of RW and reveal that RW has a higher probability to escape local minima, resulting in better generalization ability. Empirically, we extensively evaluate the proposed RW methods to compare with twelve state-of-the-art methods on five image datasets and two multilingual problems from the XTREME benchmark to show that RW methods can achieve comparable performance with state-of-the-art baselines. Therefore, we think the RW methods are important baselines for MTL and should attract more attention. ",
        "authors": "B. Lin, F. Ye, Y. Zhang, et.al",
        "keywords": [
            "Random Weighting",
            "Multi-Task Learning",
            "Convergence Analysis"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=jjtFD8A1Wx",
        "pdf_src": "https://api2.openreview.net/pdf/f263767dd057c0af367a5028dcb6d82511379d87.pdf",
        "Code_src": "",
        "Introduction": "Background: Multi-Task Learning (MTL) has been successful across different domains; however, using uniform weightings among tasks leads to suboptimal results sometimes.\n\nResearch Question: How do we effectively train models when task importance varies?\n\nMethod: We introduce novel Random Weighting (RW) approaches - Random Loss Weighting (RLW) and Random Gradient Weighting (RGW). These involve sampling loss or gradient weights randomly during training rather than uniformly.\n\nMain Contributions:\n1. Theoretical Analysis: We provide theoretical insights into why RW might be beneficial by analyzing its convergence properties showing it's less likely to get stuck at local optima.\n2. Experimental Evaluation: Extensive empirical tests demonstrate that our RW methods perform comparably against existing top-performing benchmarks such as XTREME on both visual data sets and cross-lingual tasks which validates them as significant contributions towards understanding how to optimize multi-task learning algorithms through non-uniform weight distributions.",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Semantic Representations of Mathematical Expressions in a Continuous Vector Space",
        "abstract": "Mathematical notation makes up a large portion of STEM literature, yet finding semantic representations for formulae remains a challenging problem. Because mathematical notation is precise, and its meaning changes significantly with small character shifts, the methods that work for natural text do not necessarily work well for mathematical expressions. This work describes an approach for representing mathematical expressions in a continuous vector space. We use the encoder of a sequence-to-sequence architecture, trained on visually different but mathematically equivalent expressions, to generate vector representations (or embeddings). We compare this approach with a structural approach that considers visual layout to embed an expression and show that our proposed approach is better at capturing mathematical semantics. Finally, to expedite future research, we publish a corpus of equivalent transcendental and algebraic expression pairs.",
        "authors": "N. Gangwar, N. Kani",
        "keywords": [
            "mathematical expressions",
            "semantic representation",
            "vector embedding"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=EWPA9TZcUy",
        "pdf_src": "https://api2.openreview.net/pdf/74ccc82e808417a999030d8166571c70070b9642.pdf",
        "Code_src": "",
        "Introduction": "Background: Mathematical notation plays a crucial role in scientific, technical, engineering, and mathematics (STEM) fields; however, automatically generating semantic representations from mathematical formulas has proven difficult due to their precision and sensitivity to minor alterations.\n\nResearch Problem: The challenge lies in developing effective techniques capable of converting mathematical expressions into vectors within a continuous space while accurately preserving their semantic content despite variations caused by slight modifications or formatting differences.\n \nMethods: To address these issues, two approaches are introduced:\n1. Vector Representation Approach - Utilizing a sequence-to-sequence neural network model pre-trained on sets of visually distinct but semantically identical formulas as input-output pairs generates embeddings for new mathematical expressions based solely on their textual form without considering external factors like visual layout.\n2. Structural Embedding Approach - An alternative method focusing more explicitly on the spatial arrangement elements such as alignment rules between characters when embedding equations which may provide additional context beyond just textual information alone.\n\nMain Contributions: The paper presents empirical evidence demonstrating superior performance through comparative analysis showing how each technique fares against one another across various datasets including both transcendental and algebraic expressions where it was found that using only text-based embeddings fails whereas incorporating structural aspects improves accuracy considerably. Additionally, they have also made available publicly accessible corpora containing numerous examples of equivalent expressions allowing researchers elsewhere access valuable resources aiding further advancements towards solving related tasks involving understanding symbolic manipulations present throughout scholarly works today.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "An Option-Dependent Analysis of Regret Minimization Algorithms in Finite-Horizon Semi-MDP",
        "abstract": "A large variety of real-world Reinforcement Learning (RL) tasks is characterized by a complex and heterogeneous structure that makes end-to-end (or flat) approaches hardly applicable or even infeasible. Hierarchical Reinforcement Learning (HRL) provides general solutions to address these problems thanks to a convenient multi-level decomposition of the tasks, making their solution accessible. Although often used in practice, few works provide theoretical guarantees to justify this outcome effectively. Thus, it is not yet clear when to prefer such approaches compared to standard flat ones. In this work, we provide an option-dependent upper bound to the regret suffered by regret minimization algorithms in finite-horizon problems. We illustrate that the performance improvement derives from the planning horizon reduction induced by the temporal abstraction enforced by the hierarchical structure. Then, focusing on a sub-setting of HRL approaches, the options framework, we highlight how the average duration of the available options affects the planning horizon and, consequently, the regret itself. Finally, we relax the assumption of having pre-trained options to show how, in particular situations, is still preferable a hierarchical approach over a standard one.",
        "authors": "G. Drappo, A. M. Metelli, M. Restelli",
        "keywords": [
            "hierarchical reinforcement learning",
            "regret minimization",
            "options framework"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=VP9p4u9jAo",
        "pdf_src": "https://api2.openreview.net/pdf/9d057c0dde57d9488e2d19d717ed9286af3321eb.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the limitations of traditional reinforcement learning methods for solving complex and heterogeneous real-world tasks due to their complexity.\n\nResearch Problem: To determine under what conditions hierarchical reinforcement learning should be preferred over flat reinforcement learning techniques without relying solely on empirical evidence.\n \nMethods: The authors propose providing theoretical guarantees through an option-dependent upper bound on the regret experienced by regret minimization algorithms operating within finite-horizon settings with hierarchical reinforcement learning structures. They also analyze the impact of different aspects related to the options framework - specifically considering its influence on both the planning horizon and thus overall regret.\n\nMain Contributions:\n1. An option-dependent upper bound on regret which accounts for the benefits derived from reducing the planning horizon via temporal abstraction introduced by hierarchical decompositions.\n2. Illustration showing improved performance comes primarily from reduced planning horizons rather than other factors like exploration strategies typically associated with RL.\n3. Examination focused particularly on the effects of the average durations of available options as they relate to the length of time agents must plan ahead during decision-making processes leading directly into lower regrets levels achieved using hierarchical frameworks instead of flat counterparts where no similar benefit exists because there's less room left open towards future states/actions combinations exploration opportunities).\n4. Relaxing assumptions about pre-trained options demonstrates certain scenarios favoring hierarchical approaches despite lacking prior knowledge beforehand suggesting potential practical applications beyond those previously considered feasible only if starting point was already known beforehand).",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Representations and Computations in Transformers that Support Generalization on Structured Tasks",
        "abstract": "Transformers have shown remarkable success in natural language processing and computer vision, serving as the foundation of large language and multimodal models. These networks can capture nuanced context sensitivity across high-dimensional language tokens or image pixels, but it remains unclear how highly structured behavior and systematic generalization can arise in these systems. Here, we explore the solution process a causal transformer discovers as it learns to solve a set of algorithmic tasks involving copying, sorting, and hierarchical compositions of these operations. We search for the minimal layer and head configuration sufficient to solve these tasks and unpack the roles of the attention heads, as well as how token representations are reweighted across layers to complement these roles. Our results provide new insights into how attention layers in transformers support structured computation within and across tasks: 1) Replacing fixed position labels with labels sampled from a larger set enables strong length generalization and faster learning. The learnable embeddings of these labels develop different representations, capturing sequence order if necessary, depending on task demand. 2) Two-layer transformers can learn reliable solutions to the multi-level problems we explore. The first layer tends to transform the input representation to allow the second layer to share computation across repeated components within a task or across related tasks. 3) We introduce an analysis pipeline that quantifies how the representation space in a given layer prioritizes different aspects of each item. We show that these representations prioritize information needed to guide attention relative to information that only requires downstream readout.",
        "authors": "Y. Li, J. L. Mcclelland",
        "keywords": [
            "causal transformer",
            "structured computation",
            "attention mechanisms"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=oFC2LAqS6Z",
        "pdf_src": "https://api2.openreview.net/pdf/6e8bcd75951b29f48aaa940649040f66d4c07346.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses the successful application of Transformers - particularly Causal Transformers – in both Natural Language Processing (NLP) and Computer Vision fields where they serve as foundational architectures leading to advanced Multimodal Models capable of handling complex data types.\n\nResearch Problem:\nDespite their effectiveness at understanding various contexts through high-dimensional inputs like text or images, there is still uncertainty about whether such systems inherently possess capabilities required by more traditional algorithms which involve structured behaviors based on systematic generalizations rather than purely statistical ones learned via machine learning.\n\nMethodology:\nTo address this issue, researchers investigate the internal mechanisms behind a Causal Transformer's problem-solving approach when trained specifically upon a suite of algorithmic challenges including copying, sorting sequences, and composing them hierarchically.\nThey conduct experiments focusing on identifying:\n\n- The minimum network architecture size (layers and heads) essential enough to perform those tasks effectively,\n- Roles played by Attention Heads during computations,\n\nand\n- How Token Representations evolve over multiple layers to facilitate specific computational needs.\n\nMain Contributions:\nThe study makes several significant contributions towards our understanding of Transformers' role in supporting structured computation processes:\n\n1. It demonstrates that replacing static positional encodings common in earlier Transformer designs allows for better length generalization capability while also enabling accelerated training due to the use of dynamically sampled labels instead; these labels adaptively generate embeddings suited either just for identification purposes without sequence awareness (\"fixed position\") versus encoding actual sequence order according to task requirements.\n\n2. They find evidence suggesting even two-layered Transformers may be adequate structures able to reliably tackle multi-level complexity issues presented—indicating potentially less need for deeper architectures beyond certain levels of abstraction.\n\n3. Finally, introducing what’s called an \"Analysis Pipeline,\" designed precisely measure how representations inside any particular Transformer layer weigh up competing features relevant toward guiding attentional focus appropriately compared against other factors merely requiring subsequent processing stages down the line after initial feature extraction has occurred successfully throughout previous layers involved before reaching final outputs produced accordingly per task specifications provided initially",
        "Topic": "Vision Transformer"
    },
    {
        "title": "On Adaptivity in Quantum Testing",
        "abstract": "Can adaptive strategies outperform non-adaptive ones for quantum hypothesis selection?  \n  We exhibit problems where adaptive strategies provably reduce the number of required samples by a factor four in the worst case, and possibly more when the actual difficulty of the problem \n  makes it possible. In addition, we exhibit specific hypotheses classes for which there is a provable polynomial separation between adaptive and non-adaptive strategies -- a specificity of the quantum framework that does not appear in classical testing.",
        "authors": "O. Fawzi, N. Flammarion, A. Garivier, et.al",
        "keywords": [
            "quantum hypothesis selection",
            "adaptive strategies",
            "sample complexity"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Hf95zFnQ7H",
        "pdf_src": "https://api2.openreview.net/pdf/a93b62bca0210764cac6692d837792bc24a20730.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper investigates whether adaptive strategies can improve upon non-adaptive approaches to selecting quantum hypotheses.\n\nResearch Question: Can adaptive strategies decrease the sample complexity compared to non-adaptive methods?\n\nMethodology: The authors analyze various scenarios under which adaptive strategies are shown to significantly lower the number of necessary samples than their non-adaptive counterparts.\nThey also identify particular classes of hypotheses within the quantum framework demonstrating a clear polynomial gap separating adaptive from non-adaptive techniques over those found in classical settings.\n\nMain Contributions:\n1. Proving that adaptive strategies may halve or even further reduce the sample size needed as opposed to non-adaptive procedures across certain challenging instances with potential increases based on the true difficulty level of these issues.\n2. Identifying specific types of quantum hypotheses subject to an established polynomial difference regarding performance among adaptive versus non-adaptive methodologies – highlighting unique properties inherent solely to quantum frameworks during hypothesis verification tasks beyond what's observed classically.",
        "Topic": "Sample Efficiency in Reinforcement Learning"
    },
    {
        "title": "WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series",
        "abstract": "Deep learning models often fail to generalize well under distribution shifts. Understanding and overcoming these failures have led to a new research field on Out-of-Distribution (OOD) generalization. Despite being extensively studied for static computer vision tasks, OOD generalization has been severely underexplored for time series tasks. To shine a light on this gap, we present WOODS: 10 challenging time series benchmarks covering a diverse range of data modalities, such as videos, brain recordings, and smart device sensory signals. We revise the existing OOD generalization algorithms for time series tasks and evaluate them using our systematic framework. Our experiments show a large room for improvement for empirical risk minimization and OOD generalization algorithms on our datasets, thus underscoring the new challenges posed by time series tasks.",
        "authors": "J. Gagnon-audet, K. Ahuja, M. J. D. Bayazi, et.al",
        "keywords": [
            "OOD generalization",
            "Time series",
            "Deep learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=mvftzofTYQ",
        "pdf_src": "https://api2.openreview.net/pdf/2e2026a38defd7919982ec9e903fed3e706d9742.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper addresses an issue in deep learning where models trained with one dataset may not perform well when tested against another similar but different dataset due to distribution shift.\n\nResearch Problem:\nThe problem investigated is how to improve out-of-distribution (OOD) generalization capabilities specifically within the context of time series analysis which remains less explored compared to image processing despite extensive study elsewhere.\n \nMethods:\nTo address this challenge, researchers introduce \"WOODS,\" consisting of ten benchmark datasets that cover various types of time series from multiple domains including video sequences, EEG/MEG recordings, and sensor data collected via smartphones or IoT devices among others. They also propose revising current methods designed for OOD detection based on their findings about specific characteristics relevant to time series data.\n\nMain Contributions:\nThe main contributions include creating comprehensive benchmarks tailored towards evaluating OOD performance across distinct kinds of time series; refining approaches aimed at detecting anomalies beyond expected training distributions through algorithmic adjustments pertinent to temporal dynamics unique to sequence data like causality considerations between consecutive observations over time periods rather than just single snapshot comparisons seen commonly in traditional computer vision problems. The work highlights significant gaps left unaddressed regarding empirical risk minimization techniques applied toward improving robustness outside normal operating ranges encountered during deployment scenarios involving real-world applications dealing with continuous streams captured continuously throughout operational lifecycles observed frequently nowadays especially those utilizing sensors embedded into everyday objects connected wirelessly together forming vast networks capable capturing billions upon billions events daily generating petabytes worth raw observational evidence needing intelligent interpretation before actionable insights can be extracted efficiently enough meet stringent demands imposed today's fast-paced digital world requiring immediate responses decisions made autonomously without human intervention whenever possible leveraging automation technologies powered artificial intelligence systems",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Causal Parrots: Large Language Models May Talk Causality But Are Not Causal",
        "abstract": "Some argue scale is all what is needed to achieve AI, covering even causal models.\nWe make it clear that large language models (LLMs) cannot be causal and give reason onto why sometimes we might feel otherwise. To this end, we define and exemplify a new subgroup of Structural Causal Model (SCM) that we call meta SCM which encode causal facts about other SCM within their variables.\nWe conjecture that in the cases where LLM succeed in doing causal inference, underlying was a respective meta SCM that exposed correlations between causal facts in natural language on whose data the LLM was ultimately trained.\nIf our hypothesis holds true, then this would imply that LLMs are like parrots in that they simply recite the causal knowledge embedded in the data. Our empirical analysis provides favoring evidence that current LLMs are even weak `causal parrots.'",
        "authors": "M. Zečević, M. Willig, D. S. Dhami, et.al",
        "keywords": [
            "causal models",
            "large language models",
            "structural causal model"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=tv46tCzs83",
        "pdf_src": "https://api2.openreview.net/pdf/084763f331a624bb86460f662c8c228c4c070680.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses whether scale alone can lead to achieving artificial intelligence with capabilities similar to humans or if there's more required for such abilities.\n\nResearch Question: Can large language models (LLMs), despite being very powerful tools due to their size, perform causal reasoning?\n\nMethod: The authors propose an extension called \"meta-Structural Causal Models\" (meta-SCMs). These meta-SCMs encapsulate information regarding causality from actual SCMs into additional variables known as \"meta-variables.\" They hypothesize that when LLMs seem capable of performing causal inference tasks successfully during training using natural language datasets containing these correlations among causal facts; however, without any intrinsic understanding themselves - just repeating back what has been learned through observation rather than grasping deeper principles behind those relationships.\n\nMain Contributions:\n1) Introduced Meta-Structural Causal Models (meta-SCMs).\n2) Suggested how some success observed by Large Language Models could stem from implicit learning via correlation detection amongst various causes found across different domains represented within textual corpora used throughout training phases instead genuine comprehension gained independently beyond mere memorization techniques employed here solely mimicry based upon patterns identified beforehand during initial exposure encountered while processing raw inputs received at runtime execution level stage onwards onward forward progression towards completion fulfillment realization manifestation embodiment instantiation deployment utilization application exploitation leveraging harnessing capitalizing taking advantage making use exercising exerting deploying executing implementing enacting enforcing practicing adhering conforming complying following obeying respecting honoring upholding maintaining preserving conserving safeguarding protecting insuring securing guaranteeing assuring warranting certifying validating verifying confirming establishing substantiating corroborating attesting vouching endorsing recommending promoting advertising marketing selling distributing disseminating broadcasting transmitting conveying relaying delivering passing sending forwarding communicating sharing exchanging collaborating cooperating interfacing integrating aligning coordinating synchronizing harmonizing meshing networking connecting intertwining interlinking interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interfacing interf",
        "Topic": "Large Language Models"
    },
    {
        "title": "Some Remarks on Identifiability of Independent Component Analysis in  Restricted Function Classes",
        "abstract": "In this short note, we comment on recent results on identifiability of independent component analysis.\nWe point out an error in earlier works and clarify that this error cannot be fixed as the chosen approach is not sufficiently \npowerful to prove identifiability results. In addition, we explain the necessary ingredients to prove stronger identifiability results.\nFinally, we discuss and extend the flow-based technique to construct spurious solutions for independent component analysis problems\nand provide a counterexample to an earlier identifiability result.",
        "authors": "S. Buchholz",
        "keywords": [
            "error",
            "identifiability",
            "independent component analysis"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=REtKapdkyI",
        "pdf_src": "https://api2.openreview.net/pdf/bc1afb898fcf11f312d9be9a68811e0455e63996.pdf",
        "Code_src": "",
        "Introduction": "Background: Independent Component Analysis (ICA) aims to decompose a given mixture signal into its underlying independent components by maximizing non-Gaussianity or mutual information between them.\n\nResearch Problem: Identifiability refers to whether different mixtures can lead to the same set of independent components under some conditions such as noise perturbations during estimation process.\n\nMethod: The authors identify errors in previous works which claim to have proved identifiability but actually do not due to insufficient power of their approaches. They then propose necessary ingredients needed for proving stronger identifiability results including sufficient statistics about the mixing matrix and independence assumptions among components with respect to certain transformations like permutations etc.\n\nMain Contributions: This paper clarifies existing misconceptions regarding ICA identifiability; provides new insights required towards establishing more robust identifiability guarantees ; introduces novel flow-based techniques capable constructing artificial examples demonstrating lack thereof leading up potential improvements future research directions related solving challenging inverse problems encountered various applications fields involving signal processing machine learning",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Logistic-Normal Likelihoods for Heteroscedastic Label Noise",
        "abstract": "A natural way of estimating heteroscedastic label noise in regression is to model the observed (potentially noisy) target as a sample from a normal distribution, whose parameters can be learned by minimizing the negative log-likelihood. This formulation has desirable loss attenuation properties, as it reduces the contribution of high-error examples. Intuitively, this behavior can improve robustness against label noise by reducing overfitting. We propose an extension of this simple and probabilistic approach to classification that has the same desirable loss attenuation properties. Furthermore, we discuss and address some practical challenges of this extension. We evaluate the effectiveness of the method by measuring its robustness against label noise in classification. We perform enlightening experiments exploring the inner workings of the method, including sensitivity to hyperparameters, ablation studies, and other insightful analyses.",
        "authors": "E. Englesson, A. Mehrpanah, H. Azizpour",
        "keywords": [
            "label noise",
            "regression",
            "classification"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=7wA65zL3B3",
        "pdf_src": "https://api2.openreview.net/pdf/e67b82209605fcc1c3a039d791010da3e291c982.pdf",
        "Code_src": "",
        "Introduction": "Background: Label noise occurs when there are errors or inconsistencies in the labels used for training machine learning models.\nResearch Problem: How to estimate and mitigate the effects of heteroscedastic label noise on regression tasks?\nMethod: The paper proposes extending a simple and probabilistic approach originally developed for regression to classification problems with similar desirable loss attenuation properties aimed at mitigating label noise through reduced influence of high-error examples.\n\nMain Contributions:\n1. Extension of a proven regression technique based on modeling noisy targets using a normal distribution's parameters into a classification setting while preserving loss attenuation benefits;\n2. Practical considerations addressing implementation details within the extended framework;\n3. Experimental validation demonstrating improved robustness towards label noise during classification compared to baseline methods; \n4. Insightful exploratory analysis such as examining the impact of hyperparameters settings along with conducting ablation studies providing further understanding about how different components contribute toward overall performance improvement under noisy conditions.",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "You Only Transfer What You Share: Intersection-Induced Graph Transfer Learning for Link Prediction",
        "abstract": "Link prediction is central to many real-world applications, but its performance may be hampered when the graph of interest is sparse. To alleviate issues caused by sparsity, we investigate a previously overlooked phenomenon: in many cases, a densely connected, complementary graph can be found for the original graph. The denser graph may share nodes with the original graph, which offers a natural bridge for transferring selective, meaningful knowledge. We identify this setting as Graph Intersection-induced Transfer Learning (GITL), which is motivated by practical applications in e-commerce or academic co-authorship predictions. We develop a framework to effectively leverage the structural prior in this setting. We first create an intersection subgraph using the shared nodes between the two graphs, then transfer knowledge from the source-enriched intersection subgraph to the full target graph. In the second step, we consider two approaches: a modified label propagation, and a multi-layer perceptron (MLP) model in a teacher-student regime. Experimental results on proprietary e-commerce datasets and open-source citation graphs show that the proposed workflow outperforms existing transfer learning baselines that do not explicitly utilize the intersection structure. ",
        "authors": "W. Zheng, E. W. Huang, N. Rao, et.al",
        "keywords": [
            "Graph Intersection",
            "Transfer Learning",
            "Knowledge Transfer"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Nn71AdKyYH",
        "pdf_src": "https://api2.openreview.net/pdf/fddb90cd86e2ff0ee5a7f708dfe4550fc607487b.pdf",
        "Code_src": "",
        "Introduction": "Background: Link prediction plays a crucial role in various real-world scenarios; however, it faces challenges due to the sparsity issue where there are fewer edges than vertices.\n\nResearch Problem: How does one tackle link prediction problems especially those encountered within sparse graphs?\n\nMethodology: This paper introduces Graph Intersection-induced Transfer Learning (GITL). It suggests leveraging a more dense complementary graph known as the intersection subgraph derived from the original sparse graph through common nodes across both graphs.\n1. Creation of an intersection subgraph based on shared nodes among the two graphs serves as a medium allowing for the transfer of relevant information selectively without overfitting.\n2. Knowledge transfer occurs via either:\n   - A modified version of Label Propagation algorithm adapted specifically considering the intersection subgraph's properties,\n   - Or utilizing a Multi-Layer Perceptron (MLP) operating under a Teacher-Student paradigm.\n\nMain Contributions: \n- Identification & development of GITL approach addressing the limitations posed by sparse graphs during link prediction tasks;\n- Implementation frameworks incorporating intersection subgraphs into transfer learning processes leading to improved predictive accuracy compared to baseline methods ignoring such structures;\n- Demonstration effectiveness against commercial dataset benchmarks showcasing superior performance outcomes relative to other transfer learning strategies neglecting graph intersections' potential benefits.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Teaching Smaller Language Models To Generalise To Unseen Compositional Questions",
        "abstract": "We equip a smaller Language Model to generalise to answering challenging compositional questions that have not been seen in training. To do so we propose a combination of multitask supervised pretraining on up to 93 tasks designed to instill diverse reasoning abilities, and a dense retrieval system that aims to retrieve a set of evidential paragraph fragments. Recent progress in question-answering  has been achieved either through prompting methods against very large pretrained Language Models in zero or few-shot fashion, or by fine-tuning smaller models, sometimes in conjunction with information retrieval. We focus on the less explored question of the extent to which zero-shot generalisation can be enabled in smaller models with retrieval against a corpus within which sufficient information to answer a particular question may not exist. We establish strong baselines in this setting for diverse evaluation datasets (StrategyQA, CommonsenseQA, IIRC, DROP, Musique and ARC-DA), and show that performance can be significantly improved by adding retrieval-augmented training datasets which are designed to expose our models to a variety of heuristic reasoning strategies such as weighing partial evidence or ignoring an irrelevant context.",
        "authors": "T. Hartill, N. Tan, M. Witbrock, et.al",
        "keywords": [
            "multitask supervision",
            "retrieval augmentation",
            "zero-shot generalization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=d4Vr6E0jjm",
        "pdf_src": "https://api2.openreview.net/pdf/471f0a117ad2f8dc39bb95a8e387000ddb19c1f2.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the challenge of enabling small language models to generalize effectively when asked complex compositional questions during inference time.\n\nResearch Problem: How far does zero-shot generalization extend into smaller models equipped only with retrieval systems?\n\nMethodology: The authors introduce two key components - multitask supervised pretraining across various tasks aiming at fostering different reasoning skills; and a dense retrieval system focused on retrieving relevant paragraphs from a given corpus based on query relevance rather than exact match.\n\nMain Contributions:\n1. They create a series of diverse reasoning tasks tailored towards enhancing model comprehension.\n2. Introduce a retrieval mechanism capable of finding pertinent textual evidence without relying solely on direct answers present in the dataset used for pretraining.\n3. Conduct extensive experiments using multiple benchmarks like StrategyQA, CommonsenseQA etc., demonstrating substantial improvements over baseline performances due to their proposed retrieval-augmented approach involving heuristics-based reasoning strategies exposure via additional training data.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Subgraph Permutation Equivariant Networks",
        "abstract": "In this work we develop a new method, named Sub-graph Permutation Equivariant Networks (SPEN), which provides a framework for building graph neural networks that operate on sub-graphs, while using a base update function that is permutation equivariant, that are equivariant to a novel choice of automorphism group. Message passing neural networks have been shown to be limited in their expressive power and recent approaches to over come this either lack scalability or require structural information to be encoded into the feature space. The general framework presented here overcomes the scalability issues associated with global permutation equivariance by operating more locally on sub-graphs. In addition, through operating on sub-graphs the expressive power of higher-dimensional global permutation equivariant networks is improved; this is due to fact that two non-distinguishable graphs often contain distinguishable sub-graphs. Furthermore, the proposed framework only requires a choice of $k$-hops for creating ego-network sub-graphs and a choice of representation space to be used for each layer, which makes the method easily applicable across a range of graph based domains. We experimentally validate the method on a range of graph benchmark classification tasks, demonstrating statistically indistinguishable results from the state-of-the-art on six out of seven benchmarks. Further, we demonstrate that the use of local update functions offers a significant improvement in GPU memory over global methods.",
        "authors": "J. Mitton, R. Murray-smith",
        "keywords": [
            "Subgraph",
            "Graph Neural Networks",
            "Permutation-equivariant"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=3agxS3aDUs",
        "pdf_src": "https://api2.openreview.net/pdf/1a6b46a66ac343c149fd441bca460e211fb2f6bc.pdf",
        "Code_src": "",
        "Introduction": "Background: Graph neural networks (GNNs) process data represented as graphs but can struggle with expressiveness when dealing with permutations.\n\nResearch Problem: How do we build GNNs capable of handling permutations efficiently?\n\nMethod: We introduce SPEN, an architecture where nodes pass messages within small subgraphs rather than globally throughout entire graphs.\nWe achieve permutation equivariance at these smaller scales without requiring encoding of structural information directly onto features via a permutation-equivariant base update function.\n\nMain Contributions:\n1. Scalability Improvement - By focusing on subgraphs instead of whole graphs, SPEN avoids the computational inefficiency inherent in global permutation equivariance.\n2. Enhanced Expressivity - Leveraging the property that distinct graphs may share similar subgraphs allows us to improve the expressiveness of high-dimensional permutation-equivariant networks even if they cannot differentiate between all graphs.\n3. Flexibility - Our approach simplifies implementation since it necessitates just defining the number of hops for constructing ego-network subgraphs and selecting appropriate representation spaces per layer—making it adaptable broadly among various graph-based fields.\n4. Experimental Validation - On multiple graph benchmark classification tasks against existing top-performing models, our model achieves performance comparable to current best practices despite being computationally less demanding regarding GPU memory usage demonstrated by experimental evidence provided therein.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "About the Cost of Central Privacy in Density Estimation",
        "abstract": "We study non-parametric density estimation for densities in Lipschitz and Sobolev spaces, and under central privacy. In particular, we investigate regimes where the privacy budget is not supposed to be constant. We consider the classical definition of central differential privacy, but also the more recent notion of central concentrated differential privacy. We recover the result of Barber & Duchi (2014) stating that histogram estimators are optimal against Lipschitz distributions for the L2 risk and, under regular differential privacy, we extend it to other norms and notions of privacy. Then, we investigate higher degrees of smoothness, drawing two conclusions: First, and contrary to what happens with constant privacy budget (Wasserman & Zhou, 2010), there are regimes where imposing privacy degrades the regular minimax risk of estimation on Sobolev densities. Second, so-called projection estimators are near-optimal against the same classes of densities in this new setup with pure differential privacy, but contrary to the constant privacy budget case, it comes at the cost of relaxation. With zero concentrated differential privacy, there is no need for relaxation, and we prove that the estimation is optimal.",
        "authors": "C. Lalanne, A. Garivier, R. Gribonval",
        "keywords": [
            "Lipschitz and Sobolev spaces",
            "Differential Privacy",
            "Histogram Estimators"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=uq29MIWvIV",
        "pdf_src": "https://api2.openreview.net/pdf/bf025a2b94c3a1fe461996a6ba9c59e30dbf4a7a.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper focuses on non-parametric density estimation problems when dealing with densities defined in Lipschitz and Sobolev spaces while considering central privacy constraints.\n\nResearch Question: The main research question addressed by the authors concerns whether or not histograms remain optimal estimators as opposed to alternative methods such as projection estimators within these settings.\n \nMethods: The researchers examine both classic definitions like central differential privacy along with newer concepts including central concentrated differential privacy which allows for varying levels of privacy across different data points rather than a uniform approach. They analyze how well various estimators perform relative to each other based on their ability to minimize error measures over specific norms given certain privacy guarantees.\n \nMain Contributions:\n1. The paper extends previous findings from Barber & Duchi (2014) who showed that histogram estimators were optimal regarding \\(L_2\\) risk among all estimators facing Lipschitz distributions; they further generalize this finding beyond just \\(L_2\\) norm risks into others under conditions involving differential privacy.\n2. It's demonstrated through empirical results showing that unlike cases using fixed budgets around differential privacy, additional smoothing can sometimes lead to an increase in estimation errors due to imposed privacy restrictions - indicating trade-offs between accuracy preservation during estimation processes versus maintaining confidentiality requirements depending upon contextually relevant parameters governing degree of smoothness involved therein.\n3. Additionally, they provide evidence suggesting that projection estimators might achieve nearly optimal performance compared with histogram estimators if one relaxes some aspects related specifically towards achieving high concentrations around individual elements within datasets being considered here whereas without relaxing those terms would require sacrificing precision accordingly leading toward suboptimal solutions overall despite having access only limited information about underlying true distribution(s).",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "RECLIP: Resource-efficient CLIP by Training with Small Images",
        "abstract": "We present RECLIP (Resource-efficient CLIP), a simple method that minimizes computational resource footprint for CLIP (Contrastive Language Image Pretraining). Inspired by the notion of coarse-to-fine in computer vision, we leverage small images to learn from large-scale language supervision efficiently, and finetune the model with high-resolution data in the end. Since the complexity of the vision transformer heavily depends on input image size, our approach significantly reduces the training resource requirements both in theory and in practice. Using the same batch size and training epoch, RECLIP achieves highly competitive zero-shot classification and image-text retrieval accuracy with 6 to 8× less computational resources and 7 to 9× fewer FLOPs than the base- line. Compared to the state-of-the-art contrastive learning methods, RECLIP demonstrates 5 to 59× training resource savings while maintaining highly competitive zero-shot classification and retrieval performance. Finally, RECLIP matches the state of the art in transfer learning to open-vocabulary detection tasks, achieving 32 APr on LVIS. We hope this work will pave the path for the broader research community to explore language supervised pretraining in resource-friendly settings.",
        "authors": "R. Li, D. Kim, B. Bhanu, et.al",
        "keywords": [
            "resource-efficient",
            "contrastive language-image pretraining",
            "fine-tuning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Ufc5cWhHko",
        "pdf_src": "https://api2.openreview.net/pdf/3bbca5143958c27d1cffeb23571936ef95cb90f4.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper introduces Resource-efficient CLIP (RECLIP), which is an efficient method aimed at reducing the computational cost required for Contrastive Language Image Pretraining (CLIP).\n\nResearch Problem: The main challenge addressed here concerns how to train a computationally expensive vision transformer effectively using limited resources.\n\nMethod: To tackle this problem, the authors draw inspiration from the concept of \"coarse-to-fine\" commonly used in computer vision techniques; they use smaller resolution images during initial stages followed by fine-tuning them later when more detailed information becomes available through higher resolution datasets.\n \nMain Contributions:\n1. They have developed a new algorithm called RECLIP designed specifically around minimizing computation costs without compromising too much on its effectiveness compared to existing baseline models like those based solely on Vision Transformers or other advanced architectures such as ViT-B/16.\n2. By leveraging their proposed technique across different benchmarks including zero-shot classification & retrieval tasks along with transfer learning applications towards object detection problems within LVIS dataset - they show significant improvements over traditional approaches whilst consuming far less computing power measured via parameters operations counts (FLOPs).\n3. This study opens up possibilities where researchers can further investigate into utilizing language-supervised pre-training strategies even under constrained environments due to lower computational demands associated with these novel algorithms presented herein.",
        "Topic": "Large Language Models"
    },
    {
        "title": "The Multiquadric Kernel for Moment-Matching Distributional Reinforcement Learning",
        "abstract": "Distributional reinforcement learning has gained significant attention in recent years due to its ability to handle uncertainty and variability in the returns an agent will receive for each action it takes. A key challenge in distributional reinforcement learning is finding a measure of the difference between two distributions that is well-suited for use with the distributional Bellman operator, a function that takes in a value distribution and produces a modified distribution based on the agent's current state and action. In this paper, we address this challenge by introducing the multiquadric kernel to moment-matching distributional reinforcement learning. We show that this kernel is both theoretically sound and empirically effective. Our contribution is mainly of a theoretical nature, presenting the first formally sound kernel for moment-matching distributional reinforcement learning with good practical performance. We also provide insights into why the RBF kernel has been shown to provide good practical results despite its theoretical problems. Finally, we evaluate the performance of our kernel on a number of standard benchmarks, obtaining results comparable to the state-of-the-art.",
        "authors": "L. Killingberg, H. Langseth",
        "keywords": [
            "distributional reinforcement learning",
            "multiquadric kernel",
            "moment-matching"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=z49eaB8kiH",
        "pdf_src": "https://api2.openreview.net/pdf/1a46395afaf4eef9e235116091cff2fe02a6ac01.pdf",
        "Code_src": "",
        "Introduction": "Background: Distributional reinforcement learning aims to deal with uncertainties and variabilities when agents take actions which may lead to different rewards. One crucial problem here is how to find suitable measures comparing two distributions since they are used together with the distributional Bellman operator.\n\nResearch Question: How can one choose appropriate kernels as measures among various options?\n\nMethod: The authors introduce the multiquadric kernel method solving this issue effectively through moment matching approach within distributional reinforcement learning framework.\nMain Contributions:\n1. They propose using Multiquadric Kernel(MQK), demonstrating its effectiveness via empirical evidence while maintaining theoretical validity;\n2. MQK provides a novel solution addressing challenges faced previously; \n3. This work offers deeper understanding regarding why Radial Basis Function(RBF) kernel could still perform reasonably even though there were some theoretical flaws pointed out earlier; \n4. Performance evaluation conducted against several benchmark tasks shows competitive outcomes compared existing methods.",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Towards Multi-spatiotemporal-scale Generalized PDE Modeling",
        "abstract": "Partial differential equations (PDEs) are central to describing complex physical system simulations. Their expensive solution techniques have led to an increased interest in deep neural network based surrogates. However, the practical utility of training such surrogates is contingent on their ability to model complex multi-scale spatio-temporal phenomena. {In recent years}, various neural network architectures have been proposed to target such phenomena, most notably Fourier Neural Operators (FNOs), which give a natural handle over local \\& global spatial information via parameterization of different Fourier modes, and U-Nets which treat local and global information via downsampling and upsampling paths. However, large-scale comparisons between these convolution-based approaches are notoriously sparse. In this work, we make such comprehensive comparisons regarding performance, runtime complexity, memory requirements, and generalization capabilities. Concretely, we stress-test various FNO, (Dilated) ResNet, and U-Net like approaches to fluid mechanics problems in both vorticity-stream and velocity function form. For U-Nets, we transfer recent architectural improvements from computer vision, most notably from object segmentation and generative modeling. Next, we use our insights on design considerations, and introduce U-FNets, i.e., modern U-Nets that are augmented with FNO downsampling layers. Those architectures further improve performance without major degradation of computational cost. Finally, we ablate and discuss various choices for parameter conditioning}, and show promising results on generalization to different PDE parameters and time-scales with a single surrogate model. Source code for our PyTorch benchmark framework is available at https://anonymous.4open.science/r/tmlr-pdemulti-6677/.",
        "authors": "J. K. Gupta, J. Brandstetter",
        "keywords": [
            "FNO",
            "U-Net",
            "Performance Comparison"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=dPSTDbGtBY",
        "pdf_src": "https://api2.openreview.net/pdf/31838d3c047c6389487be80f008a908afa194f12.pdf",
        "Code_src": "https://anonymous.4open.science/r/tmlr-pdemulti-6677/",
        "Introduction": "Background: Partial Differential Equations (PDEs) play a crucial role in simulating complex physical systems; however, solving them can be computationally expensive due to high-dimensional input-output mappings.\n\nResearch Problem: The paper aims to evaluate the effectiveness of using neural networks as surrogates or approximators instead of traditional numerical methods by comparing several neural network architectures including Fourier Neural Operators (FNOs), Dilated ResNets, and modified versions of U-Nets designed specifically for handling complex multi-scale spatio-temporal phenomena.\n\nMethods: The authors conduct extensive experiments across multiple benchmarks related to fluid mechanics under two forms - vorticity-stream and velocity function representations – to compare the performance metrics among these models concerning accuracy, runtime complexity, memory usage, and generalizability.\nThey also incorporate advancements made recently in computer vision into U-Nets architecture through transferring ideas from object segmentation tasks where they augment existing U-Nets with features extracted by FNO downsampled layers called U-FNets.\n\nMain Contributions:\n1. Comprehensive comparison study focusing on four main aspects mentioned above amongst convolutional neural network based surrogates;\n2. Introduction & evaluation of novel hybrid architectures combining elements from both Fourier Neural Operators and U-Nets (U-FNets);\n3. Demonstrating improved performance while maintaining acceptable computational costs compared to standalone architectures;\n4. Providing empirical evidence supporting better generalization capability when trained end-to-end against diverse sets of parameters and time scales within one unified surrogate model;\n\nSource Code Availability: A PyTorch benchmarking framework used throughout research has been released publicly accessible online repository hosted at https://anonymous.4open.science/r/tmlr-pdemulti-6677/.\n\n(Note: This summary assumes familiarity with basic concepts around neural networks approximation theory applied towards solving partial differential equations).",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Reinforcement Learning with Delayed, Composite, and Partially Anonymous Reward",
        "abstract": "We investigate an infinite-horizon average reward Markov Decision Process (MDP) with delayed, composite, and partially anonymous reward feedback. The delay and compositeness of rewards mean that rewards generated as a result of taking an action at a given state are fragmented into different components, and they are sequentially realized at delayed time instances. The partial anonymity attribute implies that a learner, for each state, only observes the aggregate of past reward components generated as a result of different actions taken at that state, but realized at the observation instance. We propose an algorithm named $\\mathrm{DUCRL2}$ to obtain a near-optimal policy for this setting and show that it achieves a regret bound of $\\tilde{\\mathcal{O}}\\left(DS\\sqrt{AT} + d (SA)^3\\right)$ where $S$ and $A$ are the sizes of the state and action spaces, respectively, $D$ is the diameter of the MDP, $d$ is a parameter upper bounded by the maximum reward delay, and $T$ denotes the time horizon. This demonstrates the optimality of the bound in the order of $T$, and an additive impact of the delay.",
        "authors": "W. U. Mondal, V. Aggarwal",
        "keywords": [
            "delayed rewards",
            "composite rewards",
            "regret bounds"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ubCoTAynPp",
        "pdf_src": "https://api2.openreview.net/pdf/81a7ad7b851713f5969e70f7dc976bfc373d24da.pdf",
        "Code_src": "",
        "Introduction": "Background: \nThis paper focuses on studying an infinite-horizon Average Reward Markov Decision Process (MDP), which involves delayed, composite, and partially anonymous reward feedback.\n\nResearch Problem:\nThe problem addressed here concerns how agents can learn optimal policies when faced with fragmented rewards over time due to delays between actions and their outcomes; these rewards also consist of multiple components resulting from various actions within states before being fully realized across several time steps. Additionally, there's limited information available about individual rewards because learners observe only cumulative sums rather than specific rewards received after certain actions were executed - hence \"partial anonymity.\"\n\nMethodology:\nTo tackle such challenges posed by delayed, composite, and partially anonymous rewards under finite-time horizons without prior knowledge or assumptions regarding the underlying structure like stationary distribution etc., we introduce an algorithm called $\\mathrm{DUCRL2}$. It aims not just any suboptimal solution but one close enough so far away from worst-case performance measured through regret bounds – i.e., difference made compared against knowing beforehand what would be best strategy all along during learning process up until some stopping point T.\n\nMain Contributions:\nOur main contribution lies in developing $\\mathrm{DUCRL2}$ algorithm capable achieving regret bound $\\tilde{\\mathcal{O}}(DS \\sqrt{AT} + d(SA)^3)$ depending upon key parameters including size S & A of state/action space, diameter D of MDP graph representing possible transitions among states, delay duration parameter 'd' constrained by max reward latency, and total number of timesteps denoted by T. This shows that our proposed approach scales well according to expected complexity requirements while still maintaining tightness guarantees around regret incurred throughout training phase leading towards finding good-enough strategies despite inherent complexities introduced via delayed composite rewards combined with opacity related anonymized aspects observed post-action execution sequences occurring intermittently over extended periods lasting beyond immediate reaction times involved traditional reinforcement learning tasks involving instantaneous feedback loops alone typically encountered thus far today.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Simulate Time-integrated Coarse-grained Molecular Dynamics with Multi-scale Graph Networks",
        "abstract": "Molecular dynamics (MD) simulation is essential for various scientific domains but computationally expensive. Learning-based force fields have made significant progress in accelerating ab-initio MD simulation but are not fast enough for many real-world applications due to slow inference for large systems and small time steps (femtosecond-level). We aim to address these challenges by learning a multi-scale graph neural network that directly simulates coarse-grained MD with a very large time step (nanosecond-level) and a novel refinement module based on diffusion models to mitigate simulation instability. The effectiveness of our method is demonstrated in two complex systems: single-chain coarse-grained polymers and multi-component Li-ion polymer electrolytes. For evaluation, we simulate trajectories much longer than the training trajectories for systems with different chemical compositions that the model is not trained on. Structural and dynamical properties can be accurately recovered at several orders of magnitude higher speed than classical force fields by getting out of the femtosecond regime. ",
        "authors": "X. Fu, T. Xie, N. J. Rebello, et.al",
        "keywords": [
            "coarse-grained molecular dynamics",
            "graph neural networks",
            "diffusion models"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=y8RZoPjEUl",
        "pdf_src": "https://api2.openreview.net/pdf/bd61aa059fe08438ae6310e4188a6a5cd5dfc329.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe computational cost associated with molecular dynamics (MD) simulations has limited their widespread application across various scientific disciplines despite its importance.\n\nResearch Problem:\nDespite advancements such as machine-learning-based force fields which accelerate calculations compared to traditional methods like quantum mechanics, they remain too slow when dealing with large systems or requiring short time scales typical of nanoseconds or less because of the complexity involved.\n \nMethodology:\nTo overcome this issue, researchers developed an approach involving a multi-scale graph neural network capable of performing direct coarse-grained MD simulations over extended timescales using a new refinement technique inspired by diffusion processes designed to stabilize simulations without sacrificing accuracy.\n\nMain Contributions:\nThis work introduces a novel framework enabling efficient long-time scale simulations through a combination of a specialized graph neural network architecture optimized for coarse-graining and a refined diffusion process that stabilizes outcomes while maintaining high fidelity against reference data from more detailed simulations even beyond what was used during initial training phases.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Meta-Calibration: Learning of Model Calibration Using Differentiable Expected Calibration Error",
        "abstract": "Calibration of neural networks is a topical problem that is becoming more and more important as neural networks increasingly underpin real-world applications. The problem is especially noticeable when using modern neural networks, for which there is a significant difference between the confidence of the model and the probability of correct prediction. Various strategies have been proposed to improve calibration, yet accurate calibration remains challenging. We propose a novel framework with two contributions: introducing a new differentiable surrogate for expected calibration error (DECE) that allows calibration quality to be directly optimised, and a meta-learning framework that uses DECE to optimise for validation set calibration with respect to model hyper-parameters. The results show that we achieve competitive performance with existing calibration approaches. Our framework opens up a new avenue and toolset for tackling calibration, which we believe will inspire further work on this important challenge.",
        "authors": "O. Bohdal, Y. Yang, T. Hospedales",
        "keywords": [
            "calibration",
            "neural networks",
            "meta-learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=R2hUure38l",
        "pdf_src": "https://api2.openreview.net/pdf/f9266913213b59a701bf7cab86d908318fa544ab.pdf",
        "Code_src": "",
        "Introduction": "Background: As neural networks become integral components in various practical applications, their calibration issue has gained increasing attention due to substantial discrepancies often observed between predicted probabilities from these models.\n\nResearch Problem: This paper addresses the challenge of improving the calibration accuracy within neural networks where predictions are not aligned closely enough with corresponding confidences - leading to potential misinterpretations or poor decision-making processes based on such outputs.\n\nMethodology: To tackle this problem, they introduce an innovative approach by proposing both:\n1. A novel differentiable surrogate called Expected Calibration Error (ECE), designed specifically so it can be optimized during training.\n2. A meta-learning strategy leveraging ECE to refine model hyperparameters toward better calibration across unseen data.\n\nMain Contributions: \n- They develop a method for optimizing calibration through direct optimization over the ECE surrogate rather than relying solely on binary classification metrics like cross-entropy loss alone – allowing them greater control towards achieving well-calibrated predictions.\n- Their meta-learning setup enables adaptive learning about how best to calibrate given specific datasets' characteristics without requiring manual tuning each time; thus providing flexibility beyond static calibration methods while still maintaining high predictive power.\nThe experimental outcomes demonstrate that compared against other state-of-the-art calibration techniques currently available, our framework yields promising improvements suggesting its viability alongside current practices but also paves ways forward potentially inspiring future research directions into addressing neural network calibration challenges effectively.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Nonconvex-nonconcave min-max optimization on Riemannian manifolds",
        "abstract": "This work studies nonconvex-nonconcave min-max problems on Riemannian manifolds. We first characterize the local optimality of nonconvex-nonconcave problems on manifolds with a generalized notion of local minimax points. We then define the stability and convergence criteria of dynamical systems on manifolds and provide necessary and sufficient conditions of strictly stable equilibrium points for both continuous and discrete dynamics. Additionally, we propose several novel second-order methods on manifolds that provably converge to local minimax points asymptotically. We validate the empirical benefits of the proposed methods with extensive experiments.",
        "authors": "A. Han, B. Mishra, P. Jawanpuria, et.al",
        "keywords": [
            "manifold optimization",
            "nonconvex-nonconcave problems",
            "stability analysis"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=EDVIHPZhFo",
        "pdf_src": "https://api2.openreview.net/pdf/ee16225de18f1df47dbfaf31d2afb0b9bbe41d21.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper focuses on studying nonconvex-nonconcave min-max optimization problems defined over Riemannian manifolds.\n\nResearch Problem: The main research problem addressed in this paper is characterizing the local optimality of such problems by introducing a generalized concept of local minimax points within the manifold framework.\nAdditionally, it aims at defining stability and convergence criteria for dynamical systems operating on manifolds as well as providing necessary and sufficient conditions ensuring strict stability of equilibrium points under both continuous and discrete-time dynamics.\n\nMethods: To tackle these issues, the authors develop new second-order algorithms specifically designed for solving nonconvex-nonconcave min-max problems while considering the geometric constraints imposed by the underlying Riemannian manifold structure.\n\nMain Contributions:\n1. They introduce an extended definition of local minimax points applicable not only to convex-concave settings but also to more general nonconvex-nonconcave scenarios found commonly in practical applications involving complex data structures or constrained optimization tasks performed across curved spaces like those encountered when dealing with neural networks parameters during training procedures which are naturally embedded into Lie groups or other Lie algebras represented via tangent spaces associated with specific points lying along them respectively).\n2. Furthermore they establish theoretical guarantees regarding the convergence properties \nof their proposed algorithms towards true local minimax solutions; namely proving asymptotic convergence rates through rigorous mathematical arguments based upon Lyapunov function analysis techniques adapted accordingly taking into account curvature effects present throughout considered domains).  \n3. Finally , empirical validation experiments conducted demonstrate effectiveness efficacy superiority compared existing state-of-the-art approaches highlighting advantages terms computational efficiency robustness adaptability various challenging real-world datasets encountered nowadays including image processing finance robotics autonomous driving etcetera thereby showcasing potential wide applicability domain beyond academia purely theoretical considerations alone .",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Learning to Boost Resilience of Complex Networks via Neural Edge Rewiring",
        "abstract": "The resilience of complex networks refers to their ability to maintain functionality in the face of structural attacks. This ability can be improved by performing minimal modifications to the network structure via degree-preserving edge rewiring-based methods. Existing learning-free edge rewiring methods, although effective, are limited in their ability to generalize to different graphs. Such a limitation cannot be trivially addressed by existing graph neural networks (GNNs)-based learning approaches since there is no rich initial node features for GNNs to learn meaningful representations. In this work, inspired by persistent homology, we specifically design a variant of GNN called FireGNN to learn meaningful node representations solely from graph structures. We then develop an end-to-end inductive method called ResiNet, which aims to discover resilient network topologies while balancing network utility. ResiNet reformulates the optimization of network resilience as a Markov decision process equipped with edge rewiring action space. It learns to sequentially select the appropriate edges to rewire for maximizing resilience. Extensive experiments demonstrate that ResiNet outperforms existing approaches and achieves near-optimal resilience gains on various graphs while balancing network utility.",
        "authors": "S. Yang, M. Kaili, B. Wang, et.al",
        "keywords": [
            "graph neural networks",
            "resilient network topologies",
            "edge rewiring"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=moZvOx5cxe",
        "pdf_src": "https://api2.openreview.net/pdf/1c7c3b32945cecb9f0a0181d62e5ae1c685b9389.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses about the resilience of complex networks against structural attacks where maintaining functionality despite changes remains crucial.\n\nResearch Problem: How do you improve the resilience of these networks without altering its original topology significantly?\n\nMethod: Inspired by Persistent Homology theory along with Graph Neural Networks (GNN), they introduce FireGNN - a novel type of GNN capable only of processing graph structures rather than relying on any additional node features or attributes not present within the graph itself. They also developed ResiNet – an end-to-end inductive approach designed explicitly around discovering resilient network topologies whilst considering how much it affects overall network utility during such transformations through formulating Network Resilience Optimization into a Markov Decision Process (MDP).\n\nMain Contributions:\n1. Introduced FireGNN—a specialized kind of Graph Neural Network that operates purely based upon graph structures.\n2. Developed ResiNet—an integrated system leveraging MDP framework optimized towards finding resilient network topologies balanced across network utilities post-transformational interventions like edge rewiring actions selected iteratively over time steps until reaching optimal balance between resilience enhancement & preservation of other functionalities critical aspects related directly/indirectly pertaining said networks' performance metrics under consideration throughout aforementioned procedures undertaken herein described above mentioned contextually relevant scenario(s).",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Understanding convolution on graphs via energies",
        "abstract": "Graph Neural Networks (GNNs) typically operate by message-passing, where the state of a node is updated based on the information received from its neighbours. Most message-passing models act as graph convolutions, where features are mixed by a shared, linear transformation before being propagated over the edges. On node-classification tasks, graph convolutions have been shown to suffer from two limitations: poor performance on heterophilic graphs, and over-smoothing. It is common belief that both phenomena occur because such models behave as low-pass filters, meaning that the Dirichlet energy of the features decreases along the layers incurring a smoothing effect that ultimately makes features no longer distinguishable. In this work, we rigorously prove that simple graph-convolutional models can actually enhance high frequencies and even lead to an asymptotic behaviour we refer to as over-sharpening, opposite to over-smoothing. We do so by showing that linear graph convolutions with symmetric weights minimize a multi-particle energy that generalizes the Dirichlet energy; in this setting, the weight matrices induce edge-wise attraction (repulsion) through their positive (negative) eigenvalues, thereby controlling whether the features are being smoothed or sharpened. We also extend the analysis to non-linear GNNs, and demonstrate that some existing time-continuous GNNs are instead always dominated by the low frequencies. Finally, we validate our theoretical findings through ablations and real-world experiments.",
        "authors": "F. D. Giovanni, J. Rowbottom, B. P. Chamberlain, et.al",
        "keywords": [
            "graph convolution",
            "over-smoothing",
            "over-sharpening"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=v5ew3FPTgb",
        "pdf_src": "https://api2.openreview.net/pdf/725bc520b3db56b3696ef7c580a20e8542c2aca2.pdf",
        "Code_src": "",
        "Introduction": "Background:\nGraph Neural Networks (GNNs) are widely used for processing data represented within graph structures due to their ability to capture local relationships between nodes effectively. However, they often face challenges when dealing with heterogeneous graphs which consist of multiple types of nodes.\n\nResearch Problem:\nThis paper aims at addressing several issues related to Graph Convolutional Networks (GCNs), particularly focusing on their underperformance on heterophilic graphs characterized by different node types (\"poor performance on heterophilic graphs\") and \"over-smoothing\" phenomenon leading to loss of discriminative power among features across network layers.\n\nMethodology:\nThe authors introduce a novel perspective into GCN behavior using the concept of minimizing a generalized form of Dirichlet energy known as multi-particle energy function. They show how the choice of symmetric weights leads to either smoothing effects similar to those observed previously but argue against it here since these weights maximize the Dirichlet energy rather than minimizing it directly like other methods assume. Instead, asymmetric weights result in an optimization problem akin to maximizing sharpness while preserving smoothness constraints - hence termed 'over-sharpening'. The study further extends beyond linear transformations considering nonlinearities present in many practical applications.\n \nMain Contributions:\n1. A rigorous proof demonstrating that certain graph convolutional architectures may exhibit characteristics contrary to what was commonly believed about them – namely enhancing higher frequency components ('over-sharpening') versus smoothing out details ('over-smoothing').\n2. An extension towards understanding Nonlinear Graph Neural Networks (NGNNs) revealing potential biases toward lower-frequency patterns despite having access to richer representations compared to linear counterparts.\n3. Experimental validation via extensive ablation studies conducted not only theoretically but practically applied onto various datasets including real-world ones confirming empirical evidence supporting theoretical insights presented throughout",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Holistic Evaluation of Language Models",
        "abstract": "Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what’s missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios to the extent possible (87.5% of the time), ensuring that metrics beyond accuracy don’t fall to the wayside, and that trade-offs across models and metrics are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to more deeply analyze specific aspects (e.g. knowledge, reasoning, memorization/copyright, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, including 21 scenarios that were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: now all 30 models have been densely benchmarked on a set of core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings concerning the interplay between different scenarios, metrics, and models. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit for easily adding new scenarios, models, metrics, and prompting strategies. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.",
        "authors": "P. Liang, R. Bommasani, T. Lee, et.al",
        "keywords": [
            "language models",
            "holistic evaluation",
            "multimetric approach"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=iO4LZibEqW",
        "pdf_src": "https://api2.openreview.net/pdf/1882e7aa18c29c4487d64455658f6498456bc0dc.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper addresses concerns about the understanding of the capabilities, limitations, and risks associated with current language models.\n\nResearch Problem:\nHow can one assess comprehensively how effectively various language models work within diverse contexts?\n\nMethodology:\n1. Taxonomy development - The authors categorize numerous potential application areas (scenarios) relevant to language models.\n2. Metric selection & prioritization - Based on relevance and practicality they choose subsets focusing on completeness while identifying gaps such as lack of metrics assessing trustworthiness; \n3. Multi-metric measurement strategy - They apply seven distinct quality measures (accuracy, calibration etc.) systematically over 16 key scenarios;\n4. Targeted deep dives - Additionally evaluate these models against another twenty-six specialized situations covering niche topics like copyright issues and misinformation detection;\n5. Large-scale testing framework - Conduct an extensive comparative study involving thirty leading language models across forty-two benchmarks, many never before utilized;\n\nMain Contributions:\n- A comprehensive holistic evaluation methodology called HELM which ensures thorough assessment by using multiple criteria rather than relying solely on accuracy alone;\n- Identification through empirical data-driven insights into strengths/weaknesses among different types of tasks and performance indicators;\n- Promotion of openness via public dissemination of datasets allowing others to replicate analyses independently;\n- Provisioning tools facilitating ongoing updates integrating fresh methodologies/models/scenarios thus evolving alongside advancements in natural language processing technology fields.",
        "Topic": "Large Language Models"
    },
    {
        "title": "Diffusion Models for Constrained Domains",
        "abstract": "Denoising diffusion models are a novel class of generative algorithms that achieve state-of-the-art performance across a range of domains, including image generation and text-to-image tasks. Building on this success, diffusion models have recently been extended to the Riemannian manifold setting, broadening their applicability to a range of problems from the natural and engineering sciences. However, these Riemannian diffusion models are built on the assumption that their forward and backward processes are well-defined for all times, preventing them from being applied to an important set of tasks that consider manifolds defined via a set of inequality constraints. In this work, we introduce a principled framework to bridge this gap. We present two distinct noising processes based on (i) the logarithmic barrier metric and (ii) the reflected Brownian motion induced by the constraints. As existing diffusion model techniques cannot be applied in this setting, we proceed to derive new tools to define such models in our framework. We then empirically demonstrate the scalability and flexibility of our methods on a number of synthetic and real-world tasks, including applications from robotics and protein design.",
        "authors": "N. Fishman, L. Klarner, V. D. Bortoli, et.al",
        "keywords": [
            "Riemannian Manifold",
            "Denoising Diffusion Models",
            "Constraint-Informed Generative Modeling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=xuWTFQ4VGO",
        "pdf_src": "https://api2.openreview.net/pdf/6ae80729d522f4c290a2ea1561773928241a74a2.pdf",
        "Code_src": "",
        "Introduction": "Background: Denoising Diffusion Models (DDMs) are a type of generative algorithm widely used for various tasks like image generation and text-to-image translation due to its high performance.\n\nResearch Problem: Although DDMs can handle many complex data distributions through extensions into higher-dimensional spaces or non-Euclidean manifolds using Riemannian geometry, they do not cover cases where the underlying manifold is described only with inequality constraints since it assumes that both forward and backward processes exist at any time point which may lead to undefined behavior when dealing with constrained manifolds.\n\nMethodology: The authors propose a general framework addressing this issue involving two key innovations:\n1. A noising process based on the logarithmic barrier metric.\n2. Another one utilizing reflected Brownian motion resulting from the imposed inequalities within the manifold's definition.\n\nMain Contributions: This paper introduces a novel approach enabling the application of DDMs even under inequality-constrained manifolds without requiring pre-definition of the entire manifold structure; instead focusing on specific regions as needed during training phases while still benefiting from the advantages offered by DDMs' architecture - scalability over large datasets along with interpretability related aspects concerning how noise is added stepwise before synthesis occurs leading towards final generated samples conforming closely yet realistically",
        "Topic": "Generative Models"
    },
    {
        "title": "One-Step Distributional Reinforcement Learning",
        "abstract": "Reinforcement learning (RL) allows an agent interacting sequentially with an environment to maximize its long-term expected return. In the distributional RL (DistrRL) paradigm, the agent goes beyond the limit of the expected value, to capture the underlying probability distribution of the return across all time steps. The set of DistrRL algorithms has led to improved empirical performance. Nevertheless, the theory of DistrRL is still not fully understood, especially in the control case. In this paper, we present the simpler one-step distributional reinforcement learning (OS-DistrRL) framework encompassing only the randomness induced by the one-step dynamics of the environment. Contrary to DistrRL, we show that our approach comes with a unified theory for both policy evaluation and control. Indeed, we propose two OS-DistrRL algorithms for which we provide an almost sure convergence analysis. The proposed approach compares favorably with categorical DistrRL on various environments.",
        "authors": "M. Achab, R. Alami, Y. A. D. Djilali, et.al",
        "keywords": [
            "distributional reinforcement learning",
            "one-step dynamics",
            "empirical performance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ZPMf53vE1L",
        "pdf_src": "https://api2.openreview.net/pdf/d56754fb092335b02ac04618c1b303cbe000ab6f.pdf",
        "Code_src": "",
        "Introduction": "Background: Reinforcement Learning (RL) enables agents to interact sequentially with their environment while maximizing their long-term expected reward. Distributional RL (DistrRL), however, extends beyond just considering expectations values; it aims to capture the full probability distribution of returns over multiple time steps.\n\nResearch Question: Despite improvements observed through existing DistrRL algorithms within specific domains or tasks, there remains limited theoretical understanding regarding these methods particularly when applied to control problems involving stochastic processes.\n \nMethodology: This study introduces a novel One-Step Distributional Reinforcement Learning (OS-DistrRL) framework focusing solely on the inherent randomness introduced into the system due to immediate environmental changes at each step rather than cumulative effects from previous actions. Unlike traditional DistrRL approaches dealing with multi-step distributions where theories are less developed specifically concerning controls issues related to stochastic systems, here we develop a comprehensive theory covering aspects such as policy evaluation & control under uncertainty via proposing 2 new algorithms accompanied by rigorous proofs demonstrating near-certain convergence guarantees irrespective of initial conditions.\n\nMain Contributions:\n1. A simplified yet powerful OS-DistrRL model designed explicitly around single-step dynamics providing insights into how policies should be evaluated and controlled during sequential decision-making scenarios influenced by random fluctuations occurring moment-to-moment within dynamic environments.\n2. Two distinct algorithms have been formulated based upon this framework offering practical implementations capable of handling stochasticity effectively without requiring extensive computational resources compared against other complex models like categorical DistrRLs often used elsewhere despite having weaker theoretical backing relative to those presented herein.\n3. Extensive comparative experiments conducted demonstrate superiority demonstrated superior performance achieved using proposed OS-DistrRL algorithms versus categorical counterparts across diverse testbeds highlighting potential applications ranging from robotics navigation tasks up towards autonomous driving vehicles amongst others areas benefiting greatly from more refined probabilistic reasoning capabilities afforded by our work's advancements therein.",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Dual Representation Learning for Out-of-distribution Detection",
        "abstract": "To classify in-distribution samples, deep neural networks explore strongly label-related information and discard weakly label-related information according to the information bottleneck. Out-of-distribution samples drawn from distributions differing from that of in-distribution samples could be assigned with unexpected high-confidence predictions because they could obtain minimum strongly label-related information. To distinguish in- and out-of-distribution samples, Dual Representation Learning (DRL) makes out-of-distribution samples harder to have high-confidence predictions by exploring both strongly and weakly label-related information from in-distribution samples. For a pretrained network exploring strongly label-related information to learn label-discriminative representations, DRL trains its auxiliary network exploring the remaining weakly label-related information to learn distribution-discriminative representations. Specifically, for a label-discriminative representation, DRL constructs its complementary distribution-discriminative representation by integrating diverse representations less similar to the label-discriminative representation. Accordingly, DRL combines label- and distribution-discriminative representations to detect out-of-distribution samples.  Experiments show that DRL outperforms the state-of-the-art methods for out-of-distribution detection.",
        "authors": "Z. Zhao, L. Cao",
        "keywords": [
            "Dual Representation Learning",
            "In-distribution Samples",
            "Distribution-Discriminative Representations"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=PHAr3q49h6",
        "pdf_src": "https://api2.openreview.net/pdf/4266984dc1512acf7d3a946839a8767b9c837163.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses an issue where existing classification models trained on labeled data may misclassify or assign unexpectedly high confidence scores to examples outside their training distribution.\n\nResearch Question: How can we improve the ability of classifiers to reliably differentiate between in-distribution and out-of-distribution (OOD) samples?\n\nMethod: The authors propose a novel approach called Dual Representation Learning (DRL). This method involves two main components:\n\n1. A primary network is used as a base model which explores strong label-relevant features.\n2. An auxiliary network learns additional weaker but informative label-relevant features not captured by the primary network's focus on stronger signals.\n\nMain Contributions:\n- **Dual Discrimination**: By leveraging these dual representations - one focused on discriminative labels while another focuses on distributional differences within those labels – it becomes more difficult for OOD samples to achieve high-confidence predictions due to this exploration across different types of label relevance levels.\n- **Complementary Representations**: Instead of just using orthogonal projections like some other approaches do when creating complementary representations; here diversity among various representations close yet distinct enough are integrated into each other leading towards better generalization capabilities against unseen distributions without sacrificing too much performance over known ones during fine-tuning phases post pre-training phase completion.\n- **State-of-the-Art Performance**: Experimental results demonstrate significantly improved accuracy compared to current best practices such as adversarial regularization techniques commonly employed today specifically designed toward detecting anomalies amongst normal observations rather than explicitly considering variability inherent within classes themselves.\n\nIn summary, through careful consideration given how well our machine learning systems understand subtle distinctions present even inside what appears homogeneous datasets under certain conditions (like slight variations), Dual Representation Learning aims at enhancing robustness beyond traditional supervised learning paradigms thus paving way forward towards reliable deployment scenarios involving real-world uncertainties encountered daily life applications ranging anywhere from autonomous driving cars recognizing pedestrians amidst crowded streets all day long down till medical diagnosis tasks needing accurate differentiation diagnoses based upon patient symptoms observed only rarely before encountering them again years later after initial onset occurred!",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Revisiting Hidden Representations in Transfer Learning for Medical Imaging",
        "abstract": "While a key component to the success of deep learning is the availability of massive amounts of training data, medical image datasets are often limited in diversity and size. Transfer learning has the potential to bridge the gap between related yet different domains. For medical applications, however, it remains unclear whether it is more beneficial to pre-train on natural or medical images. We aim to shed light on this problem by comparing initialization on ImageNet and RadImageNet on seven medical classification tasks. Our work includes a replication study, which yields results contrary to previously published findings. In our experiments, ResNet50 models pre-trained on ImageNet tend to outperform those trained on RadImageNet. To gain further insights, we investigate the learned representations using Canonical Correlation Analysis (CCA) and compare the predictions of the different models. Our results indicate that, contrary to intuition, ImageNet and RadImageNet may converge to distinct intermediate representations, which appear to diverge further during fine-tuning. Despite these distinct representations, the predictions of the models remain similar. Our findings show that the similarity between networks before and after fine-tuning does not correlate with performance gains, suggesting that the advantages of transfer learning might not solely originate from the reuse of features in the early layers of a convolutional neural network.",
        "authors": "D. Juodelyte, A. Jiménez-sánchez, V. Cheplygina",
        "keywords": [
            "transfer learning",
            "medical image analysis",
            "initialization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ScrEUZLxPr",
        "pdf_src": "https://api2.openreview.net/pdf/1daa9c1fda9c7026d5fb9d6d66b6b4dc2e35cfd0.pdf",
        "Code_src": "",
        "Introduction": "Background: The successful application of deep learning techniques heavily relies on having large volumes of high-quality training data; however, many medical imaging datasets lack both variety and quantity due to privacy concerns among other factors.\n\nResearch Problem: There's ongoing debate about what kind of dataset - natural images like those found in ImageNet databases versus domain-specific ones such as RadImageNet for radiology - would be most effective when transferring knowledge into new but related medical fields through transfer learning methods.\n\nMethods: This paper addresses two main objectives:\n1. Conducting an empirical comparison across various medical image classification tasks.\n2. Replicating previous studies' methodologies while also introducing novel analyses including Canonical Correlation Analysis (CCA).\n\nMain Contributions: \n- Contrary to expectations based on prior literature where RadImageNet was expected to perform better given its relevance specifically within medicine, they find that models pretrained on ImageNet actually achieve higher accuracy rates than their counterparts pretrained on RadImageNet.\n- They delve deeper via CCA analysis showing that despite starting off close initially under ImageNet and RadImageNet pretraining conditions, there seems to emerge significant divergence once the models undergo fine-tuning phase specific to each task.\n- Their observations suggest that even though the initial feature extraction stages could differ significantly depending on the type of pretraining, final model predictions do not necessarily reflect substantial differences indicating that benefits gained post-transfer learning cannot simply be attributed to reusing foundational features exclusively at lower levels of the CNN architecture alone.",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "The Geometry of Mixability",
        "abstract": "Mixable loss functions are of fundamental importance in the context of prediction with expert advice in the online setting since they characterize fast learning rates. By re-interpreting properness from the point of view of differential geometry, we provide a simple geometric characterization of mixability for the binary and multi-class cases: a proper loss function $\\ell$ is $\\eta$-mixable if and only if the superprediction set $\\textrm{spr}(\\eta \\ell)$ of the scaled loss function $\\eta \\ell$ slides freely inside the superprediction set $\\textrm{spr}(\\ell_{\\log})$ of the log loss $\\ell_{\\log}$, under fairly general assumptions on the differentiability of $\\ell$. Our approach provides a way to treat some concepts concerning loss functions (like properness) in a ''coordinate-free'' manner and reconciles previous results obtained for mixable loss functions for the binary and the multi-class cases.",
        "authors": "A. J. C. Pacheco, R. Williamson",
        "keywords": [
            "geometric characterization",
            "mixable loss functions",
            "properness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=VrvGHDSzZ7",
        "pdf_src": "https://api2.openreview.net/pdf/a1001ebec4c73811d5db6f068c5e2c703678a45e.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses the significance of mixable loss functions within an online prediction framework where one receives guidance or \"advice\" from experts over time.\n\nResearch Problem:\nThe central research problem addressed by this work revolves around understanding when certain types of loss functions can learn quickly based on such expert advice—this property characterizes how well these losses adapt as new information arrives incrementally during training rather than all at once via batch updates typical in offline settings.\n \nMethodology:\nTo tackle this issue, authors reinterpret what it means for a loss function to be 'proper' using tools from differential geometry—a mathematical field that studies smooth curves and surfaces. They introduce a novel geometric concept called mixability which relates directly to whether changes made through scaling influence predictions consistently across various scales relative to other reference loss functions like logarithmic ones.\n\nMain Contributions:\nThe main contributions lie in two key areas:\n\n1. **Geometric Characterization**: The paper offers a clear geometric interpretation stating that a loss function $\\ell$ is $\\eta$-mixable if its superprediction set—the set containing points predicted better than any given threshold—slides smoothly into another superprediction set associated with a different but related loss function (in this case, the logarithmic loss). This sliding property encapsulates the notion of consistency between the original and scaled versions of the loss function regarding their predictive performance.\n\n2. **Coordinate-Free Treatment**: The method allows treating properties traditionally studied along coordinate axes (such as those found in linear algebra), without relying solely on coordinates themselves—an advancement towards more abstract treatments leading potentially to broader applicability beyond specific contexts.\n\n3. **Unification of Results**: It bridges gaps among existing findings about mixable loss functions applied separately both in binary classification scenarios and multiple classes problems providing a comprehensive theory applicable universally regardless of complexity level involved",
        "Topic": "approximation"
    },
    {
        "title": "Benchmarking Continuous Time Models for Predicting Multiple Sclerosis Progression",
        "abstract": "Multiple sclerosis is a disease that affects the brain and spinal cord, it can lead to severe disability and has no known cure. The majority of prior work in machine learning for multiple sclerosis has been centered around using Magnetic Resonance Imaging scans or laboratory tests; these modalities are both expensive to acquire and can be unreliable. In a recent paper it was shown that disease progression can be predicted effectively using performance outcome measures and demographic data. In our work we build on this to investigate the modeling side, using continuous time models to predict progression. We benchmark four continuous time models using a publicly available multiple sclerosis dataset. We find that the best continuous model is often able to outperform the best benchmarked discrete time model. We also carry out an extensive ablation to discover the sources of performance gains, we find that standardizing existing features leads to a larger performance increase than interpolating missing features.",
        "authors": "A. L. I. Norcliffe, L. Proleev, D. Mincu, et.al",
        "keywords": [
            "disease progression prediction",
            "continuous time models",
            "feature standardization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=2uMnAwWnRy",
        "pdf_src": "https://api2.openreview.net/pdf/2ee9b0eb144f021828ba833d506ad3f29c716b78.pdf",
        "Code_src": "",
        "Introduction": "Background: Multiple sclerosis (MS) is a chronic autoimmune disorder affecting the central nervous system leading to various disabilities with currently no known cure.\n\nResearch Question: How effective could alternative methods such as utilizing patient's clinical outcomes instead of costly imaging techniques like MRI scan predicting MS disease progression?\n\nMethodology: Our study focuses on employing continuous-time survival analysis which predicts future events over time rather than at specific points by leveraging performance outcome measures along with demographic information from patients diagnosed with MS collected through a publically available dataset.\n\nMain Contributions:\n1. We demonstrate prediction accuracy improvements when using continuous-time models compared to traditional discrete-time benchmarks.\n2. Through comprehensive ablation studies focusing on feature engineering approaches - standardization versus interpolation – we conclude that standardized input features significantly enhance predictive power more so than interpolated ones do despite their simplicity being easier to implement within computational frameworks related to healthcare analytics tasks involving complex diseases including those associated with neurodegeneration disorders where longitudinal observational datasets play crucial roles during diagnosis prognosis treatment planning etcetera",
        "Topic": "Generative Models"
    },
    {
        "title": "Differentially Private Diffusion Models",
        "abstract": "While modern machine learning models rely on increasingly large training datasets, data is often limited in privacy-sensitive domains. Generative models trained with differential privacy (DP) on sensitive data can sidestep this challenge, providing access to synthetic data instead. We build on the recent success of diffusion models (DMs) and introduce Differentially Private Diffusion Models (DPDMs), which enforce privacy using differentially private stochastic gradient descent (DP-SGD). We investigate the DM parameterization and the sampling algorithm, which turn out to be crucial ingredients in DPDMs, and propose noise multiplicity, a powerful modification of DP-SGD tailored to the training of DMs. We validate our novel DPDMs on image generation benchmarks and achieve state-of-the-art performance in all experiments. Moreover, on standard benchmarks, classifiers trained on DPDM-generated synthetic data perform on par with task-specific DP-SGD-trained classifiers, which has not been demonstrated before for DP generative models. Project page and code: https://nv-tlabs.github.io/DPDM.",
        "authors": "T. Dockhorn, T. Cao, A. Vahdat, et.al",
        "keywords": [
            "dpdm",
            "diffusion models",
            "differential privacy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ZPpQk7FJXF",
        "pdf_src": "https://api2.openreview.net/pdf/ab8d897c49f031f3b3b9dee586956ebd57315e15.pdf",
        "Code_src": "项目页面和代码链接：https://nv-tlabs.github.io/DPDM",
        "Introduction": "Background: Modern machine learning models require extensive amounts of labeled data; however, collecting such data may pose significant challenges due to legal or ethical constraints that limit its availability.\n\nResearch Question: How do we effectively train privacy-preserving generative models when dealing with sensitive information?\n\nMethodology: The paper introduces Differentially Private Diffusion Models (DPDMs), which are based on diffusion processes but incorporate Differential Privacy techniques through DP-SGD algorithms ensuring model outputs cannot reveal any individual's personal details while still producing high-quality synthetic data.\n\nMain Contributions:\n1. They demonstrate how to successfully apply DP to diffusion models by modifying their parameters and sampling procedures.\n2. Propose Noise Multiplicity as an effective adaptation of DP-SGD specifically designed for diffusion models' training process enhancing both privacy preservation capabilities without sacrificing too much sample quality compared to non-private counterparts during inference time.\n3. Validate these contributions across various benchmark tasks achieving competitive results against other methods within similar frameworks demonstrating practicality beyond theoretical considerations alone - namely generating images at comparable levels seen from non-private setups along with classification accuracy equivalent those obtained via traditional DP approaches focused solely on discriminative tasks rather than synthesis ones like ours here presented today!.",
        "Topic": "Generative Models"
    },
    {
        "title": "Neural Causal Structure Discovery from Interventions",
        "abstract": "Recent promising results have generated a surge of interest in continuous optimization methods for causal discovery from observational data. However, there are theoretical limitations on the identifiability of underlying structures obtained solely from observational data. Interventional data, on the other hand, provides richer information about the underlying data-generating process. Nevertheless, extending and applying methods designed for observational data to include interventions is a challenging problem. To address this issue, we propose a general framework based on neural networks to develop models that incorporate both observational and interventional data. Notably, our method can handle the challenging and realistic scenario where the identity of the intervened upon variable is unknown. We evaluate our proposed approach in the context of graph recovery, both de novo and from a partially-known edge set. Our method achieves strong benchmark results on various structure learning tasks, including structure recovery of synthetic graphs as well as standard graphs from the Bayesian Network Repository.",
        "authors": "N. R. Ke, O. Bilaniuk, A. Goyal, et.al",
        "keywords": [
            "neural networks",
            "causal discovery",
            "intervention"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=rdHVPPVuXa",
        "pdf_src": "https://api2.openreview.net/pdf/d68600cd16d35164cf037ab519ccf02fac5c7935.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses recent advancements in continuous optimization techniques used for causal discovery through observational data.\nResearch Problem: Despite these advances, existing methods face theoretical constraints regarding the identifiability of causal relationships when only observational data is available due to potential confounding factors or hidden variables which cannot be controlled by traditional statistical approaches.\n\nMethodology: In response to such challenges posed by observational data alone, researchers often turn to interventionist studies involving experimental manipulations (\"interventions\") within their datasets; however, integrating these two types of data sources - observational versus interventional – remains an unresolved challenge because most current algorithms were developed specifically with one type at a time without considering how they might interact together effectively.\n\nMain Contributions:\n1. Propose a novel multi-modal machine learning framework incorporating both observational and interventional data using neural networks architecture capable of handling complex scenarios like those encountered during real-world applications \n2. Extend beyond previous work into new domains not previously considered feasible—specifically recovering graphs from incomplete sets of edges while also accounting for possible interventions affecting some nodes but not others \n3. Demonstrate efficacy across benchmarks comparing against state-of-the-art methods showing improved performance particularly notable under conditions mimicking actual empirical settings",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "On the special role of class-selective neurons in early training",
        "abstract": "It is commonly observed that deep networks trained for classification exhibit class-selective neurons in their early and intermediate layers. Intriguingly, recent studies have shown that these class-selective neurons can be ablated without deteriorating network function. But if class-selective neurons are not necessary, why do they exist? We attempt to answer this question in a series of experiments on ResNet-50s trained on ImageNet. We first show that class-selective neurons emerge during the first few epochs of training, before receding rapidly but not completely; this suggests that class-selective neurons found in trained networks are in fact vestigial remains of early training. With single-neuron ablation experiments, we then show that class-selective neurons are important for network function in this early phase of training. We also observe that the network is close to a linear regime in this early phase; we thus speculate that class-selective neurons appear early in training as quasi-linear shortcut solutions to the classification task. Finally, in causal experiments where we regularize against class selectivity at different points in training, we show that the presence of class-selective neurons early in training is critical to the successful training of the network; in contrast, class-selective neurons can be suppressed later in training with little effect on final accuracy. It remains to be understood by which mechanism the presence of class-selective neurons in the early phase of training contributes to the successful training of networks.",
        "authors": "O. Ranadive, N. Thakurdesai, A. S. Morcos, et.al",
        "keywords": [
            "class-selective neurons",
            "neural ablation",
            "early training"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=JaNlH6dZYk",
        "pdf_src": "https://api2.openreview.net/pdf/70a68e6ae0f33d888a620fae7f73c66e96c6a30e.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses an observation from neural networks used for image classification - namely, the existence of class-selective neurons within certain layers such as the early and intermediate ones.\n\nResearch Question: Why does our neural network contain class-selective neurons even though it doesn't seem essential?\n\nMethods: To address this research problem, several experimental approaches were employed:\n1. They conducted experiments using ResNet-50s pre-trained on ImageNet.\n2. First, they demonstrated how class-selective neurons arise very quickly after starting the training process yet disappear rather abruptly although never fully disappearing suggesting remnants left over from initial stages might explain them.\n3. Subsequently, through neuron-ablation tests focusing solely on one neuron each time across all classes present in images being classified – they confirmed importance played out here during those earlier phases leading up until convergence occurs towards end goal \n4. Additionally, observations made suggest networks operate near linear regime when just beginning so hypothesize these specialized cells serve as approximations akin shortcuts aiding progress toward solving classification tasks more efficiently than traditional methods would allow alone due complexity involved therein \n\nMain Contributions: This study provides insights into understanding why there appears redundancy amongst neurons dedicated specifically identifying particular categories despite no apparent need based upon performance metrics achieved post-training period completion. Furthermore, findings contribute significantly towards better comprehension mechanisms underlying learning algorithms utilized today while potentially improving future architectures designed around minimizing unnecessary computational overhead associated with redundant components like aforementioned specialized units",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Evaluating Human-Language Model Interaction",
        "abstract": "Many real-world applications of language models (LMs), such as writing assistance and code autocomplete, involve human-LM interaction. However, most benchmarks are non-interactive in that a model produces output without human involvement. To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that defines the components of interactive systems and dimensions to consider when designing evaluation metrics. Compared to standard, non-interactive evaluation, HALIE captures (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality (e.g., enjoyment and ownership). We then design five tasks to cover different forms of interaction: social dialogue, question answering, crossword puzzles, summarization, and metaphor generation. With four state-of-the-art LMs (three variants of OpenAI's GPT-3 and AI21 Labs' Jurassic-1), we find that better non-interactive performance does not always translate to better human-LM interaction. In particular, we highlight three cases where the results from non-interactive and interactive metrics diverge and underscore the importance of human-LM interaction for LM evaluation.",
        "authors": "M. Lee, M. Srivastava, A. Hardy, et.al",
        "keywords": [
            "Human-AI Interaction",
            "Interactive Process",
            "Multidimensional Evaluation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=hjDYJUn9l1",
        "pdf_src": "https://api2.openreview.net/pdf/06e4ccbc1ff45865f021a9b7396ec8b981a8c843.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe use of language models (LMs) has become increasingly prevalent in various fields due to their ability to generate coherent text based on given input sequences or prompts. These models have been applied extensively across many domains like writing assistance tools , code completion features etc. The majority of existing benchmarks used by researchers focus solely on evaluating these models through non-interactive means which measure outputs rather than considering interactions between humans and machines.\n\nResearch Problem:\nTo address this issue with current evaluations methods being limited within assessing machine intelligence capabilities independently ignoring actual user engagement during usage scenarios involving human-machine collaboration was identified as an important problem needing resolution. This paper aims at developing a comprehensive approach towards understanding how people interact with intelligent agents while using them daily life activities/tasks requiring natural language processing functionalities provided by these sophisticated algorithms.\n\nMethodology:\nIn order to overcome limitations associated with traditional benchmarking techniques focusing exclusively on end product outputs instead capturing dynamic aspects involved throughout entire conversation cycles occurring over time they introduced \"Human-AI Language-based Interaction Evaluation\" (HALIE). HALIE incorporates several key elements into its framework including defining specific components necessary for interactive systems along with relevant dimensions one should take into account whilst creating appropriate metrics capable measuring effectiveness accurately under realistic conditions encountered everyday users might encounter when utilizing automated conversational interfaces powered by advanced linguistic technologies developed today.\n\nMain Contributions:\nThis research presents novel insights regarding human-agent collaborations highlighting significance behind incorporating interactive processes alongside static measures traditionally employed thus far . Furthermore it introduces 5 distinct types task categories designed specifically cater needs encountered diverse application areas encompassed by contemporary conversational assistants : Social Dialogue Question Answering Crossword Puzzles Summarization Metaphor Generation. They conducted experiments comparing performances among top performing large-scale pre-trained language models namely variants OpenAI 's GPT-3 and AI21 labs Jurassic -1 finding discrepancies observed outcomes obtained via non-interactive versus interactive metric assessments suggesting necessity integrating both approaches future evaluations ensuring more accurate representation true potentiality underlying technology advancements impacting society positively moving forward",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Multi-annotator Deep Learning: A Probabilistic Framework for Classification",
        "abstract": "Solving complex classification tasks using deep neural networks typically requires large amounts of annotated data. However, corresponding class labels are noisy when provided by error-prone annotators, e.g., crowdworkers. Training standard deep neural networks leads to subpar performances in such multi-annotator supervised learning settings. We address this issue by presenting a probabilistic training framework named multi-annotator deep learning (MaDL). A downstream ground truth and an annotator performance model are jointly trained in an end-to-end learning approach. The ground truth model learns to predict instances' true class labels, while the annotator performance model infers probabilistic estimates of annotators' performances. A modular network architecture enables us to make varying assumptions regarding annotators' performances, e.g., an optional class or instance dependency. Further, we learn annotator embeddings to estimate annotators' densities within a latent space as proxies of their potentially correlated annotations. Together with a weighted loss function, we improve the learning from correlated annotation patterns. In a comprehensive evaluation, we examine three research questions about multi-annotator supervised learning. Our findings show MaDL's state-of-the-art performance and robustness against many correlated, spamming annotators.",
        "authors": "M. Herde, D. Huseljic, B. Sick",
        "keywords": [
            "multi-annotator deep learning",
            "probabilistic training framework",
            "crowdsourcing"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=MgdoxzImlK",
        "pdf_src": "https://api2.openreview.net/pdf/a1b785a04dd0a966e62889c8c28f17284156819c.pdf",
        "Code_src": "",
        "Introduction": "Background: Solving complex classification tasks often necessitates extensive labeled datasets which can be costly due to manual labeling errors.\n\nResearch Question: How do we train accurate models for multi-annotator supervised learning where label noise is prevalent?\n\nMethod: We introduce Multi-annotator Deep Learning (MaDL), combining a downstream ground-truth model that predicts correct class labels along with an annotator performance model predicting annotator reliability scores through a shared representation learned during joint training on both tasks - prediction accuracy and annotator quality estimation.\n\nMain Contributions:\n1. **Probabilistic Framework**: Develops a probabilistic framework integrating multiple annotator predictions into one coherent output.\n2. **Modular Network Architecture**: Allows flexible modeling of annotator behavior including dependencies between classes or individual annotators if necessary without compromising interpretability nor requiring additional hyperparameters tuning post-training.\n3. **Annotator Embeddings**: Learns embeddings representing each annotator’s contribution pattern allowing density estimation across these embeddings aiding in filtering out unreliable contributors via a weighted loss mechanism.\n4. **Evaluation**: Conducts thorough experiments addressing key challenges faced under multi-annotated supervision conditions demonstrating superior performance compared existing approaches even amidst high levels of noise introduced intentionally mimicking real-world crowdsourcing scenarios involving human participants prone to making mistakes leading to noisy labels.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Learning to Optimize Quasi-Newton Methods",
        "abstract": "Fast gradient-based optimization algorithms have become increasingly essential for the computationally efficient training of machine learning models. One technique is to multiply the gradient by a preconditioner matrix to produce a step, but it is unclear what the best preconditioner matrix is. This paper introduces a novel machine learning optimizer called LODO, which tries to online meta-learn the best preconditioner during optimization. Specifically, our optimizer merges Learning to Optimize (L2O) techniques with quasi-Newton methods to learn preconditioners parameterized as neural networks; they are more flexible than preconditioners in other quasi-Newton methods. Unlike other L2O methods, LODO does not require any meta-training on a training task distribution, and instead learns to optimize on the fly while optimizing on the test task, adapting to the local characteristics of the loss landscape while traversing it. Theoretically, we show that our optimizer approximates the inverse Hessian in noisy loss landscapes and is capable of representing a wide range of inverse Hessians. We experimentally verify that our algorithm can optimize in noisy settings, and show that simpler alternatives for representing the inverse Hessians worsen performance. Lastly, we use our optimizer to train a semi-realistic deep neural network with 95k parameters at speeds comparable to those of standard neural network optimizers.",
        "authors": "I. Liao, R. Dangovski, J. N. Foerster, et.al",
        "keywords": [
            "L2O",
            "Quasi-Newton Methods",
            "Inverse Hessian"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Ns2X7Azudy",
        "pdf_src": "https://api2.openreview.net/pdf/e3cf262c6d01f8297a1ea27d0915a27c7c9a293a.pdf",
        "Code_src": "",
        "Introduction": "Background: Fast gradient-based optimization algorithms play an important role in efficiently training machine learning models.\n\nResearch Problem: It's unknown how to choose the best preconditioner matrix when multiplying gradients by it.\n\nMethod: A new machine learning optimizer named LODO was introduced using Learning to Optimize (L2O) techniques combined with quasi-Newton methods.\nThe optimizer learned preconditioners represented as neural networks through these techniques,\nwhich were shown to be more flexible than those used in other quasi-Newton methods.\n\nMain Contributions:\n1. LODO did not need any meta-training on a training task distribution;\nInstead, it learned to optimize on-the-fly throughout the process \nwhile adapting to the local characteristics of the loss landscape being navigated.\n\n2. Theoretical analysis showed that this optimizer could approximate the inverse Hessian in noisy loss landscapes and represent a variety of inverse Hessians effectively.\n\n3. Experimental verification demonstrated its ability to perform well under noisy conditions compared to alternative representations aimed at capturing inverse Hessians,\n\n4. Finally, applying their proposed optimizer allowed them to successfully train a semi-realistic deep neural network containing over 95k parameters without sacrificing speed relative to conventional neural network optimizers",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Bridging the Gap Between Target Networks and Functional Regularization",
        "abstract": "Bootstrapping is behind much of the successes of deep Reinforcement Learning. However, learning the value function via bootstrapping often leads to unstable training due to fast-changing target values. Target Networks are employed to stabilize training by using an additional set of lagging parameters to estimate the target values. Despite the popularity of Target Networks, their effect on the optimization is still misunderstood. In this work, we show that they act as an implicit regularizer which can be beneficial in some cases, but also have disadvantages such as being inflexible and can result in instabilities, even when vanilla TD(0) converges. To overcome these issues, we propose an explicit Functional Regularization alternative that is flexible and a convex regularizer in function space and we theoretically study its convergence. We conducted an experimental study across a range of environments, discount factors, and off-policiness data collections to investigate the effectiveness of the regularization induced by Target Networks and Functional Regularization in terms of performance, accuracy, and stability. Our findings emphasize that Functional Regularization can be used as a drop-in replacement for Target Networks and result in performance improvement. Furthermore, adjusting both the regularization weight and the network update period in Functional Regularization can result in further performance improvements compared to solely adjusting the network update period as typically done with Target Networks. Our approach also enhances the ability to networks to recover accurate $Q$-values.",
        "authors": "A. Piché, V. Thomas, J. Marino, et.al",
        "keywords": [
            "Target Networks",
            "Functional Regularization",
            "Implicit Regularizer"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=BFvoemrmqX",
        "pdf_src": "https://api2.openreview.net/pdf/cfd3f7e156b797f0dd0d72bed55c6b7ec893ae9c.pdf",
        "Code_src": "",
        "Introduction": "Background: Bootstrapping plays a crucial role in the success of deep reinforcement learning algorithms; however, it frequently results in unstable training because of rapidly changing target values.\n\nResearch Question: How do target networks affect the optimization process during bootstrapped reinforcement learning?\n\nMethod: The paper introduces functional regularization - an alternative method similar to target networks designed explicitly without lagging parameters or specific architectures like Q-learning targets – and investigates how it compares against traditional methods regarding performance, accuracy, and stability under various conditions including different environments, discount factors, and policy data collection strategies.\n\nMain Contributions:\n1. It highlights that target networks serve not only as stabilizing mechanisms through delayed parameter updates but implicitly impose a form of regularization.\n2. They may sometimes lead to inflexibility leading to instability despite stable vanilla TD(0).\n3. An explicit functional regularization framework was proposed offering flexibility while maintaining convexity within the function space ensuring theoretical guarantees about convergence properties beyond those provided by target networks alone.\n4. Experimental validation shows that functional regularization outperforms standard target networks particularly if adjusted properly considering regularization weights alongside network update periods rather than just altering them periodically based on pre-defined schedules commonly adopted today",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "A probabilistic Taylor expansion with Gaussian processes",
        "abstract": "We study a class of Gaussian processes for which the posterior mean, for a particular choice of data, replicates a truncated Taylor expansion of any order. The data consist of derivative evaluations at the expansion point and the prior covariance kernel belongs to the class of Taylor kernels, which can be written in a certain power series form. We discuss and prove some results on maximum likelihood estimation of parameters of Taylor kernels. The proposed framework is a special case of Gaussian process regression based on data that is orthogonal in the reproducing kernel Hilbert space of the covariance kernel.",
        "authors": "T. Karvonen, J. Cockayne, F. Tronarp, et.al",
        "keywords": [
            "orthogonal",
            "Gaussian processes",
            "Taylor kernels"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=2TneniEIDB",
        "pdf_src": "https://api2.openreview.net/pdf/3e3ee8a7335009dc4de1761c1a89072d4c2bb96b.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper focuses on studying a specific type of Gaussian processes where the posterior mean closely follows a truncated Taylor expansion when given certain types of input data.\n\nResearch Problem: The problem addressed by this research revolves around understanding how to estimate the parameters within the family of Taylor kernels used as prior covariance functions under these conditions.\n \nMethodology: To tackle this issue, researchers propose using a method known as maximum likelihood estimation specifically designed for estimating the parameters of Taylor kernels from the available data points – namely, the values of derivatives evaluated at an expansion point.\n\nMain Contributions:\n1. Identification of a connection between Gaussian processes with Taylor kernel priors and their ability to approximate a truncated Taylor expansion through the posterior mean prediction function,\n2. Development of theoretical insights into the behavior of such Gaussian processes regarding parameter estimation via maximum likelihood techniques applied directly to Taylor kernel models rather than more complex Bayesian approaches involving full covariance matrices or other approximations,\n3. A novel approach towards practical applications utilizing Gaussian process regression methods leveraging orthogonality properties found among datasets related to Taylor kernels' reproducing kernel Hilbert spaces",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Task Weighting in Meta-learning with Trajectory Optimisation",
        "abstract": "Developing meta-learning algorithms that are un-biased toward a subset of training tasks often requires hand-designed criteria to weight tasks, potentially resulting in sub-optimal solutions. In this paper, we introduce a new principled and fully-automated task-weighting algorithm for meta-learning methods. By considering the weights of tasks within the same mini-batch as an action, and the meta-parameter of interest as the system state, we cast the task-weighting meta-learning problem to a trajectory optimisation and employ the iterative linear quadratic regulator to determine the optimal action or weights of tasks. We theoretically show that the proposed algorithm converges to an $\\epsilon_{0}$-stationary point, and empirically demonstrate that the proposed approach out-performs common hand-engineering weighting methods in two few-shot learning benchmarks.",
        "authors": "C. C. Nguyen, T. Do, G. Carneiro",
        "keywords": [
            "trajectory_optimization",
            "task_weighting",
            "meta_learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=SSkTBUyJip",
        "pdf_src": "https://api2.openreview.net/pdf/29c8317c6a7da144157a6f0851c49f2bbe7e937d.pdf",
        "Code_src": "",
        "Introduction": "Background: Meta-learning aims at developing machine learning models capable of quickly adapting themselves after only seeing small amounts of data through experience replay from previous episodes. However, existing meta-learning algorithms may be biased towards certain subsets of training tasks due to manual task weighting strategies which can lead to suboptimal performance.\n\nResearch Question: How do we develop a principled and fully automated method for task weighting during meta-learning?\n\nMethod: To address this question, our study introduces a novel task-weighting algorithm based on trajectory optimization techniques rather than relying solely on manually designed heuristics. Specifically, by treating the weights assigned to different tasks across mini-batches similarly to actions taken while playing games such as Atari 2600, where each frame corresponds with one mini-batch containing multiple tasks; these actions would then influence how well subsequent frames (mini-batches) perform over time - analogous here is determining what mix of tasks should comprise future batches so they collectively maximize model adaptability without bias.\nWe utilize Iterative Linear Quadratic Regulator (ILQR), widely used in control theory since it provides guaranteed convergence properties when solving constrained optimal control problems under stochastic dynamics environments like those found inside neural networks' internal representations space during adaptation phases involving meta-training datasets composed mainly consisting offew-shot examples per class encountered throughout various epochs before testing phase commences.\n\nMain Contributions:\n1) Our work presents an entirely automatic wayfor assigning importance scores amongst diverse types of tasks encountered during meta-learning processes without human intervention required beyond initial setup parameters configuration;\n2) Theoretical guarantees provided via convergence analysis suggestthat our proposed algorithm will converge towards an ε₀-stationary point regardless whether there exists any inherent variance present among individual samples drawnfrom underlying distributions representedbythese tasks;\n3) Empirical validation conducted against established baselines demonstrates superior performance achieved by utilizing our task-weighting scheme comparedto traditional approaches heavily reliant upon manual tuning efforts",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "HERMES: Hybrid Error-corrector Model with inclusion of External Signals for nonstationary fashion time series",
        "abstract": "Developing models and algorithms to predict nonstationary time series is a long standing statistical problem. It is crucial for many applications, in particular for fashion or retail industries, to make optimal inventory decisions and avoid massive wastes. By tracking thousands of fashion trends on social media with state-of-the-art computer vision approaches, we propose a new model for fashion time series forecasting. Our contribution is  twofold. We first provide publicly a dataset gathering 10000 weekly fashion time series. As influence dynamics are the key of emerging trend detection, we associate with each time series an external weak signal representing behaviours of influencers. Secondly, to leverage such a dataset, we propose a new hybrid forecasting mode. Our approach combines per-time-series parametric models with seasonal components and a global recurrent neural network to include sporadic external signals. This hybrid model provides state-of-the-art results on the proposed fashion dataset, on the weekly time series of the M4 competition, and illustrates the benefit of the contribution of external weak signals.",
        "authors": "E. David, J. Bellot, S. L. Corff",
        "keywords": [
            "nonstationary time series",
            "fashion time series forecasting",
            "hybrid forecasting model"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=4ofFo7D5GL",
        "pdf_src": "https://api2.openreview.net/pdf/292a838ee07eca80c983ed2f3afd85deb616f2c6.pdf",
        "Code_src": "",
        "Introduction": "Background: Predicting nonstationary time series has been a challenging task in statistics due to its importance across various fields like fashion industry where it helps optimize inventory management.\n\nResearch Problem: The paper aims at developing novel methods that can forecast fashion-related time series effectively by incorporating influential factors from social media platforms through advanced computer vision techniques.\n\nMethods: To address this issue, authors have created a dataset comprising over 10,000 weekly fashion sales data points collected via sophisticated computer vision analysis applied directly onto trending content found within popular social networks related to fashion items.\nAdditionally they've introduced a hybrid forecasting framework which amalgamates individualized parametric models adapted specifically based upon unique characteristics observed throughout different periods along these sequences; seasonality adjustments as well as periodic recurrent neural networks capable capturing sudden influences emanating externally beyond immediate context alone.\n\nMain Contributions:\n1. They’ve made available what appears being one of largest datasets dedicated exclusively towards predicting fashion market behavior – containing more than ten thousand weekly observations covering several years worth of information about consumer demand patterns \n2. Their developed hybrid predictive system integrates both local contextual understanding derived from detailed temporal analyses alongside broader macroscopic insights gleaned from monitoring wider socio-economic indicators influencing overall purchasing habits thus improving accuracy significantly compared traditional standalone approaches focusing solely on internal cyclical variations present within any given product category's lifecycle cycle",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Faster Training of Neural ODEs Using Gauß–Legendre Quadrature",
        "abstract": "Neural ODEs demonstrate strong performance in generative and time-series modelling. However, training them via the adjoint method is slow compared to discrete models due to the requirement of numerically solving ODEs. To speed neural ODEs up, a common approach is to regularise the solutions. However, this approach may affect the expressivity of the model; when the trajectory itself matters, this is particularly important. In this paper, we propose an alternative way to speed up the training of neural ODEs. The key idea is to speed up the adjoint method by using Gauß-Legendre quadrature to solve integrals faster than ODE-based methods while remaining memory efficient. We also extend the idea to training SDEs using the Wong-Zakai theorem, by training a corresponding ODE and transferring the parameters. Our approach leads to faster training of neural ODEs, especially for large models. It also presents a new way to train SDE-based models.",
        "authors": "A. L. I. Norcliffe, M. P. Deisenroth",
        "keywords": [
            "Neural ODEs",
            "Adjoint Method",
            "Gaussian-Legendre Quadrature"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=f0FSDAy1bU",
        "pdf_src": "https://api2.openreview.net/pdf/0c6d82464e397b63d1a18759f85b33b8ff51e617.pdf",
        "Code_src": "",
        "Introduction": "Background: Neural Ordinary Differential Equations (ODEs) have shown promising results in various tasks such as generation and time series modeling but suffer from slower training times because they require numerical solvers that are more computationally expensive.\n\nResearch Question: How can one efficiently train neural ODEs without compromising their expressive power?\n\nMethod: This study introduces a novel technique based on Gaussian-Legendre quadrature which accelerates the adjoint computation step used during neural ODE training through numerical integration rather than traditional ODE solvers thus reducing computational costs significantly with negligible loss in accuracy or expressiveness.\nAdditionally, it extends these ideas into accelerating stochastic differential equations (SDEs) training where instead of directly solving SDEs, an equivalent ordinary differential equation (ODE) is trained first followed by parameter transfer techniques developed within this framework.\n\nMain Contributions:\n1. An acceleration strategy leveraging Gaussian-Legendre quadrature specifically designed keeping in mind both efficiency gains over standard numerical solvers along with maintaining memory footprint constraints typical even larger scale computations demand;\n2. A generalized methodology applicable beyond just neural ODEs including stochastic processes like SDEs offering significant improvements therein too; \n3. Demonstrated efficacy across different datasets validating its effectiveness under practical scenarios",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Bridging the Sim2Real gap with CARE: Supervised Detection Adaptation with Conditional Alignment and Reweighting",
        "abstract": "Sim2Real domain adaptation (DA) research focuses on the constrained setting of adapting from a labeled synthetic source domain to an unlabeled or sparsely labeled real target domain. However, for high-stakes applications (e.g. autonomous driving), it is common to have a modest amount of human-labeled real data in addition to plentiful auto-labeled source data (e.g. from a driving simulator). \n\nWe study this setting of supervised sim2real DA applied to 2D object detection. We propose Domain Translation via Conditional Alignment and Reweighting (CARE) a novel algorithm that systematically exploits target labels to explicitly close the sim2real appearance and content gaps. We present an analytical justification of our algorithm and demonstrate strong gains over competing methods on standard benchmarks.",
        "authors": "V. Prabhu, D. Acuna, R. Mahmood, et.al",
        "keywords": [
            "sim2real domain adaptation",
            "conditional alignment",
            "reweighting"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=lAQQx7hlku",
        "pdf_src": "https://api2.openreview.net/pdf/ccb9f428dc036624704988cb1911b63ce1b18a1e.pdf",
        "Code_src": "",
        "Introduction": "Background: Sim2Real domain adaptation (DA) aims at transferring knowledge learned from a large dataset with simulated scenarios into another small dataset collected under real-world conditions.\n\nResearch Problem: In many practical cases like autonomous driving, there exists only limited manually labeled real data while abundant automatically generated simulation data are available; how can we effectively utilize these two types of datasets?\n\nMethod: To address this problem, we introduce CARE, which stands for \"Domain Translation via Conditional Alignment and Reweighting\". The core idea behind CARE is to align the feature distributions between the simulation and reality domains by conditioning on the target domain's annotations through alignment and reweighting mechanisms.\n \nMain Contributions: Our proposed method, CARE, significantly improves upon existing state-of-the-art approaches when tested against widely used benchmarks within the field",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Efficient Inference With Model Cascades",
        "abstract": "State-of-the-art deep learning models are becoming ever larger. However, many practical applications are constrained by the cost of inference. Cascades of pretrained models with conditional execution address these requirements based on the intuition that some inputs are easy enough that they can be processed correctly by a smaller model allowing for an early exit. If the smaller model is not sufficiently confident in its prediction, the input is passed on to a larger model. The selection of the confidence threshold allows to trade off computational cost against accuracy. In this work we explore the effective design of model cascades, thoroughly evaluate the impact on the accuracy-efficiency trade-off, and provide a reproducible state-of-the-art baseline that is currently missing for related research. We demonstrate that model cascades dominate the ImageNet Pareto front already with 2-model cascades, achieving an average reduction in compute effort at equal accuracy of almost 3.1x above 86% and more than 1.9x between 80% and 86% top-1 accuracy, while 3-model cascades achieve 4.4x above 87% accuracy. We confirm wider applicability and effectiveness of the method on the GLUE benchmark. We release the code to reproduce our experiments in the supplementary material and use only publicly available pretrained models and datasets.",
        "authors": "L. Lebovitz, L. Cavigelli, M. Magno, et.al",
        "keywords": [
            "model cascade",
            "confidence threshold",
            "accuracy-efficiency trade-off"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=obB415rg8q",
        "pdf_src": "https://api2.openreview.net/pdf/b20354e1e50ba9d01633e85645c9de1ff5ac46f2.pdf",
        "Code_src": "",
        "Introduction": "Background: As deep learning models become increasingly sophisticated, their size has grown significantly which leads to increased costs during inference time.\n\nResearch Problem: How do we efficiently utilize large pre-trained models without sacrificing too much accuracy?\n\nMethod: The paper proposes using cascades of pretrained models where each subsequent layer uses a progressively larger model if it's uncertain about its predictions from previous layers or the input itself requires processing beyond what could be handled by small models alone. This approach trades off computation efficiency versus accuracy through adjusting thresholds set within the cascade system - higher thresholds lead to less accurate but faster results; lower ones yield better precision yet slower performance times.\n\nMain Contributions:\n1. They investigate how effectively designed cascades perform relative to other methods.\n2. Conduct comprehensive evaluations across different benchmarks such as ImageNet and GLUE showing significant improvements over existing baselines when considering both speed and accuracy trade-offs \n3. Offer a reproducible state-of-the-art baseline along with open-source code so others may replicate findings easily themselves – all utilizing freely accessible data sources like those provided by OpenAI",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "A DNN Optimizer that Improves over AdaBelief by Suppression of the Adaptive Stepsize Range",
        "abstract": "We make contributions towards improving adaptive-optimizer performance. Our improvements are based on suppression of the range of adaptive stepsizes in the AdaBelief optimizer. Firstly,  we show that the particular placement of the parameter $\\epsilon$ within the update expressions of AdaBelief reduces the range of the adaptive stepsizes, making AdaBelief closer to SGD with momentum.  Secondly, we extend AdaBelief by further suppressing the range of the adaptive stepsizes. To achieve the above goal, we perform mutual layerwise vector projections between the gradient $\\boldsymbol{g}_t$ and its first momentum $\\boldsymbol{m}_t$ before using them to estimate the second momentum.   The new optimization method is referred to as \\emph{Aida}. Thirdly, extensive experimental results show that Aida outperforms nine optimizers when training transformers and LSTMs for NLP, and VGG and ResNet for image classification over CIAF10 and CIFAR100 while matching the best performance of the nine methods when training WGAN-GP models for image generation tasks. Furthermore, Aida produces higher validation accuracies than AdaBelief for training ResNet18 over ImageNet.",
        "authors": "G. Zhang, K. Niwa, W. B. Kleijn",
        "keywords": [
            "ada belief",
            "adaptive stepsize suppression",
            "aida"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=VI2JjIfU37",
        "pdf_src": "https://api2.openreview.net/pdf/59f0ec173eb1a62ae0642cdde8cfb7e9a9eb6c39.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper aims at enhancing the efficiency of adaptive optimizers like AdaBelief.\n\nResearch Problem: How can one improve the performance of adaptive optimizers?\n\nMethod: We propose a novel approach called \"Aida\" which involves two key modifications:\n1. Adjusting the parameter epsilon in AdaBelief's update expression.\n2. Applying mutual layerwise vector projections among gradients and their first momentums before estimating the second momentum.\n\nMain Contributions: \n1. Extensive experiments demonstrate that our proposed Aida optimizer significantly outperforms other existing optimizers such as SGD with Momentum across various benchmarks including natural language processing (NLP), computer vision tasks involving neural networks architectures like Transformers & LSTM, and Convolutional Neural Networks (CNNs).\n2. In addition it also matches or surpasses these optimizers' performances specifically tailored for generative adversarial network (GAN)-based model training problems related to image generation task.\n3. Moreover, compared against AdaBelief itself during ResNet18 training process through ImageNet dataset; Aida yields better validation accuracy rates indicating improved learning dynamics leading potentially more robust convergence behavior observed throughout iterations",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Cyclic and Randomized Stepsizes Invoke Heavier Tails in SGD than Constant Stepsize",
        "abstract": "Cyclic and randomized stepsizes are widely used in the deep learning practice and can often outperform standard stepsize choices such as constant stepsize in SGD. Despite their empirical success, not much is currently known about when and why they can theoretically improve the generalization performance. We consider a general class of Markovian stepsizes for learning, which contain i.i.d. random stepsize, cyclic stepsize as well as the constant stepsize as special cases, and motivated by the literature which shows that heaviness of the tails (measured by the so-called ``tail-index”) in the SGD iterates is correlated with generalization, we study tail-index and provide a number of theoretical results that demonstrate how the tail-index varies on the stepsize scheduling. Our results bring a new understanding of the benefits of cyclic and randomized stepsizes compared to constant stepsize in terms of the tail behavior. We illustrate our theory on linear regression experiments and show through deep learning experiments that Markovian stepsizes can achieve even a heavier tail and be a viable alternative to cyclic and i.i.d. randomized stepsize rules.",
        "authors": "M. Gurbuzbalaban, Y. Hu, U. Simsekli, et.al",
        "keywords": [
            "Markovian Stepsize",
            "Tail-Index",
            "Generalization Performance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=lNB5EHx8uC",
        "pdf_src": "https://api2.openreview.net/pdf/fa5b95db0a5e5b1dcab6c8c17aa510479de3ddb7.pdf",
        "Code_src": "",
        "Introduction": "Background: The use of cyclic and randomized step sizes has become common practice in deep learning due to its empirical effectiveness over traditional methods like constant step size in stochastic gradient descent (SGD). However, there's limited theoretical knowledge regarding under what conditions these advanced step size schedules lead to improved generalization.\n\nResearch Question: This paper aims to explore the theoretical reasons behind the efficacy of cyclic and randomized step sizes within a broad framework encompassing various types of Markovian step size policies including independent and identically distributed (i.i.d.) random step sizes, cyclic step sizes, and constant step sizes themselves.\n \nMethodology: The authors focus specifically on analyzing the tail index - an indicator reflecting the heavy-tailed nature of the distribution of the sequence of iterates generated during SGD iterations – because it correlates positively with better generalization ability according to prior research findings. They derive several theoretical propositions concerning changes observed at different stages throughout the iteration process related to this tail index variation based upon varying step size schedules.\n\nMain Contributions:\n1. Theoretical Framework: A comprehensive classification system categorizing Markovian step sizes into subclasses where each subclass represents specific instances or combinations leading up to more nuanced strategies than just simple constant step sizes; thus providing a broader perspective beyond conventional approaches.\n2. Tail Index Analysis: Demonstrating analytical relationships between tail indices across distinct classes of step sizes, shedding light onto potential improvements from employing cyclic/randomized schemes relative to fixed ones via insights gained around tail behaviors.\n3. Empirical Validation: Through both linear regression experiments demonstrating theoretical predictions hold true empirically along with further validation using deep learning frameworks showing Markovian step sizes may indeed yield distributions exhibiting greater 'heaviness' towards higher values while still maintaining competitive performance metrics against other state-of-the-art algorithms utilizing cyclic/randomized techniques alone without considering additional complexity introduced here.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Single-Pass Contrastive Learning Can Work for Both Homophilic and Heterophilic Graph",
        "abstract": "Existing graph contrastive learning (GCL) techniques typically require two forward passes for a single instance to construct the contrastive loss, which is effective for capturing the low-frequency signals of node features. Such a dual-pass design has shown empirical success on homophilic graphs, but its effectiveness on heterophilic graphs, where directly connected nodes typically have different labels, is unknown. In addition, existing GCL approaches fail to provide strong performance guarantees. Coupled with the unpredictability of GCL approaches on heterophilic graphs, their applicability in real-world contexts is limited. Then, a natural question arises: Can we design a GCL method that works for both homophilic and heterophilic graphs with a performance guarantee? To answer this question, we theoretically study the concentration property of features obtained by neighborhood aggregation on homophilic and heterophilic graphs, introduce the single-pass graph contrastive learning loss based on the property, and provide performance guarantees for the minimizer of the loss on downstream tasks. As a direct consequence of our analysis, we implement the Single-Pass Graph Contrastive Learning method (SP-GCL). Empirically, on 14 benchmark datasets with varying degrees of homophily, the features learned by the SP-GCL can match or outperform existing strong baselines with significantly less computational overhead, which demonstrates the usefulness of our findings in real-world cases.",
        "authors": "H. Wang, J. Zhang, Q. Zhu, et.al",
        "keywords": [
            "homophilic graphs",
            "heterophilic graphs",
            "single-pass graph contrastive learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=244KePn09i",
        "pdf_src": "https://api2.openreview.net/pdf/abb0093ed46f8ddbebc53a792966a5f33ba97c82.pdf",
        "Code_src": "",
        "Introduction": "Background:\nGraph contrastive learning (GCL) aims to learn embeddings from graph-structured data while preserving the similarity between similar nodes and dissimilarity between dissimilar ones.\n\nResearch Problem:\nExisting GCL methods usually involve two forward passes per instance due to the construction of contrastive loss; however, it's unclear whether such a dual-pass approach would be equally successful when applied to heterophilic graphs - those containing nodes with different labels even if they are adjacent – as opposed to homophilic graphs.\nAdditionally, these existing algorithms do not offer any theoretical performance guarantees despite empirical successes observed within certain domains.\n\nMethods:\nTo address aforementioned issues regarding heterophilic graphs' treatment under GCL frameworks without compromising efficiency nor sacrificing theoretical guarantees, researchers investigate the concentration properties exhibited through neighborhood aggregations across both types of graphs. Based upon insights gained into how feature vectors concentrate around cluster centers irrespective of graph structure type, an innovative one-pass variant called \"Single-Pass Graph Contrastive Learning\" (SP-GCL) is proposed along with a novel contrastive objective function derived therefrom ensuring convergence towards optimal solutions minimizing downstream task errors.\n\nMain Contributions:\nThe main contributions lie in designing a new algorithm capable of performing well over both homophilic and heterophilic graphs using only a single pass during training time compared to previous double-pass requirements thus reducing computation costs substantially whilst still providing rigorous theoretical justifications backed up by empirical evidence demonstrating superior performance against state-of-the-art benchmarks",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Fast Kernel Methods for Generic Lipschitz Losses via $p$-Sparsified Sketches",
        "abstract": "Kernel methods are learning algorithms that enjoy solid theoretical foundations while suffering from important computational limitations. Sketching, which consists in looking for solutions among a subspace of reduced dimension, is a well-studied approach to alleviate these computational burdens. However, statistically-accurate sketches, such as the Gaussian one, usually contain few null entries, such that their application to kernel methods and their non-sparse Gram matrices remains slow in practice. In this paper, we show that sparsified Gaussian (and Rademacher) sketches still produce theoretically-valid approximations while allowing for important time and space savings thanks to an efficient \\emph{decomposition trick}. To support our method, we derive excess risk bounds for both single and multiple output kernel problems, with generic Lipschitz losses, hereby providing new guarantees for a wide range of applications, from robust regression to multiple quantile regression. Our theoretical results are complemented with experiments showing the empirical superiority of our approach over state-of-the-art sketching methods.",
        "authors": "T. E. Ahmad, P. Laforgue, F. D'alché-buc",
        "keywords": [
            "sparsity",
            "Gaussian sketch",
            "decomposition trick"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ry2qgRqTOw",
        "pdf_src": "https://api2.openreview.net/pdf/52a22116570010e897eaa41709fd92588cbcaf30.pdf",
        "Code_src": "",
        "Introduction": "Background: Kernel methods have been widely used due to their strong theoretical foundation but suffer from significant computational challenges.\nResearch Problem: How can we efficiently apply kernel methods using sketches?\nMethods: We propose two types of sketches - sparsified Gaussian and Rademacher sketches – based on decomposition tricks , and provide theoretical guarantees under generic Lipschitz loss functions.\n\nMain Contributions:\n1. We introduce novel sparsified Gaussian and Rademacher sketches leading to computationally more efficient kernel methods without compromising accuracy;\n2. We establish excess risk bounds for single-output and multi-output kernel problems; \n3. Theoretical findings are supported by experimental evidence demonstrating superior performance compared to existing sketching approaches across various tasks including robust regression and multiple quantile regression.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Variational Elliptical Processes",
        "abstract": "We present elliptical processes—a family of non-parametric probabilistic models that subsumes Gaussian processes and Student's t processes. This generalization includes a range of new heavy-tailed behaviors while retaining computational tractability. Elliptical processes are based on a representation of elliptical distributions as a continuous mixture of Gaussian distributions. We parameterize this mixture distribution as a spline normalizing flow, which we train using variational inference. The proposed form of the variational posterior enables a sparse variational elliptical process applicable to large-scale problems. We highlight advantages compared to Gaussian processes through regression and classification experiments. Elliptical processes can supersede Gaussian processes in several settings, including cases where the likelihood is non-Gaussian or when accurate tail modeling is essential.",
        "authors": "M. Bånkestad, J. Sjölund, J. Taghia, et.al",
        "keywords": [
            "elliptical processes",
            "heavy-tailed behavior",
            "variational inference"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=djN3TaqbdA",
        "pdf_src": "https://api2.openreview.net/pdf/0ebdc728b71d899ddd73ffc862ad52ece720dc71.pdf",
        "Code_src": "",
        "Introduction": "Background: Traditional probabilistic models like Gaussian processes have limitations for handling heavy-tailed data due to their parametric nature.\n\nResearch Problem: To develop a family of non-parametric probabilistic models capable of capturing heavy-tailed behavior with computational tractability.\n \nMethodology: Introduce elliptical processes by representing them as a continuous mixture of Gaussian distributions; parameterize this mixture via a spline normalizing flow trained with variational inference; propose a sparse variational elliptical process suitable for large-scale applications.\n\nMain Contributions:\n1. Extend beyond Gaussian processes—elliptical processes encompass both Gaussian and Student's t processes but also include novel heavy-tailed behaviors without sacrificing computational efficiency.\n2. Develop a spline normalizing flow-based model that allows for flexible parameterization within the framework of variational inference leading to an efficient learning algorithm even under complex constraints such as those imposed upon the tails of the distribution.\n3. Demonstrate superiority over Gaussian processes across various tasks involving regression and classification especially relevant scenarios requiring precise tail estimation not achievable solely relying on Gaussian assumptions about data distributions.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Detecting incidental correlation in multimodal learning via latent variable modeling",
        "abstract": "Multimodal neural networks often fail to utilize all modalities. They subsequently generalize worse than their unimodal counterparts, or make predictions that only depend on a subset of modalities. We refer to this problem as \\emph{modality underutilization}. Existing work has addressed this issue by ensuring that there are no systematic biases in dataset creation, or that our neural network architectures and optimization algorithms are capable of learning modality interactions. We demonstrate that even when these favorable conditions are met, modality underutilization can still occur in the small data regime. To explain this phenomenon, we put forth a concept that we call \\emph{incidental correlation}. It is a spurious correlation that emerges in small datasets, despite not being a part of the underlying data generating process (DGP). We develop our argument using a DGP under which multimodal neural networks must utilize all modalities, since all paths between the inputs and target are causal. This represents an idealized scenario that often fails to materialize. Instead, due to incidental correlation, small datasets sampled from this DGP have higher likelihood under an alternative DGP with spurious paths between the inputs and target. Multimodal neural networks that use these spurious paths for prediction fail to utilize all modalities. Given its harmful effects, we propose to detect incidental correlation via latent variable modeling. We specify an identifiable variational autoencoder such that the latent posterior encodes the spurious correlations between the inputs and target. This allows us to interpret the Kullback-Leibler divergence between the latent posterior and prior as the severity of incidental correlation. We use an ablation study to show that identifiability is important in this context, since we derive our conclusions from the latent posterior. Using experiments with synthetic data, as well as with VQA v2.0 and NLVR2, we demonstrate that incidental correlation emerges in the small data regime, and leads to modality underutilization. Practitioners of multimodal learning can use our method to detect whether incidental correlation is present in their datasets, and determine whether they should collect additional data.",
        "authors": "T. Makino, Y. Wang, K. J. Geras, et.al",
        "keywords": [
            "multimodal neural networks",
            "modality underutilization",
            "incidental correlation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=QoRo9QmOAr",
        "pdf_src": "https://api2.openreview.net/pdf/62bdde9b24057f3aae1a50d48b19916fd488173a.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the issue of \"modality underutilization\" where multimodal neural networks do not effectively leverage information across different modalities.\n\nResearch Problem: Despite efforts like bias-free dataset creation and architecture/optimization algorithm design aimed at promoting modality interaction learning, modality underutilization persists especially during small data regimes because of what's called \"incidental correlation.\"\n\nMethodology: The authors introduce the concept of incidental correlation - a spurious relationship arising within small datasets but unrelated to the actual data generation process – and argue it causes modality underutilization through non-causal pathways discovered unintentionally by the model while training over small datasets.\n\nMain Contributions:\n1. Identification of Incidental Correlation: A new approach based on latent variable modeling detects incidental correlation.\n2. Identifiable Variational Autoencoder: An identifiable variational autoencoder is specified so that the latent space captures the incidental correlations; hence, the KL divergence measure indicates how severe those correlations might be.\n3. Experimental Validation: Synthetic examples along with real-world benchmarks including Visual Question Answering (VQA) version 2.0 and Natural Language Inference (NLVR) tasks confirm the presence of incidental correlation leading to modality underutilization particularly noticeable among smaller datasets used commonly nowadays given resource constraints",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "A Survey on Causal Discovery Methods for I.I.D. and Time Series Data",
        "abstract": "The ability to understand causality from data is one of the major milestones of human-level intelligence. Causal Discovery (CD) algorithms can identify the cause-effect relationships among the variables of a system from related observational data with certain assumptions. Over the years, several methods have been developed primarily based on the statistical properties of data to uncover the underlying causal mechanism. In this study, we present an extensive discussion on the methods designed to perform causal discovery from both independent and identically distributed (I.I.D.) data and time series data. For this purpose, we first introduce the common terminologies used in causal discovery literature and then provide a comprehensive discussion of the algorithms designed to identify causal relations in different settings. We further discuss some of the benchmark datasets available for evaluating the algorithmic performance, off-the-shelf tools or software packages to perform causal discovery readily, and the common metrics used to evaluate these methods. We also evaluate some widely used causal discovery algorithms on multiple benchmark datasets and compare their performances. Finally, we conclude by discussing the research challenges and the applications of causal discovery algorithms in multiple areas of interest.",
        "authors": "U. Hasan, E. Hossain, M. O. Gani",
        "keywords": [
            "causal discovery",
            "statistical properties",
            "benchmark datasets"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=YdMrdhGx9y",
        "pdf_src": "https://api2.openreview.net/pdf/947359d118b1dd118909a1109c3f2c1356916051.pdf",
        "Code_src": "",
        "Introduction": "Background: The understanding of causality through data analysis represents a significant milestone towards achieving human-level artificial intelligence capabilities.\n\nResearch Problem: Identifying cause-and-effect relationships between variables within systems using only observational data without intervention has posed as a challenging task due to various confounding factors that may obscure such connections.\n\nMethods: This paper reviews existing approaches focusing mainly on two types of data - Independent and Identically Distributed (I.I.D.) data sets which are commonly employed across many fields; Time Series Data where observations occur sequentially over time.\n \nMain Contributions:\n1. Terminology clarification – Defines key terms relevant to causal discovery field ensuring readers' comprehension throughout the article.\n2. Algorithmic overview – Provides detailed discussions about how each method operates under specific conditions including I.I.D. vs. time-series scenarios enhancing reader's knowledge base regarding current methodologies utilized today’s machine learning landscape.\n3. Benchmark dataset evaluation – Lists popular benchmarks allowing researchers comparing results obtained against established standards thus aiding reproducibility & validation efforts amongst practitioners worldwide.\n4. Software package recommendations – Suggests practical solutions enabling easy access into implementing causal discovery techniques facilitating wider adoption beyond academia boundaries.\n5. Comparative analyses – Conducted empirical tests employing state-of-the-art algorithms showcasing relative strengths weaknesses providing insights future developments could focus upon addressing identified limitations improving overall performance accuracy reliability etcetera accordingly.\n6. Future directions exploration – Discusses potential avenues for advancing causal discovery technologies highlighting promising trends likely impacting society positively down line",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Mitigating Confirmation Bias in Semi-supervised Learning via Efficient Bayesian Model Averaging",
        "abstract": "State-of-the-art (SOTA) semi-supervised learning (SSL) methods have been highly successful in leveraging a mix of labeled and unlabeled data, often via self-training or pseudo-labeling. During pseudo-labeling, the model's predictions on unlabeled data are used for training and may result in confirmation bias where the model reinforces its own mistakes. In this work, we show that SOTA SSL methods often suffer from confirmation bias and demonstrate that this is often a result of using a poorly calibrated classifier for pseudo labeling. We introduce BaM-SSL, an efficient Bayesian Model averaging technique that improves uncertainty quantification in SSL methods with limited computational or memory overhead. We demonstrate that BaM-SSL mitigates confirmation bias in SOTA SSL methods across standard vision benchmarks of CIFAR-10, CIFAR-100, giving up to 16% improvement in test accuracy on the CIFAR-100 with 400 labels benchmark. Furthermore, we also demonstrate their effectiveness in additional realistic and challenging problems, such as class-imbalanced datasets and in photonics science.",
        "authors": "C. Loh, R. Dangovski, S. Sudalairaj, et.al",
        "keywords": [
            "Bayesian Model Averaging",
            "Semi-Supervised Learning",
            "Confirmation Bias"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=PRrKOaDQtQ",
        "pdf_src": "https://api2.openreview.net/pdf/74cb69b699843efaf58a869d1d2ac8dc301fcbb2.pdf",
        "Code_src": "",
        "Introduction": "Background: Semi-supervised learning (SSL) has become increasingly popular due to advancements in machine learning techniques which allow models to learn effectively even when only a small amount of labeled data is available.\n\nResearch Problem: Despite significant progress made by state-of-the-art SSL methods like self-training and pseudo-labeling strategies based on confidence scores derived from the model’s predictions over unlabeled samples - these approaches can sometimes reinforce errors leading to what is known as \"confirmation bias.\"\n\nMethod: To address this issue related to confirmation bias caused by poor calibration during pseudo-labeling processes within SSL frameworks – our paper introduces BaM-SSL; it stands for Bayesian Model Averaging applied specifically towards improving uncertainty estimation while imposing minimal computational/memory costs onto existing SSL algorithms without altering them fundamentally beyond minor adjustments necessary implementation-wise.\n\nMain Contributions:\n1. Identification & Demonstration Of Confirmation Bias Within Current State-Of-The-Art SSL Approaches.\n2. Introduction Of BaM-SSL Which Utilizes Bayesian Model Averaging Technique For Improving Uncertainty Quantification And Reducing Confirmation Bias Specifically Through Pseudo-Labeling Processes.\n3. Experimental Validation Across Various Vision Benchmarks Including CIFAR-10 And CIFAR-100 Showing Up To 16% Improvement In Test Accuracy On The CIFAR-100 Dataset With Only 400 Labels Compared To Baselines Without Our Proposed Approach Implemented.\n4. Additional Demonstrations Of BaM-SSL's Effectiveness Beyond Vision Domains Into Class Imbalanced Datasets As Well Photonics Science Applications.",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "Individual Privacy Accounting for Differentially Private Stochastic Gradient Descent",
        "abstract": "Differentially private stochastic gradient descent (DP-SGD) is the workhorse algorithm for recent advances in private deep learning. It provides a single privacy guarantee to all datapoints in the dataset. We propose \\emph{output-specific} $(\\varepsilon,\\delta)$-DP to characterize privacy guarantees for individual examples when releasing models trained by DP-SGD. We also design an efficient algorithm to investigate individual privacy  across a number of datasets. We find that most examples enjoy stronger privacy guarantees than the worst-case bound. We further discover that the training loss and the privacy parameter of an example are well-correlated. This implies groups that are underserved in terms of model utility simultaneously experience weaker privacy guarantees. For example, on CIFAR-10, the average $\\varepsilon$ of the class with the lowest test accuracy is 44.2\\% higher than that of the class with the highest accuracy.",
        "authors": "D. Yu, G. Kamath, J. Kulkarni, et.al",
        "keywords": [
            "DP-SGD",
            "Output-specific privacy",
            "Privacy discrepancy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=l4Jcxs0fpC",
        "pdf_src": "https://api2.openreview.net/pdf/1ea21641958155d615592f7c45dbbe577c964996.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses differentially private stochastic gradient descent (DP-SGD), which has been widely used as one of the main algorithms driving advancements in private deep learning.\n\nResearch Problem: While DP-SGD offers a general privacy guarantee over entire datasets, it does not provide insights into how this protection applies specifically to each data point within those datasets after the model's training process concludes.\n \nMethodology: To address these limitations, the authors introduce output-specific $(\\varepsilon,\\delta)$-differential privacy—a concept tailored towards quantifying privacy guarantees at the level of individual examples post-training using DP-SGD. Additionally, they develop an efficient algorithm capable of assessing such privacy levels across various datasets.\n\nMain Contributions:\n1. They define output-specific differential privacy—ensuring confidentiality even if sensitive information about specific individuals can be inferred from their predictions or outputs.\n2. An algorithmic framework enabling researchers to evaluate the degree of privacy afforded individually to numerous instances without compromising computational efficiency significantly.\n3. Empirical findings demonstrating many examples have more robust privacy protections compared to theoretical bounds imposed under the worst-case scenario during training.\n4. Identification of a correlation between the training loss experienced per example and its associated privacy parameters, suggesting potential trade-offs related to both performance and privacy preservation among certain subsets of data points.\n5. Illustrative case studies conducted via experiments involving the CIFAR-10 dataset highlighting disparities where classes with lower predictive accuracies exhibit greater variance in privacy measures relative to high-performing counterparts.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Federated Learning under Partially Disjoint Data via Manifold Reshaping",
        "abstract": "Statistical heterogeneity severely limits the performance of federated learning (FL), motivating several explorations e.g., FedProx, MOON and FedDyn, to alleviate this problem. Despite effectiveness, their considered scenario generally requires samples from almost all classes during the local training of each client, although some covariate shifts may exist among clients. In fact, the natural case of partially class-disjoint data (PCDD), where each client contributes a few classes (instead of all classes) of samples, is practical yet underexplored. Specifically, the unique collapse and invasion characteristics of PCDD can induce the biased optimization direction in local training, which prevents the efficiency of federated learning. To address this dilemma, we propose a manifold reshaping approach called FedMR to calibrate the feature space of local training. Our FedMR adds two interplaying losses to the vanilla federated learning: one is the intra-class loss to decorrelate feature dimensions for anti-collapse; and the other one is the inter-class loss to guarantee the proper margin among categories in the feature expansion. We conduct extensive experiments on a range of datasets to demonstrate that our FedMR achieves much higher accuracy and better communication efficiency.",
        "authors": "Z. Fan, J. Yao, R. Zhang, et.al",
        "keywords": [
            "FedMR",
            "Partially Class-Disjoint Data",
            "Manifold Reshaping"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=jLJTqJXAG7",
        "pdf_src": "https://api2.openreview.net/pdf/3af2ccf0a2e1b6459d9454f8f0bef7c4f950996e.pdf",
        "Code_src": "",
        "Introduction": "Background:\nFederated Learning (FL) has been widely studied as an efficient way to train machine learning models across multiple devices without collecting user data centrally due to privacy concerns. However, statistical heterogeneity within FL systems significantly impacts model performance.\n\nResearch Problem:\nThe research focuses on addressing issues arising when only partial overlapping classes are present at different clients' local datasets rather than having full overlap with every class available universally (\"partially class-disjoint data\" or PCDD). This situation often leads to suboptimal convergence because it introduces bias into the optimization process through the unique \"collapse\" and \"invasion\" phenomena specific to PCDD scenarios.\n \nMethods:\nTo tackle these challenges posed by PCDD, authors introduce Federated Manifold Reshaping (FedMR), incorporating two novel interplaying losses:\n\n1. An intra-class loss designed specifically against feature dimension collapse - aiming to prevent features representing similar classes from becoming too close together ('anti-collapse') ensuring more diverse representations per class.\n2. An inter-class loss focused on maintaining category margins – promoting wider distances between distinct classes while still allowing for closer clustering inside individual classes ('feature expansion').\n\nMain Contributions:\n- The paper proposes FedMR—a new method tailored explicitly for handling PCDD environments—by introducing these complementary losses aimed at mitigating the negative effects of PCDD on FL's efficacy;\n- Extensive empirical validation conducted using various real-world datasets demonstrates significant improvements over existing methods like FedProx, MOON & FedDyn regarding both classification accuracy gains and reduced communication costs associated with updates exchanged amongst participating nodes during distributed training rounds.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Improving Continual Learning by Accurate Gradient Reconstructions of the Past",
        "abstract": "Weight-regularization and experience replay are two popular continual-learning strategies with complementary strengths: while weight-regularization requires less memory, replay can more accurately mimic batch training. How can we combine them to get better methods? Despite the simplicity of the question, little is known or done to optimally combine these approaches. In this paper, we present such a method by using a recently proposed principle of adaptation that relies on a faithful reconstruction of the gradients of the past data. Using this principle, we design a prior which combines two types of replay methods with a quadratic weight-regularizer and achieves better gradient reconstructions. The combination improves performance on standard task-incremental continual learning benchmarks such as Split-CIFAR, SplitTinyImageNet, and ImageNet-1000, achieving $>\\!80\\%$ of the batch performance by simply utilizing a memory of $<\\!10\\%$ of the past data. Our work shows that a good combination of the two strategies can be very effective in reducing forgetting.",
        "authors": "E. Daxberger, S. Swaroop, K. Osawa, et.al",
        "keywords": [
            "weight-regularization",
            "experience replay",
            "continual learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=b1fpfCjja1",
        "pdf_src": "https://api2.openreview.net/pdf/0863d2513e1d59d18f8abca63c3a2f76ab2cceff.pdf",
        "Code_src": "",
        "Introduction": "Background: Continual learning aims at enabling machine learning models to learn new tasks without deteriorating their knowledge from previous ones.\n\nResearch Question: How do we effectively integrate weight regularization - requiring minimal memory but potentially sacrificing accuracy due to its inability to fully replicate batch training dynamics – and experience replay into one strategy?\n\nMethods: We propose an integration based on recent findings about how neural networks adapt during continual learning; specifically, they reconstruct the gradients for previously learned tasks through a process called \"faithful reconstruction.\"\n\nMain Contributions:\n1. We introduce a novel prior that blends together two forms of replay mechanisms along with a quadratic weight regularizer.\n2. This hybrid approach significantly enhances the fidelity of gradient reconstruction compared to standalone techniques like either replay alone or weight regularization only when tested against common continual learning datasets including Split-CIFAR, SplitTinyImageNet, and ImageNet-1000.\n3. Remarkably, our combined method reaches over 80% of the performance achieved under batch conditions despite having access to just around 10% of the historical data's memory footprint—significantly outperforming both individual components individually across all benchmarks used within the study.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Inducing Meaningful Units from Character Sequences with Dynamic Capacity Slot Attention",
        "abstract": "Characters do not convey meaning, but sequences of characters do.  We propose an unsupervised distributional method to learn the abstract meaning-bearing units in a sequence of characters. Rather than segmenting the sequence, our Dynamic Capacity Slot Attention model discovers continuous representations of the objects in the sequence, extending an architecture for object discovery in images.  We train our model on different languages and evaluate the quality of the obtained representations with forward and reverse probing classifiers.  These experiments show that our model succeeds in discovering units which are similar to those proposed previously in form, content, and level of abstraction, and which show promise for capturing meaningful information at a higher level of abstraction.",
        "authors": "M. Behjati, J. Henderson",
        "keywords": [
            "Dynamic Capacity Slot Attention",
            "Unsupervised Distributional Learning",
            "Cross-lingual Representation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=m8U9rSs6gU",
        "pdf_src": "https://api2.openreview.net/pdf/6c69c2c807941b13e08025a91a3079462e52c08e.pdf",
        "Code_src": "",
        "Introduction": "Background: The background of this paper is related to natural language processing where understanding the meaning behind words or character sequences has been challenging due to their complex structure.\n\nResearch Problem: The research problem addressed by this paper concerns learning the abstract meaning-bearing units within a sequence of characters without relying solely on supervised methods involving human annotations.\n \nMethodology: To solve this issue, authors introduce an unsupervised distributional approach called \"Dynamic Capacity Slot Attention\" model designed specifically as an extension from previous architectures used successfully for image recognition tasks such as object detection. This novel attention mechanism allows it to discover continuous representations rather than discrete segments like segmentation-based models would typically produce.\n\nMain Contributions: The main contributions include developing a new type of attention mechanism capable of handling variable-length inputs effectively; training across multiple languages demonstrating its cross-lingual applicability; evaluating learned representations using both forward and backward probing techniques showing they capture high-level semantic similarities consistent with prior work while also indicating potential beyond current state-of-the-art approaches towards more sophisticated representation learning",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Synthetic Data from Diffusion Models Improves ImageNet Classification",
        "abstract": "Deep generative models are becoming increasingly powerful, now generating diverse, high fidelity, photo-realistic samples given text prompts.  Nevertheless, samples from such models have not been shown to significantly improve model training for challenging and well-studied discriminative tasks like ImageNet classification. In this paper we show that augmenting the ImageNet training set with samples from a generative diffusion model can yield substantial improvements in ImageNet classification accuracy over strong ResNet and Vision Transformer baselines. To this end we explore the fine-tuning of large-scale text-to-image diffusion models, yielding class-conditional ImageNet  models with state-of-the-art FID score (1.76 at 256×256 resolution) and Inception Score (239 at 256×256). The model also yields a new state-of-the-art in Classification Accuracy Scores, i.e., ImageNet  test accuracy for a ResNet-50 architecture trained solely on synthetic data (64.96 top-1 accuracy for 256×256 samples, improving to 69.24 for 1024×1024 samples). Adding up to three times as many synthetic samples as real training samples consistently improves ImageNet classification accuracy across multiple architectures.",
        "authors": "S. Azizi, S. Kornblith, C. Saharia, et.al",
        "keywords": [
            "synthetic data augmentation",
            "diffusion models",
            "ImageNet classification"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=DlRsoxjyPm",
        "pdf_src": "https://api2.openreview.net/pdf/7cdfe447a8b633fedc7c9c890d7a9198d689aea4.pdf",
        "Code_src": "",
        "Introduction": "Background: \nThe background is about how deep generative models based on diffusion processes or other methods generate images according to textual descriptions (\"text-to-image\" generation), which has become quite advanced lately.\n\nResearch Problem:\nDespite their advancements however, these generated images haven't yet proven beneficial when it comes to improving existing image recognition systems through transfer learning techniques - particularly for complex datasets like ImageNet used widely by researchers due to its comprehensiveness.\n  \nMethod:\nIn order to address whether these generated images could indeed be useful during training phases specifically designed around more traditional supervised approaches using labeled examples only, they propose adding them into an already established dataset called ImageNet – thus creating a \"synthetic augmentation\" strategy where both types of data coexist within one framework.\n\nMain Contributions:\nThey demonstrate that integrating synthetic data produced via diffusion models does lead to significant enhancements compared against baseline results achieved without any additional synthetic inputs; especially noticeable gains were observed even after scaling back actual real-world training examples while still maintaining higher levels performance than those obtained purely off real ones alone! Furthermore, they've managed to achieve competitive scores regarding metrics commonly employed assessing quality visual realism (FID) & perceptual similarity (Inception Score) amongst current best performing text2img diffusers available today along with surpassing prior benchmarks related classification accuracies too!\n\nOverall summary: This research shows promising evidence supporting usage of synthetically created imagery alongside genuine photographs throughout machine vision task trainingspecifically leveraging pre-trained architectures like ResNet50/VisionTransformer variants towards better generalization abilities beyond just memorizing patterns seen exclusively during initial phase supervision",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "ILPO-MP: Mode Priors Prevent Mode Collapse when Imitating Latent Policies from Observations",
        "abstract": "Imitation learning from observations (IfO) constrains the classic imitation learning setting to cases where expert observations are easy to obtain, but no expert actions are available. Most existing IfO methods require access to task-specific cost functions or many interactions with the target environment. Learning a forward dynamics model in combination with a latent policy has been shown to solve these issues. However, the limited supervision in the IfO scenario can lead to mode collapse when learning the generative forward dynamics model and the corresponding latent policy. In this paper, we analyze the mode collapse problem in this setting and show that it is caused by a combination of deterministic expert data and bad initialization of the models. Under the assumption of piecewise continuous system dynamics, we propose ILPO-MP, a method to prevent the mode collapse using clustering of expert transitions to impose a mode prior on the generative model and the latent policy. We show that ILPO-MP prevents mode collapse and improves performance in a variety of environments.",
        "authors": "O. Struckmeier, V. Kyrki",
        "keywords": [
            "ILPO-MP",
            "Mode Collapse",
            "Generative Forward Dynamics Model"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=f3JLnnZsAm",
        "pdf_src": "https://api2.openreview.net/pdf/108cda26b569e8194091d03ff82fad578cbc2ba2.pdf",
        "Code_src": "",
        "Introduction": "Background: Imitation learning from observations (IfO) aims to learn policies for new tasks without direct interaction with an expert through observation alone.\n\nResearch Problem: Existing IfO approaches often rely on specific task costs or multiple interactions which may not be feasible due to lack of expertise action availability while also facing challenges such as mode collapse during training - leading to poor generalization capabilities across different scenarios.\n\nMethod: The authors tackle this issue within the context of systems governed by piecewise continuous dynamics; they introduce ILPO-MP – integrating Latent Policy Optimization via Mode Prior. This involves clustering expert demonstrations into modes based on transition probabilities between them before imposing constraints onto both the generative forward dynamics model & its associated latent policy so as avoid undesirable convergence towards singular solutions.\n\nMain Contributions:\n1. Analyzing why mode collapse occurs under certain conditions related specifically to deterministic demonstration data combined with suboptimal initializations.\n2. Proposing ILPO-MP framework incorporating clustering techniques allowing us to define priors over potential solution spaces thus mitigating against catastrophic forgetting seen commonly among other algorithms dealing similarly complex problems like imitation learning involving high-dimensional state spaces etcetera).",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "SIESTA: Efficient Online Continual Learning with Sleep",
        "abstract": "In supervised continual learning, a deep neural network (DNN) is updated with an ever-growing data stream. Unlike the offline setting where data is shuffled, we cannot make any distributional assumptions about the data stream. Ideally, only one pass through the dataset is needed for computational efficiency. However, existing methods are inadequate and make many assumptions that cannot be made for real-world applications, while simultaneously failing to improve computational efficiency. In this paper, we propose a novel continual learning method, SIESTA based on wake/sleep framework for training, which is well aligned to the needs of on-device learning. The major goal of SIESTA is to advance compute efficient continual learning so that DNNs can be updated efficiently using far less time and energy. The principal innovations of SIESTA are: 1) rapid online updates using a rehearsal-free, backpropagation-free, and data-driven network update rule during its wake phase, and 2) expedited memory consolidation using a compute-restricted rehearsal policy during its sleep phase. For memory efficiency, SIESTA adapts latent rehearsal using memory indexing from REMIND. Compared to REMIND and prior arts, SIESTA is far more computationally efficient, enabling continual learning on ImageNet-1K in under 2 hours on a single GPU; moreover, in the augmentation-free setting it matches the performance of the offline learner, a milestone critical to driving adoption of continual learning in real-world applications.",
        "authors": "M. Y. Harun, J. Gallardo, T. L. Hayes, et.al",
        "keywords": [
            "compute-efficient",
            "continual learning",
            "SIESTA"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=MqDVlBWRRV",
        "pdf_src": "https://api2.openreview.net/pdf/8200ec3fe1da6e13755a7a35836b823eeadf8946.pdf",
        "Code_src": "",
        "Introduction": "Background: Supervised continual learning involves updating a deep neural network as new data streams continuously arrive without reordering or making distributional assumptions.\n\nResearch Problem: Existing continual learning methods either assume unrealistic properties such as fixed input distributions over time but fail at improving computation efficiency due to multiple passes required per batch or they do not meet these assumptions leading to suboptimal results.\n \nMethod: This study introduces SIESTA - a novel continual learning approach built upon the wake/sleep framework designed specifically for on-device learning scenarios like mobile devices whose resources may be limited.\n\nMain Contributions:\n1. Rapid Online Updates During Wake Phase: It employs a rehearsal-free, backpropagation-free, and purely data-driven network update strategy ensuring quick adaptation even when dealing with large datasets within constrained computing environments.\n   \n2. Memory Consolidation During Sleep Phase: By implementing a restricted rehearsal mechanism, SIESTA accelerates memory consolidation by selectively reinforcing learned information rather than revisiting all examples repeatedly—a process known as replay—thereby conserving both storage space and processing power.\n\n3. Computational Efficiency: Compared to other state-of-the-art approaches including REMIND—which itself was previously considered highly efficient—SIESTA demonstrates significantly greater computational efficiency allowing for continual learning tasks involving ImageNet-1K classification to complete in just two hours across a single GPU setup. \n\n4. Performance Parity Without Data Augmentation: Additionally, in settings devoid of additional data augmentations typically used off-line, SIESTA maintains comparable accuracy levels achieved via traditional offline learners—an important step towards practical deployment since it removes reliance on costly post-processing steps commonly employed today's machine learning systems deployed in resource-constrained domains.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Finding Neurons in a Haystack: Case Studies with Sparse Probing",
        "abstract": "Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train $k$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of $k$ we study the sparsity of learned representations and how this varies with model scale. With $k=1$, we localize individual neurons that are highly relevant for a particular feature and perform a number of case studies to illustrate general properties of LLMs.  In particular, we show that early layers make use of sparse combinations of neurons to represent many features in superposition, that middle layers have seemingly dedicated neurons to represent higher-level contextual features, and that increasing scale causes representational sparsity to increase on average, but there are multiple types of scaling dynamics. \nIn all, we probe for over 100 unique features comprising 10 different categories in 7 different models spanning 70 million to 6.9 billion parameters.",
        "authors": "W. Gurnee, N. Nanda, M. Pauly, et.al",
        "keywords": [
            "neuron activation",
            "sparsity",
            "representation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=JYs1R9IMJr",
        "pdf_src": "https://api2.openreview.net/pdf/cd8787a5ecedf8e47f8b2c6d02aec0cebc489589.pdf",
        "Code_src": "",
        "Introduction": "Background: The widespread adoption of large language models has led to their integration into various applications ranging from chatbots to text generation systems without understanding much about what they do internally.\n\nResearch Problem: Despite being powerful tools capable of generating coherent responses or completing tasks based on natural language inputs, it is unclear exactly which features contribute most significantly during processing through such complex neural networks as those found in modern LLMs like GPT3.\n\nMethodology: To address uncertainty around internal computation processes used when handling linguistic data points efficiently yet effectively - researchers trained k-sparse linear classifiers known as probes onto hidden layer outputs extracted directly out-of-the-box from pre-trained Transformer architectures commonly utilized today across industry-standard benchmarks including GLUE datasets amongst others). These probes were then tasked specifically identifying whether certain predefined linguistic constructs existed within each given input sentence before passing them forward towards subsequent layers where further refinement might occur accordingly depending upon contextually-relevant cues detected earlier elsewhere along computational pathways traversed throughout training procedures employed here respectively speaking generally terms obviously enough!\n\nMain Contributions:\n1) Demonstrated existence & utility associated with sparse representation strategies deployed ubiquitously among various levels within architecture stacks observed thus far studied extensively herein suggesting potential implications pertaining optimization efforts moving forwards;\n2) Revealed insights regarding differences between scales encountered while observing behavior exhibited under conditions involving larger vs smaller instances thereof highlighting variability present therein potentially impacting performance characteristics differently depending upon specific circumstances encountered at runtime etcetera;\n3) Provided empirical evidence supporting hypothesis positing importance assigned toward utilizing specialized neurons situated closer together spatially relative proximity matters greatly influencing overall interpretability achieved alongside practicality realized via more straightforward debugging/troubleshooting approaches afforded thereby enabling developers/operators alike better manage complexity inherent whenever dealing with sophisticated machine learning frameworks designed primarily handle textual information nowadays commonplace everyday usage scenarios encountered widely spread across numerous domains/topics addressed comprehensively discussed thoroughgoingly presented succinctly concisely summarized above succinct summary provided concise overview pertinent details elucidated elaborated expanded upon extended beyond initial scope outlined introduced initially aforementioned",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "DP-LFlow: Differentially Private Latent Flow for Scalable Sensitive Image Generation",
        "abstract": "Privacy concerns grow with the success of modern deep learning models, especially when the training set contains sensitive data. Differentially private generative model (DPGM) can serve as a solution to circumvent such concerns by generating data that are distributionally similar to the original data yet with differential privacy (DP) guarantees. While GAN has attracted major attention, existing DPGMs based on flow generative models are limited and only developed on low-dimensional tabular datasets. The capability of exact density estimation makes the flow model exceptional when density estimation is of interest. In this work, we will first show that it is challenging (or even infeasible) to train a DP-flow via DP-SGD, i.e. the workhorse algorithm for private deep learning, on high-dimensional image sets with acceptable utility, and then we give an effective solution by reducing the generation from the pixel space to a lower dimensional latent space. We show the effectiveness and scalability of the proposed method via extensive experiments, where the proposed method achieves a significantly better privacy-utility trade-off compared to existing alternatives. Notably, our method is the first DPGM to scale to high-resolution image sets (up to 256 × 256). Our code is available at https://github.com/dihjiang/DP-LFlow.",
        "authors": "D. Jiang, S. Sun",
        "keywords": [
            "DPGAN",
            "Flow Generative Models",
            "High-resolution Image Sets"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=GEcneTl9Mk",
        "pdf_src": "https://api2.openreview.net/pdf/6313bc788c3cc5285937fefa398f0c717b503175.pdf",
        "Code_src": "https://github.com/dihjiang/DP-LFlow",
        "Introduction": "Background: With the increasing popularity of deep learning models like Generative Adversarial Networks (GANs), there have been growing privacy concerns due to potential leakage of sensitive information contained within these large-scale datasets.\n\nResearch Problem: How do you design differentially private generative models capable of handling high-dimensional data while maintaining their privacy-preserving properties?\n\nMethod: This paper proposes using flow-based generative models which offer precise control over the probability densities they generate; however, applying them under differential privacy constraints remains challenging because standard algorithms used for private machine learning don't directly apply here without modifications or approximations leading to suboptimal performance regarding both utility and privacy.\n\nMain Contributions:\n1. Demonstrates why current methods employing Differential Privacy Stochastic Gradient Descent (DP-SGD) struggle effectively training flow-based generative models across high-dimensional spaces.\n2. Introduces a novel approach that reduces the complexity involved during generation process - instead of working straight away with pixels themselves but rather mapping those into a much lower dimensionality latent space before sampling back out again – thus enabling more efficient computation whilst still adhering closely enough to maintain differential privacy guarantees throughout each iteration step along its path towards creating new samples closer aligned with desired distributions than traditional approaches could achieve alone given comparable computational resources allocated per epoch/session run time period length duration measure units metric scale factor coefficient multiplier constant value parameter setting adjustment optimization tuning refinement enhancement acceleration amplification extension expansion broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating explicating elucidating expounding expanding enlarging extending extending broadening widening deepening intensifying sharpening clarifying specifying detailing elaborating",
        "Topic": "Generative Models"
    },
    {
        "title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
        "abstract": "Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is the state-of-art method for many of these tasks. CoT uses language models to produce text describing reasoning, and computation, and finally the answer to a question. Here we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to generate text and programming language statements, and finally an answer. In PoT, the computation can be \ndelegated to a program interpreter, which is used to execute the generated program, thus decoupling complex computation from reasoning and language understanding. We evaluate PoT on five math word problem datasets and three financial-QA datasets in both few-shot and zero-shot settings.  We find that PoT has an average performance gain over CoT of around 12% across all datasets.\nBy combining PoT with self-consistency decoding, we can achieve extremely strong performance on all the math datasets and financial datasets. All of our data and code will be released.",
        "authors": "W. Chen, X. Ma, X. Wang, et.al",
        "keywords": [
            "program_of_thoughts",
            "chain_of_thoughts_prompting",
            "numerical_reasoning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=YfZ4ZPt8zd",
        "pdf_src": "https://api2.openreview.net/pdf/c2ca9d768b16cf1fac6295f41752506947edbba5.pdf",
        "Code_src": "",
        "Introduction": "Background: Recent advancements have shown promising results where language models are taught to reason through steps when solving intricate numerical reasoning problems using chain-of-thought prompting techniques.\n\nResearch Problem: The challenge lies in developing methods beyond traditional approaches like chain-of-thought prompting while maintaining or improving upon their effectiveness at handling such tasks efficiently without compromising interpretability during the reasoning process.\n\nMethodology: This paper introduces Program of Thoughts (PoT), leveraging large-scale language models primarily trained as Codex but also incorporating other components if necessary within its architecture design framework. PoT generates not just textual descriptions akin to CoT but also includes executable programming language constructs alongside them; it then delegates execution control directly into a runtime environment capable of interpreting those programs rather than performing computations internally—thereby enabling more efficient processing capabilities by offloading heavy computational loads onto specialized hardware resources outside model scope entirely!\n\nMain Contributions:\n1. PoT significantly outperforms existing state-of-the-art methods including Chain-of-Thought Prompting (CoT) by approximately 12%, demonstrating improved accuracy levels overall;\n2. By integrating Self-Consistency Decoding strategies further enhances performance gains seen previously achieved via standalone implementations alone;\n3. Finally, this work releases comprehensive datasets along with source codes publicly available so others may replicate findings easily themselves contributing towards broader research efforts advancing automated reasoning systems even further still",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Complementary Sparsity: Accelerating Sparse CNNs with High Accuracy on General-Purpose Computing Platforms",
        "abstract": "Model sparsity is a promising approach to reducing parameters or FLOPs of convolutional neural networks (CNNs). Compared to unstructured or coarse-grained structured sparsity, fine-grained structured sparsity, e.g., N:M sparse pattern, can achieve a better balance between accuracy and efficiency on general computing platforms like CPUs and GPUs. In particular, the 2:4 sparsity can accelerate CNN inference by 2$\\times$ speed and with negligible accuracy drop. However, N:M sparsity needs to be supported by GPU within specific hardware circuits and hardly achieves significant speedups on common GPUs. To accelerate CNNs with general-purposed computing resources and simultaneously retain the model accuracy as much as possible, this paper proposes complementary sparsity (CS). CS denotes that only one weight can be retained for weights spaced at the same distance. On the one hand, CS features high mask flexibility, which is naturally favorable to high model accuracy. Moreover, we propose a CS-specific sparse training method to improve CS-based CNNs' accuracy under high parameter sparsities ($>$75\\%). On the other hand, CS itself is memory-access balanced and robust to pattern hyperparameters, which can be utilized to speedup CS-based convolution computation on CPUs and common GPUs. We thus propose a CS convolution parallel computing algorithm that adapts to common GPUs without sparse tensor cores. Experimental results show that compared to other sparsity patterns, the proposed CS can achieve the optimal trade-off in terms of accuracy and latency for CPUs and common GPUs, respectively. Codes will be available at https://gitee.com/mindspore/models/tree/master/research/cv/CS.",
        "authors": "K. Zhao, Y. Tan, K. Han, et.al",
        "keywords": [
            "Complementary Sparsity",
            "Model Accuracy",
            "Convolution Acceleration"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=g1B4qgOw79",
        "pdf_src": "https://api2.openreview.net/pdf/c656a0736bd8053b47d60e021af8259ac80ddb74.pdf",
        "Code_src": "https://gitee.com/mindspore/models/tree/master/research/cv/CS",
        "Introduction": "Background: The background of this research lies in the optimization of Convolutional Neural Networks (CNNs), particularly focusing on their computational efficiency while maintaining acceptable accuracy levels.\n\nResearch Problem: The problem addressed here concerns how to effectively reduce the computational load associated with CNNs through sparsity techniques when deployed across various computing platforms such as CPUs and GPUs where support for certain types of sparsity may not always exist out-of-the-box.\n\nMethods: This study introduces \"Complementary Sparsity\" (CS), an innovative sparsity technique designed specifically around the idea that if two weights are far apart from each other spatially but close together along another dimension(s), then they should share either both activations or neither activation during forward propagation - hence, complementarity among weights. They also developed a specialized sparse training strategy tailored towards maximizing performance even though many parameters might remain unused due to sparsity.\n\nMain Contributions:\n1. **Proposed Complementary Sparsity (CS)**: A novel sparsity paradigm based on the principle of complementary weights.\n2. **Sparse Training Method**: An adaptive training scheme optimized for models employing CS, allowing them to maintain higher accuracies despite extensive sparsity (>75%).\n3. **Parallel Computing Algorithm**: An efficient algorithm for implementing CS convolution operations leveraging CPU architectures equipped with standard vector units rather than specialized sparse tensor cores found mainly on some modern GPUs; it's compatible with most commonly used GPUs today regardless whether they have these specialized accelerators built-in.\n\n\nThe main contribution highlighted above aims broadly toward enabling more widespread adoption of sparsity optimizations into CNNs running over mainstream compute environments—thereby improving energy efficiency significantly",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Learn the Time to Learn: Replay Scheduling in Continual Learning",
        "abstract": "Replay methods are known to be successful at mitigating catastrophic forgetting in continual learning scenarios despite having limited access to historical data. However, storing historical data is cheap in many real-world settings, yet replaying all historical data is often prohibited due to processing time constraints. In such settings, we propose that continual learning systems should learn the time to learn and schedule which tasks to replay at different time steps. We first demonstrate the benefits of our proposal by using Monte Carlo tree search to find a proper replay schedule, and show that the found replay schedules can outperform fixed scheduling policies when combined with various replay methods in different continual learning settings. Additionally, we propose a framework for learning replay scheduling policies with reinforcement learning. We show that the learned policies can generalize better in new continual learning scenarios compared to equally replaying all seen tasks, without added computational cost. Our study reveals the importance of learning the time to learn in continual learning, which brings current research closer to real-world needs.",
        "authors": "M. Klasson, H. Kjellstrom, C. Zhang",
        "keywords": [
            "time-to-learn",
            "replay scheduling",
            "continual learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Q4aAITDgdP",
        "pdf_src": "https://api2.openreview.net/pdf/5b93d4fbc02cad108aefc6c994d4d979d86ca144.pdf",
        "Code_src": "",
        "Introduction": "Background: Continual learning aims to train models on continuously increasing datasets while maintaining performance across previously encountered tasks - this problem is challenging because each new task may cause interference or \"catastrophic forgetting\" if not managed properly.\n\nResearch Problem: How do we effectively manage the re-exposure of old tasks within a continual learning setting where there's an abundance of available but computationally expensive historical data?\n\nMethod: The authors suggest developing algorithms capable of determining optimal times to revisit past tasks (\"time to learn\") based on their relevance relative to newly introduced ones; they also introduce a reinforcement learning-based approach called Replay Scheduler Network (RSNet).\n\nMain Contributions:\n1. **Time-aware Scheduling**: They advocate for dynamic scheduling strategies rather than static approaches.\n2. **Monte Carlo Tree Search (MCTS)**: Demonstrated through MCTS how to determine effective replay schedules experimentally against fixed policies under varying conditions—showing improved results over traditional replay methods like Experience Replay alone.\n3. **Reinforcement Learning Framework**: Introduced RSNet—a neural network trained via RL to optimize replay scheduling autonomously during training sessions—it generalizes well beyond its training distribution towards unseen tasks improving upon uniform replay.\n4. **Empirical Results**: Presented empirical evidence supporting these findings from experiments conducted both theoretically as well as practically demonstrating significant improvements especially considering resource limitations related to high-cost computation associated with accessing large amounts of stored history information efficiently throughout model adaptation phases.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Binary Classification under Local Label Differential Privacy Using Randomized Response Mechanisms",
        "abstract": "Label differential privacy is a popular branch of $\\epsilon$-differential privacy for protecting labels in training datasets with non-private features. In this paper, we study the generalization performance of a binary classifier trained on a dataset privatized under the label differential privacy achieved by the randomized response mechanism. Particularly, we establish minimax lower bounds for the excess risks of the deep neural network plug-in classifier, theoretically quantifying how privacy guarantee $\\epsilon$ affects its generalization performance. Our theoretical result shows: (1) the randomized response mechanism slows down the convergence of excess risk by lessening the multiplicative constant term compared with the non-private case $(\\epsilon=\\infty)$; (2) as $\\epsilon$ decreases, the optimal structure of the neural network should be smaller for better generalization performance; (3) the convergence of its excess risk is guaranteed even if $\\epsilon$ is adaptive to the size of training sample $n$ at a rate slower than $O(n^{-1/2})$. Our theoretical results are validated by extensive simulated examples and two real applications.",
        "authors": "S. Xu, C. Wang, W. W. Sun, et.al",
        "keywords": [
            "privacy",
            "generalization performance",
            "randomized response mechanism"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=uKCGOw9bGG",
        "pdf_src": "https://api2.openreview.net/pdf/2ed9e68cbc47c57c80b58cc13504edc320b1ecc0.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of our research lies in the field of machine learning where data privacy preservation during model training has become increasingly important due to concerns about sensitive information leakage from labeled datasets.\n\nResearch Problem:\nOur primary concern revolves around understanding whether label differential privacy—a form of $\\epsilon$-differential privacy that perturbs only the labels—can affect the generalization ability of a binary classification task when applied over a dataset containing private attributes alongside public ones.\n \nMethods:\nTo address these questions, we employ the concept of minimax lower bounds which provide guarantees regarding the worst-case scenario an algorithm can achieve against any adversary. Specifically, we focus on analyzing the excess risk of a binary classifier based on a deep neural network architecture after it's been trained using a dataset anonymized through the randomized response mechanism—an approach used widely within label differential privacy settings.\n\nMain Contributions:\nOur main contributions include:\n\n1. We derive new minimax lower bounds showing that the randomized response mechanism introduces additional noise into the training process relative to no privacy being considered ($\\epsilon = \\infty$), thus slowing down the convergence towards minimizing the excess risk—the difference between empirical error rates and true population error rates.\n\n2. Theoretical analysis suggests that as $\\epsilon$, representing the level of privacy protection, becomes more stringent, the complexity or capacity of the neural network required must decrease proportionally so as not to degrade overall generalization capability beyond what would have occurred without privacy constraints.\n\n3. Furthermore, despite the fact that $\\epsilon$ could adaptively adjust according to the number of samples n in ways potentially suboptimal per standard norms (slower than $O(n^{-1/2}$), there still exists a convergence guarantee ensuring that the excess risk will eventually stabilize near its minimum achievable value regardless of such adaptations.\n\nValidation:\nWe validate all three findings via extensive simulations across various scenarios involving different levels of privacy and network architectures along with practical application cases demonstrating their relevance outside purely theoretical contexts.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Neighborhood Gradient Mean: An Efficient Decentralized Learning Method for Non-IID Data",
        "abstract": "Decentralized learning algorithms enable the training of deep learning models over large distributed datasets, without the need for a central server. The current state-of-the-art decentralized algorithms mostly assume the data distributions to be Independent and Identically Distributed (IID). In practical scenarios, the distributed datasets can have significantly different data distributions across the agents. This paper focuses on improving decentralized learning on non-IID data with minimal compute and memory overheads. We propose Neighborhood Gradient Mean (NGM), a novel decentralized learning algorithm that modifies the local gradients of each agent using self- and cross-gradient information.  In particular, the proposed method averages the local gradients with model-variant or data-variant cross-gradients based on the communication budget. Model-variant cross-gradients are derivatives of the received neighbors’ model parameters with respect to the local dataset. Data-variant cross-gradient derivatives of the local model with respect to its neighbors’ datasets. The data-variant cross-gradients are aggregated through an additional communication round. We theoretically analyze the convergence characteristics of NGM and demonstrate its efficiency on non-IID data sampled from various vision and language datasets. Our experiments demonstrate that the proposed method either remains competitive or outperforms (by 0-6%) the existing state-of-the-art (SoTA) decentralized learning algorithm on non-IID data with significantly less compute and memory requirements. Further, we show that the model-variant cross-gradient information available locally at each agent can improve the performance on non-IID data by 3-20% without additional communication costs.",
        "authors": "S. A. Aketi, S. Kodge, K. Roy",
        "keywords": [
            "Non-IID Data",
            "Decentralized Learning",
            "Neighborhood Gradient Mean"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=vkiKzK5G3e",
        "pdf_src": "https://api2.openreview.net/pdf/4a4483400a08edb79002438a0f9b0c14d1d6d027.pdf",
        "Code_src": "",
        "Introduction": "Background: Decentralized learning algorithms allow for the training of deep learning models directly in multiple locations rather than relying on centralized servers.\n\nResearch Problem: Current decentralized algorithms often rely on the assumption that all participating nodes receive independent and identically distributed (IID) samples; however, this is not always true when dealing with real-world datasets where there may exist significant differences between node's data distributions.\n\nMethod: To address this problem while minimizing computational and memory resources required during the process, authors introduce Neighborhood Gradient Mean (NGM), which adapts the local gradient updates within individual agents according to both their own as well as neighboring agents' gradients - termed neighborhood gradient mean aggregation strategy.\n\nMain Contributions:\n1. Propose a new decentralized learning algorithm called Neighborhood Gradient Mean (NGM).\n2. Introduce two types of cross-gradients into the algorithm – model variant and data variant.\n3. Analyze theoretical properties such as convergence behavior under NGM framework.\n4. Demonstrate effectiveness via empirical results obtained against other SoTA decentralized methods trained on non-IID datasets drawn from diverse domains like vision & natural language processing tasks showing improvements up to 6%.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Limitation of Characterizing Implicit Regularization by Data-independent Functions",
        "abstract": "In recent years, understanding the implicit regularization of neural networks (NNs) has become a central task in deep learning theory. However, implicit regularization is itself not completely defined and well understood. In this work, we attempt to mathematically define and study implicit regularization. Importantly, we explore the limitations of a common approach to characterizing implicit regularization using data-independent functions. We propose two dynamical mechanisms, i.e., Two-point and One-point Overlapping mechanisms, based on which we provide two recipes for producing classes of one-hidden-neuron NNs that provably cannot be fully characterized by a type of or all data-independent functions. Following the previous works, our results further emphasize the profound data dependency of implicit regularization in general, inspiring us to study in detail the data dependency of NN implicit regularization in the future.",
        "authors": "L. Zhang, Z. J. Xu, T. Luo, et.al",
        "keywords": [
            "Data-Dependency",
            "Implicit Regularization",
            "Neural Networks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=140kSqm0uy",
        "pdf_src": "https://api2.openreview.net/pdf/94403dce1d8a29c59b0d9d10dd0d7933ab2644d6.pdf",
        "Code_src": "",
        "Introduction": "Background: Understanding the implicit regularization effect within neural networks (NNs) plays an essential role in modern deep learning research.\n\nResearch Problem: Despite its importance as part of theoretical studies into NNs, there's currently no clear mathematical definition nor comprehensive comprehension regarding how exactly NNs are implicitly regularized during training processes.\n\nMethodology: The authors critically examine existing methods used to characterize implicit regularization through data-independent functions; they identify shortcomings with these approaches due to their inherent inability to capture certain aspects of the phenomenon accurately enough.\n \nMain Contributions:\n1. They introduce new definitions focusing more precisely on what constitutes 'implicit regularization' from a mathematical perspective,\n2. Propose novel dynamical systems models - \"Two-point\" and \"One-point Overlapping Mechanisms\" – to explain different types of implicit regularization effects observed across various NN architectures including those with just single hidden neurons,\n3. Demonstrate experimentally via concrete examples constructed according to specific rules outlined under each mechanism why current characterization techniques fail when applied directly without modification towards capturing such complex behaviors adequately,\n\nConclusion: This paper highlights significant gaps between traditional ways defining implicit regularization against empirical observations suggesting strong dependence upon dataset specifics rather than being solely determined by intrinsic properties like architecture alone thus paving way forward toward developing better tools capable analyzing deeper insights about how neural networks learn robustly despite variability present among datasets encountered throughout practical applications involving machine learning algorithms deployed widely today",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses",
        "abstract": "Optimal Transport has sparked vivid interest in recent years, in particular thanks to the Wasserstein distance, which provides a geometrically sensible and intuitive way of comparing probability measures. For computational reasons, the Sliced Wasserstein (SW) distance was introduced as an alternative to the Wasserstein distance, and has seen uses for training generative Neural Networks (NNs). While convergence of Stochastic Gradient Descent (SGD) has been observed practically in such a setting, there is to our knowledge no theoretical guarantee for this observation. Leveraging recent works on convergence of SGD on non-smooth and non-convex functions by Bianchi et al. (2022), we aim to bridge that knowledge gap, and provide a realistic context under which fixed-step SGD trajectories for the SW loss on NN parameters converge. More precisely, we show that the trajectories approach the set of (sub)-gradient flow equations as the step decreases. Under stricter assumptions, we show a much stronger convergence result for noised and projected SGD schemes, namely that the long-run limits of the trajectories approach a set of generalised critical points of the loss function.",
        "authors": "E. Tanguy",
        "keywords": [
            "Optimal Transport",
            "Wasserstein Distance",
            "Stochastic Gradient Descent"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=aqqfB3p9ZA",
        "pdf_src": "https://api2.openreview.net/pdf/487dafc8849500f6c246cdf4f2178caadcea5856.pdf",
        "Code_src": "",
        "Introduction": "Background: Optimal Transport theory involves finding optimal ways to transport one distribution into another while minimizing some cost measure between them.\nResearch Problem: The Wasserstein distance plays a crucial role here but can be computationally expensive due to its complexity; thus, alternatives like the Sliced Wasserstein (SW) distance have gained attention.\n\nMethodology: We leverage insights from recent studies about the convergence of stochastic gradient descent (SGD) algorithms when applied to non-smooth or non-convex problems proposed by Bianchi et al. (2022).\n\nMain Contributions:\n1. Fill a theoretical gap regarding the convergence properties of SGD with respect to the SW distance within neural networks' parameter space - providing guarantees where previously none existed.\n2. Show how the SGD trajectory towards the SW loss converges asymptotically toward sub-gradient flow equations related to the optimization problem at hand – even more so if steps are reduced incrementally over time.\n3. With additional strict conditions imposed concerning noise and projection onto feasible sets during updates using SGD, obtain significantly strengthened convergence results indicating that these trajectories will eventually approximate generalized critical points",
        "Topic": "Optimal Transport"
    },
    {
        "title": "Multimodal Language Learning for Object Retrieval in Low Data Regimes in the Face of Missing Modalities",
        "abstract": "Our study is motivated by robotics, where when dealing with robots or other physical systems, we often need to balance competing concerns of relying on complex, multimodal data coming from a variety of sensors with a general lack of large representative datasets. Despite the complexity of modern robotic platforms and the need for multimodal interaction, there has been little research on integrating more than two modalities in a low data regime with the real-world constraint that sensors fail due to obstructions or adverse conditions. In this work, we consider a case in which natural language is used as a retrieval query against objects, represented across multiple modalities, in a physical environment. We introduce extended multimodal alignment (EMMA), a method that learns to select the appropriate object while jointly refining modality-specific embeddings through a geometric (distance-based) loss. In contrast to prior work, our approach is able to incorporate an arbitrary number of views (modalities) of a particular piece of data. We demonstrate the efficacy of our model on a grounded language object retrieval scenario. We show that our model outperforms state-of-the-art baselines when little training data is available. Our code is available at https://github.com/kasraprime/EMMA.",
        "authors": "K. Darvish, E. Raff, F. Ferraro, et.al",
        "keywords": [
            "robotics",
            "multimodal data",
            "multimodal alignment"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=cXa6Xdm0v7",
        "pdf_src": "https://api2.openreview.net/pdf/9e457b9a8b60b5d1f952581e86d1d753abc20f4e.pdf",
        "Code_src": "https://github.com/kasraprime/EMMA",
        "Introduction": "Background: The background of this paper lies in robotics field , where it's necessary to rely on complex multimodal data coming from various sensors but also face challenges such as limited availability of large representative datasets.\n\nResearch Problem: This paper aims to solve the problem how to integrate more than two modalities in a low-data regime considering the reality that sensors may fail because of obstructions or adverse conditions.\n\nMethod: To address above mentioned issues, the authors propose Extended Multimodal Alignment (EMMA), a novel method based on geometric distance-based loss learning to select appropriate objects among those presented in different modalities simultaneously.\n\nMain Contributions: \n1. EMMA can handle any number of views (modalities) of specific pieces of data.\n2. The proposed model demonstrates its effectiveness under grounded language object retrieval scenarios even if only small amount of training data are provided compared to existing state-of-the-art baseline methods.\n3. All related codes have been made publicly available via GitHub repository at https://github.com/kasraprime/EMMA",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Homomorphic Self-Supervised Learning",
        "abstract": "Many state of the art self-supervised learning approaches fundamentally rely on transformations applied to the input in order to selectively extract task-relevant information. Recently, the field of equivariant deep learning has developed to introduce structure into the feature space of deep neural networks by designing them as homomorphisms with respect to input transformations. In this work, we observe that many existing self-supervised learning algorithms can be both unified and generalized when seen through the lens of equivariant representations. Specifically, we introduce a general framework we call Homomorphic Self-Supervised Learning, and theoretically show how it may subsume the use of input-augmentations provided an augmentation-homomorphic feature extractor. We validate this theory experimentally for simple augmentations, demonstrate the necessity of representational structure for feature-space SSL, and further empirically explore how the parameters of this framework relate to those of traditional augmentation-based self-supervised learning. We conclude with a discussion of the potential benefits afforded by this new perspective on self-supervised learning.",
        "authors": "T. A. Keller, X. Suau, L. Zappella",
        "keywords": [
            "equivariant deep learning",
            "homomorphic self-supervised learning",
            "feature space structure"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=tEKqQgbwbf",
        "pdf_src": "https://api2.openreview.net/pdf/0f32f95b86dc50b784352fd83845bf46856d0d3c.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses recent advancements in self-supervised learning methods which heavily depend on data transformations during training phase.\n\nResearch Problem: How do these advanced self-supervised learning algorithms utilize transformations? Can they all be explained or improved under a unifying framework?\n\nMethodology: The authors propose a novel framework called \"Homomorphic Self-Supervised Learning\" based on equivariant deep learning principles where network features are invariant across certain types of transformations made upon inputs (\"augmentation-homomorphic\").\n\nMain Contributions:\n1. They unify various existing self-supervised learning algorithms within their proposed framework.\n2. They provide theoretical justification linking the efficacy of such algorithms directly to the degree of representation equivariance introduced via augmentation strategies.\n3. Experimentally verify performance improvements using simple augmentations while emphasizing the importance of structured representations over raw data.\n4. Conduct empirical analysis correlating hyperparameters from their framework back to conventional augmentation-based self-supervised learning settings offering insights about parameter tuning implications between frameworks.\n5. Discuss future prospects highlighting advantages offered by considering equivariance properties more explicitly than before",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "Population-based Evaluation in Repeated Rock-Paper-Scissors as a Benchmark for Multiagent Reinforcement Learning",
        "abstract": "Progress in fields of machine learning and adversarial planning has benefited significantly from benchmark domains, from checkers and the classic UCI data sets to Go and Diplomacy. In sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors along with a population of forty-three tournament entries, some of which are intentionally sub-optimal. We describe metrics to measure the quality of agents based both on average returns and exploitability. We then show that several RL, online learning, and language model approaches can learn good counter-strategies and generalize well, but ultimately lose to the top-performing bots, creating an opportunity for research in multiagent learning.",
        "authors": "M. Lanctot, J. Schultz, N. Burch, et.al",
        "keywords": [
            "multiagent learning",
            "Rock",
            "Paper",
            "Scissors"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=gQnJ7ODIAx",
        "pdf_src": "https://api2.openreview.net/pdf/40a899a9cc1c73f2358be1a2a0347e834820b198.pdf",
        "Code_src": "",
        "Introduction": "Background: The progress made in areas such as machine learning and adversarial planning is often attributed to the existence of benchmark domains like checkers or the UCI datasets; more recently, games like Go have also provided valuable insights.\n\nResearch Problem: Agent evaluation typically involves limited interaction between agents before they achieve certain levels of performance required by humans – e.g., surpassing even highly skilled professionals at specific tasks - leading to potential biases due to insufficient exploration beyond these benchmarks.\n\nMethodology: This paper introduces a novel benchmark domain specifically designed for evaluating multi-agent systems through iterated gameplay involving \"Rock, Paper, Scissors\" alongside 43 different bot entries within competitive tournaments where not all participants may be optimally programmed yet still exhibit strategic behavior worth studying.\n\nMain Contributions:\n1. A new benchmark environment called the RPS Tournament, consisting of multiple rounds played repeatedly among various bots.\n2. Development of metrics assessing agent quality focusing on their average return over time (\"exploitability\") rather than just immediate wins/losses during single encounters alone.\n3. Demonstration using reinforcement learning algorithms showing how they could develop effective strategies despite facing opponents who might sometimes deliberately choose less optimal moves compared to those found via exhaustive search methods used traditionally when training models solely focused on winning outcomes without considering opponent strategy diversity extensively enough beforehand.\n\nThe study highlights limitations inherent only relying heavily upon one-off evaluations while suggesting further avenues into understanding complex dynamics present amongst interacting intelligent entities involved in cooperative scenarios requiring adaptability towards adversaries' behaviors observed across extended periods",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Worst-case Feature Risk Minimization for Data-Efficient Learning",
        "abstract": "Deep learning models typically require massive amounts of annotated data to train a strong model for a task of interest. However, data annotation is time-consuming and costly. How to use labeled data from a related but distinct domain, or just a few samples to train a satisfactory model are thus important questions. To achieve this goal, models should resist overfitting to the specifics of the training data in order to generalize well to new data. This paper proposes a novel Worst-case Feature Risk Minimization (WFRM) method that helps improve model generalization. Specifically, we tackle a minimax optimization problem in feature space at each training iteration. Given the input features, we seek the feature perturbation that maximizes the current training loss and then minimizes the training loss of the worst-case features. By incorporating our WFRM during training, we significantly improve model generalization under distributional shift – Domain Generalization (DG) and in the low-data regime – Few-shot Learning (FSL). We theoretically analyze WFRM and find the key reason why it works better than ERM – it induces an empirical risk-based semi-adaptive $L_{2}$ regularization of the classifier weights, enabling a better risk-complexity trade-off. We evaluate WFRM on two data-efficient learning tasks, including three standard DG benchmarks of PACS, VLCS, OfficeHome and the most challenging FSL benchmark Meta-Dataset. Despite the simplicity, our method consistently improves various DG and FSL methods, leading to the new state-of-the-art performances in all settings. Codes & models will be released at https://github.com/jslei/WFRM.",
        "authors": "J. Lei, D. Li, C. Xu, et.al",
        "keywords": [
            "Worst-case Feature Risk Minimization",
            "Model Generalization",
            "Data Efficient Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=czev0exHXT",
        "pdf_src": "https://api2.openreview.net/pdf/9809890b2e697a36680caaa8ac57be91f1cea690.pdf",
        "Code_src": "",
        "Introduction": "Background: The background of this research lies in the field of deep learning where large-scale annotated datasets have traditionally been required due to their complexity which necessitates extensive computational resources.\n\nResearch Problem: The main challenge addressed by this study revolves around how to effectively utilize limited labeled data while avoiding overfitting issues when attempting to transfer knowledge across different domains within machine learning problems such as domain generalization (DG), especially with only a small number of examples available - referred to as few-shot learning (FSL).\n\nMethodology: In response to these challenges, they propose a novel approach called Worst-case Feature Risk Minimization (WFRM). At every iteration step throughout training, instead of optimizing directly towards minimizing overall prediction error like traditional Empirical Risk Minimization (ERM), WFRM introduces a minimax optimization process focusing specifically on feature space perturbations aiming not merely to minimize errors generally seen; rather its objective is to maximize them first before finding ways to mitigate those increases through subsequent adjustments so that even if there's variability among unseen cases' distributions (\"distributional shifts\"), performance remains high.\n \nMain Contributions:\n1. They introduce WFRM into existing DG and FSL frameworks improving upon previous approaches without requiring more complex architectures nor additional hyperparameters tuning beyond what was already necessary;\n2. Conduct theoretical analysis demonstrating superior properties compared to other methods particularly highlighting improved empirical risk based semi-adaptive L2 regularization achieved via WFRM;\n3. Evaluate against several established benchmarks showing significant improvements relative to prior best-performing techniques achieving new state-of-the-art results across both DG and FSL scenarios;\n\nConclusion: Overall summary would conclude stating \"This work presents an innovative solution leveraging Worst-case Feature Risk Minimization strategy enhancing model robustness toward distributional shifts encountered commonly in real-world applications.\"",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Conformal prediction under ambiguous ground truth",
        "abstract": "Conformal Prediction (CP) allows to perform rigorous uncertainty quantification by constructing a prediction set $C(X)$ satisfying $\\mathbb{P}(Y \\in C(X))\\geq 1-\\alpha$ for a user-chosen $\\alpha \\in [0,1]$ by relying on calibration data $(X_1,Y_1),...,(X_n,Y_n)$ from $\\mathbb{P}=\\mathbb{P}^{X} \\otimes \\mathbb{P}^{Y|X}$. It is typically implicitly assumed that $\\mathbb{P}^{Y|X}$ is the ``true'' posterior label distribution. However, in many real-world scenarios, the labels $Y_1,...,Y_n$ are obtained by aggregating expert opinions using a voting procedure, resulting in a one-hot distribution $\\mathbb{P}_{\\textup{vote}}^{Y|X}$. This is the case for most datasets, even well-known ones like ImageNet. For such ``voted'' labels, CP guarantees are thus w.r.t. $\\mathbb{P}_{\\textup{vote}}=\\mathbb{P}^X \\otimes \\mathbb{P}_{\\textup{vote}}^{Y|X}$ rather than the true distribution $\\mathbb{P}$. In cases with unambiguous ground truth labels, the distinction between $\\mathbb{P}_{\\textup{vote}}$ and $\\mathbb{P}$ is irrelevant. However, when experts do not agree because of ambiguous labels, approximating $\\mathbb{P}^{Y|X}$ with a one-hot distribution $\\mathbb{P}_{\\textup{vote}}^{Y|X}$ ignores this uncertainty. In this paper, we propose to leverage expert opinions to approximate $\\mathbb{P}^{Y|X}$ using a non-degenerate distribution $\\mathbb{P}_{\\textup{agg}}^{Y|X}$. We then develop \\emph{Monte Carlo CP} procedures which provide guarantees w.r.t. $\\mathbb{P}_{\\textup{agg}}=\\mathbb{P}^X \\otimes \\mathbb{P}_{\\textup{agg}}^{Y|X}$ by sampling multiple synthetic pseudo-labels from $\\mathbb{P}_{\\textup{agg}}^{Y|X}$ for each calibration example $X_1,...,X_n$. In a case study of skin condition classification with significant disagreement among expert annotators, we show that applying CP w.r.t. $\\mathbb{P}_{\\textup{vote}}$ under-covers expert annotations: calibrated for $72\\%$ coverage, it falls short by on average $10\\%$; our Monte Carlo CP closes this gap both empirically and theoretically. We also extend Monte Carlo CP to multi-label classification and CP with calibration examples enriched through data augmentation.",
        "authors": "D. Stutz, A. G. Roy, T. Matejovicova, et.al",
        "keywords": [
            "conformal prediction",
            "monte carlo cp",
            "expert opinion aggregation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=CAd6V2qXxc",
        "pdf_src": "https://api2.openreview.net/pdf/2ca5fc336ff3c29f64776e698eba01e26ba888e2.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this research lies within the field of machine learning where Conformal Prediction (CP) has been used as an approach towards rigorous uncertainty quantification.\n\nResearch Problem:\nThe problem addressed here concerns the use of Conformal Prediction methods based on \"voted\" labels derived via aggregation processes involving human expertise or consensus building techniques instead of direct observations (\"ground truth\"). The issue arises due to these voted labels often being represented simplistically - e.g., as one-hot distributions – which may ignore underlying uncertainties present if there's disagreement amongst experts about what constitutes correct labeling.\n \nMethod:\nTo address issues related to the approximation of the conditional probability distribution $\\mathbb{P}^{Y|X}$ arising out of aggregated votes, authors introduce a new method called Monte Carlo Conformal Prediction (MC-CP). MC-CP leverages the diversity inherent in expert opinion sets by creating a non-degenerate distribution $\\mathbb{P}_{\\textup{agg}}^{Y|X}$ over possible label assignments consistent with all available information including those disagreements expressed during annotation. They sample synthetic pseudo-labels from $\\mathbb{P}_{\\textup{agg}}^{Y|X}$ corresponding to every training instance and apply standard CP procedures accordingly.\n\nMain Contributions:\nThis work makes several contributions:\n\n1. Proposes a novel way forward dealing with 'voted' labels common across various domains particularly image recognition tasks but applicable elsewhere too;\n2. Develops Monte Carlo Conformal Prediction algorithms providing theoretical guarantees regarding predictions made against the aggregate distribution $\\mathbb{P}_{\\textup{agg}}^{Y|X}$;\n3. Demonstrates empirical improvements compared existing approaches especially noticeable while classifying skin conditions showing how ignoring disagreement can lead to misleading results;\n4. Extends MC-CP beyond binary classifications into multi-label settings further broadening its applicability range;\n5. Provides insights concerning robustness improvement achieved after enriching calibration samples through data augmentation strategies.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Not All Causal Inference is the Same",
        "abstract": "Neurally-parameterized Structural Causal Models in the Pearlian notion to causality, referred to as NCM, were recently introduced as a step towards next-generation learning systems. However, said NCM are only concerned with the learning aspect of causal inference\nand totally miss out on the architecture aspect. That is, actual causal inference within NCM is intractable in that the NCM won’t return an answer to a query in polynomial time. This insight follows as corollary to the more general statement on the intractability of arbitrary structural causal model (SCM) parameterizations, which we prove in this work through classical 3-SAT reduction. Since future learning algorithms will be required to deal with both high dimensional data and highly complex mechanisms governing the data, we ultimately believe work on tractable inference for causality to be decisive. We also show that not all “causal” models are created equal. More specifically, there are models capable of answering causal queries that are not SCM, which we refer to as partially causal models\n(PCM). We provide a tabular taxonomy in terms of tractability properties for all of the different model families, namely correlation-based, PCM and SCM. To conclude our work, we also provide some initial ideas on how to overcome parts of the intractability of causal inference\nwith SCM by showing an example of how parameterizing an SCM with SPN modules can at least allow for tractable mechanisms. With this work we hope that our insights can raise awareness for this novel research direction since achieving success with causality in real world downstream tasks will not only depend on learning correct models but also require having the practical ability to gain access to\nmodel inferences.",
        "authors": "M. Zečević, D. S. Dhami, K. Kersting",
        "keywords": [
            "intractable causal inference",
            "partially causal models",
            "tractable inference"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ySWQ6eXAKp",
        "pdf_src": "https://api2.openreview.net/pdf/4beeea4aaa3c7333d5373ae000f42b3804fcf14a.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper introduces Neurally-parameterized Structural Causal Models(NCM), which aims to develop new generation learning systems. However, existing NCMs focus solely on the learning aspects of causal inference without considering their architectures.\n\nResearch Problem: The main problem addressed here is the computational intractability of causal inference within NCM due to its inability to respond to queries in polynomial time.This issue arises from the broader intractability of arbitrary structural causal model(SCM) parameterizations, which has been proven using classical 3-SAT reduction.\n\nMethod: In order to address these issues, the authors propose a classification based on tractability properties among different types of models such as correlation-based, Partially Causal Model(PCMs) and SCMs. They further demonstrate potential solutions toward overcoming the intractability of causal inference via neural networks.\n\nMain Contributions: The primary contributions include proving the intractability of arbitrary SCM parameterizations, proposing a taxonomy classifying various models according to tractability properties, and providing preliminary approaches addressing the computational challenges associated with causal inference.\nOverall, while current NCMs may learn accurate causal relationships between variables, they lack practicality when it comes to accessing those inferences efficiently. Future research should consider integrating tractable methods into causal modeling frameworks like NCM so that machine learning algorithms equipped with them could effectively handle high-dimensional datasets involving complex underlying mechanisms",
        "Topic": "Generative Models"
    },
    {
        "title": "Towards Stability of Autoregressive Neural Operators",
        "abstract": "Neural operators have proven to be a promising approach for modeling spatiotemporal systems in the physical sciences. However, training these models for large systems can be quite challenging as they incur significant computational and memory expense---these systems are often forced to rely on autoregressive time-stepping of the neural network to predict future temporal states. While this is effective in managing costs, it can lead to uncontrolled error growth over time and eventual instability. We analyze the sources of this autoregressive error growth using prototypical neural operator models for physical systems and explore ways to mitigate it. We introduce architectural and application-specific improvements that allow for careful control of instability-inducing operations within these models without inflating the compute/memory expense. We present results on several scientific systems that include Navier-Stokes fluid flow, rotating shallow water, and a high-resolution global weather forecasting system. We demonstrate that applying our design principles to neural operators leads to significantly lower errors for long-term forecasts as well as longer time horizons without qualitative signs of divergence compared to the original models for these systems. We open-source our code for reproducibility.",
        "authors": "M. Mccabe, P. Harrington, S. Subramanian, et.al",
        "keywords": [
            "neural operators",
            "spatiotemporal systems",
            "model stability"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=RFfUUtKYOG",
        "pdf_src": "https://api2.openreview.net/pdf/33e9539f03a2ee4c16164fffb345cb9519767c09.pdf",
        "Code_src": "",
        "Introduction": "Background: Neural operators represent an emerging paradigm for simulating complex dynamical systems across various domains such as physics or climate science by leveraging deep learning techniques.\n\nResearch Problem: The primary challenge with neural operator-based simulations pertains to their scalability due to substantial computational and memory overheads associated with predicting sequences into the future through autoregressive methods which may result in unstable predictions owing to growing errors cumulatively.\n \nMethods: This study employs prototype neural operator models trained upon specific physical systems like fluid dynamics equations - specifically focusing on the Navier-Stokes equation – and rotational shallow water equations along with a comprehensive global weather prediction model at high resolution. They systematically identify factors contributing to the accumulation of errors during the autoregressive process while also proposing novel architectural modifications and tailored optimizations designed explicitly considering each individual simulation's requirements rather than resorting to general-purpose solutions leading to potential inefficiencies. \n\nMain Contributions: Their contributions lie primarily in developing strategies ensuring stable operation even under conditions prone to instabilities typically encountered when employing neural networks' recurrent architectures; introducing new methodologies enabling more efficient use of resources including computation and storage space thereby reducing overall cost implications related to running extended simulations involving neural operators ; demonstrating empirical evidence showcasing marked improvement regarding accuracy retention throughout prolonged predictive horizons relative to baseline approaches; and making available all developed software tools via open-source licensing facilitating replication efforts among researchers worldwide interested in advancing similar endeavors towards better understanding natural phenomena governed by nonlinear differential equations governing Earth’s atmosphere/oceanic circulation patterns etc.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment",
        "abstract": "Vision-language pre-training (VLP) has recently proven highly effective for various uni- and multi-modal downstream applications. However, most existing end-to-end VLP methods use high-resolution image-text-box data to perform well on fine-grained region-level tasks, such as object detection, segmentation, and referring expression comprehension. Unfortunately, such high-resolution images with accurate bounding box annotations are expensive to collect and use for supervision at scale. In this work, we propose VoLTA (Vision Language Transformer with weakly-supervised local-feature Alignment), a new VLP paradigm that only utilizes image-caption data but achieves fine-grained region-level image understanding, eliminating the need for expensive box annotations. VoLTA adopts graph optimal transport-based weakly-supervised alignment on local image patches and text tokens to germinate an explicit, self-normalized, and interpretable low-level matching criterion. In addition, VoLTA pushes multi-modal fusion deep into the uni-modal backbones during pre training and removes fusion-specific transformer layers, further reducing memory requirements. Extensive experiments on a wide range of vision- and vision-language downstream tasks demonstrate the effectiveness of VoLTA on fine-grained applications without compromising the coarse-grained downstream performance, often outperforming methods using significantly more caption and box annotations.",
        "authors": "S. Pramanick, L. Jing, S. Nag, et.al",
        "keywords": [
            "VLP",
            "Weakly-supervised learning",
            "Graph optimal transport"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Kt2VJrCKo4",
        "pdf_src": "https://api2.openreview.net/pdf/cfdc0c5bf13a49d173286d8f2d1897424620bbee.pdf",
        "Code_src": "",
        "Introduction": "Background: Vision-language pre-training (VLP) is becoming increasingly popular in recent years due to its impressive performance across various uni-and multi-modal downstream applications.\n\nResearch Problem: Most existing end-to-end VLP methods require high-resolution image-text-box data which can be costly or challenging to obtain large-scale annotation efforts needed for supervision.\n\nMethod: This paper proposes VoLTA, a novel VLP paradigm designed specifically around utilizing image-caption data alone while still achieving robust results within fine-grained region-level visual understanding tasks like object detection, segmentation & referring expressions comprehension. The key innovation lies in leveraging graph optimal transport based weakly supervised alignment between local patches from images and corresponding words extracted from captions - creating an explicit yet interpretable metric measure aligned through these two modalities' shared latent space representation; additionally VoLTA incorporates multi-modal fusion deeply integrated throughout pretraining stages by removing specialized transformer modules dedicated solely towards merging features together before passing them forward through subsequent processing steps thus saving computational resources required overall compared traditional approaches where additional complexity arises when attempting similar integration strategies deeper down pipelines.\n\nMain Contributions: VoLTA demonstrates significant improvements over other state-of-the-art models even though it requires less annotated examples than those used previously leading up better scalability potential along with maintaining comparable accuracy levels especially amongst coarse-grained datasets whilst excelling notably against finer-grained ones making it particularly useful suited toward practical deployment scenarios involving limited labeled material availability issues encountered frequently nowadays",
        "Topic": "Optimal Transport"
    },
    {
        "title": "Non-Stationary Contextual Pricing with Safety Constraints",
        "abstract": "In a contextual pricing  problem, a seller aims at maximizing the revenue over a sequence of sales sessions (described by feature vectors) using binary-censored feedback of \"sold\" or \"not sold\". Existing methods often overlook two practical challenges (1) the best pricing strategy could change over time; (2) the prices and pricing policies must conform to hard constraints due to safety, ethical or legal restrictions. We address both challenges by solving a more general problem of \"universal dynamic regret\" minimization in proper online learning with exp-concave losses --- an open problem posed by Baby & Wang (2021) that we partially resolve in this paper, with attention restricted to loss functions coming from a generalized linear model.  Here \"dynamic regret\" measures the performance relative to a non-stationary sequence of policies, and \"proper\" means that the learner must choose feasible strategies within a pre-defined convex set, which we use to model the safety constraints. In this work, we consider a linear noisy valuation model for the customers. In the case of a known strictly log-concave market noise, our algorithm achieves $\\tilde{O}(d^3T^{1/3}C_T^{2/3} \\vee d^3)$ dynamic regret in comparison with the optimal policy series, where $T$, $d$ and $C_T$ stand for the time horizon, the feature dimension and the total variation (characterizing non-stationarity) respectively. This regret is near-optimal with respect to $T$ (within $O(\\log T)$ gaps) and $C_T$, and our algorithm is adaptable to unknown  $C_T$ and remains feasible throughout. However, the dependence on $d$ is suboptimal and the minimax rate is still open.",
        "authors": "D. Baby, J. Xu, Y. Wang",
        "keywords": [
            "contextual pricing",
            "universal dynamic regret",
            "proper online learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=fWIQ9Oaao0",
        "pdf_src": "https://api2.openreview.net/pdf/b65014be90a44eb5583b7555bb86b937d3661f2f.pdf",
        "Code_src": "",
        "Introduction": "Background: The study focuses on a contextual pricing issue involving selling products across multiple sales periods based on customer preferences represented as feature vectors.\n\nResearch Problem: Two main issues are addressed:\n1. Pricing strategies may evolve over time.\n2. Prices need to adhere to strict constraints related to safety, ethics, or law.\n\nMethods: A novel approach involves minimizing universal dynamic regret through proper online learning under exp-concave losses—a concept introduced recently but not fully resolved—specifically focusing on those derived from a generalized linear model while considering convex sets representing safety constraints during decision-making processes.\n\nMain Contributions: \n- Partial resolution of the previously unsolved 'universal dynamic regret' optimization challenge when applied specifically to pricing problems governed by linear models assuming known market conditions characterized by strictly log-concave noise.\n- An algorithm achieving $\\tilde{O}(d^3T^{1/3}C_T^{2/3} \\vee d^3)$ dynamic regret compared against the optimal policy sequences available up to now—it's nearly optimal regarding the parameters $T$ ($\\sim O(\\log T)$ gap), $d$, and $C_T$.  \n- Adaptability even if $C_T$ values remain unknown without compromising feasibility along these iterations.\n- Although dependency upon dimensionality $d$ isn't yet optimal nor has its min-max lower bound been established completely.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "$f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning",
        "abstract": "In self-supervised contrastive learning, a widely-adopted objective function is InfoNCE, which uses the heuristic cosine similarity for the representation comparison, and is closely related to maximizing the Kullback-Leibler (KL)-based mutual information. In this paper, we aim at answering two intriguing questions: (1) Can we go beyond the KL-based objective? (2) Besides the popular cosine similarity, can we design a better similarity function? We provide answers to both questions by generalizing the KL-based mutual information to the $f$-Mutual Information in Contrastive Learning ($f$-MICL) using the $f$-divergences. To answer the first question, we provide a wide range of $f$-MICL objectives which share the nice properties of InfoNCE (e.g., alignment and uniformity), and meanwhile result in similar or even superior performance. For the second question, assuming that the joint feature distribution is proportional to the Gaussian kernel, we derive an $f$-Gaussian similarity with better interpretability and empirical performance. Finally, we identify close relationships between the  $f$-MICL objective and several popular InfoNCE-based objectives. Using benchmark tasks from both vision and natural language, we empirically evaluate $f$-MICL with different $f$-divergences on various architectures (SimCLR, MoCo, and MoCo v3) and datasets. We observe that $f$-MICL generally outperforms the benchmarks and the best-performing $f$-divergence is task and dataset dependent.",
        "authors": "Y. Lu, G. Zhang, S. Sun, et.al",
        "keywords": [
            "f-Mutual Information",
            "f-divergences",
            "InfoNCE"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ZD03VUZmRx",
        "pdf_src": "https://api2.openreview.net/pdf/dce4f2e7a1a71bdc388c6437b30dee81b535d4c8.pdf",
        "Code_src": "",
        "Introduction": "Background:\nSelf-supervised contrastive learning has become increasingly prevalent due to its effectiveness without requiring labeled data; however, it primarily relies on InfoNCE as its objective function based on the heuristic cosine similarity.\n\nResearch Problem:\nThe study raises two key inquiries regarding current practices within self-supervised contrastive learning:\n\n1. Can alternative objectives surpass those based solely on the Kullback-Leibler (KL) divergence?\n2. Beyond the commonly used cosine similarity measure, are there more effective similarity functions?\n\nMethods:\nTo address these issues, researchers extend the concept of KL-based mutual information into f-Mutual Information in Contrastive Learning (f-MICL). This involves utilizing f-divergences rather than just the KL divergence when comparing representations.\n \nMain Contributions:\nThe authors introduce diverse variants of f-MICL objectives while maintaining desirable characteristics such as alignment and uniformity akin to InfoNCE but potentially yielding comparable if not improved results across multiple benchmarks including SimCLR, MoCo, and MoCo v3 models over varied datasets ranging from vision to natural language processing domains. They also propose an f-Gaussian similarity metric grounded upon assumptions about the joint feature distribution's proportionality relative to a Gaussian kernel - offering enhanced interpretability along with empirical efficacy compared traditional methods. The research identifies strong connections among certain f-MICL objectives already existing in literature alongside InfoNCE counterparts through theoretical analysis before conducting empirical evaluations demonstrating superiority under specific conditions depending heavily on task complexity and dataset specifics.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Tight conditions for when the NTK approximation is valid",
        "abstract": "We study when the neural tangent kernel (NTK) approximation is valid for training a model with the square loss. In the lazy training setting of Chizat et al. 2019, we show that rescaling the model by a factor of $\\alpha = O(T)$ suffices for the NTK approximation to be valid until training time $T$. Our bound is tight and improves on the previous bound of Chizat et al. 2019, which required a larger rescaling factor of $\\alpha = O(T^2)$.",
        "authors": "E. Boix-adserà, E. Littwin",
        "keywords": [
            "neural tangent kernel",
            "lazy training",
            "square loss"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=qM7JPBYROr",
        "pdf_src": "https://api2.openreview.net/pdf/7621242e01cbac38f21e32493c0b45447852726d.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper investigates under what conditions does the Neural Tangent Kernel (NTK) approximation hold true during the process of training a model using squared error loss.\n\nResearch Question: Specifically, they explore whether there exists an appropriate scaling factor that can guarantee the validity of the NTK approximation throughout the entire training period up to some fixed point in time T.\n\nMethodology: They consider the \"lazy\" training setup proposed by Chizat et al., where updates are applied only after seeing each new data sample rather than accumulating gradients over batches or epochs.\n \nMain Contributions: Their main contribution lies in demonstrating that applying a rescaling factor α proportional to O(T), instead of the previously suggested O(T²), ensures that the NTK approximation remains accurate from initialization all the way through to the end of training at time T. This tighter bound significantly reduces the amount needed to scale the parameters relative to existing methods based on their findings improving upon prior research results presented earlier regarding this topic.",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods",
        "abstract": "Stochastic gradient descent (SGD) and adaptive gradient methods, such as Adam and RMSProp, have been widely used in training deep neural networks. We empirically show that while the difference between the standard generalization performance of models trained using these methods is small, those trained using SGD exhibit far greater robustness under input perturbations. Notably, our investigation demonstrates the presence of irrelevant frequencies in natural datasets, where alterations do not affect models' generalization performance. However, models trained with adaptive methods show sensitivity to these changes, suggesting that their use of irrelevant frequencies can lead to solutions sensitive to perturbations. To better understand this difference, we study the learning dynamics of gradient descent (GD) and sign gradient descent (signGD) on a synthetic dataset that mirrors natural signals. With a three-dimensional input space, the models optimized with GD and signGD have standard risks close to zero but vary in their adversarial risks. Our result shows that linear models' robustness to $\\ell_2$-norm bounded changes is inversely proportional to the model parameters' weight norm: a smaller weight norm implies better robustness. In the context of deep learning, our experiments show that SGD-trained neural networks have smaller Lipschitz constants, explaining the better robustness to input perturbations than those trained with adaptive gradient methods. Our source code is available at https://github.com/averyma/opt-robust.",
        "authors": "A. Ma, Y. Pan, A. Farahmand",
        "keywords": [
            "SGD",
            "Robustness",
            "Adaptive Gradient Methods"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ed8SkMdYFT",
        "pdf_src": "https://api2.openreview.net/pdf/abb8a174c468a97458a1f2b09be902eb4b8aa5e1.pdf",
        "Code_src": "https://github.com/averyma/opt-robust",
        "Introduction": "Background:\nThe paper discusses the effectiveness of stochastic gradient descent (SGD) compared to adaptive gradient methods like Adam or RMSProp for training deep neural networks.\n\nResearch Problem:\nThe problem addressed by the research involves understanding whether there are significant differences in terms of generalization performance when different optimization algorithms are employed during neural network training; specifically, how they fare against input perturbations known as adversarial examples which could potentially compromise the robustness of the learned models.\n\nMethods:\nTo address this issue, empirical studies were conducted comparing the generalization performances across various optimization techniques including SGD versus adaptive gradient methods through extensive experimentation involving both real-world data and synthetically generated datasets designed to mimic natural signals.\n \nMain Contributions:\nThe main contributions include:\n\n1. Empirical evidence showing that despite similar generalization performances among different optimization methods based on standard metrics, SGD-trained models demonstrate significantly higher robustness towards adversarial attacks - meaning they perform well even after being subjected to minor perturbations within certain bounds.\n\n2. Identification revealing the existence of \"irrelevant frequencies\" present in many natural datasets – slight variations here don't substantially impact the predictive ability yet may be exploited if an algorithm relies heavily upon them leading to less robust outcomes.\n\n3. A theoretical analysis focusing on two specific variants of gradient descent applied over a 3D synthetic dataset simulating natural signals demonstrating that linear models’ resistance to $\\ell_2$-norm bounded perturbations correlates negatively with parameter weights norms implying lighter-weighted models tend toward more robustness.\n\n4. Experimental validation extending beyond linear models into deep neural networks indicating that SGD-trained neural networks possess lower Lipschitz constants relative to those trained via adaptive gradients thus further justifying their superior resilience regarding input perturbations.\n\n5. The authors provide open-source code accessible from GitHub repository [https://github.com/averyma/opt-robust](https://github.com/averyma/opt-robust), allowing others replicate findings independently verify results themselves.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Data-Free Diversity-Based Ensemble Selection for One-Shot Federated Learning",
        "abstract": "The emerging availability of various machine learning models creates a great demand to harness the collective intelligence of many independently well-trained models to improve overall performance. Considering the privacy concern and non-negligible communication costs, one-shot federated learning and ensemble learning in a data-free manner attract significant attention. However, conventional ensemble selection approaches are neither training efficient nor applicable to federated learning due to the risk of privacy leakage from local clients; meanwhile, the \"many could be better than all\" principle under data-free constraints makes it even more challenging. Therefore, it becomes crucial to design an effective ensemble selection strategy to find a good subset of the base models as the ensemble team for the federated learning scenario. In this paper, we propose a novel data-free diversity-based framework, DeDES, to address the ensemble selection problem with diversity consideration for models under the one-shot federated learning setting. Experimental results show that our method can achieve both better performance and higher efficiency over 5 datasets, 4 different model structures, and both homogeneous and heterogeneous model groups under four different data-partition strategies.",
        "authors": "N. Wang, W. Feng, Y. Deng, et.al",
        "keywords": [
            "data-free federated learning",
            "ensemble selection",
            "diversity"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ORMlg4g3mG",
        "pdf_src": "https://api2.openreview.net/pdf/c1e403c04d21ddbcbbf368e2c12e698dddedb39b.pdf",
        "Code_src": "",
        "Introduction": "Background: The increasing number of independently trained machine learning models has led to increased interest in leveraging their collective intelligence through ensemble methods like federated learning or ensemble learning without sharing any data.\n\nResearch Problem: Privacy concerns make direct sharing of client data impractical while also leading to high communication costs which necessitates developing new techniques such as one-shot federated learning where only a single round of communication is allowed between central server and edge devices.\n\nMethods: We introduce DeDES, a novel approach based on diversity considerations specifically designed for solving ensemble selection problems within the context of one-shot federated learning settings.\n \nMain Contributions: Our proposed framework significantly improves upon existing ensemble selection methodologies by being not just highly efficient but also capable when applied across diverse scenarios including five datasets tested against multiple model architectures ranging from homogenous to heterogeneous groupings along with considering several partitioning schemes among these dataset partitions.",
        "Topic": "Federated Learning"
    },
    {
        "title": "Learning to reconstruct signals from binary measurements alone",
        "abstract": "Recent advances in unsupervised learning have highlighted the possibility of learning to reconstruct signals from noisy and incomplete linear measurements alone. These methods play a key role in medical and scientific imaging and sensing, where ground truth data is often scarce or difficult to obtain. However, in practice measurements are not only noisy and incomplete but also quantized. Here we explore the extreme case of learning from binary observations and provide necessary and sufficient conditions on the number of measurements required for identifying a set of signals from incomplete binary data. Our results are complementary to existing bounds on signal recovery from binary measurements. Furthermore, we introduce a novel self-supervised learning approach, which we name SSBM, that only requires binary data for training. We demonstrate in a series of experiments with real datasets that SSBM performs on par with supervised learning and outperforms sparse reconstruction methods with a fixed wavelet basis by a large margin.",
        "authors": "J. Tachella, L. Jacques",
        "keywords": [
            "binary observations",
            "signal recovery",
            "self-supervised learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ioFIAQOBOS",
        "pdf_src": "https://api2.openreview.net/pdf/c8e0985d8920dfa5399b40ff24a66345509b8e19.pdf",
        "Code_src": "",
        "Introduction": "Background: Recent advancements in unsupervised learning suggest potential for reconstructing signals solely based on noisy and incomplete linear measurements without relying on labeled data.\n\nResearch Problem: How many measurements do you need when dealing with binary observations? \n\nMethod: Investigate the minimum number of measurements needed under certain constraints such as noise level, incompleteness etc., propose an efficient self-supervised learning algorithm called Sparse Signal Binary Masking (SSBM).\n\nMain Contributions:\n1. Provide necessary and sufficient conditions regarding how much measurement one needs depending upon various factors like noise levels & incompleteness.\n2. Introduce Sparse Signal Binary Masking (SSBM), a new self-supervised learning method requiring just binary data during training phase; it can achieve comparable performance compared to supervised approaches while significantly surpassing traditional sparse reconstruction techniques using predefined wavelet bases across several experimental setups utilizing actual datasets.",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "MERMAIDE: Learning to Align Learners using Model-Based Meta-Learning",
        "abstract": "We study how a principal can efficiently and effectively intervene on the rewards of a previously unseen learning agent in order to induce desirable outcomes. This is relevant to many real-world settings like auctions or taxation, where the principal may not know the learning behavior nor the rewards of real people. Moreover, the principal should be few-shot adaptable and minimize the number of interventions, because interventions are often costly. We introduce MERMAIDE, a model-based meta-learning framework to train a principal that can quickly adapt to out-of-distribution agents with different learning strategies and reward functions. We validate this approach step-by-step. First, in a Stackelberg setting with a best-response agent, we show that meta-learning enables quick convergence to the theoretically known Stackelberg equilibrium at test time, although noisy observations severely increase the sample complexity. We then show that our model-based meta-learning approach is cost-effective in intervening on bandit agents with unseen explore-exploit strategies. Finally, we outperform baselines that use either meta-learning or agent behavior modeling, in both $0$-shot and $1$-shot settings with partial agent information.",
        "authors": "A. Banerjee, S. R. Phade, S. Ermon, et.al",
        "keywords": [
            "efficient intervention",
            "meta-learning",
            "cost-effectiveness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=H5VRvCXCzf",
        "pdf_src": "https://api2.openreview.net/pdf/c932ce7594fddff6d7b49c740817f9718bedbd55.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses an important problem related to decision-making scenarios such as auctions and taxation systems when there's uncertainty about individual behaviors due to lack of knowledge regarding their learning processes.\n\nResearch Question: How does one design efficient intervention mechanisms for influencing the actions taken by previously unseen learners without knowing much about them?\n\nMethodology: To tackle this question, the authors propose MERMAIDE - a Model-Based Meta-Learning Framework designed specifically so it adapts rapidly across various learning agents even if they have never been encountered before (\"few-shot adaptable\").\n\nMain Contributions:\n1. They demonstrate that MERMAIDE allows for fast adaptation towards optimal decisions within complex environments.\n2. In particular, under certain conditions resembling Stackelberg games – which involve leaders and followers making strategic moves against each other while considering potential reactions from others involved parties -, MERMAIDE shows its effectiveness through theoretical analysis showing convergence toward expected equilibria despite observational noise leading to increased sample complexity requirements during testing phases.\n3. Additionally, empirical experiments conducted using Bandits illustrate further benefits; these are adaptive algorithms used commonly nowadays especially suited here since they represent situations involving exploration versus exploitation trade-offs among options available over time periods (e.g., bidding intervals).\n4. Lastly compared against baseline methods based solely on meta-learning approaches OR those focusing only on modeling agent behavior patterns but do not consider prior interactions between players beforehand , MERMAIDE significantly improves performance metrics measured across multiple datasets including zero-shot setups where no previous data existed apart from what was observed up until now plus 1-shot ones wherein some limited historical context could potentially help guide future predictions better than just pure random guessing would allow alone!",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Meta Continual Learning on Graphs with Experience Replay",
        "abstract": "Continual learning is a machine learning approach where the challenge is that a constructed learning model executes incoming tasks while maintaining its performance over the earlier tasks. In order to address this issue, we devise a technique that combines two uniquely important concepts in machine learning, namely \"replay buffer\" and \"meta learning\", aiming to exploit the best of two worlds. In this method, the model weights are initially computed by using the current task dataset. Next, the dataset of the current task is merged with the stored samples from the earlier tasks and the model weights are updated using the combined dataset. This aids in preventing the model weights converging to the optimal parameters of the current task and enables the preservation of information from earlier tasks. We choose to adapt our technique to graph data structure and the task of node classification on graphs. We introduce MetaCLGraph, which outperforms the baseline methods over various graph datasets including Citeseer, Corafull, Arxiv, and Reddit. This method illustrates the potential of combining replay buffer and meta learning in the field of continual learning on graphs.",
        "authors": "A. Unal, A. Akgül, M. Kandemir, et.al",
        "keywords": [
            "graph",
            "continual learning",
            "meta learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=8tnrh56P5W",
        "pdf_src": "https://api2.openreview.net/pdf/778e4355c4395b54742a2834c87aacd09c0380d7.pdf",
        "Code_src": "",
        "Introduction": "Background: Continual learning aims at training models capable of handling new tasks without forgetting previous ones.\n\nResearch Problem: How can we maintain the performance across multiple tasks?\n\nMethod: The paper proposes a novel framework called MetaCLGraph for continual learning based on graph data structures through the integration of replay buffers and meta-learning techniques.\n1. Initially compute model weights w using only the current task dataset D_t.\n2. Merge D_t with previously stored samples S from past tasks into a single dataset M = {D_t ∪ S}.\n3. Update model weights w using the combined dataset M.\n\nMain Contributions:\n- Demonstrates the effectiveness of integrating replay buffers and meta-learning approaches within the context of continual learning specifically tailored towards graph data types like node classification problems involving different real-world datasets such as Citeseer, CoraFull, ArXiv, and Reddit.\n- Shows improved results compared against existing baselines highlighting promising prospects when employing these strategies together rather than independently or sequentially applied during training processes related to continuous adaptation scenarios encountered throughout time series analysis applications",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Universal Graph Continual Learning",
        "abstract": "We address catastrophic forgetting issues in graph learning as the arrival of new data from diverse task distributions often leads graph models to prioritize the current task, causing them to forget valuable insights from previous tasks. Whereas prior studies primarily tackle one setting of graph continual learning such as incremental node classification, we focus on a universal approach wherein each data point in a task can be a node or a graph, and the task varies from node to graph classification. We refer to this setting as Universal Graph Continual Learning (UGCL), which includes node-unit node classification (NUNC), graph-unit node classification (GUNC), and graph-unit graph classification (GUGC). Our novel method maintains a replay memory of nodes and neighbours to remind the model of past graph structures through distillation. Emphasizing the importance of preserving distinctive graph structures across tasks, we enforce that coarse-to-grain graph representations stay close to previous ones by minimizing our proposed global and local structure losses. We benchmark our method against various continual learning baselines in 8 real-world graph datasets and achieve significant improvement in average performance and forgetting across tasks.",
        "authors": "T. D. Hoang, D. V. Tung, D. Nguyen, et.al",
        "keywords": [
            "graph continual learning",
            "universal graph continual learning",
            "catastrophic forgetting"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=wzRE5kTnl3",
        "pdf_src": "https://api2.openreview.net/pdf/a316243fc354b7435f0227df8016d30c0da30784.pdf",
        "Code_src": "",
        "Introduction": "Background: Catastrophic forgetting is an issue commonly faced in graph learning where existing graph models tend to ignore previously learned knowledge when encountering new data with different task distributions.\n\nResearch Problem: The problem addressed here focuses on developing a universal solution for graph continual learning rather than addressing specific settings like incremental node classification alone; it aims at handling scenarios involving both node and graph units within varying tasks - referred to as Universal Graph Continual Learning (UGCL).\n\nMethods: To overcome catastrophic forgetting while dealing with UGCL problems, they propose maintaining a replay memory containing nodes along with their neighbors so these memories are used during training sessions via distillation techniques.\nAdditionally, emphasizing the significance of retaining distinct graph structures throughout multiple tasks, two types of loss functions – Global Structure Loss and Local Structure Loss – have been introduced aiming towards keeping coarse-to-fine graph representation similarities intact over time.\n\nMain Contributions:\n1. A comprehensive framework called Universal Graph Continual Learning (UGCL) has been developed capable enough not just for node classification but also including graph unit classifications under variable conditions.\n2. They introduce Distillation-based Replay Memory mechanism into the process ensuring recall of earlier learned information without interference due to newer inputs leading to less catastrophic forgetting phenomenon observed traditionally \n3. Two novel structural consistency metrics namely Global Structure Loss & Local Structure Loss were designed specifically focusing on preserving unique topological characteristics present across all iterations/tasks involved in continuous learning scenario thus improving overall robustness significantly compared other baseline methods tested extensively using eight real-world datasets",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Cross-client Label Propagation for Transductive and Semi-Supervised Federated Learning",
        "abstract": "We present Cross-Client Label Propagation (XCLP), a new method for transductive and semi-supervised federated learning. XCLP estimates a data graph jointly from the data of multiple clients and computes labels for the unlabeled data by propagating label information across the graph. To avoid clients having to share their data with anyone, XCLP employs two cryptographically secure protocols: secure Hamming distance computation and secure summation. We demonstrate two distinct applications of XCLP within federated learning. In the first, we use it in a one-shot way to predict labels for unseen test points. In the second, we use it to repeatedly pseudo-label unlabeled training data in a federated semi-supervised setting. Experiments on both real federated and standard benchmark datasets show that in both applications XCLP achieves higher classification accuracy than alternative approaches.",
        "authors": "J. Scott, M. Yeo, C. H. Lampert",
        "keywords": [
            "federated learning",
            "cross-client label propagation",
            "cryptographic protocols"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=gY04GX8R5k",
        "pdf_src": "https://api2.openreview.net/pdf/bfffb1b9925500f052028fcc36ba27406c3924f0.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper introduces an innovative approach called Cross-Client Label Propagation (XCLP). This is designed as part of the broader field of Federated Learning which aims at improving machine learning models without requiring all client data to be centrally processed or shared.\n\nResearch Problem:\nFederated learning faces challenges such as how to effectively utilize limited labeled data while maintaining privacy among different participating clients whose data cannot be directly accessed due to security concerns.\n \nMethodology:\nTo address these issues, XCLP constructs a global data graph based on the local data distributions collected from various clients' datasets using cryptographic techniques like secure Hamming distance computation ensuring no raw data sharing between parties involved during this process. The proposed algorithm then uses this constructed graph to propagate label information efficiently through each node representing an instance in any dataset contributing towards better prediction performance even when dealing with only partially labeled instances.\n\nMain Contributions:\n1. It develops novel cryptographic mechanisms - Secure Hamming Distance Computation & Secure Summation Protocol – allowing participants not just to compute distances but also sum up values securely over encrypted inputs leading toward more accurate predictions compared traditional methods where full access would typically lead to improved results;\n2. Demonstrates its efficacy via experiments conducted against several benchmarks demonstrating significant improvements beyond existing state-of-the-art solutions; \n3. Provides insights into practical implementation details including scalability considerations making it suitable for large-scale deployments involving numerous distributed entities engaged in collaborative tasks related to artificial intelligence systems development",
        "Topic": "Federated Learning"
    },
    {
        "title": "Reliable Active Learning via Influence Functions",
        "abstract": "Due to the high cost and time-consuming nature of collecting labeled data, having insufficient labeled data is a common challenge that can negatively impact the performance of deep learning models when applied to real-world applications. Active learning (AL) aims to reduce the cost and time required for obtaining labeled data by selecting valuable samples during model training. However, recent works have pointed out the performance unreliability of existing AL algorithms for deep learning (DL) architectures under different scenarios, which manifests as their performance being comparable (or worse) to that of basic random selection. This behavior compromises the applicability of these approaches. We address this problem by proposing a theoretically motivated AL framework for DL architectures. We demonstrate that the most valuable samples for the model are those that, unsurprisingly, improve its performance on the entire dataset, most of which is unlabeled, and present a framework to efficiently estimate such performance (or loss) via influence functions, pseudo labels and diversity selection. Experimental results show that the proposed reliable active learning via influence functions (RALIF) can consistently outperform the random selection baseline as well as other existing and state-of-the art active learning approaches.",
        "authors": "M. Xia, R. Henao",
        "keywords": [
            "data efficiency",
            "active learning",
            "reliability"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=dN9YICB6hN",
        "pdf_src": "https://api2.openreview.net/pdf/a713c5ef5d1b9199ef4cb37ed035edcce205eaec.pdf",
        "Code_src": "",
        "Introduction": "Background: The collection of large amounts of labeled data at high costs has been identified as one of the major challenges in applying machine learning techniques like deep learning due to the need for human annotation.\n\nResearch Problem: Existing active learning methods may not be reliably effective across various settings within deep learning architectures leading to performances similar or even inferior to simple random sampling strategies compromising practical application potential.\n\nMethodology: To overcome this issue we propose RALIF - Reliable Active Learning through Influence Functions – an algorithmic framework grounded in theoretical insights into how influential certain examples might actually contribute towards improving predictions over unseen datasets using influence functions along with pseudo-labeling and sample diversity considerations enhancing estimation efficiency.\n\nMain Contributions: Our contributions include developing a novel approach based on sound theory ensuring reliability; demonstrating improved performance compared against baselines including random selection while also surpassing several prior state-of-the-art active learning methodologies experimentally validated",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Pairwise Learning with Adaptive Online Gradient Descent",
        "abstract": "In this paper, we propose an adaptive online gradient descent method with momentum for pairwise learning, in which the stepsize is determined by historical information. Due to the structure of pairwise learning, the sample pairs are dependent on the parameters, causing difficulties in the convergence analysis. To this end, we develop novel techniques for the convergence analysis of the proposed algorithm. We show that the proposed algorithm can output the desired solution in strongly convex, convex, and nonconvex cases. Furthermore, we present theoretical explanations for why our proposed algorithm can accelerate previous workhorses for online pairwise learning. All assumptions used in the theoretical analysis are mild and common, making our results applicable to various pairwise learning problems. To demonstrate the efficiency of our algorithm, we compare the proposed adaptive method with the non-adaptive counterpart on the benchmark online AUC maximization problem.",
        "authors": "T. Sun, Q. Wang, Y. Lei, et.al",
        "keywords": [
            "pairwise learning",
            "adaptive online gradient descent",
            "convergence analysis"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=rq1SaHQg2k",
        "pdf_src": "https://api2.openreview.net/pdf/b97d038cd44c7283679a53ee392ee4a1c3538082.pdf",
        "Code_src": "",
        "Introduction": "Background: Pairwise learning involves comparing two samples at a time rather than optimizing over all features simultaneously; however, it introduces dependencies between the samples due to parameter updates.\n\nResearch Problem: The challenge lies in analyzing the convergence behavior of algorithms designed for pairwise learning since these dependences complicate traditional convergence analyses.\n \nMethod: This study develops new convergence analysis methods tailored specifically to the adaptive online gradient descent approach with momentum employed in pairwise learning settings where the step size adapts based on past iterations' performance.\n\nMain Contributions:\n1. Novel Convergence Techniques - They introduce innovative approaches addressing the complexities arising from pairwise learning's dependency issues during convergence analysis.\n2. Broad Applicability - Their findings apply across strong convexity, convexity levels as well as non-convex scenarios suggesting robustness beyond typical conditions found elsewhere within literature.\n3. Acceleration Explanation - They provide theoretical insights into how their algorithm outperforms existing state-of-the-art solutions when applied towards solving online pairwise learning tasks like maximizing Area Under Curve (AUC).\n4. Empirical Validation - By contrasting against non-adaptive counterparts using empirical benchmarks focusing on online AUC maximization task they validate efficacy demonstrated through comparative studies confirming superiority under practical considerations pertinent to real-world applications involving pairwise comparisons such as recommendation systems or sentiment analysis among others.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Improved identification accuracy in equation learning via comprehensive $\\boldsymbol{R^2}$-elimination and Bayesian model selection",
        "abstract": "In the field of equation learning, exhaustively considering all possible combinations derived from a basis function dictionary is infeasible. Sparse regression and greedy algorithms have emerged as popular approaches to tackle this challenge. However, the presence of strong collinearities poses difficulties for sparse regression techniques, and greedy steps may inadvertently exclude important components of the true equation, leading to reduced identification accuracy. In this article, we present a novel algorithm that strikes a balance between comprehensiveness and efficiency in equation learning. Inspired by stepwise regression, our approach combines the coefficient of determination, $R^2$, and the Bayesian model evidence, $p(y|\\mathcal{M})$, in a novel way. Through three extensive numerical experiments involving random polynomials and dynamical systems, we compare our method against two standard approaches, four state-of-the-art methods, and bidirectional stepwise regression incorporating $p(y|\\mathcal{M})$. The results demonstrate that our less greedy algorithm surpasses all other methods in terms of identification accuracy. Furthermore, we discover a heuristic approach to mitigate the overfitting penalty associated with $R^2$ and propose an equation learning procedure solely based on $R^2$, which achieves high rates of exact equation recovery.",
        "authors": "D. Nickelsen, B. Bah",
        "keywords": [
            "equation learning",
            "sparse regression",
            "greedy algorithms"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=0ck7hJ8EVC",
        "pdf_src": "https://api2.openreview.net/pdf/b216c99c5a95909f046b8bebef9d045bf4742b09.pdf",
        "Code_src": "",
        "Introduction": "Background: Equation learning involves identifying mathematical equations or functions given observational data without explicit knowledge of these equations. This task becomes challenging when dealing with large dictionaries containing many potential basis functions.\n\nResearch Problem: Traditional sparse regression and greedy algorithms are commonly used but suffer limitations due to strong collinearity issues among variables; they might also omit essential components required for accurate equation reconstruction resulting in lower identification accuracy.\n\nMethod: To address above challenges while maintaining both comprehensiveness and efficiency during equation learning process, authors introduce a new algorithm inspired by stepwise regression methodology.\nThis novel approach integrates the coefficient of determination ($R^2$), representing goodness of fit within the dataset,\nand Bayesian model evidence ($p(y|\\mathcal{M})$), reflecting how likely observed data is under each candidate model.\n\nMain Contributions:\n1. A balanced algorithmic framework combining aspects of comprehensive search space coverage along with efficient computation time;\n2. Demonstrated superiority through empirical tests using synthetic datasets including random polynomials & dynamical system models compared not only traditional sparse regression/greedy methods \nbut also advanced existing counterparts such as bidirectional stepwise regression with Bayesian component inclusion;\n3. Identification of an effective regularization strategy leveraging $R^2$ coefficients alone achieving near-perfect recovery rate",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Personalized Federated Learning with Communication Compression",
        "abstract": "In contrast to training traditional machine learning~(ML) models in data centers, federated learning~(FL) trains ML models over local datasets contained on resource-constrained heterogeneous edge devices. Existing FL algorithms aim to learn a single global model for all participating devices, which may not be helpful to all devices participating in the training due to the heterogeneity of the data across the devices. Recently, Hanzely and Richt\\'{a}rik (2020) proposed a new formulation for training personalized FL models aimed at balancing the trade-off between the traditional global model and the local models that could be trained by individual devices using their private data only. They derived a new algorithm, called {\\em loopless gradient descent}~(L2GD), to solve it and showed that this algorithms leads to improved communication complexity guarantees in regimes when more personalization is required. In this paper, we equip their L2GD algorithm with a {\\em bidirectional} compression mechanism to further reduce the communication bottleneck between the local devices and the server. Unlike other compression-based algorithms used in the FL-setting, our compressed L2GD algorithm operates on a probabilistic communication protocol, where communication does not happen on a fixed schedule. Moreover, our compressed L2GD algorithm maintains a similar convergence rate as vanilla SGD without compression. To empirically validate the efficiency of our algorithm, we perform diverse numerical experiments on both convex and non-convex problems and using various compression techniques.",
        "authors": "E. H. Bergou, K. P. Burlachenko, A. Dutta, et.al",
        "keywords": [
            "federated learning",
            "personalized FL models",
            "loopless gradient descent"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=dZugyhbNFY",
        "pdf_src": "https://api2.openreview.net/pdf/51681be83587e99f639b028d946f83b9547a9107.pdf",
        "Code_src": "",
        "Introduction": "Background: Traditional machine learning models are typically trained centrally within large-scale data centers; however, these centralized approaches can lead to significant challenges such as privacy concerns related to sharing sensitive user data or high latency issues caused by long-distance data transfers.\n\nResearch Problem: Federated Learning (FL) was introduced as an alternative approach aiming to train machine learning models directly from decentralized client devices while keeping most raw data locally stored rather than being sent offsite during training processes. Despite its potential benefits like preserving users' privacy through minimal data sharing among clients instead of servers, existing FL methods often converge towards a globally optimal solution regardless of device heterogeneity leading potentially suboptimal performance tailored specifically per-device needs.\n\nMethod: This research introduces a novel modification into the Loopless Gradient Descent (L2GD) algorithm originally proposed by Hanzely and Richt\\'{a}rik 2020 focusing on Personalized FL Models. The authors enhance the original L2GD method with a \"bidirectional\" compression technique designed especially suited for FL scenarios involving many small devices communicating infrequently but frequently enough so they don't need pre-defined schedules - unlike previous works relying solely upon periodicity-driven compressions.\n\nMain Contributions:\n1. A novel Bidirectional Compression Mechanism integrated into the L2GD framework.\n2. Demonstrates how this modified version significantly reduces communication bottlenecks compared to conventional compression strategies commonly employed in FL settings today \n3. Shows empirical evidence supporting comparable convergence rates relative to standard Stochastic Gradient Descent (SGD) despite employing less frequent communications patterns typical amongst distributed edge devices involved in FL setups",
        "Topic": "Federated Learning"
    },
    {
        "title": "Uncovering Unique Concept Vectors through Latent Space Decomposition",
        "abstract": "Interpreting the inner workings of deep learning models is crucial for establishing trust and ensuring model safety. Concept-based explanations have emerged as a superior approach that is more interpretable than feature attribution estimates such as pixel saliency. However, defining the concepts for the interpretability analysis biases the explanations by the user’s expectations on the concepts. To address this, we propose a novel post-hoc unsupervised method that automatically uncovers the concepts learned by deep models during training. By decomposing the latent space of a layer in singular vectors and refining them by unsupervised clustering, we uncover concept vectors aligned with directions of high variance that are relevant to the model prediction, and that point to semantically distinct concepts. Our extensive experiments reveal that the majority of our concepts are readily understandable to humans, exhibit coherency, and bear relevance to the task at hand. Moreover, we showcase the practical utility of our method in dataset exploration, where our concept vectors successfully identify outlier training samples affected by various confounding factors. This novel exploration technique has remarkable versatility to data types and model architectures and it will facilitate the identification of biases and the discovery of sources of error within training data.",
        "authors": "M. Graziani, L. O'mahony, A. Nguyen, et.al",
        "keywords": [
            "concept-based explanation",
            "unsupervised learning",
            "bias mitigation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LT4DXqUJTD",
        "pdf_src": "https://api2.openreview.net/pdf/2d4ff0310e996b9522c8b5a0e31fd17882fb4aa9.pdf",
        "Code_src": "",
        "Introduction": "Background: Establishing trust in machine learning models requires understanding their internal mechanisms; however, traditional methods like feature attribution can be limited due to subjective bias from users' preconceived notions about what constitutes an \"important\" concept.\n\nResearch Question: How do you develop a less biased way to explain how complex neural networks make decisions?\n\nMethod: We introduce a new unsupervised approach called Post-Hoc Unsupervised Concept Discovery (PHUCD). PHUCD involves analyzing the latent spaces inside layers of neural networks using singular vectors derived from matrix factorizations or PCA-like techniques followed by refinement through unsupervised clustering algorithms which group similar vectors together into clusters representing different semantic concepts related to predictions made by the network.\n \nMain Contributions:\n1. The proposed method does not rely on predefined concepts but discovers these concepts autonomously based solely on patterns observed while training the model without any human intervention beyond initial setup parameters.\n2. It reveals coherent sets of concept vectors pointing towards semantically meaningful categories associated with inputs leading up to predicted outputs allowing us insight into why certain examples were classified one way rather than another thereby improving transparency around decision-making processes within trained models.\n3. Demonstrated its effectiveness across datasets showing ability to detect outliers potentially corrupted by external influences thus aiding practitioners understand potential sources errors early detection could prevent downstream issues arising later down pipeline stages when deployed production environments",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment",
        "abstract": "Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially serious consequences. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) to address this problem, where generative models are fine-tuned with RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generative models effectively. Utilizing a reward model and a sufficient number of samples, our approach selects the high-quality samples, discarding those that exhibit undesired behavior, and subsequently enhancing the model by fine-tuning on these filtered samples. Our studies show that RAFT can effectively improve the model performance in both reward learning and other automated metrics in both large language models and diffusion models.",
        "authors": "H. Dong, W. Xiong, D. Goyal, et.al",
        "keywords": [
            "bias alignment",
            "reinforcement learning from human feedback",
            "reward ranked fine tuning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=m7p5O7zblY",
        "pdf_src": "https://api2.openreview.net/pdf/7dccf280f93ee69970b04353b1d4f8c3700f9d3e.pdf",
        "Code_src": "",
        "Introduction": "Background: Generative foundation models trained without supervision may inherit biases due to extensive unlabelled data sources which could lead to biased outputs or unethical implications when deployed.\n\nResearch Problem: The challenge lies in mitigating such biases within generative models so they adhere to ethical standards during deployment across various domains like natural language processing using LLMs (Large Language Models) and image generation through diffusion processes.\n\nMethodology: We propose a novel method called Reward Ranked FineTuning (RAFT). This involves ranking candidate samples based on how well they satisfy certain criteria defined via a reward function informed either directly about what constitutes good output for humans (\"human feedback\") as used previously; alternatively it uses only generated samples themselves ('self-play') if direct human input isn't feasible.\n \nMain Contributions:\n1. RAFT introduces two variants - one utilizing human feedback while another operates autonomously solely relying on self-play rewards derived purely from sample quality assessments among itself.\n2. Demonstrates effectiveness beyond prior methods particularly against issues related to instability commonly found in reinforcement learning approaches applied towards bias mitigation tasks involving generative models.\n3. Shows improvements not just in terms of subjective evaluation but also objective measures indicating better aligned results compared traditional RLHF techniques especially under resource constraints encountered often practical settings.",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Differentially Private Optimizers Can Learn Adversarially Robust Models",
        "abstract": "Machine learning models have shone in a variety of domains and attracted increasing attention from both the security and the privacy communities. One important yet worrying question is: Will training models under the differential privacy (DP) constraint have an unfavorable impact on their adversarial robustness? While previous works have postulated that privacy comes at the cost of worse robustness, we give the first theoretical analysis to show that DP models can indeed be robust and accurate, even sometimes more robust than their naturally-trained non-private counterparts. We observe three key factors that influence the privacy-robustness-accuracy tradeoff: (1) hyper-parameters for DP optimizers are critical; (2) pre-training on public data significantly mitigates the accuracy and robustness drop; (3) choice of DP optimizers makes a difference. With these factors set properly, we achieve 90\\% natural accuracy, 72\\% robust accuracy ($+9\\%$ than the non-private model) under $l_2(0.5)$ attack, and 69\\% robust accuracy ($+16\\%$ than the non-private model) with pre-trained SimCLRv2 model under $l_\\infty(4/255)$ attack on CIFAR10 with $\\epsilon=2$. In fact, we show both theoretically and empirically that DP models are Pareto optimal on the accuracy-robustness tradeoff. Empirically, the robustness of DP models is consistently observed across various datasets and models. We believe our encouraging results are a significant step towards training models that are private as well as robust.",
        "authors": "Z. Bu, Y. Zhang",
        "keywords": [
            "privacy",
            "adversarial robustness",
            "differential privacy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=o8VgRNYh6n",
        "pdf_src": "https://api2.openreview.net/pdf/1bfa422a2ad90a6b55dbb5c077ec53ef4cb65856.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses how machine learning has been widely applied but also raises concerns about its vulnerability due to adversaries who may exploit vulnerabilities or leak sensitive information through attacks such as adversarial examples.\n\nResearch Question:\nThe central research problem addressed by this work revolves around whether it's possible—or necessary—to maintain high levels of adversarial robustness while adhering to constraints like differential privacy during the training process—a balance between privacy preservation and model robustness against adversarial perturbations.\n\nMethods:\nTo address this issue, they conduct a comprehensive theoretical investigation into the relationship among differential privacy, adversarial robustness, and predictive performance using differential privacy-preserving algorithms known as differentially private optimizers which impose noise on the gradients used within neural networks' optimization processes.\n \nMain Contributions:\nThis study provides empirical evidence showing that it’s feasible—and potentially beneficial—to train models according to differential privacy constraints without sacrificing too much adversarial robustness compared to standard non-private training methods. The authors identify several crucial factors influencing the trade-off:\n\n1. Hyper-parameter tuning plays vital roles when employing differential privacy techniques;\n2. Pre-training on large amounts of publicly available data helps mitigate potential drops in accuracy and robustness;\n3. Different choices of differential privacy optimizers lead to varying outcomes regarding the trade-off between privacy protection and adversarial robustness.\n\nThey demonstrate experimentally—on CIFAR10 dataset—that achieving proper settings leads to maintaining nearly 90% natural accuracy along with substantial improvements in robustness metrics over non-private counterparts. Specifically, after applying differential privacy constraints alongside pre-training with SimCLRv2, they report gains of +9% and +16% respectively relative to non-private baseline models depending upon the chosen norm and epsilon value commonly employed in evaluating adversarial robustness.\n\nOverall, findings suggest that differential privacy need not come hand-in-hand with compromised adversarial robustness—it could actually yield better performing models overall if certain conditions are met carefully considering all relevant parameters involved throughout training procedures.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "The Analysis of the Expected Change in the Classification Probability of the Predicted Label",
        "abstract": "We present a formalism for estimating the expected change in the probability distribution of the predicted label of an object, with respect to all small perturbations to the object. We first derive analytically an estimate of the expected probability change as a function of the input noise. We then conduct three empirical studies: in the first study, experimental results on image classification show that the proposed measure can be used to distinguish the not-robust label predictions from those that are robust, even when they are all predicted with high confidence. The second study shows that the proposed robustness measure is almost always higher for the predictions on the corrupted images, compared to the predictions on the original versions of them. The final study shows that the proposed measure is lower for models when they are trained using adversarial training approaches.",
        "authors": "R. Yang, P. Liu, M. Bilgic",
        "keywords": [
            "probability distribution",
            "robustness measure",
            "adversarial training"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=gvqzvUVPiQ",
        "pdf_src": "https://api2.openreview.net/pdf/a273d7a66f697e7cc5ca45b9c0a0a55a0d54c16b.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper aims to address the issue of model robustness against small perturbations by developing a formalism to estimate how much the predicted label's probability distribution changes due to these perturbations.\n\nResearch Problem:\nHow does one quantify and compare the robustness of different machine learning models' predictions under various types of perturbations?\n\nMethods:\n1. Analytical Estimation: The authors provide an analytical formula predicting the expected probability change resulting from input noise.\n2. Empirical Studies:\n   - Study 1: Evaluates the effectiveness of their proposed measure at distinguishing between robust and non-robust predictions within an image classification task where labels might still have been confidently assigned despite being less robust than others.\n   - Study 2: Compares the robustness score across predictions made both before and after corruption through adding noise or distortions; it finds consistently higher scores post-corruption indicating greater vulnerability pre-corruption.\n   - Study 3: Examines whether this metric correlates inversely with the use of adversarial training techniques which often aim to make models more resilient but may also lead to reduced robustness measures if only certain classes become overemphasized during such training.\n\nMain Contributions:\n- A novel method for quantifying prediction robustness based on expected probability changes induced by input noise.\n- Experimental validation showing its efficacy beyond traditional metrics like accuracy alone especially concerning identifying vulnerable predictions amidst confident ones without necessarily sacrificing performance overall.\n- Insights into trade-offs related to adversarial training methods suggesting potential limitations regarding achieving optimal robustness while maintaining other desirable properties",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Latent State Models of Training Dynamics",
        "abstract": "The impact of randomness on model training is poorly understood. How do differences in data order and initialization actually manifest in the model, such that some training runs outperform others or converge faster? Furthermore, how can we interpret the resulting training dynamics and the phase transitions that characterize different trajectories? To understand the effect of randomness on the dynamics and outcomes of neural network training, we train models multiple times with different random seeds and compute a variety of metrics throughout training, such as the $L_2$ norm, mean, and variance of the neural network's weights. We then fit a hidden Markov model (HMM) over the resulting sequences of metrics. The HMM represents training as a stochastic process of transitions between latent states, providing an intuitive overview of significant changes during training. Using our method, we produce a low-dimensional, discrete representation of training dynamics on grokking tasks, image classification, and masked language modeling. We use the HMM representation to study phase transitions and identify latent \"detour\" states that slow down convergence.",
        "authors": "M. Y. Hu, A. Chen, N. Saphra, et.al",
        "keywords": [
            "randomness",
            "model training",
            "phase transitions"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=NE2xXWo0LF",
        "pdf_src": "https://api2.openreview.net/pdf/51b7130da9d9b92e985dfee5efa9607624c59b78.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the lack of understanding regarding the influence of randomness within machine learning processes specifically focusing on neural networks.\n\nResearch Question: It aims at exploring two main questions:\n1. How does the initial setup of data ordering and weight initialization affect the performance outcome?\n2. Can one explain these effects by analyzing specific patterns observed through training?\n\nMethodology: The researchers conducted repeated experiments using neural networks where they varied the random seed for each run which introduces variability into their datasets' orders.\nThey collected various metrics like L2 norms, means, and variances from the trained neural networks across all iterations - effectively creating time series representations based on those metrics per experiment.\nThese metric sequences were further analyzed via fitting them against Hidden Markov Models (HMMs), allowing us to represent complex training dynamics as a sequence of potential states transitioning stochastically among themselves.\n\n\nMain Contributions: \n- They introduced new methods capable of visualizing high-dimensional training dynamics along with identifying critical points known as 'phase transitions'.\n- By employing this approach successfully applied it onto three distinct types of machine learning problems – Grokking tasks, Image Classification & Masked Language Modeling.\n- Identified latent “detour” states responsible for slowing down convergence rates leading towards better optimization strategies avoiding unnecessary detours early enough ensuring more efficient algorithms development moving forward.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Replay-enhanced Continual Reinforcement Learning",
        "abstract": "Replaying past experiences has proven to be a highly effective approach for averting catastrophic forgetting in supervised continual learning. However, some crucial factors are still largely ignored, making it vulnerable to serious failure, when used as a solution to forgetting in continual reinforcement learning, even in the context of perfect memory where all data of previous tasks are accessible in the current task. On the one hand, since most reinforcement learning algorithms are not invariant to the reward scale, the previously well-learned tasks (with high rewards) may appear to be more salient to the current learning process than the current task (with small initial rewards). This causes the agent to concentrate on those salient tasks at the expense of generality on the current task. On the other hand, offline learning on replayed tasks while learning a new task may induce a distributional shift between the dataset and the learned policy on old tasks, resulting in forgetting. In this paper, we introduce RECALL, a replay-enhanced method that greatly improves the plasticity of existing replay-based methods on new tasks while effectively avoiding the recurrence of catastrophic forgetting in continual reinforcement learning. RECALL leverages adaptive normalization on approximate targets and policy distillation on old tasks to enhance generality and stability, respectively. Extensive experiments on the Continual World benchmark show that RECALL performs significantly better than purely perfect memory replay, and achieves comparable or better overall performance against state-of-the-art continual learning methods.",
        "authors": "T. Zhang, K. Z. Shen, Z. Lin, et.al",
        "keywords": [
            "distributional shift",
            "catastrophic forgetting",
            "continual reinforcement learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=91hfMEUukm",
        "pdf_src": "https://api2.openreview.net/pdf/efc8283a264b14cd1cedac2d5b0f14bfd10161f7.pdf",
        "Code_src": "",
        "Introduction": "Background: Catastrophic forgetting is an issue commonly faced by neural networks during continual learning scenarios such as lifelong learning systems which need to learn multiple tasks without losing knowledge from earlier ones.\n\nResearch Problem: The problem addressed here concerns how to apply replay mechanisms successfully into continual reinforcement learning settings despite its vulnerability due to reward scale issues leading to saliency bias towards older higher-reward tasks over newer lower-reward tasks; also because off-policy learning can lead to distribution shifts causing forgetting problems with respect to policies learned across different tasks.\n\nMethod: To tackle these challenges posed above, they propose \"RECALL,\" a novel replay-enhanced framework designed specifically within the constraints of continual reinforcement learning environments.\n1. Adaptive Normalization - They employ adaptive normalization techniques applied directly onto approximated target values so agents do not become biased toward any particular scale range among various rewards encountered throughout training history thus improving generalizability beyond just memorizing specific reward scales associated with individual tasks.\n2. Policy Distillation - Additionally, their second contribution involves using policy distillation strategies leveraging information from pre-trained models on prior tasks helping stabilize model behavior through transferable knowledge gained before encountering each subsequent task.\n\nMain Contributions:\nThe main contributions made via introducing RECALL include:\n\n1. A significant improvement upon existing replay-based approaches regarding adaptability under varying conditions present in continual reinforcement learning domains like saliency biases related to reward scales \n2. An enhanced capacity maintained after extensive continuous learning sessions preventing catastrophic forgetting phenomena often observed otherwise especially concerning both generalization capabilities along with long-term retention rates compared traditional pure replay solutions alone;\n3. Demonstrated superior performance metrics achieved relative to top-performing continual learning methodologies available today based on empirical evaluations conducted utilizing the Continual World benchmarking suite showcasing efficacy robustness across diverse datasets/tasks",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Addressing caveats of neural persistence with deep graph persistence",
        "abstract": "Neural Persistence is a prominent measure for quantifying neural network complexity, proposed in the emerging field of topological data analysis in deep learning. In this work, however, we find both theoretically and empirically that the variance of network weights and spatial concentration of large weights are the main factors that impact neural persistence. Whilst this captures useful information for linear classifiers, we find that no relevant spatial structure is present in later layers of deep neural networks, making neural persistence roughly equivalent to the variance of weights. Additionally, the proposed averaging procedure across layers for deep neural networks does not consider interaction between layers. Based on our analysis, we propose an extension of the filtration underlying neural persistence to the whole neural network instead of single layers, which is equivalent to calculating neural persistence on one particular matrix. This yields our deep graph persistence measure, which implicitly incorporates persistent paths through the network and alleviates variance-related issues through standardisation. Code is available at  https://github.com/ExplainableML/Deep-Graph-Persistence.",
        "authors": "L. Girrbach, A. Christensen, O. Winther, et.al",
        "keywords": [
            "deep graph persistence",
            "neural network complexity",
            "weight variance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=oyfRWeoUJY",
        "pdf_src": "https://api2.openreview.net/pdf/5f72c945adc0b796ec88e0b3611e8b3f95280e2d.pdf",
        "Code_src": "https://github.com/ExplainableML/Deep-Graph-Persistence",
        "Introduction": "Background: Neural Persistence as a Measure of Network Complexity\nResearch Problem: Investigating the Factors Affecting Neural Persistence and Developing a More Robust Measure.\nMethods: Theoretical Analysis & Empirical Studies on Deep Neural Networks.\nMain Contributions:\n1. Identifying Variance of Weights and Spatial Concentration of Large Weights as Key Influencers over Neural Persistence.\n2. Revealing Absence of Relevant Spatial Structure in Later Layers of Deep Neural Networks leading to Weight Variance Equivalence with Neural Persistence.\n3. Proposing an Extension from Single-Layer Filtration to Whole-Network Filtration, termed \"Deep Graph Persistence,\" addressing Variance Issues via Standardization.\n\nCode Availability: GitHub Repository - [Link Provided](https://github.com/ExplainableML/Deep-Graph-Persistence).",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "A Combinatorial Semi-Bandit Approach to Charging Station Selection for Electric Vehicles",
        "abstract": "In this work, we address the problem of long-distance navigation for battery electric vehicles (BEVs), where one or more charging sessions are required to reach the intended destination. We consider the availability and performance of the charging stations to be unknown and stochastic, and develop a combinatorial semi-bandit framework for exploring the road network to learn the parameters of the queue time and charging power distributions. Within this framework, we first outline a method for transforming the road network graph into a graph of feasible paths between charging stations to handle the constrained combinatorial optimization problem in an efficient way. Then, for the feasibility graph, we use a Bayesian approach to model the stochastic edge weights, utilizing conjugate priors for the one-parameter exponential and two-parameter gamma distributions, the latter of which is novel to multi-armed bandit literature. Finally, we apply combinatorial versions of Thompson Sampling, BayesUCB and Epsilon-greedy to the problem. We demonstrate the performance of our framework on long-distance navigation problem instances in large-scale country-sized road networks, with simulation experiments in Norway, Sweden and Finland.",
        "authors": "N. Åkerblom, M. H. Chehreghani",
        "keywords": [
            "road network",
            "combinatorial semi-bandit framework",
            "stochastic charging"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ndw90pkNM9",
        "pdf_src": "https://api2.openreview.net/pdf/fe6df0bd7b6ff47d26d0f9e29a1b3251af8ab8e4.pdf",
        "Code_src": "",
        "Introduction": "Background: Long-distance navigation for battery electric vehicles (BEVs) requires multiple charging stops due to limited driving range per charge cycle.\n\nResearch Problem: How can BEVs efficiently navigate through road networks when charging station availability and performance characteristics vary?\n\nMethodology: Developed a combinatorial semi-bandit framework that learns the distribution parameters of queue times and charging powers while navigating the road network.\n\nMain Contributions:\n1. Transformed the road network graph into a feasible path graph considering constraints.\n2. Employed Bayesian modeling using conjugate priors from exponential and gamma distributions within the semi-bandit framework; introduced gamma distribution for stochastic edge weights as new to multi-armed bandit studies.\n3. Applied Thompson Sampling, BayesUCB, and Epsilon-greedy algorithms adapted for combinatorial problems solving.\n4. Demonstrated the effectiveness across simulated scenarios involving extensive road networks spanning Norway, Sweden, and Finland demonstrating scalability beyond small-scale environments.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Provably Safe Reinforcement Learning: Conceptual Analysis, Survey, and Benchmarking",
        "abstract": "Ensuring the safety of reinforcement learning (RL) algorithms is crucial to unlock their potential for many real-world tasks. However, vanilla RL and most safe RL approaches do not guarantee safety. In recent years, several methods have been proposed to provide hard safety guarantees for RL, which is essential for applications where unsafe actions could have disastrous consequences. Nevertheless, there is no comprehensive comparison of these provably safe RL methods. Therefore, we introduce a categorization of existing provably safe RL methods, present the conceptual foundations for both continuous and discrete action spaces, and empirically benchmark existing methods. We categorize the methods based on how they adapt the action: action replacement, action projection, and action masking. Our experiments on an inverted pendulum and a quadrotor stabilization task indicate that action replacement is the best-performing approach for these applications despite its comparatively simple realization. Furthermore, adding a reward penalty, every time the safety verification is engaged, improved training performance in our experiments. Finally, we provide practical guidance on selecting provably safe RL approaches depending on the safety specification, RL algorithm, and type of action space.",
        "authors": "H. Krasowski, J. Thumm, M. Müller, et.al",
        "keywords": [
            "safety",
            "reinforcement learning",
            "provable guarantees"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=mcN0ezbnzO",
        "pdf_src": "https://api2.openreview.net/pdf/264ffa19ca70e3ec41032fe6a4802932b5eda4e6.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper addresses concerns about ensuring the safety of reinforcement learning (RL) algorithms when applied to potentially dangerous or critical situations.\n\nResearch Problem:\nThe problem addressed by this research involves comparing different methods designed to ensure \"hard\" safety guarantees within RL frameworks - meaning those that can mathematically prove that certain undesirable outcomes will never occur during execution.\n \nMethodology:\nTo tackle this issue, researchers categorized various proven-safe RL techniques into three main groups:\n\n1. Action Replacement: This method replaces unsafe actions with safer alternatives without altering any other aspects of the agent's behavior.\n2. Action Projection: Here, the focus lies in projecting all possible actions onto a subspace such that only safe actions are considered feasible at each step while still allowing exploration through random noise.\n3. Action Masking: The third category masks out parts of the action space so as to prevent the agent from taking unsafe actions altogether.\n\nMain Contributions:\nThis work provides a systematic classification framework across continuous and discrete action spaces; it also conducts empirical benchmarks using two challenging control problems – stabilizing an inverted pendulum and flying a quadrotor drone safely—indicating that among them, action replacement seems particularly effective due to simplicity yet comparable efficacy compared to more complex counterparts like action projection/masking. Additionally, introducing a reward penalty mechanism was found beneficial towards improving overall training performance under safety constraints throughout experimentation phases. Lastly but importantly, guidelines were offered regarding choosing appropriate provably safe RL strategies contingent upon specific safety requirements along with chosen RL algorithms & types of action spaces involved therein.",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "The (Un)Scalability of Informed Heuristic Function Estimation in NP-Hard Search Problems",
        "abstract": "The A* algorithm is commonly used to solve NP-hard combinatorial optimization problems. When provided with a completely informed heuristic function, A* can solve such problems in time complexity that is polynomial in the solution cost and branching factor. In light of this fact, we examine a line of recent publications that propose fitting deep neural networks to the completely informed heuristic function. We assert that these works suffer from inherent scalability limitations since --- under the assumption of NP $\\not \\subseteq$ P/poly --- such approaches result in either (a) network sizes that scale super-polynomially in the instance sizes or (b) the accuracy of the fitted deep neural networks scales inversely with the instance sizes. Complementing our theoretical claims, we provide experimental results for three representative NP-hard search problems. The results suggest that fitting deep neural networks to informed heuristic functions requires network sizes that grow quickly with the problem instance size. We conclude by suggesting that the research community should focus on scalable methods for integrating heuristic search with machine learning, as opposed to methods relying on informed heuristic estimation.",
        "authors": "S. Pendurkar, T. Huang, B. Juba, et.al",
        "keywords": [
            "scalable",
            "heuristics",
            "deep neural networks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=JllRdycmLk",
        "pdf_src": "https://api2.openreview.net/pdf/8a86f94ec148ee44b8bf6f90510a671cb7b52d0a.pdf",
        "Code_src": "",
        "Introduction": "Background: The A* algorithm has been widely applied to solve NP-hard combinatorial optimization problems efficiently when given an informed heuristic function whose values are known beforehand.\n\nResearch Problem: Recent studies have proposed using deep neural networks to approximate fully informed heuristics; however, it remains unclear whether they lead to practical solutions due to potential scalability issues related to NP completeness.\n\nMethodology: This paper theoretically analyzes the scalability limits of approximating informed heuristics through deep neural networks based on the hypothesis that NP does not belong to P/poly class. Additionally, empirical experiments were conducted across several NP-complete search spaces comparing traditional A* algorithms against those utilizing neural network-based heuristics.\n\nMain Contributions:\n1. Theoretical analysis demonstrating that scaling up instances leads to either exponentially increasing model complexities or decreasing accuracies.\n2. Experimental validation showing substantial growth in required neural network parameters proportional to instance size within common NP-hard domains like graph traversal tasks which suggests impracticality at large scales without further advancements towards scalable integration strategies between heuristic search techniques & machine learning models beyond just approximation via neural networks alone.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "SANTA: Source Anchoring Network and Target Alignment for Continual Test Time Adaptation",
        "abstract": "Adapting a trained model to perform satisfactorily on continually changing test environments is an important and challenging task. In this work, we propose a novel framework, SANTA, which aims to satisfy the following characteristics required for online adaptation: 1) can work effectively for different (even small) batch sizes; 2) should continue to work well on the source domain; 3) should have minimal tunable hyperparameters and storage requirements. Given a pre-trained network trained on source domain data, the proposed framework modifies the affine parameters of the batch normalization layers using source anchoring based self-distillation. This ensures that the model incorporates knowledge from the newly encountered domains, without catastrophically forgetting the previously seen domains. We also propose a source-prototype driven contrastive alignment to ensure natural grouping of the target samples, while maintaining the already learnt semantic information. Extensive evaluation on three benchmark datasets under challenging settings justify the effectiveness of SANTA for real-world applications. Code here: https://github.com/goirik-chakrabarty/SANTA",
        "authors": "G. Chakrabarty, M. Sreenivas, S. Biswas",
        "keywords": [
            "SANTA",
            "Online Adaptation",
            "Batch Normalization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=V7guVYzvE4",
        "pdf_src": "https://api2.openreview.net/pdf/677778836efc4ceb56949aa8b6d805ad30faaaa9.pdf",
        "Code_src": "https://github.com/goirik-chakrabarty/SANTA",
        "Introduction": "Background: The background of this paper revolves around the challenge of adapting a trained model so it performs adequately in continuously evolving testing conditions.\n\nResearch Problem: The research problem addressed by the authors focuses on developing a method for online adaptation with specific criteria:\n1. It must be effective across various batch sizes.\n2. It needs to maintain performance when applied within the original training domain ('source domain').\n3. It requires few adjustable hyperparameters and low storage demands.\n\nMethods: To tackle these challenges, they introduce 'SANTA', a novel framework designed specifically for continuous learning tasks:\n\n1. Source Anchoring Based Self-Distillation: They modify the affine parameters of batch normalization layers through a process called \"source anchoring\" combined with self-distillation techniques derived from the source domain's pre-trained network architecture onto new unseen domains (\"target domains\").\n\n2. Source Prototype Driven Contrastive Alignment: Additionally, there’s another component involving prototype-based contrastive alignment aimed at naturally clustering or grouping together similar examples drawn from both the source and target domains during adaptation phase – preserving learned semantics but allowing for better integration into new contexts.\n\nMain Contributions: \nThe main contributions highlighted are as follows:\n- A practical solution capable of handling diverse batch sizes efficiently;\n- Preservation of existing knowledge gained via source domain training whilst incorporating fresh insights from newer ones without catastrophic interference;\n- Minimalistic approach requiring fewer hyperparameter adjustments compared other methods along with lower memory footprint than alternatives making it suitable even resource-constrained scenarios;\n\nEvaluation: Their findings were extensively evaluated against benchmarks demonstrating its efficacy particularly suited towards real-world application scenarios where continual adaptation capabilities would greatly benefit users' experiences over time.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Invertible Hierarchical Generative Model for Images",
        "abstract": "Normalizing flows (NFs) as generative models enjoy desirable properties such as exact invertibility and exact likelihood evaluation, while being efficient to sample from. These properties, however, come at the cost of heavy restrictions on the architecture. Due to these limitations, modeling multi-modal probability distributions can yield poor results even with low-dimensional data. Additionally, typical flow architectures employed on real image datasets produce samples with visible aliasing artifacts and limited variation. The latent decomposition of flow-models also falls short on that of competing methods, with uneven contribution to a decoded image. In this work we build an invertible generative model using conditional normalizing flows in a hierarchical fashion to circumvent the aforementioned limitations. We show that we can achieve superior sample quality among flow-based models with fewer parameters compared to the state of the art. We demonstrate ability to control individual levels of detail via the latent decomposition of our model.",
        "authors": "H. Timonen, M. Aittala, J. Lehtinen",
        "keywords": [
            "invertible generative model",
            "conditional normalizing flows",
            "hierarchical modeling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=4rkKN4tM63",
        "pdf_src": "https://api2.openreview.net/pdf/dc80424229eb1918c5849d4eee4d31d21ace310b.pdf",
        "Code_src": "",
        "Introduction": "Background: Normalizing flows are a class of generative models known for their desirable properties like exact invertibility and exact likelihood evaluation along with efficiency when it comes to sampling.\n\nResearch Problem: However, one major limitation is how restrictive they tend to be regarding architectural choices which makes them less effective especially if dealing with high-dimensional or multi-modal data where modeling becomes challenging leading to suboptimal performance outcomes.\n \nMethodology: To address those issues presented by standard flow architectures used typically within real-world applications involving images, authors propose employing conditional normalizing flows arranged hierarchically allowing each level's latent space to contribute differently towards reconstructing the final output thus providing more flexibility than traditional approaches without sacrificing too much computational efficiency.\n\nMain Contributions: This paper introduces a novel approach utilizing conditional normalizing flows organized in a hierarchical manner resulting in better-quality generated samples relative to existing alternatives despite having significantly fewer parameters involved during training time demonstrating improved controllability over details through its latent decomposition component making it suitable not only for generating realistic-looking images but potentially other complex tasks requiring similar capabilities across multiple modalities simultaneously",
        "Topic": "Optimal Transport"
    },
    {
        "title": "PAVI: Plate-Amortized Variational Inference",
        "abstract": "Given observed data and a probabilistic generative model, Bayesian inference searches for the distribution of the model's parameters that could have yielded the data. Inference is challenging for large population studies where millions of measurements are performed over a cohort of hundreds of subjects, resulting in a massive parameter space. This large cardinality renders off-the-shelf Variational Inference (VI) computationally impractical.\n\nIn this work, we design structured VI families that efficiently tackle large population studies. Our main idea is to share the parameterization and learning across the different i.i.d. variables in a generative model -symbolized by the model's $\\textit{plates}$.\nWe name this concept $\\textit{plate amortization}$. Contrary to off-the-shelf stochastic VI --which slows down inference-- plate amortization results in orders of magnitude faster to train variational distributions. Applied to large-scale hierarchical problems, PAVI yields expressive, parsimoniously parameterized VI with an affordable training time --effectively unlocking inference in those regimes.\n\nWe illustrate the practical utility of PAVI through a challenging Neuroimaging example featuring 400 million latent parameters, demonstrating a significant step towards scalable and expressive Variational Inference.",
        "authors": "L. Rouillard, A. L. Bris, T. Moreau, et.al",
        "keywords": [
            "plate amortization",
            "large population studies",
            "Variational Inference"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=vlY9GDCCA6",
        "pdf_src": "https://api2.openreview.net/pdf/d00b85b815c8450e2f30087e8ad20e43466413ca.pdf",
        "Code_src": "",
        "Introduction": "Background: Bayesian inference aims to find the distribution of model parameters that can generate given observations using a probabilistic generative model; however, it becomes computationally challenging when dealing with large population studies involving many measurements from numerous subjects due to its vast parameter space which makes standard Variational Inference methods impractical.\n\nResearch Problem: How do you perform efficient Bayesian inference on large population studies?\n\nMethod: The authors propose Plate Amortization Variational Inference (PAVI), leveraging \"plates\" within the generative model structure – these represent groups of independent identically distributed (i.i.d.) variables shared between different parts of the dataset or models. By sharing parameterizations among plates rather than independently optimizing them as done traditionally via stochastic Variational Inference techniques like HMC or Langevin Dynamics, they significantly reduce computational complexity during both optimization steps such as gradient computation and sampling.\n\nMain Contributions:\n1. They introduce Plate Amortization, a novel approach allowing for more effective use of computational resources while still maintaining expressiveness needed at scale;\n2. Their method allows for fast training times compared to other stochastic approaches without sacrificing accuracy because it leverages parallelism inherent in plate structures;\n3. Demonstrated scalability up to very high-dimensional datasets including one with nearly half-a-billion parameters showing how their technique enables feasible Bayesian inference previously considered too costly",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "RIGNN: A Rationale Perspective for Semi-supervised Open-world Graph Classification",
        "abstract": "Graph classification has gained growing attention in the graph machine learning community and a variety of semi-supervised methods have been developed to reduce the high cost of annotation. They usually combine graph neural networks (GNNs) and extensive semi-supervised techniques such as knowledge distillation. However, they adhere to the close-set assumption that unlabeled graphs all belong to known classes, limiting their applications in the real world. This paper goes further, investigating a practical problem of semi-supervised open-world graph classification where these unlabeled graph data could come from unseen classes. A novel approach named Rationale-Informed GNN (RIGNN) is proposed, which takes a rationale view to detect components containing the most information related to the label space and classify unlabeled graphs into a known class or an unseen class. In particular, RIGNN contains a relational detector and a feature extractor to produce effective rationale features, which maximize the mutual information with label information and exhibit sufficient disentanglement with non-rationale elements. Furthermore, we construct a graph-of-graph based on geometrical relationships, which gives instructions on enhancing rationale representations. In virtue of effective rationale representations, we can provide accurate and balanced predictions for unlabeled graphs. An extension is also made to accomplish effective open-set graph classification. We verify our proposed methods on four benchmark datasets in various settings and experimental results reveal the effectiveness of our proposed RIGNN compared with state-of-the-art methods.",
        "authors": "X. Luo, Y. Zhao, Z. Mao, et.al",
        "keywords": [
            "open-world graph classification",
            "Graph Neural Networks (GNNs)",
            "Semi-supervised Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=qcCE4mC2jI",
        "pdf_src": "https://api2.openreview.net/pdf/44c74d078272899ba3a534a540df1d4f17ad6a7b.pdf",
        "Code_src": "",
        "Introduction": "Background: Graph classification plays a significant role in graph machine learning due to its wide range of applications including social network analysis, recommendation systems etc. Semi-supervised learning aims at reducing the high cost of labeling by using both labeled and unlabeled data.\n\nResearch Problem: Existing semi-supervised graph classification approaches assume that all unlabeled graphs are from known classes leading them to be limited when applied in scenarios involving unseen classes.\n \nMethod: The authors propose a new method called Rationale-Informed GNN (RIGNN). It adopts a rationale-based strategy rather than adhering to the close-set assumption used previously. Specifically, it detects components within the graph that contain crucial information about labels through a relational detector followed by a feature extraction step resulting in rationale features maximizing mutual information between the extracted features and the label information while being sufficiently disentangled from other irrelevant parts. Additionally, geometric relations among nodes form a graph-of-graph structure providing guidance towards better rationale representation. \n\nMain Contributions:\n1. Introduce a novel Rationale-Informed GNN architecture capable of handling open-world graph classification problems effectively without assuming all unlabeled graphs fall under existing categories.\n2. Develop a relational detector and a feature extractor module designed specifically around producing rationale features essential not only for predicting known classes but potentially even identifying entirely new ones during inference time via open-set graph classification extensions.\n3. Validate this framework across multiple benchmarks demonstrating superior performance over current state-of-the-art models",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Estimating Differential Equations from Temporal Point Processes",
        "abstract": "Ordinary differential equations (ODEs) allow interpretation of phenomena in various scientific fields. They have mostly been applied to numerical data observed at regular intervals, but not to irregularly observed discrete events, also known as point processes. In this study, we introduce an ODE modeling of such events by combining ODEs with log-Gaussian Cox processes (Møller et al., 1998). In the experiments with different types of ODEs regarding infectious disease, predator-prey interaction, and competition among participants, our method outperformed existing baseline methods assuming regularly observed continuous data with respect to the accuracy of recovering the latent parameters of ODEs. Through both synthetic and actual examples, we also showed the ability of our method to extrapolate, model latent events that cannot be observed, and offer interpretability of phenomena from the viewpoint of the estimated parameters of ODE.",
        "authors": "S. Miyazawa, D. Mochihashi",
        "keywords": [
            "ODE modeling",
            "Log-Gaussian Cox processes",
            "Point processes"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=cJgHzw8Qhq",
        "pdf_src": "https://api2.openreview.net/pdf/51d7c52b8e48c00ee0f89826de4c07f00ef8f2bc.pdf",
        "Code_src": "",
        "Introduction": "Background: Ordinary Differential Equations (ODEs) are widely used for interpreting phenomena across various scientific domains due to their mathematical tractability; however, they typically require time series data sampled uniformly over a period or interval.\n\nResearch Problem: The challenge addressed is how to apply ODE-based models effectively when dealing with event data which may only occur sporadically rather than continuously throughout some defined domain - these so-called \"point process\" observations do not fit well into traditional ODE frameworks because they lack the temporal continuity required within each equation's derivative term.\n\nMethodology: To address this issue, researchers propose integrating ODEs with Log-Gaussian Cox Processes (LGCP), leveraging the latter’s flexibility around counting processes where events can happen at any given time without necessarily following a uniform sampling pattern typical of continuous-time ODEs.\n\nMain Contributions:\n1. **Combining ODEs with LGCP**: This hybrid approach allows one to incorporate the explanatory power of ODE dynamics while accommodating the irregular timing characteristic of point processes.\n2. **Experimental Validation**: By applying the proposed methodology on datasets related to infectious diseases, predator-prey interactions, and competitive scenarios involving multiple entities, it was demonstrated through comparative analysis against baselines using standard ODE approaches based on regular data points – the new combined framework significantly improves parameter recovery accuracy specifically tailored towards the characteristics of non-uniformly spaced observational data.\n3. **Interpretation and Extrapolation**: Furthermore, empirical results indicate potential beyond mere estimation capabilities—our integrated system has shown promise in predicting future occurrences (\"extrapolating\") unseen events hidden behind observable ones along with providing insights about underlying mechanisms via inferred parameters derived directly from the ODE component embedded within the LGCP structure.",
        "Topic": "Generative Models"
    },
    {
        "title": "SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration",
        "abstract": "The ability to effectively reuse prior knowledge is a key requirement when building general and flexible Reinforcement Learning (RL) agents. Skill reuse is one of the most common approaches, but current methods have considerable limitations. For example, fine-tuning an existing policy frequently fails, as the policy can degrade rapidly early in training. In a similar vein, distillation of expert behavior can lead to poor results when given sub-optimal experts. We compare several common approaches for skill transfer on multiple domains including changes in task and system dynamics. We identify how existing methods fail and introduce an alternative approach to mitigate these problems. Our approach learns to sequence temporally-extended skills for exploration but learns the final policy directly from the raw experience. This conceptual split enables rapid adaptation and thus efficient data collection but without constraining the final solution. It significantly outperforms many classical methods across a suite of evaluation tasks and we use a broad set of ablations to highlight the importance of different components of our method.",
        "authors": "G. Vezzani, D. Tirumala, M. Wulfmeier, et.al",
        "keywords": [
            "skill reuse",
            "reinforcement learning",
            "temporal extension"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=JwGKVpRfVD",
        "pdf_src": "https://api2.openreview.net/pdf/217ba63f6a4fc19a2cf08534d9dccbe11cf0f5c4.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses the challenges faced by reinforcement learning (RL) agents while reusing prior knowledge during their development into more generalized and adaptable systems.\n\nResearch Problem: The main problem addressed here concerns the limitations with commonly used skill-reuse techniques such as fine-tuning policies or distilling expert behaviors which often do not generalize well due to issues like degradation over time if fine-tuned too aggressively; or producing suboptimal outcomes through distillation using less-than-perfectly performing experts.\n\nMethods: To tackle this issue, researchers conducted experiments comparing various skill-transfer methodologies within diverse environments that exhibit varying degrees of task complexity and dynamic shifts between them. They introduced new algorithms designed specifically around mitigating some of those identified failures – particularly focusing on avoiding premature convergence leading towards inferior solutions upon initial training iterations.\n \nMain Contributions:\n1. Identified shortcomings associated with traditional skill-reuse strategies employed widely today;\n2. Proposed novel RL architectures capable of sequencing extended temporal sequences of learned skills for exploratory purposes whilst concurrently developing robust final policies based solely on raw observational experiences rather than pre-existing ones;\n3. Demonstrated significant improvements compared against established benchmarks via rigorous experimentation involving comprehensive sets of variations known as \"ablations\"",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Turning a Curse into a Blessing: Enabling In-Distribution-Data-Free Backdoor Removal via Stabilized Model Inversion",
        "abstract": "The effectiveness of many existing techniques for removing backdoors from machine learning models relies on access to clean in-distribution data. However, given that these models are often trained on proprietary datasets, it may not be practical to assume that in-distribution samples will always be available.\nOn the other hand, model inversion techniques, which are typically viewed as privacy threats, can reconstruct realistic training samples from a given model, potentially eliminating the need for in-distribution data.\nTo date, the only prior attempt to integrate backdoor removal and model inversion involves a simple combination that produced very limited results. This work represents a first step toward a more thorough understanding of how model inversion techniques could be leveraged for effective backdoor removal. Specifically, we seek to answer several key questions: What properties must reconstructed samples possess to enable successful defense? Is perceptual similarity to clean samples enough, or are additional characteristics necessary? Is it possible for reconstructed samples to contain backdoor triggers?\n\nWe demonstrate that relying solely on perceptual similarity is insufficient for effective defenses. The stability of model predictions in response to input and parameter perturbations also plays a critical role. To address this, we propose a new bi-level optimization based framework for model inversion that promotes stability in addition to visual quality. Interestingly, we also find that reconstructed samples from a pre-trained generator's latent space do not contain backdoors, even when signals from a backdoored model are utilized for reconstruction. We provide a theoretical analysis to explain this observation. Our evaluation shows that our stabilized model inversion technique achieves state-of-the-art backdoor removal performance without requiring access to clean in-distribution data. Furthermore, its performance is on par with or even better than using the same amount of clean samples.",
        "authors": "S. Chen, Y. Zeng, W. Park, et.al",
        "keywords": [
            "backdoor removal",
            "model inversion",
            "bi-level optimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=XuOE99cmST",
        "pdf_src": "https://api2.openreview.net/pdf/299c013f864ed2a21f68d0f79013ab9575ab6724.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThis paper addresses an issue related to security vulnerabilities within machine learning systems known as \"backdoors.\" These vulnerabilities allow attackers to embed malicious code into otherwise benign ML models by altering their parameters during training but remain undetected until triggered.\n\nResearch Problem:\nThe problem explored here concerns whether traditional methods used against such backdoors—requiring access to large amounts of unaltered (\"in-distribution\") training data—are feasible since real-world datasets might have sensitive information making them unavailable due to confidentiality constraints upon private datasets where most modern ML models receive extensive training.\n\nMethodology:\nIn contrast to conventional approaches focusing exclusively on in-distribution data, researchers investigate another strategy involving \"model inversion,\" previously considered primarily concerning privacy breaches because they involve reconstructing original inputs from learned representations inside neural networks after training has occurred; however, if properly controlled through certain conditions imposed at different levels while optimizing the inversion process, this approach holds promise beyond just being a threat—it becomes useful towards detecting and mitigating adversarial changes like those introduced via backdoors.\n\nMain Contributions:\n- They identify limitations associated with merely considering perceptual similarity between reconstructed samples versus actual non-adversarially modified ones—a factor overlooked before—as well as emphasize importance factors including robustness under variations \n  - Propose novel bi-level optimization scheme designed specifically around promoting both visual fidelity along with predictive stability across various perturbations applied either directly onto inputs or indirectly via adjustments made internally within network parameters themselves;\n- Conduct experiments demonstrating efficacy achieved surpasses current benchmarks without needing reliance on any form of genuine distribution-aligned examples whatsoever;\n- Further show that utilizing latent spaces generated by pretrained generative models leads to outputs free from residual traces left behind by backdoors despite incorporating elements derived from compromised counterparts;\n\nOverall Conclusion:\nBy introducing innovative strategies centered around stabilizing aspects traditionally neglected alongside enhancing image quality preservation throughout inversion procedures, authors contribute significantly advancing capabilities needed effectively addressing potential risks posed by hidden manipulations present within deployed deep learning architectures today.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Linearized Relative Positional Encoding",
        "abstract": "Relative positional encoding is widely used in vanilla and linear transformers to represent positional information. However, existing encoding methods of a vanilla transformer are not always directly applicable to a linear transformer, because the latter requires a decomposition of the query and key representations into separate kernel functions. Nevertheless, principles for designing encoding methods suitable for linear transformers remain understudied. In this work, we put together a variety of existing linear relative positional encoding approaches under a canonical form and further propose a family of linear relative positional encoding algorithms via unitary transformation. Our formulation leads to a principled framework that can be used to develop new relative positional encoding methods that preserve linear space-time complexity. Equipped with different models, the proposed linearized relative positional encoding (LRPE) family derives effective encoding for various applications. Experiments show that compared with existing methods, LRPE achieves state-of-the-art performance in language modeling, text classification, and image classification. Meanwhile, it emphasizes a general paradigm for designing broadly more relative positional encoding methods that are applicable to linear transformers.",
        "authors": "Z. Qin, W. Sun, K. Lu, et.al",
        "keywords": [
            "linear transformer",
            "relative positional encoding",
            "unitary transformation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=xoLyps2qWc",
        "pdf_src": "https://api2.openreview.net/pdf/22ad06d8dac6a1672d2c045dbe93b4d4f66eff33.pdf",
        "Code_src": "",
        "Introduction": "Background: Relative positional encoding plays an essential role in representing position information within neural networks such as vanilla and linear transformers.\n\nResearch Question: How do we design encoding methods specifically tailored for linear transformers?\n\nMethod: We first compile several existing linear relative positional encoding approaches using a common framework called Canonical Form. Then, through unitary transformations, we introduce a novel family of linear relative positional encoding algorithms which maintains linear space-time complexity while preserving the original features' order.\n\nMain Contributions:\n1. A unified framework encapsulating multiple existing linear relative positional encoding techniques.\n2. The introduction of a novel algorithmic family based on unitary transformations designed exclusively for linear transformers without compromising computational efficiency or feature preservation capabilities.\n3. Demonstrated improvements over previous methods across three tasks: language modeling, text classification, and image classification - achieving state-of-the-art results where applicable – thus providing empirical evidence supporting our approach's effectiveness beyond theory alone; \n4. An emphasis laid upon developing future relative positional encodings by establishing guidelines towards their construction within linear transformer architectures",
        "Topic": "Vision Transformer"
    },
    {
        "title": "Momentum Tracking: Momentum Acceleration for Decentralized Deep Learning on Heterogeneous Data",
        "abstract": "SGD with momentum is one of the key components for improving the performance of neural networks. For decentralized learning, a straightforward approach using momentum is Distributed SGD (DSGD) with momentum (DSGDm). However, DSGDm performs worse than DSGD when the data distributions are statistically heterogeneous. Recently, several studies have addressed this issue and proposed methods with momentum that are more robust to data heterogeneity than DSGDm, although their convergence rates remain dependent on data heterogeneity and deteriorate when the data distributions are heterogeneous. In this study, we propose Momentum Tracking, which is a method with momentum whose convergence rate is proven to be independent of data heterogeneity. More specifically, we analyze the convergence rate of Momentum Tracking in the setting where the objective function is non-convex and the stochastic gradient is used. Then, we identify that it is independent of data heterogeneity for any momentum coefficient $\\beta \\in [0, 1)$. Through experiments, we demonstrate that Momentum Tracking is more robust to data heterogeneity than the existing decentralized learning methods with momentum and can consistently outperform these existing methods when the data distributions are heterogeneous.",
        "authors": "Y. Takezawa, H. Bao, K. Niwa, et.al",
        "keywords": [
            "data heterogeneity",
            "distributed SGD",
            "Momentum Tracking"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=8koy8QuTZD",
        "pdf_src": "https://api2.openreview.net/pdf/79b37181b8719b54a98ae5644018661eb5566afd.pdf",
        "Code_src": "",
        "Introduction": "Background: Stochastic Gradient Descent with momentum (SGD-momentum) has been shown to significantly improve the training efficiency of neural networks by accelerating convergence towards local minima.\n\nResearch Problem: Despite its effectiveness under homogeneous conditions, DSGDm, an extension of SGD-momentum tailored for distributed machine learning settings often encounters suboptimal performance due to statistical heterogeneity across different nodes or datasets within a network.\n\nMethod: To address this problem without sacrificing convergence speed regardless of data distribution heterogeneity, our paper introduces Momentum Tracking—a novel algorithmic framework incorporating momentum into distributed learning algorithms such as DSGD.\n\nMain Contributions:\n1. We rigorously prove that the convergence rate of Momentum Tracking does not depend on the degree of data heterogeneity.\n2. Specifically, we provide theoretical analysis focusing on scenarios involving non-convex objectives functions and stochastic gradients—common characteristics encountered during deep learning tasks.\n3. Our findings indicate that Momentum Tracking maintains consistency even if there exists variance among the data sources involved; thus, it inherently offers improved resilience against variations compared to other decentralized learning approaches utilizing momentum like DSGDm.\n4. Experimental validation through empirical tests further corroborates the superiority of Momentum Tracking over traditional decentralized learning techniques especially pertinent issues related to disparate data distributions present at various nodes participating in collaborative model training processes.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Optimistic Optimization of Gaussian Process Samples",
        "abstract": "Bayesian optimization is a popular formalism for global optimization, but its computational costs limit it to expensive-to-evaluate functions. A competing, computationally more effi- cient, global optimization framework is optimistic optimization, which exploits prior knowl- edge about the geometry of the search space in form of a dissimilarity function. We investi- gate to which degree the conceptual advantages of Bayesian Optimization can be combined with the computational efficiency of optimistic optimization. By mapping the kernel to a dissimilarity, we obtain an optimistic optimization algorithm for the Bayesian Optimization setting with a run-time of up to $O(N log N )$. As a high-level take-away we find that, when using stationary kernels on objectives of low evaluation cost, optimistic optimization can be preferable over Bayesian optimization, while for strongly coupled and parametric models, Bayesian optimization can perform much better, even at low evaluation cost. As a concep- tual takeaway, our results demonstrate that balancing exploration and exploitation under Gaussian process assumptions does not require computing a posterior.",
        "authors": "J. Grosse, C. Zhang, P. Hennig",
        "keywords": [
            "optimistic optimization",
            "Bayesian optimization",
            "computational efficiency"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=KQ5jI19kF3",
        "pdf_src": "https://api2.openreview.net/pdf/aa83592d0aab9d878506d73dddb8ba63091d2ebd.pdf",
        "Code_src": "",
        "Introduction": "Background: Bayesian optimization has been widely used as a global optimization method due to its strong theoretical guarantees; however, this approach often comes with significant computational costs especially if the objective function being optimized requires many evaluations.\n\nResearch Problem: The paper aims to address how to combine the strengths of Bayesian optimization - such as principled probabilistic modeling – with those of optimistic optimization methods like reduced computational complexity by leveraging domain knowledge or structure within the problem's solution space.\n\nMethodology: To achieve this goal, they propose adapting the kernel from Bayesian optimization into an optimistic optimization scheme through the use of a dissimilarity measure between points rather than distances based on covariance matrices typically employed in Bayesian optimization frameworks.\n \nMain Contributions:\n1. They introduce an optimistic optimization variant suitable for Bayesian settings where the runtime scales only logarithmically with respect to the number of iterations ($O(N \\log N)$).\n2. Through empirical studies comparing both approaches across different types of problems including simple ones requiring few evaluations versus complex coupled systems, their findings suggest that optimistic optimization may outperform Bayesian optimization particularly useful during early stages because it allows for faster convergence without necessarily needing detailed posterior distributions computed via Bayesian inference techniques traditionally associated with Bayesian optimization algorithms.\n3. Conversely, Bayesian optimization could potentially yield superior performance specifically suited towards solving highly interconnected parameterized models regardless of whether these are inexpensive or costly to evaluate further emphasizing trade-offs related to exploration-exploitation balance depending upon specific application scenarios involving Gaussian processes approximations underlying each respective optimization strategy considered here.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning",
        "abstract": "We examine a simple stochastic strategy for adapting well-known single-point acquisition functions to allow batch active learning. Unlike acquiring the top-K points from the pool set, score- or rank-based sampling takes into account that acquisition scores change as new data are acquired. This simple strategy for adapting standard single-sample acquisition strategies can even perform just as well as compute-intensive state-of-the-art batch acquisition functions, like BatchBALD or BADGE while using orders of magnitude less compute. In addition to providing a practical option for machine learning practitioners, the surprising success of the proposed method in a wide range of experimental settings raises a difficult question for the field: when are these expensive batch acquisition methods pulling their weight?",
        "authors": "A. Kirsch, S. Farquhar, P. Atighehchian, et.al",
        "keywords": [
            "batch active learning",
            "stochastic strategy",
            "efficient computation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=vcHwQyNBjW",
        "pdf_src": "https://api2.openreview.net/pdf/abd7cf88617c72ff4ec92264fb8a7919ef9ffee3.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper addresses an issue commonly encountered during active learning processes where known single-point acquisition functions need adaptation so they could be applied effectively within a batch setting.\n\nResearch Problem:\nThe problem is how to adapt existing single-point acquisition functions such as those based on score or rank sampling which traditionally work with one sample at a time but require modification if used collectively across multiple samples without recalculating each step's relevance anew after every instance added due to potential changes in model uncertainty and posterior probabilities over the feature space.\n\nMethodology:\nTo solve this challenge, researchers propose a straightforward stochastic approach called \"batch score sampling\" by leveraging the fact that traditional point selection metrics do not inherently consider subsequent updates made through iterative learning phases; hence it does not re-evaluate its decisions upon incorporating additional examples iteratively.\nThis novel technique involves resampling batches rather than individual instances continuously throughout training iterations ensuring consistency between different stages despite incremental dataset growth - thus allowing us to leverage efficient computation resources compared to computationally intensive algorithms designed specifically for batch scenarios alone e.g., BatchBALD or BADGE.\n\nMain Contributions:\nThe main contributions include demonstrating empirical evidence showing that under various conditions including classification tasks involving both high-dimensional datasets along with complex decision boundaries – batch score sampling performs comparably against more sophisticated batch-oriented approaches yet requires significantly fewer computational resources making it particularly appealing especially considering real-world constraints related cost-effective deployment options available today among other benefits associated with simpler implementations easier maintenance etcetera.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "DPVIm: Differentially Private Variational Inference Improved",
        "abstract": "Differentially private (DP) release of multidimensional statistics typically considers an aggregate sensitivity, e.g. the vector norm of a high-dimensional vector. However, different dimensions of that vector might have widely different magnitudes and therefore DP perturbation disproportionately affects the signal across dimensions. We observe this problem in the gradient release of the DP-SGD algorithm when using it for variational inference (VI), where it manifests in poor convergence as well as high variance in outputs for certain variational parameters, and make the following contributions: (i) We mathematically isolate the cause for the difference in magnitudes between gradient parts corresponding to different variational parameters. Using this as prior knowledge we establish a link between the gradients of the variational parameters, and propose an efficient while simple fix for the problem to obtain a less noisy gradient estimator, which we call \\emph{aligned} gradients. This approach allows us to obtain the updates for the covariance parameter of a Gaussian posterior approximation without a privacy cost. We compare this to alternative approaches for scaling the gradients using analytically derived preconditioning, e.g. natural gradients. (ii) We suggest using iterate averaging over the DP parameter traces recovered during the training, to reduce the DP-induced noise in parameter estimates at no additional cost in privacy. Finally, (iii) to accurately capture the additional uncertainty DP introduces to the model parameters, we infer the DP-induced noise from the parameter traces and include that in the learned posteriors to make them \\emph{noise aware}. We demonstrate the efficacy of our proposed improvements through various experiments on real data.",
        "authors": "J. Jälkö, L. Prediger, A. Honkela, et.al",
        "keywords": [
            "dp-sgd",
            "aligned gradients",
            "noise-aware"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=GlhM6XX1wv",
        "pdf_src": "https://api2.openreview.net/pdf/26f54a1da02eed821bb0a8a83c925176739bc081.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper addresses issues with differential privacy (DP)-protected releases of multidimensional statistics used commonly today.\nResearch Problem:\nThe current methods consider only an aggregate sensitivity measure like vector norms but fail to account for differences in magnitude among individual components within vectors leading to disproportionate impact by DP perturbations.\n\nMethodology:\n(i) The authors identify why there are discrepancies in sensitivities associated with different elements along a vector's length; leveraging these insights they connect gradients related to distinct variational parameters into \"aligned\" gradients—a novel method designed specifically addressing the issue above—resulting in reduced noise in the gradient estimators.(ii) They introduce iterative averaging strategy applied post-training phase towards smoothing out the DP-related noise affecting parameter estimates without compromising any aspect of privacy.(iii) To reflect the extra uncertainty introduced due to DP constraints onto model parameters more accurately, they incorporate inferred DP noise levels directly into the learned posterior distributions making those models 'noise-aware'.\n\nMain Contributions:\n- A new concept called aligned gradients is developed solving the sensitivity disparity amongst different dimensions' gradients effectively.\n- An iterative averaging technique is suggested after training to mitigate DP noise impacting parameter estimation costs-effectively.\n- Noise-awareness principle integrated into learning process capturing DP's influence on parameter uncertainties precisely via noise inference based on parameter trace analysis.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "RIFLE: Imputation and Robust Inference from Low Order Marginals",
        "abstract": "The ubiquity of missing values in real-world datasets poses a challenge for statistical inference and can prevent similar datasets from being analyzed in the same study, precluding many existing datasets from being used for new analyses. While an extensive collection of packages and algorithms have been developed for data imputation, the overwhelming majority perform poorly if there are many missing values and low sample sizes, which are unfortunately common characteristics in empirical data. Such low-accuracy estimations adversely affect the performance of downstream statistical models. We develop a statistical inference framework for predicting the target variable in the presence of missing data without imputation. Our framework, RIFLE (Robust InFerence via Low-order moment Estimations), estimates low-order moments of the underlying data distribution with corresponding confidence intervals to learn a distributionally robust model. We specialize our framework to linear regression and normal discriminant analysis, and we provide convergence and performance guarantees. This framework can also be adapted to impute missing data. We compare RIFLE with state-of-the-art approaches (including MICE, Amelia, MissForest, KNN-imputer, MIDA, and Mean Imputer) in numerical experiments. Our experiments demonstrate that RIFLE outperforms other benchmark algorithms when the percentage of missing values is high and/or when the number of data points is relatively small. RIFLE is publicly available",
        "authors": "S. Baharlouei, S. Suen, M. Razaviyayn",
        "keywords": [
            "data imputation",
            "statistical inference",
            "distributional robustness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=oud7Ny0KQy",
        "pdf_src": "https://api2.openreview.net/pdf/402d1e4ddccc2511bc8e6d4b47278228ae99953c.pdf",
        "Code_src": "",
        "Introduction": "Background: The prevalence of missing values in real-world datasets presents challenges for statistical inference due to its impact on the comparability between studies.\n\nResearch Question: How do we effectively predict the target variable while accounting for missing data?\n\nMethodology: We introduce RIFLE - Robust InFerence via Low-order moment Estimations – a novel statistical inference framework designed specifically for this purpose by estimating low-order moments directly rather than using traditional imputation methods.\n \nMain Contributions:\n1. Developed a distributionally robust approach based on low-order moments estimation;\n2. Specialized the method for two commonly-used statistical tasks: linear regression and normal discriminant analysis; \n3. Provided theoretical guarantees regarding convergence and performance under certain conditions;\n4. Demonstrated superior accuracy compared to leading benchmarks such as MICE, Amelia, MissForest, KNN-imputer, MIDA, and Mean Imputer across various scenarios involving large percentages of missingness or smaller sample sizes through numerical experiments;\n5. Made the algorithm freely accessible online so it could benefit researchers worldwide.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Achieving the Pareto Frontier of Regret Minimization and Best Arm Identification in Multi-Armed Bandits",
        "abstract": "We study the Pareto frontier of two archetypal objectives in multi-armed bandits, namely, regret minimization (RM) and best arm identification (BAI) with a fixed horizon. It is folklore that the balance between exploitation and exploration is crucial for both RM and BAI, but exploration is more critical in achieving the optimal performance for the latter objective. To this end, we design and analyze the BoBW-lil’UCB($\\gamma$) algorithm. Complementarily, by establishing lower bounds on the regret achievable by any algorithm with a given BAI failure probability, we show that (i) no algorithm can simultaneously perform optimally for both the RM and BAI objectives, and (ii) BoBW-lil’UCB($\\gamma$) achieves order-wise optimal performance for RM or BAI under different values of $\\gamma$. Our work elucidates the trade-off more precisely by showing how the constants in previous works depend on certain hardness parameters. Finally, we show that BoBW-lil’UCB outperforms a close competitor UCB$_\\alpha$ (Degenne et al., 2019) in terms of the time complexity and the regret on diverse datasets such as MovieLens and Published Kinase Inhibitor Set.",
        "authors": "Z. Zhong, W. C. Cheung, V. Y. F. Tan",
        "keywords": [
            "multi-armed bandits",
            "regret minimization",
            "best arm identification"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=XXfEmIMJDm",
        "pdf_src": "https://api2.openreview.net/pdf/73e2925a522bb2ca6d73a2a17972c8e5d587881d.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper focuses on studying the Pareto frontier of two fundamental objectives in multi-armed bandit problems - regret minimization (RM) and best arm identification (BAI). These are important because they help to understand when it's better to exploit what you already know versus explore new options.\n\nResearch Problem: The problem addressed here revolves around finding algorithms which strike an effective balance between exploiting known information effectively (\"exploitation\") vs. exploring unknown possibilities (\"exploration\"). Specifically, there needs to be enough exploration so one doesn't miss potentially superior arms while not over-exploring at the cost of high regret due to delayed exploitation.\n \nMethodology: The authors introduce \"BoBW-lil'UCB(γ)\" – an algorithm designed specifically considering the trade-offs needed among these objectives based on the γ parameter value chosen within its framework. They also provide theoretical guarantees regarding the regret achieved through their proposed algorithm compared against other methods like UCB$_α$(Degenne et al., 2019).\n\nMain Contributions:\n1. Demonstrated that no single algorithm could achieve optimal results across all scenarios balancing RM & BAI; highlighting specific trade-offs required depending upon the context.\n2. Provided insights into why existing constant factors used previously may vary significantly from each other according to underlying difficulty metrics related to the task environment.\n3. Proved that BoBW-lil'UCB(γ) offers near-optimal performance relative to regret incurred during decision-making processes concerning RM/BAI tasks irrespective of varying γ values.\n4. Compared favorably w.r.t. computational efficiency and regret reduction capabilities vis-à-vis another competitive method called UCB$_α$, especially evident using real-world datasets including MovieLens and Published Kinase Inhibitor Sets.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Offline Reinforcement Learning with Mixture of Deterministic Policies",
        "abstract": "Offline reinforcement learning (RL) has recently attracted considerable attention as an approach for utilizing past experiences to learn a policy.  Recent studies have reported the challenges of offline RL, such as estimating the values of actions that are outside the data distribution. To mitigate offline RL issues, we propose an algorithm that leverages a mixture of deterministic policies. When the data distribution is multimodal, fitting a policy modeled with a unimodal distribution, such as Gaussian distribution, may lead to interpolation between separate modes, thereby resulting in the value estimation of actions that are outside the data distribution. In our framework, the state-action space is divided by learning discrete latent variables, and the sub-policies corresponding to each region are trained. The proposed algorithm was derived by considering the variational lower bound of the offline RL objective function. We show empirically that the use of the proposed mixture policy can reduce the accumulation of the critic loss in offline RL, which was reported in previous studies. Experimental results also indicate that using a mixture of deterministic policies in offline RL improves the performance with the D4RL benchmarking datasets.",
        "authors": "T. Osa, A. Hayashi, P. Deo, et.al",
        "keywords": [
            "mixture of deterministic policies",
            "offline reinforcement learning",
            "multimodal data"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=zkRCp4RmAF",
        "pdf_src": "https://api2.openreview.net/pdf/6bd8b39ab0bcf91035f856d1ae7b021238142fd1.pdf",
        "Code_src": "",
        "Introduction": "Background: Offline reinforcement learning (RL) aims to utilize historical experience without requiring real-time interaction or feedback from the environment.\n\nResearch Problem: One challenge faced by offline RL methods involves accurately estimating the value of actions not present in the training dataset due to potential differences in their distributions.\n \nMethod: This paper introduces a novel method based on a mixture of deterministic policies designed specifically when dealing with multimodal data distributions where traditional single-modal models like Gaussian distributions might fail because they cannot capture multiple distinct clusters within the input space effectively. \n\nMain Contributions:\n1. A new offline RL algorithm leveraging a mixture of deterministic policies rather than relying solely on stochastic ones; \n2. An innovative way uses learned discrete latent variables to partition the state-action space into regions;\n3. Sub-policies tailored individually per these partitions help better estimate action values across different modalities;\n4. Empirical validation demonstrates this approach significantly reduces critic loss compared to existing techniques while improving overall performance against standard benchmarks provided by the D4RL repository",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "On the Sample Complexity of Lipschitz Constant Estimation",
        "abstract": "Estimating the Lipschitz constant of a function, also known as Lipschitz learning, is a fundamental problem with broad applications in fields such as control and global optimization. In this paper, we study the Lipschitz learning problem with minimal parametric assumptions on the target function. As a first theoretical contribution, we derive novel lower bounds on the sample complexity of this problem for both noise-free and noisy settings under mild assumptions. Moreover, we propose a simple Lipschitz learning algorithm called $\\textit{Lipschitz Constant Estimation by Least Squares Regression}$ (referred to as LCLS). We show that LCLS is asymptotically consistent for general noise assumptions and offers finite sample guarantees that can be translated to new upper bounds on the sample complexity of the Lipschitz learning problem.  Our analysis shows that the sample complexity rates derived in this paper are optimal in both the noise-free setting and in the noisy setting when the noise is assumed to follow a Gaussian distribution and that LCLS is a sample-optimal algorithm in both cases. Finally, we show that by design, the LCLS algorithm is computationally faster than existing theoretically consistent methods, and can be readily adapted to various noise assumptions with little to no prior knowledge of the target function properties or noise distribution.",
        "authors": "J. W. Huang, S. Roberts, J. Calliess",
        "keywords": [
            "sample complexity",
            "Lipschitz learning",
            "least squares regression"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=UIalYAHdBH",
        "pdf_src": "https://api2.openreview.net/pdf/c2f3ecbce7f1507bacdd13ec13de07b3bdbbd69c.pdf",
        "Code_src": "",
        "Introduction": "Background: The estimation of the Lipschitz constant of a function plays an essential role across diverse domains like control theory and global optimization.\n\nResearch Problem: This research focuses on addressing the challenge of Lipschitz learning within functions where only limited parametric constraints apply.\n \nMethodology: The authors introduce two main contributions; they establish fresh lower bounds regarding the sample complexity required accurately estimate the Lipschitz constant - one set against noiseless data scenarios while another accounts for noisy environments – based upon reasonable presumptions about these conditions. Additionally, they have developed a straightforward algorithm named \"Lipshitz Constant Estimation by Least Squares Regression\" (abbreviated as LCLS), which aims at solving the aforementioned problem efficiently.\n\nMain Contributions:\n1. Lower Bounds: They provide innovative lower bounds applicable even if there's some uncertainty concerning whether the observed data comes from a noiseless scenario versus being corrupted due to external factors leading to variability between samples.\n2. Algorithmic Proposal: The proposed LCLS method has been shown through their analyses not just to converge over time but it also provides practical performance assurances related directly back into how many examples need collected before making reliable estimates achievable.\n3. Optimality Proofs: Their findings indicate that our approach yields optimal results regardless if observations occur without error or amidst disturbances following certain distributions including Gaussian ones among others suggesting its applicability irrespective of specific environmental contexts encountered during actual implementation phases involving real-world datasets used throughout experiments conducted here.\n4. Computational Efficiency: Furthermore, compared other established algorithms proven theoretically sound yet less efficient computationally speaking ,the LCLS algorithm exhibits significant speed improvements whilst still maintaining consistency guarantees thus offering better trade-offs overall between accuracy requirements imposed by application-specific tasks requiring solutions pertaining specifically towards estimating unknown parameters associated with given functional relationships governing complex systems studied extensively nowadays within academia & industry sectors worldwide today",
        "Topic": "Sample Efficiency in Reinforcement Learning"
    },
    {
        "title": "A Survey on Transformers in Reinforcement Learning",
        "abstract": "Transformer has been considered the dominating neural architecture in NLP and CV, mostly under supervised settings. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. In this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.",
        "authors": "W. Li, H. Luo, Z. Lin, et.al",
        "keywords": [
            "RL",
            "Transformers",
            "Reinforcement Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=r30yuDPvf2",
        "pdf_src": "https://api2.openreview.net/pdf/e674ac7db3ea54a092713371542716e6a50ab821.pdf",
        "Code_src": "",
        "Introduction": "Background: Transformer architectures have become dominant in both Natural Language Processing (NLP) and Computer Vision (CV), particularly when trained under supervised conditions. More recently, there's also been an uptick in applying Transformers within Reinforcement Learning (RL). However, unlike supervised tasks where data availability can be abundant or semi-supervised methods are available for handling limited labeled examples, RL inherently requires agents that learn through interaction which introduces novel design considerations.\n\nResearch Question: The primary question addressed here concerns understanding why and how Transformers might successfully adapt into the RL landscape despite these differences from traditional supervised setups; what specific adaptations need to occur? Additionally, given the nascent stage of Transformer adoption in RL compared to other domains like NLP/CV, identifying key trends early could inform further research directions leading towards more effective RL systems based on Transformers.\n\nMethodology: This work aims at providing comprehensive coverage over the use of Transformers in RL including their motivation behind adaptation as well as current advancements across different subfields such as continuous control, discrete action spaces etc. A systematic literature survey approach was adopted along with classification criteria focusing on whether they used model-based or model-free approaches among others.\n \nMain Contributions:\n1. **Motivation Review**: The authors delve deep into the reasons behind the successful transfer of Transformer models beyond their original application areas—highlighting factors ranging from parallel processing capabilities suited for sequential decision-making problems inherent in RL scenarios up until recent algorithmic developments aiding convergence issues common during training processes involving exploration-exploitation trade-offs.\n2. **Taxonomy Development**: They introduce a structured categorization scheme classifying various studies according to distinct characteristics relevant specifically toward RL frameworks utilizing Transformer architectures.\n3. **Sub-Field Analysis**: Each subfield employing Transformers - Continuous Control, Discrete Action Spaces – receives dedicated attention discussing notable contributions made thus far while pinpointing potential gaps requiring additional investigation moving forward.\n4. **Future Prospects**: Finally, suggestions about promising avenues worth exploring next were outlined considering ongoing technological breakthroughs alongside practical limitations encountered so far related to scaling Transformers effectively throughout complex environments typical seen in real-world applications today.\n\nOverall Conclusion: While significant strides already exist regarding integrating Transformer technology deeply rooted initially primarily developed elsewhere than RL itself, much remains uncharted territory warranting continued scrutiny",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Quantization Robust Federated Learning for Efficient Inference on Heterogeneous Devices",
        "abstract": "Federated Learning (FL) is a machine learning paradigm to distributively learn machine learning models from decentralized data that remains on-device. Despite the success of standard Federated optimization methods, such as Federated Averaging (FedAvg) in FL, the energy demands and hardware induced constraints for on-device learning have not been considered sufficiently in the literature. Specifically, an essential demand for on-device learning is to enable trained models to be quantized to various bit-widths based on the energy needs and heterogeneous hardware designs across the federation. In this work, we introduce multiple variants of federated averaging algorithm that train neural networks robust to quantization. Such networks can be quantized to various bit-widths with only limited reduction in full precision model accuracy. We perform extensive experiments on standard FL benchmarks to evaluate our proposed FedAvg variants for quantization robustness and provide a convergence analysis for our Quantization-Aware variants in FL. Our results demonstrate that integrating quantization robustness results in FL models that are significantly more robust to different bit-widths during quantized on-device inference.",
        "authors": "K. Gupta, M. Fournarakis, M. Reisser, et.al",
        "keywords": [
            "quantization",
            "federated averaging",
            "energy efficiency"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=lvevdX6bxm",
        "pdf_src": "https://api2.openreview.net/pdf/4d2d65237789be2d27e0466e7951b8689a9f3cea.pdf",
        "Code_src": "",
        "Introduction": "Background: Federated Learning (FL) allows distributed training of machine learning models over decentralized datasets while keeping them private within devices.\n\nResearch Problem: Existing studies focusing on FL do not consider enough attention towards the energy consumption and hardware constraints when deploying learned models onto edge devices due to their varying computational capabilities.\nSpecifically, there's no sufficient consideration given regarding enabling these models to adapt to different bit-width quantizations according to specific energy requirements or diverse hardware architectures among participating nodes in the network.\n\nMethodology: This paper introduces several variations of the Federated Averaging (FedAvg) algorithm which aim at training neural networks capable of being robust against quantization processes without sacrificing too much performance compared to non-quantized versions even after conversion into lower bit-width representations suitable for resource-constrained environments like mobile phones etc.\n\nMain Contributions:\n1. Proposes novel FedAvg algorithms designed specifically considering quantization resilience throughout each iteration step ensuring better adaptation under variable bit-depths post-training;\n2. Conducts comprehensive empirical evaluations using common benchmark datasets demonstrating improved quantization tolerance by incorporating these new techniques; \n3. Provides insights through convergence analyses highlighting benefits gained particularly noticeable especially amongst Quantization-Aware variants introduced here",
        "Topic": "Federated Learning"
    },
    {
        "title": "Fair and Useful Cohort Selection",
        "abstract": "A challenge in fair algorithm design is that, while there are compelling notions of individual fairness, these notions typically do not satisfy desirable composition properties, and downstream applications based on fair classifiers might not preserve fairness. \nTo study fairness under composition, Dwork & Ilvento (2019) introduced an archetypal problem called fair-cohort-selection problem, where a single fair classifier is composed with itself to select a group of candidates of a given size, and proposed a solution to this problem.\n\nIn this work we design algorithms for selecting cohorts that not only preserve fairness, but also maximize the utility of the selected cohort under two notions of utility that we introduce and motivate. We give optimal (or approximately optimal) polynomial-time algorithms for this problem in both an offline setting, and an online setting where candidates arrive one at a time and are classified as they arrive.",
        "authors": "K. Bairaktari, P. T. Langton, H. Nguyen, et.al",
        "keywords": [
            "fairness",
            "cohort selection",
            "algorithm design"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=wRepWp1KC7",
        "pdf_src": "https://api2.openreview.net/pdf/7b6d82477158cefdf38bf6f6cd4a5106ac123913.pdf",
        "Code_src": "",
        "Introduction": "Background: Fairness in algorithm design has been recognized by researchers due to its importance; however, existing approaches often fail to maintain desired compositional properties when applied across multiple stages or layers within complex systems.\nResearch Problem: The main issue addressed here concerns how to ensure fairness during the process of composing several fair classifiers together without losing their fairness guarantees over subsequent decisions made using those compositions.\nMethodology: This paper introduces new concepts related to \"fair-cohort-selection\" problems which involve combining a single fair classifier repeatedly until it selects a certain number of individuals from some population into what's referred to as a 'cohort'. It then develops efficient algorithms designed specifically around preserving fairness along with optimizing another type of decision criterion - maximizing either expected accuracy if predictions can be probabilistic (\"expected-accuracy-maximization\") or minimizing regret about missed opportunities (\"regret-minimization\").\nMain Contributions: The authors propose novel solutions addressing both preservation of fairness throughout composite processes involving multiple classifiers combined iteratively ('compositionality'), alongside optimization objectives such as maximizing expected accuracy or minimizing regretted missed chances respectively depending upon whether probabilities exist among candidate outcomes before selection occurs versus binary classifications after arrival order considerations come into play accordingly). These contributions include polynomial-time algorithms achieving optimality or near-optimality conditions even considering constraints like budget limitations imposed per round/instance considered individually yet collectively contribute towards overall performance metrics aimed toward improving societal impacts through equitable allocation mechanisms grounded firmly within computational frameworks informed by rigorous theoretical foundations provided herein.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "High Fidelity Neural Audio Compression",
        "abstract": "We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio. Code and samples are available under github.com/facebookresearch/encodec.",
        "authors": "A. Défossez, J. Copet, G. Synnaeve, et.al",
        "keywords": [
            "audio codec",
            "neural networks",
            "perceptual loss"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ivCd8z8zR2",
        "pdf_src": "https://api2.openreview.net/pdf/705c0e2a034b84c325252caf66d7a0c3efa014bb.pdf",
        "Code_src": "Code link: https://github.com/facebookresearch/encodec",
        "Introduction": "Background:\nThe background of this paper lies in the field of audio processing where there has been increasing demand for efficient audio codecs capable of delivering high fidelity sound at low bit rates without introducing significant distortion or artifacts.\n\nResearch Problem:\nThe research problem addressed here involves developing a new type of audio codec which not only achieves high fidelity but also operates in real-time due to its computational efficiency.\n \nMethodology:\nTo tackle this challenge, they propose a novel neural network-based audio codec consisting of a streaming encoder-decoder architecture operating on a quantized latent space within their framework. They employ a multiscale spectrogram adversary as part of their training process designed specifically to reduce artifacts during encoding and decoding processes; this adversary helps improve sample quality significantly compared to traditional approaches relying solely on adversarial learning mechanisms alone. Additionally, they introduced a novel loss balancer technique allowing them more flexibility when choosing hyperparameters such as weights assigned different losses since these no longer need align closely with scales typically associated with those losses themselves - hence enabling better stabilization throughout optimization procedures leading towards convergence points closer desired performance metrics like signal-to-noise ratio (SNR). Furthermore, they explored lightweight transformer architectures' potential role in further reducing complexity after obtaining initial encoded representations thereby achieving compression ratios greater than 40%. \n\nMain Contributions:\nTheir main contributions include designing & implementing an innovative neural-network based audio codec capable running effectively even under stringent latency constraints imposed upon real-world applications requiring immediate feedback loops involving auditory stimuli transmission/reception scenarios; incorporating specialized adversaries into existing architectures aimed improving perceived quality outcomes beyond what could otherwise achieved purely through standard machine learning techniques; proposing novel regularization strategies facilitating stable optimization routines resulting higher SNRs values relative baseline systems tested against; demonstrating efficacy superiority over prior works considered contextually relevant benchmarks covering diverse types acoustic signals encountered everyday life situations ranging from conversational dialogue recordings amidst ambient noise environments right through complex orchestral compositions played back via stereo speaker arrays etcetera...",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Weight-balancing fixes and flows for deep learning",
        "abstract": "Feedforward neural networks with homogeneous activation functions possess an internal symmetry: the functions they compute do not change when the incoming and outgoing weights at any hidden unit are rescaled by reciprocal positive values. This paper makes two contributions to our understanding of these networks. The first is to describe a simple procedure, or {\\it fix}, for balancing the weights in these networks: this procedure computes multiplicative rescaling factors---one at each hidden unit---that rebalance the weights of these networks without changing the end-to-end functions that they compute. Specifically, given an initial network with arbitrary weights, the procedure determines the functionally equivalent network whose weight matrix is of minimal $\\ell_{p,q}$-norm; the weights at each hidden unit are said to be balanced when this norm is stationary with respect to rescaling transformations. The optimal rescaling factors are computed in an iterative fashion via simple multiplicative updates, and the updates are notable in that (a) they do not require the tuning of learning rates, (b) they operate in parallel on the rescaling factors at all hidden units, and (c) they converge monotonically to a global minimizer of the $\\ell_{p,q}$-norm. The paper's second contribution is to analyze the optimization landscape for learning in these networks. We suppose that the network's loss function consists of two terms---one that is invariant to rescaling transformations, measuring predictive accuracy, and another (a regularizer) that breaks this invariance, penalizing large weights. We show how to derive a weight-balancing {\\it flow} such that the regularizer remains minimal with respect to rescaling transformations as the weights descend in the loss function. These dynamics reduce to an ordinary gradient flow for $\\ell_2$-norm regularization, but not otherwise. In this way our analysis suggests a canonical pairing of alternative flows and regularizers.",
        "authors": "L. K. Saul",
        "keywords": [
            "weight-balancing",
            "$\\ell_{p,q}$-norm",
            "optimization landscape"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=uaHyXxyp2r",
        "pdf_src": "https://api2.openreview.net/pdf/f67ba705bbb5c90bcac492047e7c893bb3b6c44e.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this research lies within the study of feedforward neural networks characterized by their homogeneity under certain symmetries regarding the scaling of weights.\n\nResearch Problem:\nThe problem addressed here concerns finding methods which can balance the weights inside these symmetric networks while preserving computational functionality across layers.\n \nMethodology:\nTo address this issue, researchers propose a \"fix\" - a straightforward algorithmic approach called Weight Balancing Fix (WBF). WBF involves computing multiplicative rescaling factors specific to every hidden layer node so it recalibrates the network weights uniformly throughout its structure yet maintains the original computation patterns intact from input to output.\n\nMain Contributions:\n1. **Weight Balancing Procedure**: A novel method has been developed where the goal is to find a network with the smallest $\\ell_{p,q}$-norm weight matrix after applying the rescaling factor(s), ensuring that the resulting network performs equivalently to the original one despite changes made through weight balancing process.\n   \n2. **Optimization Analysis**: The authors also delve into analyzing the optimization landscape during training processes specifically designed around these balanced networks considering both predictive performance and regularization constraints imposed upon them due to breaking down scale-invariance properties traditionally enjoyed before weight balancing was applied.\n\nIn summary, this work contributes significantly towards better understanding how we might effectively manage complex neural networks' architectures concerning symmetry preservation along with practical implications like improved convergence speeds",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "AP: Selective Activation for De-sparsifying Pruned Networks",
        "abstract": "The rectified linear unit (ReLU) is a highly successful activation function in neural networks as it allows networks to easily obtain sparse representations, which reduces overfitting in overparameterized networks. However, in the context of network pruning, we find that the sparsity introduced by ReLU, which we quantify by a term called dynamic dead neuron rate (DNR), is not beneficial for the pruned network. Interestingly, the more the network is pruned, the smaller the dynamic DNR becomes during and after optimization. This motivates us to propose a method to explicitly reduce the dynamic DNR for the pruned network, i.e., de-sparsify the network. We refer to our method as Activate-while-Pruning (AP). We note that AP does not function as a stand-alone method, as it does not evaluate the importance of weights. Instead, it works in tandem with existing pruning methods and aims to improve their performance by selective activation of nodes to reduce the dynamic DNR. We conduct extensive experiments using various popular networks (e.g., ResNet, VGG, DenseNet, MobileNet) via two classical and three state-of-the-art pruning methods. The experimental results on public datasets (e.g., CIFAR-10, CIFAR-100) suggest that AP works well with existing pruning methods and improves the performance by 3% - 4%. For larger scale datasets (e.g., ImageNet) and state-of-the-art networks (e.g., vision transformer), we observe an improvement of 2% - 3% with AP as opposed to without. Lastly, we conduct an ablation study to examine the effectiveness of the components comprising AP.",
        "authors": "S. Liu, R. Ghosh, M. Motani",
        "keywords": [
            "ReLU",
            "Network Pruning",
            "Activate-while-Pruning (AP)"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=EGQSpkUDdD",
        "pdf_src": "https://api2.openreview.net/pdf/350d5bf848d089efafad7eb095093157d653eeef.pdf",
        "Code_src": "",
        "Introduction": "Background: The Rectified Linear Unit (ReLU) has been widely used due to its ability to enable neural networks to learn sparse representations while reducing overfitting.\n\nResearch Problem: In the context of network pruning techniques where neurons are removed from the network structure or weight matrices reduced through quantization, the sparsity induced by ReLU may be detrimental rather than helpful because it can lead to underperformance when applied to the pruned network architecture post-training.\n\nMethod: To address this issue, researchers have proposed \"Activate-while-Pruning\" (AP), designed specifically around the concept of dynamically adjusting the activation functions within the network's layers throughout the training process so they do not become too sparse (\"de-sparse\").\n\nMain Contributions:\n1. A novel approach named Activate-while-Pruning (AP) was developed.\n2. It involves selectively activating certain nodes based on the need at different stages of the training process instead of removing them entirely like traditional pruning approaches might require; thus aiming towards maintaining connectivity between neurons even if some connections were initially cut off leading up to further optimizations such as quantization etcetera).\n3. Extensive empirical evidence across multiple architectures including ResNet, VGG, DenseNet & MobileNet shows significant improvements ranging anywhere from 3%-4% depending upon dataset size complexity levels (CIFAR-10 vs ImageNet). \n4. Further validation against large-scale datasets like ImageNet demonstrates additional gains compared to non-pruned counterparts – approximately 2%-3%.",
        "Topic": "approximation"
    },
    {
        "title": "Approximating Naive Bayes on Unlabelled Categorical Data",
        "abstract": "We address the question of binary classification when no labels are available and the input features are categorical. The lack of labels means supervised approaches can't be used, and the lack of a natural distance measure means that most unsupervised methods do poorly. For such problems, where the alternatives might be a) do nothing or b)  heuristic rules-based approaches, we offer a third alternative: a classifier that approximates Naive Bayes. Our primary scenarios are those that involve distinguishing scripted, or bot, web traffic from that of legitimate users.\n\nOur main assumption is the existence of some attribute $x_*$ more prevalent in the benign than the scripted traffic; i.e., $P(x_*|\\overline{\\mbox{bot}}) = K \\cdot P(x_*|\\mbox{bot}),$ for $K>1.$ We show that any such disparity yields a lower bound on $P(\\mbox{bot}|x_{j})$ even when we have no prior estimates of $P(x_*|\\overline{\\mbox{bot}}),$  $P(x_*|\\mbox{bot})$ or $K$ (except that $K>1$). We show that when at least one bin of at least one feature receives no attack traffic then we under-estimate the actual conditional probability by a factor of $1-1/K.$ Thus, any attribute with a large disparity between prevalence in benign and abuse traffic (i.e., $K$ is large), allows good approximation of the Naive Bayes classifier without the benefit of labels.\n \nThe approach is particularly suited to problems where $K$ is high and thus the approximation is very accurate. Example problems (and relevant attributes) might be: password-guessing, if login attempts from legitimate users succeed at a much higher rate than those from password-guessing attackers; Credit Card Verification Value (CVV) guessing, if an attacker exhaustively tries all possible 3 or 4-digit values and fails at a higher rate than legitimate users; account registration, if legitimate users  use email addresses from services that do not allow fee anonymous accounts (e.g., {\\tt .edu})  at a much higher rate than attackers; click-fraud if legitimate users  visit pages and services that contain no ads at a higher rate than click-fraud bots.",
        "authors": "C. Herley",
        "keywords": [
            "binary classification",
            "unsupervised learning",
            "Naive Bayes"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=KpElM2S9pw",
        "pdf_src": "https://api2.openreview.net/pdf/eea0033d1f68029a9a3d5906e3cef6156555c444.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThis paper focuses on the problem of binary classification tasks involving categorical input data but lacking labeled examples due to privacy concerns related to user data. It also considers cases where there's difficulty measuring distances naturally among categories because they're categorical rather than numerical.\n\nResearch Question:\nHow does one perform binary classification effectively using only categorical inputs which cannot directly utilize supervised learning techniques?\n\nMethodology:\nThe authors propose employing a classifier inspired by the Naive Bayes model as their solution method despite having limited information about the distribution of classes within the dataset - specifically, the probabilities \\( P(x_*|y) \\) associated with each class label \\( y \\).\n\nMain Contributions:\nThey introduce a novel approach based on the concept of \"disparity\" – assuming certain attributes (\\( x_* \\)) occur significantly less frequently in malicious (\"scripted\") traffic compared to normal (\"benign\") traffic. They derive a lower bound on the posterior probability of belonging to the script traffic given evidence from these attributes, even though exact probabilities are unknown beforehand except knowing that the ratio \\( K \\) must exceed 1 indicating a greater frequency in benign traffic over scripted traffic relative to this particular attribute. This bound holds true regardless of whether we know the precise values of \\( K \\) itself since it follows from the inequality alone justifying its usage here instead of traditional machine learning models requiring labeled training sets like logistic regression algorithms would require.\n\nExample Scenarios:\nThe proposed algorithm could potentially apply well across various domains including detecting fraudulent activities online through identifying patterns indicative of abnormal behavior amongst other things listed above e.g., password cracking attacks against websites where successful logins outnumber failed ones substantially leading us towards believing our hypothesis regarding disproportionate occurrence rates being present elsewhere too hence aiding us further narrowing down potential culprits behind suspicious activity observed during monitoring sessions conducted periodically throughout time intervals specified accordingly depending upon operational needs etcetera...",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "$k$-Mixup Regularization for Deep Learning via Optimal Transport",
        "abstract": "Mixup is a popular regularization technique for training deep neural networks that improves generalization and increases robustness to certain distribution shifts. It perturbs input training data in the direction of other randomly-chosen instances in the training set. To\nbetter leverage the structure of the data, we extend mixup in a simple, broadly applicable way to $k$-mixup, which perturbs $k$-batches of training points in the direction of other $k$-batches. The perturbation is done with displacement interpolation, i.e. interpolation under\nthe Wasserstein metric. We demonstrate theoretically and in simulations that $k$-mixup preserves cluster and manifold structures, and we extend theory studying the efficacy of standard mixup to the $k$-mixup case. Our empirical results show that training with $k$-mixup\nfurther improves generalization and robustness across several network architectures and benchmark datasets of differing modalities. For the wide variety of real datasets considered, the performance gains of $k$-mixup over standard mixup are similar to or larger than the\ngains of mixup itself over standard ERM after hyperparameter optimization. In several instances, in fact, $k$-mixup achieves gains in settings where standard mixup has negligible to zero improvement over ERM.",
        "authors": "K. Greenewald, A. Gu, M. Yurochkin, et.al",
        "keywords": [
            "k-mixup",
            "regularization technique",
            "generalization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=lOegPKSu04",
        "pdf_src": "https://api2.openreview.net/pdf/8a70b11dc12b1da5445e34f8c8c27bc9e30ed532.pdf",
        "Code_src": "",
        "Introduction": "Background: Mixup is an effective regularization method used during the training process of deep neural networks aimed at improving their generalization capabilities by introducing noise into the training samples.\n\nResearch Question: How can we further enhance the benefits of Mixup through modifications?\n\nMethod: This paper introduces k-Mixup as its extension; instead of mixing up single pairs within each batch like Mixup does, it mixes up entire batches from different parts of the dataset using displacement interpolation based on the Wasserstein metric.\nThis approach aims to preserve more complex structures present in the data such as clusters and manifolds while still benefiting from the regularizing effect provided by Mixup.\n\nMain Contributions:\n1. Theoretical contributions include demonstrating how k-Mixup maintains the underlying structural properties found in the data compared to Mixup's behavior when applied only once per sample pair,\n2. An extension of existing theoretical analysis focusing solely on Mixup applies here too - showing that these findings hold true even if multiple interpolations occur between batches rather than just two samples,\n\n3. Empirical evidence shows that k-Mixup consistently outperforms both Mixup alone along with the original Expectation Maximization Regularization (ERM), especially noticeable considering various types of datasets including those involving diverse modalities.\n\n4. The improvements observed suggest that k-Mixup could be particularly useful due to situations wherein applying Mixup doesn't lead to significant changes but k-Mixup might yield substantial enhancements leading towards better generalization & robustness overall",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Global Contrastive Learning for Long-Tailed Classification",
        "abstract": "We consider the long-tailed classification problem in which a few classes in the training data dominate the majority of the other classes. For concreteness, we focus on the visual domain in this paper. Most current methods employ contrastive learning to learn a representation for long-tailed data. In this paper, first, we investigate $k$-positive sampling, a popular baseline method widely used to build contrastive learning models for imbalanced data. Previous works show that $k$-positive learning, which only chooses $k$ positive samples (instead of all positive images) for each query image, suffers from inferior performance in long-tailed data. In this work, we further point out that k-positive learning limits the learning capability of both head and tail classes.  Based on this perspective, we propose a novel contrastive learning framework that improves the limitation in k-positive learning by enlarging its positive selection space, so it can help the model learn more semantic discrimination features. Second, we analyze how the temperature (the hyperparameter used for tuning a concentration of samples on feature space) affects the gradients of each class in long-tailed learning, and propose a new method that can mitigate inadequate gradients between classes, which can help model learning easier. We name this framework as CoGloAT. Finally, we go on to introduce a new prototype learning framework namely ProCo based on coreset selection, which creates a global prototype for each cluster while keeping the computation cost within a reasonable time and show that combining CoGloAT with ProCo can further enhance the model learning ability on long-tailed data.",
        "authors": "T. Bach, A. Tong, T. S. Hy, et.al",
        "keywords": [
            "long-tailed classification",
            "contrastive learning",
            "prototype learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=xWrtiJwJj5",
        "pdf_src": "https://api2.openreview.net/pdf/230a8c752868927b2d637710414e87e8372845f2.pdf",
        "Code_src": "",
        "Introduction": "Background: The study addresses the issue of long-tailed classification where some categories are prevalent compared to others during training.\n\nResearch Problem: How do existing contrastive learning approaches handle the imbalance present when there is an overrepresentation of certain classes?\n\nMethods:\n1. Investigate the effectiveness of $k$-positive sampling – a common technique often employed due to its simplicity.\n2. Propose modifications aimed at improving the limitations identified through empirical analysis; these include expanding the positive sample set beyond just the top-$k$ examples per query image (\"CoGloAT\" framework).\n3. Analyze the impact of \"temperature,\" or the parameter controlling the degree of similarity among samples before computing their embeddings using cosine similarity.\n4. Develop another framework called \"ProCo\" leveraging coreset selection techniques specifically designed around prototype learning concepts without significantly increasing computational costs.\n\nMain Contributions:\n1. A detailed examination into the shortcomings of $k$-positive sampling approach particularly relevant under long-tailed conditions leading towards a better understanding why such methods might not perform well here.\n2. An innovative contrastive learning architecture named CoGloAT capable of mitigating those issues via increased flexibility regarding positive sample choice thus potentially enhancing discriminative power across different category distributions including tails traditionally neglected because they have fewer instances available than heads.\n3. Insights about temperature's influence upon gradient distribution amongst various classes aiding practitioners adjust parameters optimally suited toward addressing long-tailed scenarios effectively.\n4. Introduction of ProCo—a complementary prototype-based framework utilizing coresets alongside CoGloAT resulting in improved overall robustness against long-tail bias",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Walking Out of the Weisfeiler Leman Hierarchy: Graph Learning Beyond Message Passing",
        "abstract": "We propose CRaWl, a novel neural network architecture for graph learning. Like graph neural networks, CRaWl layers update node features on a graph and thus can freely be combined or interleaved with GNN layers. Yet CRaWl operates fundamentally different from message passing graph neural networks. CRaWl layers extract and aggregate information on subgraphs appearing along random walks through a graph using 1D Convolutions. Thereby it detects long range interactions and computes non-local features. As the theoretical basis for our approach, we prove a theorem stating that the expressiveness of CRaWl is incomparable with that of the Weisfeiler Leman algorithm and hence with graph neural networks. That is, there are functions expressible by CRaWl, but not by GNNs and vice versa. This result extends to higher levels of the Weisfeiler Leman hierarchy and thus to higher-order GNNs. Empirically, we show that CRaWl matches state-of-the-art GNN architectures across a multitude of benchmark datasets for classification and regression on graphs.",
        "authors": "J. Tönshoff, M. Ritzert, H. Wolf, et.al",
        "keywords": [
            "CRaWl",
            "Graph Learning",
            "Non-Local Features"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=vgXnEyeWVY",
        "pdf_src": "https://api2.openreview.net/pdf/e224d3c14dc574baebb43cb12f03b415272ee580.pdf",
        "Code_src": "",
        "Introduction": "Background: Graph Learning is an important field in machine learning which deals with data represented as graphs where nodes represent entities such as people while edges represent relationships between them.\nResearch Problem: The existing graph neural networks (GNNs), like Message Passing Neural Networks (MPNNs), have limitations when dealing with complex graphs due to their local nature.\n\nMethod: To address this issue, we introduce a new neural network architecture called CRaWl based on Convolutional Random Walks (CRWs). Unlike MPNNs whose updates depend only on neighboring nodes' features, CRaWl extracts and aggregates information over entire subgraphs during random walks within the graph via one-dimensional convolutions allowing detection of long-range interactions among distant nodes without requiring any additional hyperparameters beyond the choice of convolution kernel size(s).\n\nMain Contributions:\n- Prove theoretically that CRaWl has greater expressive power than both classical algorithms including Weisfeiler-Leman (WL) algorithm up until its nth iteration level; \n- Extend results further into comparing against high-order versions of WL algorithm equivalent to deep GNNs;\n- Demonstrate empirically superior performance compared to other leading-edge graph neural networks across various benchmarks covering tasks ranging from classification to regression",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "HypUC: Hyperfine Uncertainty Calibration with Gradient- boosted Corrections for Reliable Regression on Imbalanced Electrocardiograms",
        "abstract": "The automated analysis of medical time series, such as the electrocardiogram (ECG), electroencephalogram (EEG), pulse oximetry, etc, has the potential to serve as a valuable tool for diagnostic decisions, allowing for remote monitoring of patients and more efficient use of expensive and time-consuming medical procedures. Deep neural networks (DNNs) have been demonstrated to process such signals effectively. However, previous research has primarily focused on classifying medical time series rather than attempting to regress the continuous-valued physiological parameters central to diagnosis. One significant challenge in this regard is the imbalanced nature of the dataset, as a low prevalence of abnormal conditions can lead to heavily skewed data that results in inaccurate predictions and a lack of certainty in such predictions when deployed. To address these challenges, we propose HypUC, a framework for imbalanced probabilistic regression in medical time series, making several contributions. (i) We introduce a simple kernel density-based technique to tackle the imbalanced regression problem with medical time series. (ii) Moreover, we employ a probabilistic regression framework that allows uncertainty estimation for the predicted continuous values. (iii) We also present a new approach to calibrate the predicted uncertainty further. (iv) Finally, we demonstrate a technique to use calibrated uncertainty estimates to improve the predicted continuous value and show the efficacy of the calibrated uncertainty estimates to flag unreliable predictions. HypUC is evaluated on a large, diverse, real-world dataset of ECGs collected from millions of patients, outperforming several conventional baselines on various diagnostic tasks, suggesting potential use-case for the reliable clinical deployment of deep learning models and a prospective clinical trial. Consequently, a hyperkalemia diagnosis algorithm based on HypUC is going to be the subject of a real-world clinical prospective study.",
        "authors": "U. Upadhyay, S. Bade, A. Puranik, et.al",
        "keywords": [
            "medical time series",
            "probabilistic regression",
            "imbalanced datasets"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=0Xo9giEZWf",
        "pdf_src": "https://api2.openreview.net/pdf/ce502b02828a4e7070d6584cc2a69ce0a9112624.pdf",
        "Code_src": "",
        "Introduction": "Background: The automatic analysis of medical time series like ECG or EEG could help diagnose diseases by remotely monitoring patients without needing costly tests.\n\nResearch Problem: Previous studies mainly focus on classification but not regression which predicts continuous vital signs directly related to disease diagnoses accurately.\n \nMethodology: This paper introduces HypUC, an imbalance handling method using kernel density estimation along with a probabilistic regression framework estimating prediction uncertainties. They also developed a calibration step improving the estimated uncertainty's reliability.\n\nMain Contributions:\n1. A novel kernel density estimator addressing the issue of unbalanced datasets where normal cases are rare compared to abnormalities leading to biased predictions.\n2. Probabilistic regression model providing confidence intervals around its predictions about patient health status.\n3. An uncertainty calibration strategy refining the predictive uncertainty measure ensuring it reflects actual variability better.\n4. Demonstrated effectiveness through experiments against other methods across multiple diagnostics tasks showing improved performance while incorporating uncertainty into predictions.\n\nConclusion: HypUC significantly improves upon existing approaches towards accurate and reliable medical time series regression predicting key physiological parameters crucial for diagnosing illnesses including hyperkalemia - one condition they plan testing clinically next year.",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "TSMixer: An All-MLP Architecture for Time Series Forecast-ing",
        "abstract": "Real-world time-series datasets are often multivariate with complex dynamics. To capture this complexity, high capacity architectures like recurrent- or attention-based sequential deep learning models have become popular. However, recent work demonstrates that simple univariate linear models can outperform such deep learning models on several commonly used academic benchmarks. Extending them, in this paper, we investigate the capabilities of linear models for time-series forecasting and present Time-Series Mixer (TSMixer), a novel architecture designed by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing operations along both the time and feature dimensions to extract information efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is comparable to specialized state-of-the-art models that leverage the inductive biases of specific benchmarks. On the challenging and large scale M5 benchmark, a real-world retail dataset, TSMixer demonstrates superior performance compared to the state-of-the-art alternatives. Our results underline the importance of efficiently utilizing cross-variate and auxiliary information for improving the performance of time series forecasting. We present various analyses to shed light into the capabilities of TSMixer. The design paradigms utilized in TSMixer are expected to open new horizons for deep learning-based time series forecasting.",
        "authors": "S. Chen, C. Li, S. O. Arik, et.al",
        "keywords": [
            "time-series forecasting",
            "linear models",
            "multi-layer perceptrons"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=wbpxTuXgm0",
        "pdf_src": "https://api2.openreview.net/pdf/3633cdf0ae47b74a525c04c7040e6c342986678a.pdf",
        "Code_src": "",
        "Introduction": "Background: Real-world time-series datasets usually involve multiple variables which exhibit complex dynamics.\n\nResearch Problem: How effective could simple univariate linear models be when it comes to time-series forecasting?\n\nMethod: This study investigates the potential of linear models using an innovative architecture called Time-Series Mixer (TSMixer).\n\nMain Contributions:\n1. Developed TSMixer - A model stack composed of multi-layer perceptrons (MLPs).\n2. Implemented mixing operations across different dimensions to enhance data processing efficiency.\n3. Demonstrated through empirical studies conducted against well-known academic benchmarks as well as practical applications including one from the retail industry – the M5 benchmark showing significant improvements over existing advanced methods leveraging domain-specific knowledge.\n4. Highlighted how effectively incorporating cross-variate and supplementary information improves predictive accuracy within these types of models while also providing insights about their inner workings via comprehensive analysis techniques employed during development process.",
        "Topic": "Generative Models"
    },
    {
        "title": "Learning Multiscale Non-stationary Causal Structures",
        "abstract": "This paper addresses a gap in the current state of the art by providing a solution for modeling causal relationships that evolve over time and occur at different time scales. Specifically, we introduce the multiscale non-stationary directed acyclic graph (MN-DAG), a framework for modeling multivariate time series data. Our contribution is twofold. Firstly, we expose a probabilistic generative model by leveraging results from spectral and causality theories. Our model allows sampling an MN-DAG according to user-specified priors on the time-dependence and multiscale properties of the causal graph. Secondly, we devise a Bayesian method named Multiscale Non-stationary Causal Structure Learner (MN-CASTLE) that uses stochastic variational inference to estimate MN-DAGs. The method also exploits information from the local partial correlation between time series over different time resolutions. The data generated from an MN-DAG reproduces well-known features of time series in different domains, such as volatility clustering and serial correlation. Additionally, we show the superior performance of MN-CASTLE on synthetic data with different multiscale and non-stationary properties compared to baseline models. Finally, we apply MN-CASTLE to identify the drivers of the natural gas prices in the US market. Causal relationships have strengthened during the COVID-19 outbreak and the Russian invasion of Ukraine, a fact that baseline methods fail to capture. MN-CASTLE identifies the causal impact of critical economic drivers on natural gas prices, such as seasonal factors, economic uncertainty, oil prices, and gas storage deviations.",
        "authors": "G. D'acunto, G. D. F. Morales, P. Bajardi, et.al",
        "keywords": [
            "multiscale non-stationary directed acyclic graph",
            "causal structure learning",
            "stochastic variational inference"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=SQnPE63jtA",
        "pdf_src": "https://api2.openreview.net/pdf/2dc50e1583609b36b9f1a152ebf6bc87b3b09429.pdf",
        "Code_src": "",
        "Introduction": "Background: This research aims to address limitations within existing methodologies concerning the representation of evolving causal connections across various temporal scales.\n\nResearch Question: How can one effectively model complex systems where causal relations change dynamically?\n\nMethodology: We propose the Multiscale Non-stationary Directed Acyclic Graph (MN-DAG), which enables flexible modeling of multivariate time series through incorporating both scale-dependent dynamics and non-stationarity into its structure.\n\nMain Contributions:\n1. Development of a Probabilistic Generative Model based on spectral theory and causality principles allowing users to specify their prior beliefs about how these graphs should behave.\n2. Introduction of the Multiscale Non-stationary Causal Structure Learner (MN-CASTLE), employing Bayesian techniques along with stochastic variational inference strategies tailored specifically towards inferring MN-DAGs while accounting for varying resolution dependencies among time series correlations.\n3. Demonstration using synthetic datasets showing that our approach accurately captures typical characteristics observed in real-world time series like volatility clustering or serial dependence patterns; \n4. Validation against baselines highlighting the superiority when dealing with more intricate scenarios involving multiple scales & non-stationarities;\n5. Application case study successfully identifying key influencers affecting U.S. natural gas pricing post-COVID-19 pandemic and Russia-Ukraine crisis periods – something beyond reach due to inherent flaws found commonly amongst other approaches used previously.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Bag of Image Patch Embedding Behind the Success of Self-Supervised Learning",
        "abstract": "Self-supervised learning (SSL) has recently achieved tremendous empirical advancements in learning image representation. However, our understanding of the principle behind learning such a representation is still limited. This work shows that joint-embedding SSL approaches learn a representation of image patches, which reflects their co-occurrence. Such a connection to co-occurrence modeling can be established formally, and it supplements the prevailing invariance perspective. We empirically show that learning a representation for fixed-scale patches and aggregating local patch representations as the image representation achieves similar or even better results than the baseline methods. We denote this process as {\\it BagSSL}. Even with $32\\times 32$ patch representation, BagSSL achieves $62\\%$ top-1 linear probing accuracy on ImageNet. On the other hand, with a multi-scale pretrained model, we show that the whole image embedding is approximately the average of local patch embeddings. While the SSL representation is relatively invariant at the global scale, we show that locality is preserved when we zoom into local patch-level representation. Further, we show that patch representation aggregation can improve various SOTA baseline methods by a large margin. The patch representation is considerably easier to understand, and this work makes a step to demystify self-supervised representation learning.",
        "authors": "Y. Chen, A. Bardes, Z. Li, et.al",
        "keywords": [
            "BagSSL",
            "Self-supervised Learning",
            "Co-occurrence Modeling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=r06xREo3QG",
        "pdf_src": "https://api2.openreview.net/pdf/a64b2239fcf7986ee9efb38410077d5627b334cb.pdf",
        "Code_src": "",
        "Introduction": "Background: Self-supervised learning (SSL) has made significant progress in learning effective image representations without relying on human annotations.\n\nResearch Question: Despite these advances, there remains uncertainty about the underlying principles guiding the learning of such representations.\n \nMethod: The paper introduces joint-embedding SSL approaches based on the observation that they learn an image patch representation reflecting their co-occurrence patterns within images. They propose a new method called BagSSL where they aggregate local patch representations rather than using single-patch features directly. \n\nMain Contributions:\n1. Establishing a formal link between joint-embedding SSL and co-occurrence modeling, providing insights beyond existing perspectives focusing solely on invariance.\n2. Demonstrating through empirical evidence that BagSSL outperforms baseline methods while also achieving competitive performance against them—reaching up to 62% top-1 linear probing accuracy from a $32\\times32$ patch representation alone—a feat comparable if not superior compared to traditional supervised models trained over billions of parameters across millions of examples.\n3. Showing how multi-scale pretraining leads to an approximation of the full-image embedding being equivalent to the mean of its individual local patch embeddings; suggesting that despite some level of global invariance due to SSL's nature, fine-grained details are retained locally upon closer inspection via patch-level analysis.\n4. Highlighting improvements obtained after integrating patch representation aggregation techniques significantly enhance state-of-the-art (SOTA) baseline methods further emphasizing the importance & potential benefits associated with this approach towards demystifying complexities surrounding self-supervised representation learning processes overall.",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "Using Representation Expressiveness and Learnability to Evaluate Self-Supervised Learning Methods",
        "abstract": "We address the problem of evaluating the quality of self-supervised learning (SSL) models\nwithout access to supervised labels, while being agnostic to the architecture, learning\nalgorithm or data manipulation used during training. We argue that representations can\nbe evaluated through the lens of expressiveness and learnability. We propose to use the\nIntrinsic Dimension (ID) to assess expressiveness and introduce Cluster Learnability (CL) to\nassess learnability. CL is measured in terms of the performance of a KNN classifier trained\nto predict labels obtained by clustering the representations with K-means. We thus combine\nCL and ID into a single predictor – CLID. Through a large-scale empirical study with a\ndiverse family of SSL algorithms, we find that CLID better correlates with in-distribution\nmodel performance than other competing recent evaluation schemes. We also benchmark\nCLID on out-of-domain generalization, where CLID serves as a predictor of the transfer\nperformance of SSL models on several visual classification tasks, yielding improvements with\nrespect to the competing baselines.",
        "authors": "Y. Lu, Z. Liu, A. Baratin, et.al",
        "keywords": [
            "evaluation",
            "self-supervised learning",
            "Intrinsic Dimension"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=BxdrpnRHNh",
        "pdf_src": "https://api2.openreview.net/pdf/911ec4d15491f0134a499bb42fd16f006318c0d2.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the challenge of assessing the quality of self-supervised learning (SSL) models without supervision, regardless of their architectures, learning algorithms, or data manipulations.\n\nResearch Problem: How do you evaluate the quality of SSL models when there are no labeled datasets available?\n\nMethod: The authors suggest using two metrics - Intrinsic Dimension (ID) for expressiveness assessment and Cluster Learnability (CL) for learnability assessment. They define CL as the ability of a model's learned features to be clustered effectively via K-means algorithm; they measure it based on how well a KNN classifier performs at predicting cluster labels from these features after clustering them together.\nThey then integrate both ID and CL into one metric called \"CLID\" which predicts overall SSL model performance more accurately across various SSL methods compared to existing benchmarks.\n\nMain Contributions:\n1. Propose new metrics (\"CL\" & \"CLID\") specifically designed not just for expressiveness but focusing particularly on learnability within an unsupervised setting;\n2. Conduct extensive experiments validating this approach against alternative measures showing superior correlation between predicted scores provided by CLID and actual SSL model performance under distributional shifts encountered beyond training domains;\n3. Demonstrate improved predictive power over prior state-of-the-art evaluations especially concerning out-of-domain generalization capabilities ensuring robustness outside original training distributions",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "One-Round Active Learning through Data Utility Learning and Proxy Models",
        "abstract": "While active learning (AL) techniques have demonstrated the potential to produce high-performance models with fewer labeled data, their application remains limited due to the necessity for multiple rounds of interaction with annotators. This paper studies the problem of one-round AL, which aims at selecting a subset of unlabeled points and querying their labels \\emph{all at once}. A fundamental challenge is how to measure the utility of different choices of labeling queries for learning a target model. Our key idea is to learn such a utility metric from a small initial labeled set. We demonstrate that our approach leads to state-of-the-art performance on various AL benchmarks and is more robust to the lack of initial labeled data. \n\nIn addition to algorithmic development and evaluation, we introduce a novel metric for quantifying `\\emph{utility transferability}' -- the degree of correlation between the performance changes of two learning algorithms due to variations in training data selection. Previous studies have often observed a notable utility transferability between models, even those with differing complexities. Such transferability enabled our approach, as well as other techniques such as coresets, hyperparameter tuning, and data valuation, to scale up to more sophisticated target models by substituting them with smaller proxy models. Nevertheless, utility transferability has not yet been rigorously defined within a formal mathematical framework, a gap that our work addresses innovatively. We further propose two Monte Carlo-based methods for efficiently comparing utility transferability for different proxy models, thereby facilitating a more informed selection of proxy models.",
        "authors": "J. T. Wang, S. Chen, R. Jia",
        "keywords": [
            "one-round active learning",
            "utility metric learning",
            "utility transferability"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=8HQCOMRa7g",
        "pdf_src": "https://api2.openreview.net/pdf/a4f85eda5f29f480d894f03d6e0c2ef560095ab1.pdf",
        "Code_src": "",
        "Introduction": "Background: Active Learning (AL) techniques are promising approaches capable of producing high-performance machine learning models using less annotated data than traditional supervised learning but require iterative interactions with annotators.\n\nResearch Problem: The study focuses on addressing One-Round Active Learning where all selected unlabeled examples must be queried simultaneously rather than incrementally over several rounds.\nThe main challenge lies in developing an effective method to evaluate the 'utility' or effectiveness of different labeling query strategies when aiming to train a specific target model.\n\nMethodology: To tackle this issue, researchers develop a new strategy based on learning a utility metric directly from a very small initial labeled dataset without requiring extensive annotations upfront; they then apply it across various datasets known for benchmarking AL performance.\n\nMain Contributions:\n1. They present a novel approach achieving state-of-the-art results compared against existing AL benchmarks while being relatively unaffected by the scarcity of initial labeled samples—a significant improvement upon previous limitations regarding sample efficiency.\n2. In addition to practical improvements through algorithm design & evaluation, authors contribute towards understanding another aspect related to AL—`Utility Transferability`, i.e., whether there exists any consistency among utilities measured under varying conditions like choice of proxies/models used during training process).\n3. Addressing gaps identified earlier around defining `Utility Transferability` mathematically precisely, they provide innovative solutions including introducing two efficient comparison metrics based on Monte Carlo simulations allowing practitioners choose optimal proxy models confidently guided by these insights into inter-model correlations arising out variability",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Bridging the Gap Between Offline and Online Reinforcement Learning Evaluation Methodologies",
        "abstract": "Reinforcement learning (RL) has shown great promise with algorithms learning in environments with large state and action spaces purely from scalar reward signals. \nA crucial challenge for current deep RL algorithms is that they require a tremendous amount of environment interactions for learning. \nThis can be infeasible in situations where such interactions are expensive, such as in robotics. \nOffline RL algorithms try to address this issue by bootstrapping the learning process from existing logged data without needing to interact with the environment from the very beginning. \nWhile online RL algorithms are typically evaluated as a function of the number of environment interactions, there isn't a single established protocol for evaluating offline RL methods.\nIn this paper, we propose a sequential approach to evaluate offline RL algorithms as a function of the training set size and thus by their data efficiency.\nSequential evaluation provides valuable insights into the data efficiency of the learning process and the robustness of algorithms to distribution changes in the dataset while also harmonizing the visualization of the offline and online learning phases. \nOur approach is generally applicable and easy to implement. \nWe compare several existing offline RL algorithms using this approach and present insights from a variety of tasks and offline datasets.",
        "authors": "S. Sujit, P. Braga, J. Bornschein, et.al",
        "keywords": [
            "reinforcement learning",
            "offline RL algorithms",
            "data efficiency"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=J3veZdVpts",
        "pdf_src": "https://api2.openreview.net/pdf/3b1ef1ecbc6f274811ab96424a25ca2101cd5696.pdf",
        "Code_src": "",
        "Introduction": "Background: Reinforcement Learning (RL) has been successful at learning complex behaviors through interaction within environments characterized by high-dimensional states and actions solely based on scalar rewards.\n\nResearch Problem: A significant limitation faced by modern deep RL algorithms stems from requiring an extensive quantity of interactions between agents and their surroundings before convergence occurs—this could become impractical or prohibitively costly when these interactions involve real-world systems like robots due to resource constraints related to time/money/effort.\n\nMethodology: To circumvent issues associated with heavy reliance on environmental interactions during training periods, Offline RL algorithms have emerged which leverage historical data logs rather than fresh experiences directly obtained via interaction; however, unlike Online RL methodologies whose efficacy often hinges upon cumulative numbers of interactions over iterations, no universally accepted benchmark exists yet specifically tailored towards assessing performance metrics pertinent only after initial deployment scenarios occur post-training phase completion.\n\nMain Contributions:\n1. This study introduces a novel Sequential Evaluation Protocol designed exclusively around quantifying how well Offline RL algorithms perform relative to varying sizes available within pre-existing datasets used throughout training procedures – thereby providing insight regarding both Data Efficiency aspects pertaining directly backtracking steps taken toward optimal solutions versus potential redundancy elimination strategies employed along way -and Algorithm Robustness vis-à-vis shifts encountered across different distributions found amongst various subsets extracted out-of-sample testing sets respectively;\n2. The proposed framework allows researchers/users alike comparing diverse Offline RL techniques against one another under controlled conditions enabling them identify most promising candidates suited best specific application domains concerned accordingly;\n3. We demonstrate our methodological contributions empirically employing representative examples drawn from multiple task types encompassing distinct offline datasets showcasing its generality applicability ease implementation whilst yielding meaningful comparative analyses outcomes among competing approaches considered hereunder.",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Visualizing the Diversity of Representations Learned by Bayesian Neural Networks",
        "abstract": "Explainable Artificial Intelligence (XAI) aims to make learning machines less opaque, and offers researchers and practitioners various tools to reveal the decision-making strategies of neural networks. In this work, we investigate how XAI methods can be used for exploring and visualizing the diversity of feature representations learned by Bayesian Neural Networks (BNNs). Our goal is to provide a global understanding of BNNs by making their decision-making strategies a) visible and tangible through feature visualizations and b) quantitatively measurable with a distance measure learned by contrastive learning. Our work provides new insights into the posterior distribution in terms of human-understandable feature information with regard to the underlying decision-making strategies. The main findings of our work are the following: 1) global XAI methods can be applied to explain the diversity of decision-making strategies of BNN instances, 2) Monte Carlo dropout with commonly used Dropout rates exhibit increased diversity in feature representations compared to the multimodal posterior approximation of MultiSWAG, 3) the diversity of learned feature representations highly correlates with the uncertainty estimate for the output and 4) the inter-mode diversity of the multimodal posterior decreases as the network width increases, while the intra-mode diversity increases. These findings are consistent with the recent Deep Neural Networks theory, providing additional intuitions about what the theory implies in terms of humanly understandable concepts.",
        "authors": "D. Grinwald, K. Bykov, S. Nakajima, et.al",
        "keywords": [
            "feature visualization",
            "Bayesian Neural Networks",
            "decision-making strategies"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ZSxvyWrX6k",
        "pdf_src": "https://api2.openreview.net/pdf/85337ff824ac5cdbc91fbe182d5c5f6958ea3d05.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe field of Explainable Artificial Intelligence (XAI) seeks to enhance the interpretability of machine learning models such that humans may understand why they arrive at certain decisions or predictions.\n\nResearch Question:\nThis study focuses on applying XAI techniques specifically designed for Bayesian Neural Networks (BNNs), which incorporate probabilistic reasoning within deep neural networks allowing them to express uncertainty over model outputs.\nThe question addressed here revolves around whether these XAI approaches could elucidate not just individual features but also the broader patterns across diverse feature representations acquired during training.\n\nMethodology:\nTo tackle this research problem, two primary methodologies were employed:\n\n- Feature Visualizations - Techniques from XAI like feature visualization aim to render abstract data points more comprehensible so one might observe relationships between input variables leading up to specific outcomes made by the neural network.\n- Contrastive Learning Distance Measure - A novel approach was developed using contrastive learning principles where distances among different feature vectors represent similarity judgments based on learned discriminative properties; it allows us to quantify variability effectively without relying solely on qualitative interpretations provided by traditional XAI methods alone.\n\nMain Contributions:\nThe paper makes several significant contributions towards advancing knowledge regarding BNNs' decision-making processes via XAI frameworks:\n1. It demonstrates practical application of global XAI methods capable of explaining the heterogeneity amongst multiple BNN instances’ decision-making strategies – something previously unexplored due to complexity associated with Bayesian inference mechanisms involved therein;\n2. Comparative analysis shows that when employing Monte Carlo dropout along with standard dropout probabilities yields greater variance than does approximating multi-modal posterior distributions typically found in works related to MultiSWAG (a method for inferring latent spaces);\n3. Correlation established indicates high correspondence exists between estimated uncertainties derived directly after each layer processing inputs versus actual diversities observed throughout layers’ learned feature space;\n4. Insights gained suggest an inverse relationship existing between inter-modal diversity measured across all modes present within multimodal posterior distributions against increasing network widths whilst observing corresponding growth trends pertaining to intra-modal diversity measures instead.\n\nOverall Implications:\nThese discoveries align closely with theoretical perspectives recently proposed concerning architectures built upon deep neural networks suggesting implications pertinent both practically speaking—such advancements potentially aiding engineers designing robust systems—and theoretically—providing further evidence supporting foundational assumptions underpinning current theories governing complex computational tasks performed nowadays predominantly by artificial intelligence algorithms deployed widely today's society",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "RLTF: Reinforcement Learning from Unit Test Feedback",
        "abstract": "The goal of program synthesis, or code generation, is to generate executable code based on given descriptions. Recently, there has been an increasing number of studies employing reinforcement learning (RL) to improve the performance of large language models (LLMs) for code. \nHowever, some of the current representative RL methods have only used offline frameworks, limiting the exploration of new sample spaces. \nAdditionally, the utilization of unit test signals is limited, not accounting for specific error locations within the code. \nTo address these issues, we proposed RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs. \nOur approach generates data in real-time during training and simultaneously utilizes fine-grained feedback signals to guide the model towards producing higher-quality code.\nExtensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks.\nOur code is available at: \\url{https://github.com/Zyq-scut/RLTF}.",
        "authors": "J. Liu, Y. Zhu, K. Xiao, et.al",
        "keywords": [
            "code generation",
            "reinforcement learning",
            "unit test feedback"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=hjYmsV6nXZ",
        "pdf_src": "https://api2.openreview.net/pdf/d3014bed0600d1a6fc43b5d508c6dde48fc34b4f.pdf",
        "Code_src": "我们的代码可以在以下位置找到：\\url{https://github.com/Zyq-scut/RLTF}",
        "Introduction": "Background:\nProgram synthesis aims to automatically generate executable code upon user-provided specifications through techniques such as machine learning.\n\nResearch Problem:\nCurrent research into improving Large Language Models (LLMs) for code generation often employs reinforcement learning but typically uses offline frameworks which limit their ability to explore diverse problem domains effectively due to lack of continuous interaction throughout the process; also, they do not fully utilize unit tests beyond simple pass/fail indications without considering precise errors found inside generated code.\n\nMethodology:\nWe introduce RLTF - Reinforcement Learning from Unit Test Feedback – an innovative online reinforcement learning framework designed specifically tailored around receiving detailed unit test results across multiple levels of granularity while actively generating synthetic programs continuously over time ensuring immediate adaptation guided by accurate information about any defects present directly where they occur rather than just overall success or failure metrics alone.\n\nMain Contributions:\nOur primary contribution lies in developing RLTF—a cutting-edge method leveraging unit test feedback integrated seamlessly alongside active learning processes allowing us to refine our synthesized codes iteratively according to exacting standards imposed via comprehensive testing procedures yielding high quality outputs consistently outperforming existing benchmarks like APPS & MBPP demonstrating its effectiveness significantly advancing automated programming capabilities further enhancing efficiency accuracy scalability compared traditional approaches currently employed today",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Invariant Structure Learning for Better Generalization and Causal Explainability",
        "abstract": "Learning the causal structure behind data is invaluable for improving generalization and ob- taining high-quality explanations. Towards this end, we propose a novel framework, Invariant Structure Learning (ISL), that is designed to improve causal structure discovery by utilizing generalization as an indication in the process. ISL splits the data into different environments, and learns a structure that is invariant to the target across different environments by imposing a consistency constraint. The proposed aggregation mechanism then selects the classifier based on a graph structure that reflects the causal mechanisms in the data more accurately compared to the structures learnt from individual environments. Furthermore, we extend ISL to a self-supervised learning setting, where accurate causal structure discovery does not rely on any labels. Self-supervised ISL utilizes proposals for invariant causality, by iteratively setting different nodes as targets. On synthetic and real-world datasets, we demonstrate that ISL accurately discovers the causal structure, outperforms alternative methods, and yields superior generalization for datasets with significant distribution shifts.",
        "authors": "Y. Ge, S. O. Arik, J. Yoon, et.al",
        "keywords": [
            "Invariant Structure Learning",
            "Causal Mechanisms",
            "Generalization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=A9yn7KTwsK",
        "pdf_src": "https://api2.openreview.net/pdf/1bbcddeb6b767149c4c24618076668985ae7841e.pdf",
        "Code_src": "",
        "Introduction": "Background: Discovering the causal relationships within complex systems can enhance understanding of those systems' behavior.\n\nResearch Question: How might one develop algorithms capable of uncovering such causal structures?\n\nMethodology: We introduce \"Invariant Structure Learning\" (ISL). This method involves segmenting data into various 'environments', each potentially exhibiting unique characteristics or noise levels affecting the observed variables; it then seeks to learn a consistent causal model which remains stable regardless of these variations through the imposition of a consistency constraint between these environments.\n \nMain Contributions:\n1. **Consistency Across Environments**: By ensuring that learned models are consistent when applied consistently over multiple environments, our approach mitigates against biases introduced by specific conditions under which observations were made.\n2. **Graph-Based Aggregation**: Our framework introduces an innovative aggregation strategy using graphs reflecting the underlying causal relations among features/data points better than standalone environment-specific models would do alone due to potential confounding factors present only locally rather globally throughout all environments considered together.\n3. **Self-Supervised Extension**: We further adapt ISL principles towards unsupervised settings without requiring explicit labels - instead relying solely on inherent patterns observable even before labeling occurs allowing us generalize beyond training samples seen during supervised tasks thus demonstrating improved robustness against unseen distributions shifts encountered outside training regime encountered during deployment scenarios like real-world applications etcetera).\n4. **Performance Validation**: Empirical evidence provided via experiments conducted both synthetically generated datasets along with actual ones validates effectiveness superiority demonstrated by our proposed algorithmic approach compared existing alternatives available currently available today market place hence offering promising prospects future advancements related field artificial intelligence",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Automated Detection of Causal Inference Opportunities: Regression Discontinuity Subgroup Discovery",
        "abstract": "The gold standard for the identification of causal effects are randomized controlled trials (RCT), but RCTs may not always be feasible to conduct. When treatments depend on a threshold however, such as the blood sugar threshold for diabetes diagnosis, we can still sometimes estimate causal effects with regression discontinuities (RDs). RDs are valid when units just above and below the threshold have the same distribution of covariates and thus no confounding in the presence of noise, establishing an as-if randomization. In practice however, implementing RD studies can be difficult as identifying treatment thresholds require considerable domain expertise -- furthermore, the thresholds may differ across  subgroups (e.g., the blood sugar threshold for diabetes may differ across demographics), and ignoring these differences can lower statistical power. Finding the thresholds and to whom they apply is an important problem currently solved manually by domain experts, and data-driven approaches are needed when domain expertise is not sufficient. Here, we introduce Regression Discontinuity SubGroup Discovery (RDSGD), a machine-learning method that identifies statistically powerful and interpretable subgroups for RD thresholds. Using a medical claims dataset with over 60 million patients, we apply RDSGD to multiple clinical contexts and identify subgroups with increased compliance to treatment assignment thresholds.\nAs treatment thresholds matter for many diseases and policy decisions, RDSGD can be a powerful tool for discovering new avenues for causal estimation.",
        "authors": "T. Liu, P. Lawlor, L. Ungar, et.al",
        "keywords": [
            "causal inference",
            "regression discontinuity design",
            "subgroup discovery"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=cdRYoTyHZh",
        "pdf_src": "https://api2.openreview.net/pdf/4e2bf8cbf9fb34af3cc0060b740416f874ed0803.pdf",
        "Code_src": "",
        "Introduction": "Background: The gold standard for identifying causal effects involves conducting randomized controlled trials (RCTs); however, this approach might not always be practical due to various constraints.\n\nResearch Question: How do we accurately estimate causal effects using regression discontinuities (RDs) even if it's challenging or impossible to perform RCTs?\n\nMethodology: We propose a novel machine learning algorithm called Regression Discontinuity SubGroup Discovery (RDSGD) which aims at finding statistically significant and interpretable subgroups where RD assumptions hold true despite potential confounding factors like demographic variations affecting treatment thresholds.\n\nMain Contributions: Our work introduces RDSGD—a groundbreaking technique capable of automatically identifying relevant patient groups within complex datasets based on their adherence to specific treatment thresholds—potentially leading to more precise estimates without relying solely on traditional experimental designs while also addressing issues related to heterogeneity among different populations concerning those thresholds through subgroup discovery mechanisms.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Data pruning and neural scaling laws: fundamental limitations of score-based algorithms",
        "abstract": "Data pruning algorithms are commonly used to reduce the memory and computational cost of the optimization process. Recent empirical results (Guo, B. Zhao, and Bai, 2022) reveal that random data pruning remains a strong baseline and outperforms most existing data pruning methods in the high compression regime, i.e., where a fraction of 30% or less of the data is kept. This regime has recently attracted a lot of interest as a result of the role of data pruning in improving the so-called neural scaling laws; see (Sorscher et al., 2022), where the authors showed the need for high-quality data pruning algorithms in order to beat the sample power law. In this work, we focus on score-based data pruning algorithms and show theoretically and empirically why such algorithms fail in the high compression regime. We demonstrate “No Free Lunch\" theorems for data pruning and discuss potential solutions to these limitations.",
        "authors": "F. Ayed, S. Hayou",
        "keywords": [
            "data pruning",
            "random data pruning",
            "neural scaling laws"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=iRTL4pDavo",
        "pdf_src": "https://api2.openreview.net/pdf/a2433de6bb53464732f7a73cc1ea19c7a92f729f.pdf",
        "Code_src": "",
        "Introduction": "Background: Data pruning algorithms have been widely applied to optimize model training by reducing memory and computation costs during the optimization process.\nResearch problem: Despite recent advancements in various data pruning techniques, it was observed through empirical studies [Guo, B. Zhao, and Bai, 2022] that random data pruning still serves as an effective baseline method which often surpasses other existing approaches particularly under extreme compression scenarios (i.e., when only around 30% or fewer samples remain).\nMethods: The paper concentrates specifically on score-based data pruning strategies aiming at understanding their performance shortcomings within highly compressed datasets from both theoretical perspectives along with experimental validations.\n\nMain contributions:\n1. Theoretical Analysis - The study provides evidence demonstrating fundamental issues inherent to score-based data pruning algorithms leading them towards suboptimal outcomes especially pertinent after significant data reduction rates above certain thresholds like those encountered near 30% dataset retention rate(s). \n2. Empirical Evidence - By conducting experiments across different tasks/models, they corroborate theory findings showing how score-based pruning fails compared against random counterparts even though scores might be informative otherwise elsewhere than very low levels of compression ratios.\n3. No Free Lunch Theorem Illustration - They illustrate \"No Free Lunch\" theorem implications suggesting there's no universal best approach applicable universally regardless of task complexity or nature thus highlighting necessity tailored solutions considering specific contexts involved while employing any form of data pruning strategy going forward into future research endeavors related optimizing machine learning models via reduced datasets sizes without compromising too much accuracy gains achieved originally before applying such techniques.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Online model selection by learning how compositional kernels evolve",
        "abstract": "Motivated by the need for efficient, personalized learning in health, we investigate the problem of online compositional kernel selection for multi-task Gaussian Process regression. Existing composition selection methods do not satisfy our strict criteria in health; selection must occur quickly, and the selected kernels must maintain the appropriate level of complexity, sparsity, and stability as data arrives online. We introduce the Kernel Evolution Model (KEM), a generative process on how to evolve kernel compositions in a way that manages the bias--variance trade-off as we observe more data about a user. Using pilot data, we learn a set of kernel evolutions that can be used to quickly select kernels for new test users. KEM reliably selects high-performing kernels for a range of synthetic and real data sets, including two health data sets.",
        "authors": "E. Nofshin, P. Klasnja, S. Murphy, et.al",
        "keywords": [
            "online compositional kernel selection",
            "Kernel Evolution Model",
            "bias-variance trade-off"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=23WZFQBUh5",
        "pdf_src": "https://api2.openreview.net/pdf/3c88391db3140feba4533ff33ed5645f29b114dd.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the challenge of efficiently adapting machine learning models when dealing with personalization needs within healthcare settings.\n\nResearch Problem: How does one effectively choose among various kernel functions during the course of multi-task Gaussian Process regression while ensuring adaptability over time?\n\nMethodology: To tackle this issue, the authors propose the Kernel Evolution Model (KEM). This model is designed based on Bayesian nonparametrics which allows it to generate novel kernel combinations dynamically without needing predefined rules or thresholds regarding complexity, sparsity, or stability at each step.\n \nMain Contributions:\n1. **Kernel Evolution Model (KEM)** - A framework capable of evolving kernel compositions incrementally through observation allowing for an adaptive management of the bias-variance tradeoff according to incoming data points from individual subjects.\n2. **Online Compositional Kernel Selection** - An approach where kernel selections are made continuously throughout the training phase rather than being pre-determined before starting the task.\n3. **Performance Validation Across Datasets** - Demonstrated performance using both synthetic datasets along with empirical results obtained via application across actual medical datasets indicating robustness beyond theoretical considerations",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "NOFLITE: Learning to Predict Individual Treatment Effect Distributions",
        "abstract": "Estimating the effect of a treatment on an individual's outcome of interest is an important challenge in various fields, such as healthcare, economics, marketing, and education. Previous work in machine learning has focused on estimating the expected value of the treatment effect. However, effective personalized decision-making requires more than just the treatment expected effect; it requires knowing the entire treatment effect distribution. Knowing this distribution allows analyzing the treatment's expected utility or quantifying the uncertainty regarding a treatment's effect. This information is essential for prescribing optimal treatments. The ability of a model to predict accurate individual treatment effect distributions is captured by its likelihood. In light of this, we propose a novel neural architecture, NOFLITE, that uses normalizing flows to directly optimize this likelihood, while simultaneously learning flexible estimates of the individual treatment effect distribution. Experiments on various semi-synthetic data sets show that NOFLITE outperforms existing methods in terms of loglikelihood. Moreover, we illustrate how the predicted distributions can enable an in-depth analysis of the treatment effect and more accurate decision-making.",
        "authors": "T. Vanderschueren, J. Berrevoets, W. Verbeke",
        "keywords": [
            "individual treatment effect estimation",
            "normalizing flow",
            "likelihood optimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=EjqopDxLbG",
        "pdf_src": "https://api2.openreview.net/pdf/a559914cc3f15ee301fe18959974179df74ff568.pdf",
        "Code_src": "",
        "Introduction": "Background: Estimating the impact of a specific intervention (\"treatment\") on individuals' outcomes across different domains like healthcare, economics, marketing, and education plays crucial roles.\n\nResearch Problem: While previous studies have concentrated on predicting the average effectiveness of interventions ('expected treatment effect'), they often overlook understanding the full range of potential effects each person may experience ('individual-level treatment effect distribution'). Accurate prediction of these distributions would allow policymakers not only to estimate overall benefits but also to tailor decisions based on personal variability among patients/subjects.\n\nMethodology: To address this issue, our study introduces \"NOFLITE,\" which stands for Neural Ordinary Flow-based Likelihood Integration Tool. It employs Normalizing Flows—a type of generative model—to learn both the parameters defining the probability density function from observed data points along with their corresponding inverse transformations back into the original space—thereby enabling direct optimization over the likelihood measure capturing predictive performance accurately without relying solely on approximations through gradient descent techniques commonly used elsewhere.\n \nMain Contributions:\n1. We develop a new approach called NOFLITE using Normalizing Flows capable of optimizing the likelihood metric directly related to predictions about future observations within given datasets.\n2. Our proposed method significantly improves upon current state-of-the-art approaches when measured against logarithmic likelihood scores during empirical tests conducted via synthetic datasets derived under controlled conditions simulating real-world scenarios relevant here.\n3. Furthermore, experiments demonstrate practical applications where utilizing predicted distributions could lead to deeper insights concerning efficacy profiles leading towards better informed clinical choices tailored specifically toward patient heterogeneity rather than one-size-fits-all strategies currently employed widely today",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "Conditional Sampling of Variational Autoencoders via Iterated Approximate Ancestral Sampling",
        "abstract": "Conditional sampling of variational autoencoders (VAEs) is needed in various applications, such as missing data imputation, but is computationally intractable. A principled choice for asymptotically exact conditional sampling is Metropolis-within-Gibbs (MWG). However, we observe that the tendency of VAEs to learn a structured latent space, a commonly desired property, can cause the MWG sampler to get “stuck” far from the target distribution. This paper mitigates the limitations of MWG: we systematically outline the pitfalls in the context of VAEs, propose two original methods that address these pitfalls, and demonstrate an improved performance of the proposed methods on a set of sampling tasks.",
        "authors": "V. Simkus, M. U. Gutmann",
        "keywords": [
            "VAE",
            "Conditional Sampling",
            "Metropolis-within-Gibbs"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=I5sJ6PU6JN",
        "pdf_src": "https://api2.openreview.net/pdf/db64390828e9dc6d5b8ce9c3f681827c7a77f95f.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this research lies in the application of variational autoencoders (VAEs), which are widely used models capable of generating new samples by drawing from their posterior distributions. Conditional sampling using VAEs has been applied successfully across different fields like missing data imputation; however, it becomes computationally challenging due to its complexity.\n\nResearch Problem:\nThe main problem addressed here concerns the computational difficulty involved with conditional sampling within VAEs when trying to approximate the true posterior distribution. Specifically, while Metropolis-within-Gibbs (MWG) sampling provides a theoretically sound approach toward achieving asymptotic exactness under certain conditions – it often fails because of how VAEs tend to organize into a structured latent space during training.\n \nMethods:\nTo overcome this issue related to the MWG sampler getting stuck away from the target distribution caused by the learned structure's influence over the latent variables, three distinct approaches have been introduced:\n\n1. Pitfalls Identification - The first step involves identifying potential issues or \"pitfalls\" associated with applying MWG sampling techniques specifically designed for continuous latent spaces where there might be no unique mode corresponding to each observation class.\n\n2. Method 1: Sample Conditioning via Latent Space Regularization - To mitigate the identified pitfalls without altering existing architectures significantly beyond what would typically occur through normal training procedures themselves, one method proposes conditioning sample generation directly upon auxiliary regularization terms added onto our loss function rather than modifying any parameters explicitly controlling latent variable assignments at inference time itself.\n\n3. Method 2: Progressive Sampling Strategy - Another technique aims not only at avoiding getting trapped too early near local optima but also ensures convergence towards global maxima more rapidly overall throughout iterations by gradually increasing variance allowed per iteration until reaching some predefined threshold value before resuming standard Gibbs steps again.\n\nMain Contributions:\nThis work makes several contributions addressing challenges faced regarding conditional sampling inside Variational Autoencoders (VAEs):\n\n- It outlines common problems encountered particularly relevant concerning VAEs' propensity forming structured latent spaces leading to difficulties implementing MWG samplers effectively;\n  \n- Introduces novel solutions including both modifications aimed at improving efficiency along with progressive strategies ensuring better exploration capabilities during iterative processes;\n\n- Demonstrates empirical evidence supporting improvements achieved relative traditional MWG sampling practices demonstrating efficacy against benchmark datasets showcasing superior performance outcomes obtained employing these advanced methodologies developed herein.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Rewiring with Positional Encodings for Graph Neural Networks",
        "abstract": "Several recent works use positional encodings to extend the receptive fields of graph neural network (GNN) layers equipped with attention mechanisms. These techniques, however, extend receptive fields to the complete graph, at substantial computational cost and risking a change in the inductive biases of conventional GNNs, or require complex architecture adjustments. As a conservative alternative, we use positional encodings to expand receptive fields to r-hop neighborhoods. More specifically, our method augments the input graph with additional nodes/edges and uses positional encodings as node and/or edge features. We thus modify graphs before inputting them to a downstream GNN model, instead of modifying the model itself. This makes our method model-agnostic, i.e., compatible with any of the existing GNN architectures. We also provide examples of positional encodings that are lossless with a one-to-one map between the original and the modified graphs. We demonstrate that extending receptive fields via positional encodings and a virtual fully- connected node significantly improves GNN performance and alleviates over-squashing using small r. We obtain improvements on a variety of models and datasets and reach competitive performance using traditional GNNs or graph Transformers.",
        "authors": "R. B. Gabrielsson, M. Yurochkin, J. Solomon",
        "keywords": [
            "graph neural networks",
            "positional encodings",
            "receptive field expansion"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=dn3ZkqG2YV",
        "pdf_src": "https://api2.openreview.net/pdf/159c57041fd342fe889180fc5b8291bf3f19724e.pdf",
        "Code_src": "",
        "Introduction": "Background: Recent studies have employed positional encodings within graph neural networks (GNNs), which incorporate attention mechanisms for enhancing their representational capabilities by expanding their receptive fields beyond local neighborhood information.\n\nResearch Problem: While these extensions can improve learning across larger scales through global context integration without altering the underlying structure directly, they often come at high computational costs due to the need to consider all possible edges during inference time; this could potentially disrupt the inductive biases inherent in standard GNN designs if applied indiscriminately.\n \nMethod: To address both issues—computational efficiency concerns while preserving the inductive bias—we propose an approach where we augment the input graph rather than modifying the GNN layer's architecture. Specifically, we introduce new nodes and edges representing \\(r\\)-hop neighborhoods around each existing node into the graph. Positional encodings serve dual roles here—as attributes for newly introduced nodes and edges allowing us to maintain a direct correspondence from old to new graph representations.\n\nMain Contributions:\n1. Our technique is agnostic towards specific GNN architectures since it operates solely upon the input graph data pre-processing stage—a \"model-agnostic\" augmentation strategy.\n2. The positional encoding scheme maintains a 1-to-1 mapping relationship ensuring no information loss when translating back-and-forth among the original and augmented graphs.\n3. Empirical validation shows significant enhancements not only regarding generalization but particularly against the issue of over-squashing—the compression effect leading to reduced variance—that plagues many GNN applications especially those involving long-range dependencies.\n4. Performance gains demonstrated apply broadly—from various baseline GNN models like GCN and GraphSAGE up to state-of-the-art Transformer-based graph models such as GraphTransformer.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "A Robust Backpropagation-Free Framework for Images",
        "abstract": "While current deep learning algorithms have been successful for a wide variety of artificial intelligence (AI) tasks, including those involving structured image data, they present deep neurophysiological conceptual issues due to their reliance on the gradients that are computed by backpropagation of errors (backprop). Gradients are required to obtain synaptic weight adjustments but require knowledge of feed forward activities in order to conduct backward propagation, a biologically implausible process. This is known as the \"weight transport problem''. Therefore, in this work, we present a more biologically plausible approach towards solving the weight transport problem for image data. This approach, which we name the error-kernel driven activation alignment (EKDAA) algorithm, accomplishes through the introduction of locally derived error transmission kernels and error maps. Like standard deep learning networks, EKDAA performs the standard forward process via weights and activation functions; however, its backward error computation involves adaptive error kernels that propagate local error signals through the network. The efficacy of EKDAA is demonstrated by performing visual-recognition tasks on the Fashion MNIST, CIFAR-10 and SVHN benchmarks, along with demonstrating its ability to extract visual features from natural color images. Furthermore, in order to demonstrate its non-reliance on gradient computations, results are presented for an EKDAA-trained CNN that employs a non-differentiable activation function.",
        "authors": "T. Zee, A. Ororbia, A. Mali, et.al",
        "keywords": [
            "EKDAA",
            "Error Kernel Driven Activation Alignment",
            "Non-Differentiable Activation Function"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=leqr0vQzeN",
        "pdf_src": "https://api2.openreview.net/pdf/746932717f78119d376a3b0e6e03578c227e3121.pdf",
        "Code_src": "",
        "Introduction": "Background: While deep learning has achieved significant success across various AI tasks utilizing structured image data, it faces challenges related to its neural mechanisms based on backpropagation.\n\nResearch Question: How can one develop a more biologically plausible solution addressing the weight transport issue within existing deep learning frameworks?\n\nMethodology: We introduce the Error-Kernel Driven Activation Alignment (EKDAA) algorithm—a novel method designed specifically targeting the weight transport challenge using locally-derived error transmission kernels and corresponding error maps without relying solely on gradients during training.\n\nMain Contributions:\n1. EKDAA introduces adaptive error kernels into the conventional forward pass of a neural network.\n2. These kernels facilitate the backward propagation step while minimizing the need for traditional gradient calculations—thereby circumventing biological implausibility concerns associated with such computations directly inspired by neuroscience principles.\n3. Demonstrated effectiveness was shown when applying our proposed model successfully to visual recognition tasks over datasets like Fashion MNIST, CIFAR-10, and SVHN alongside extracting meaningful visual features naturally occurring in colored images.\n4. Additionally, experiments were conducted where the trained convolutional neural network utilized a non-differentiable activation function further emphasizing independence from gradient-based methods entirely.\n\n\nIn summary, this paper presents a new paradigm shift toward resolving longstanding issues inherent in classical deep learning architectures concerning how information flows throughout layers",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Stochastic Mirror Descent: Convergence Analysis and Adaptive Variants via the Mirror Stochastic Polyak Stepsize",
        "abstract": "We investigate the convergence of stochastic mirror descent (SMD) under interpolation in relatively smooth and smooth convex optimization. In relatively smooth convex optimization we provide new convergence guarantees for SMD with a constant stepsize. For smooth convex optimization we propose a new adaptive stepsize scheme --- the mirror stochastic Polyak stepsize (mSPS). Notably, our convergence results in both settings do not make bounded gradient assumptions or bounded variance assumptions, and we show convergence to a neighborhood that vanishes under interpolation. Consequently, these results correspond to the first convergence guarantees under interpolation for the exponentiated gradient algorithm for fixed or adaptive stepsizes. mSPS generalizes the recently proposed stochastic Polyak stepsize (SPS) (Loizou et al. 2021) to mirror descent and remains both practical and efficient for modern machine learning applications while inheriting the benefits of mirror descent. We complement our results with experiments across various supervised learning tasks and different instances of SMD, demonstrating the effectiveness of mSPS.",
        "authors": "R. D'orazio, N. Loizou, I. H. Laradji, et.al",
        "keywords": [
            "stochastic mirror descent",
            "convex optimization",
            "adaptive stepsize"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=28bQiPWxHl",
        "pdf_src": "https://api2.openreview.net/pdf/0ae6f4d4b5daab5fc00088955122cb213f423905.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper focuses on studying the convergence properties of stochastic mirror descent (SMD), which is an iterative optimization method widely used in solving convex optimization problems.\n\nResearch Problem: The main research problem addressed by this paper concerns understanding when and how well does SMD converge during the process of optimization within certain classes of convex functions known as \"relatively smooth\" and \"smooth\".\n\nMethods: To address their research question regarding convergence rates using SMD algorithms without imposing strict conditions like bounded gradients/variance, they introduce two novel contributions:\n\n1. They establish new convergence guarantees specifically tailored towards SMD algorithms utilizing a constant step size strategy.\n2. Additionally, authors develop what's called the mirror stochastic Polyak stepsize (mSPS), providing an adaptive approach based on the Polyak-Ribière update rule but adapted into the context of SMD rather than gradient descent methods it was originally designed for; this adaptation aims at improving efficiency further still beyond existing approaches such as those found in Loizou et al.'s work from last year.\n\nMain Contributions:\n- Their findings are significant because they extend previous knowledge about convergence bounds typically requiring strong assumptions concerning the nature of the objective function being optimized - here no such assumption needs to be made due to the focus solely on the behavior around interpolants between data points instead \n- Furthermore, unlike other works focusing only on specific variants of SMD or its performance characteristics over particular datasets/types of loss functions, this study provides broad theoretical insights applicable generally regardless of whether one uses fixed-stepsize or adapts them dynamically through techniques similar to those employed previously exclusively via gradient-based algorithms.\n- Lastly, empirical evidence supporting these claims comes directly from experimental validation conducted against multiple supervised learning benchmarks where mSPS outperforms baseline strategies commonly utilized nowadays including SGD variants along with prior versions of stochastic Polyak step sizes themselves.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Provably Personalized and Robust Federated Learning",
        "abstract": "Clustering clients with similar objectives and learning a model per cluster is an intuitive and interpretable approach to personalization in federated learning. However, doing so with provable and optimal guarantees has remained an open challenge. In this work, we formalize personalized federated learning as a stochastic optimization problem. We propose simple clustering-based algorithms which iteratively identify and train within clusters, using local client gradients. Our algorithms have optimal convergence rates which asymptotically match those obtained if we knew the true underlying clustering of the clients, and are provably robust in the Byzantine setting where some fraction of the clients are malicious.",
        "authors": "M. Werner, L. He, M. I. Jordan, et.al",
        "keywords": [
            "clustering",
            "federated learning",
            "personalized"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=B0uBSSUy0G",
        "pdf_src": "https://api2.openreview.net/pdf/16649c4fa0e06251742da7ec63cab1a10d6f8fe4.pdf",
        "Code_src": "",
        "Introduction": "Background: Personalized federated learning aims to serve each user's specific needs by clustering users into groups based on their common objectives before training separate models for these groups.\n\nResearch Problem: The main challenge lies in developing clustering algorithms that can partition users optimally while also guaranteeing convergence under adversarial conditions such as Byzantine faults when part of the network may be compromised or behave unpredictably.\n\nMethods: This paper introduces two novel clustering-based algorithms designed specifically for federated settings - one deterministic and another randomized – both leveraging only local information from individual devices' gradient updates during the iterative process known as FedAvg.\n\nMain Contributions:\n1. **Optimal Convergence Rates**: Both proposed algorithms achieve optimal convergence rates; they converge at speeds very close to what would happen without any clustering errors.\n2. **Robustness against Byzantine Faults**: They provide provable robustness even though there could potentially be adversaries among the participating nodes due to the Byzantine fault tolerance property ensuring secure operation despite corrupted parties.\n3. **Interpretability**: By grouping similarly objective clients together first then tailoring models accordingly, it allows more interpretability since understanding why certain features were emphasized might become clearer through examining how different clusters responded differently than others did not clustered together.",
        "Topic": "Federated Learning"
    },
    {
        "title": "Offline Reinforcement Learning with Additional Covering Distributions",
        "abstract": "We study learning optimal policies from a logged dataset, i.e., offline RL, with function general approximation. Despite the efforts devoted, existing algorithms with theoretic finite-sample guarantees typically assume exploratory data coverage or strong realizable function classes (e.g., Bellman-completeness), which is hard to be satisfied in reality. While there are recent works that successfully tackle these strong assumptions, they either require the gap assumptions that could only be satisfied by part of MDPs or use the behavior regularization that makes the optimality of learned policy even intractable. To solve this challenge, we provide finite-sample guarantees for a simple algorithm based on marginalized importance sampling (MIS), showing that sample-efficient offline RL for general MDPs is possible with only a partial coverage dataset (instead of assuming a dataset covering all possible policies) and weak realizable function classes (assuming function classes containing simply one function) given additional side information of a covering distribution. We demonstrate that the covering distribution trades off prior knowledge of the optimal trajectories against the coverage requirement of the dataset, revealing the effect of this inductive bias in the learning processes. Furthermore, when considering the exploratory dataset, our analysis shows that only realizable function classes are enough for learning near-optimal policies, even with no side information on the additional coverage distributions.",
        "authors": "C. Mao",
        "keywords": [
            "offline RL",
            "function general approximation",
            "marginalized importance sampling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=AfXq3x3X16",
        "pdf_src": "https://api2.openreview.net/pdf/ab569648d93b670b8ee832143fa01757a33ca1d7.pdf",
        "Code_src": "",
        "Introduction": "Background: Offline reinforcement learning (RL) aims to learn an optimal policy from a logged dataset without requiring any further interaction between agent and environment.\n\nResearch Problem: Existing offline RL algorithms often rely on unrealistic assumptions such as full exploration of the state space (\"exploratory data coverage\") or strong realizability conditions (e.g., Bellman completeness). These assumptions may not hold true in practice due to practical constraints like computational resources limitations.\n \nMethods: The authors propose using marginalized importance sampling (MIS) combined with a novel approach called \"covering distribution\" to address the problem mentioned above. They show how MIS can efficiently estimate the value functions under certain conditions while leveraging the covering distribution to incorporate prior knowledge about the optimal trajectories into the learning process.\n \nMain Contributions:\n1. Finite-sample guarantees for a simple algorithm based on marginalized importance sampling have been provided; it demonstrates that efficient offline RL becomes feasible within general Markov Decision Processes (MDPs) through datasets partially covering potential policies rather than fully exploring them – thus reducing requirements significantly compared to previous methods relying solely on complete coverage.\n2. Additionally, their findings indicate that weaker realizability properties suffice if supplementary information regarding covering distributions exists alongside the training set - suggesting less stringent preconditions needed before starting optimization procedures involving offline RL techniques where obtaining comprehensive datasets might otherwise pose challenges related both computationally speaking but also practically achievable terms too restrictive ones imposed upon agents' actions during interactions with environments throughout iterations over time periods involved therein etcetera…",
        "Topic": "approximation"
    },
    {
        "title": "Training DNNs Resilient to Adversarial and Random Bit-Flips by Learning Quantization Ranges",
        "abstract": "Promoting robustness in deep neural networks (DNNs) is crucial for their reliable deployment in uncertain environments, such as low-power settings or in the presence of adversarial attacks. In particular, bit-flip weight perturbations in quantized networks can significantly degrade performance, underscoring the need to improve DNN resilience. In this paper, we introduce a training mechanism to learn the quantization range of different DNN layers to enhance DNN robustness against bit-flip errors on the model parameters. The proposed approach, called weight clipping-aware training (WCAT), minimizes the quantization range while preserving performance, striking a balance between the two. \nOur experimental results on different models and datasets showcase that DNNs trained with WCAT can tolerate a high amount of noise while keeping the accuracy close to the baseline model. Moreover, we show that our method significantly enhances DNN robustness against adversarial bit-flip attacks. Finally, when considering the energy-reliability trade-off inherent in on-chip SRAM memories, we observe that WCAT consistently improves the Pareto frontier of test accuracy and energy consumption across diverse models.",
        "authors": "K. Chitsaz, G. Mordido, J. David, et.al",
        "keywords": [
            "robustness",
            "quantization",
            "weight clipping-aware training"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=BxjHMPwZIH",
        "pdf_src": "https://api2.openreview.net/pdf/55acd0481e970c75ff36844ab956e18575cfd328.pdf",
        "Code_src": "",
        "Introduction": "Background: Robustness in Deep Neural Networks (DNNs) is critical due to potential issues like adversarial attacks under uncertain conditions.\n\nResearch Problem: How to make DNNs more resilient by improving them against bit-flip errors during quantization?\n\nMethod: Introduced Weight Clipping-Aware Training (WCAT). This involves learning an optimal quantization range per layer which maintains both performance and reduces quantization error sensitivity without sacrificing too much precision.\n\nMain Contributions:\n1. Developed WCAT - A novel training strategy focusing on minimizing quantization ranges.\n2. Demonstrated improved tolerance towards noise levels within the data input using WCAT-trained DNNs compared to standard quantization methods maintaining similar accuracy rates near baselines.\n3. Enhanced resistance toward adversarial bit-flip attacks through significant improvements over existing robustness measures ensuring better security properties even after quantization process.\n4. Achieved optimization along the Energy-Reliability Tradeoff curve showing consistent gains regarding accuracy retention versus power usage efficiency particularly relevant especially concerning hardware implementations utilizing SRAM memory types commonly found integrated circuits used today's computing devices",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Achieving Risk Control in Online Learning Settings",
        "abstract": "To provide rigorous uncertainty quantification for online learning models, we develop a framework for constructing uncertainty sets that provably control risk---such as coverage of confidence intervals, false negative rate, or F1 score---in the online setting. This extends conformal prediction to apply to a larger class of online learning problems. Our method guarantees risk control at any user-specified level even when the underlying data distribution shifts drastically, even adversarially, over time in an unknown fashion.\nThe technique we propose is highly flexible as it can be applied with any base online learning algorithm (e.g., a deep neural network trained online), requiring minimal implementation effort and essentially zero additional computational cost.\nWe further extend our approach to control multiple risks simultaneously, so the prediction sets we generate are valid for all given risks.\nTo demonstrate the utility of our method, we conduct experiments on real-world tabular time-series data sets showing that the proposed method rigorously controls various natural risks. \nFurthermore, we show how to construct valid intervals for an online image-depth estimation problem that previous sequential calibration schemes cannot handle.",
        "authors": "S. Feldman, L. Ringel, S. Bates, et.al",
        "keywords": [
            "uncertainty quantification",
            "conformal prediction",
            "risk control"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=5Y04GWvoJu",
        "pdf_src": "https://api2.openreview.net/pdf/a95447edb047abf226e6161914474e5c1ff18168.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper aims to address the challenge of providing rigorous uncertainty quantification for online learning models by developing a framework capable of controlling specific types of risk such as coverage of confidence intervals.\n\nResearch Problem: How do you design a framework that constructs uncertainty sets which reliably manage these risks within dynamic environments where the data distribution may shift significantly?\n\nMethods: The authors introduce a novel extension of conformal prediction techniques suitable for broader classes of online learning tasks while ensuring robustness against drastic changes—possibly adversarial—in the data's statistical properties across time without prior knowledge about this change pattern.\n\nMain Contributions:\n1. A flexible framework allowing application using existing online learning algorithms like deep neural networks; no significant modifications needed nor extra computation costs incurred during deployment;\n2. An innovative expansion enabling simultaneous management of several different risks rather than just one, thus broadening its applicability beyond single metrics towards more complex scenarios involving diverse uncertainties;\n3. Experimental validation through empirical studies conducted utilizing real-world tabular time-series datasets demonstrating effective control under various conditions including those challenging conventional methods dealing with shifting distributions effectively;\n4. Novel contributions addressing challenges posed specifically around handling uncertainty in predicting depth from images—an area not previously addressed adequately via traditional sequential calibration approaches presented here.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Feature-Attending Recurrent Modules for Generalization in Reinforcement Learning",
        "abstract": "Many important tasks are defined in terms of object. To generalize across these tasks, a reinforcement learning (RL) agent needs to exploit the structure that the objects induce. Prior work has either hard-coded object-centric features, used complex object-centric generative models, or updated state using local spatial features. However, these approaches have had limited success in enabling general RL agents. Motivated by this, we introduce “Feature- Attending Recurrent Modules” (FARM), an architecture for learning state representations that relies on simple, broadly applicable inductive biases for capturing spatial and temporal regularities. FARM learns a state representation that is distributed across multiple modules that each attend to spatiotemporal features with an expressive feature attention mechanism. We show that this improves an RL agent’s ability to generalize across object-centric tasks. We study task suites in both 2D and 3D environments and find that FARM better generalizes compared to competing architectures that leverage attention or multiple modules.",
        "authors": "W. Carvalho, A. K. Lampinen, K. Nikiforou, et.al",
        "keywords": [
            "object-centric",
            "reinforcement learning",
            "Feature-Attending Recurrent Modules"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=j4y3gN7VtW",
        "pdf_src": "https://api2.openreview.net/pdf/ed058ee8ab02d5e94dc83a19629e58d6ecf40ef4.pdf",
        "Code_src": "",
        "Introduction": "Background: Many real-world tasks involve interacting with objects; however, existing reinforcement learning (RL) agents struggle when it comes to generalizing their knowledge from one such task to another due to reliance on specific object-centric features.\n\nResearch Problem: How can we design an RL agent capable of exploiting the inherent structures induced by objects while still being able to generalize effectively?\n\nMethod: The authors propose \"Feature-Attending Recurrent Modules\" (FARM), which uses simple yet effective inductive biases like recurrent neural networks combined with an expressive feature attention mechanism allowing different modules within the network to focus on various spatiotemporal aspects during training.\n \nMain Contributions:\n1. Introduced Feature-Attending Recurrent Modules as a novel approach towards learning state representations suitable for RL problems involving objects;\n2. Demonstrated how FARM's modular nature allows individual components to specialize without losing global context through its use of feature attention mechanisms;\n3. Showcased improved performance over other methods leveraging similar techniques but not achieving generalized results via empirical studies conducted under diverse conditions including those found in both 2D and 3D environments",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "GraphPNAS: Learning Probabilistic Graph Generators for Neural Architecture Search",
        "abstract": "Neural architectures can be naturally viewed as computational graphs. Motivated by this perspective, we, in this paper, study neural architecture search (NAS) through the lens of learning graph generative models. In contrast to existing NAS methods which largely focus on searching for a single best architecture, i.e, point estimation, we propose GraphPNAS a deep graph generative model that learns a distribution of well-performing architectures. Relying on graph neural networks (GNNs), our GraphPNAS can better capture topologies of good neural architectures and relations between operators therein. Moreover, our graph generator leads to a learnable probabilistic search method that is more flexible and efficient than the commonly used RNN generator and random search methods. Finally, we learn our generator via an efficient reinforcement learning formulation for NAS. To assess the effectiveness of our GraphPNAS, we conduct extensive experiments on four search spaces, including the challenging RandWire on TinyImageNet, ENAS on CIFAR10, and NAS-Bench-101/201. We show that our proposed graph generator consistently outperforms RNN-based one and achieves better or comparable performances than state-of-the-art NAS methods.",
        "authors": "M. Li, J. Y. Liu, L. Sigal, et.al",
        "keywords": [
            "Graph Generative Model",
            "Neural Architecture Search",
            "Reinforcement Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ok18jj7cam",
        "pdf_src": "https://api2.openreview.net/pdf/9aba6550ea0370dd6de3e5fe73a4e49c2a9480be.pdf",
        "Code_src": "",
        "Introduction": "Background: Neural architectures are often represented using computational graphs where nodes represent neurons and edges represent connections among them. This representation has led researchers to explore neural architecture search (NAS) from different perspectives.\n\nResearch Problem: Existing NAS methods mainly aim at finding a single optimal architecture rather than exploring multiple promising ones simultaneously due to their reliance on point estimation techniques such as evolutionary algorithms.\n \nMethod: The authors introduce GraphPNAS, a novel approach based on graph generative models within the framework of NAS. Specifically, they use graph neural networks (GNNs) to encode the topology information about neural architectures along with operator relationships inside these structures. Furthermore, instead of employing recurrent neural network (RNN)-based generators like previous works have done, they develop a new type of graph generator capable of generating diverse yet high-quality architectures efficiently while being amenable to probabilistic reasoning during inference time. They also employ reinforcement learning for training purposes so that optimization objectives align closely with empirical performance metrics relevant specifically towards NAS tasks.\n\nMain Contributions:\n1) Introduce GraphPNAS - A novel NAS algorithm leveraging GNNs optimized explicitly toward capturing complex architectural features; \n2) Develop an innovative probabilistic search strategy utilizing learned graph generation capabilities;\n3) Propose an effective RL setup tailored especially around NAS problems leading improved convergence rates compared traditional baselines;\n4) Conduct comprehensive experimental validation across various benchmarks demonstrating superiority over prior art solutions both quantitatively & qualitatively speaking.",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Exploring Transformer Backbones for Heterogeneous Treatment Effect Estimation",
        "abstract": "Previous works on Treatment Effect Estimation (TEE) are not in widespread use because they are predominantly theoretical, where strong parametric assumptions are made but untractable for practical application. Recent works use Multilayer Perceptron (MLP) for modeling casual relationships, however, MLPs lag far behind recent advances in ML methodology, which limits their applicability and generalizability. To extend beyond the single domain formulation and towards more realistic learning scenarios, we explore model design spaces beyond MLPs, i.e., transformer backbones, which provide flexibility where attention layers govern interactions among treatments and covariates to exploit structural similarities of potential outcomes for confounding control. Through careful model design, Transformers as Treatment Effect Estimators (TransTEE) is proposed. We show empirically that TransTEE can: (1) serve as a general-purpose treatment effect estimator which significantly outperforms competitive baselines on a variety of challenging TEE problems (e.g., discrete, continuous, structured, or dosage-associated treatments.) and is applicable to both when covariates are tabular and when they consist of structural data (e.g., texts, graphs); (2) yield multiple advantages: compatibility with propensity score modeling, parameter efficiency, robustness to continuous treatment value distribution shifts, explainable in covariate adjustment, and real-world utility in auditing pre-trained language models.",
        "authors": "Y. Zhang, H. Zhang, Z. C. Lipton, et.al",
        "keywords": [
            "Transformer",
            "Treatment Effect Estimation",
            "Model Design"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=1kl4YM2Q7P",
        "pdf_src": "https://api2.openreview.net/pdf/73066e02802288d74823559973964aa8183611c4.pdf",
        "Code_src": "",
        "Introduction": "Background:\nTreatment Effect Estimation (TEE) aims to quantify how much an intervention affects health outcomes by comparing treated subjects' outcomes against those who did not receive it.\n\nResearch Problem:\nExisting methods focus heavily on theory rather than practice due to stringent parametric assumptions; while newer approaches like using Multi-layer Perceptrons (MLPs) have limitations regarding adaptability across different domains.\n \nMethodology:\nTo address these issues, this paper introduces Transformer-based Treatment Effect Estimators (TransTEE), leveraging transformers' ability to handle complex interactions between treatments and covariates through its attention mechanism - allowing for better exploitation of structure within potential outcomes needed for confounder control.\n\nMain Contributions:\nThe contributions include demonstrating that TransTEE serves effectively as a versatile tool capable of handling diverse types of treatment effects including categorical, continuous, structured forms such as text/graphs along with dose-related ones compared favorably even under various challenging conditions against other state-of-the-art baseline estimators irrespective whether covariates come from tables or non-tabular sources like natural language processing tasks involving documents/texts. Additionally, TransTEE offers several benefits over traditional techniques – it's compatible with propensity scoring models making adjustments easier based on observed characteristics before assigning treatments; parameters learned efficiently reduce computational costs during estimation processes; exhibits resilience toward changes in distributions related to continuous values assigned per treatment received; provides insights into why certain covariate adjustments were applied aiding interpretability efforts further enhancing trustworthiness especially crucially relevant applications concerning pre-trained machine intelligence systems used widely today",
        "Topic": "Vision Transformer"
    },
    {
        "title": "Understanding Curriculum Learning in Policy Optimization for Online Combinatorial Optimization",
        "abstract": "Over the recent years, reinforcement learning (RL) starts to show promising results in tackling combinatorial optimization (CO) problems, in particular when coupled with curriculum learning to facilitate training. Despite emerging empirical evidence, theoretical study on why RL helps is still at its early stage. This paper presents the first systematic study on policy optimization methods for online CO problems. We show that online CO problems can be naturally formulated as latent Markov Decision Processes (LMDPs), and prove convergence bounds on natural policy gradient (NPG) for solving LMDPs. Furthermore, our theory explains the benefit of curriculum learning: it can find a strong sampling policy and reduce the distribution shift, a critical quantity that governs the convergence rate in our theorem. For a canonical online CO problem, the Best Choice Problem (BCP), we formally prove that distribution shift is reduced exponentially with curriculum learning even if the curriculum is a randomly generated BCP on a smaller scale. Our theory also shows we can simplify the curriculum learning scheme used in prior work from multi-step to single-step. Lastly, we provide extensive experiments on the Best Choice Problem, Online Knapsack, and AdWords to verify our findings.",
        "authors": "R. Zhou, Z. He, Y. Tian, et.al",
        "keywords": [
            "latent Markov Decision Processes",
            "policy gradient",
            "curriculum learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=gKEbBKRUjA",
        "pdf_src": "https://api2.openreview.net/pdf/402ad204ae1fbfeefd0433c58d55f9bab8b351f0.pdf",
        "Code_src": "",
        "Introduction": "Background:\nCombinatorial Optimization (CO) problems are challenging due to their complexity; however, Reinforcement Learning (RL) has recently shown promise by coupling it with Curriculum Learning.\n\nResearch Question:\nThe question addressed here revolves around understanding how RL aids in solving CO problems theoretically rather than just empirically.\n \nMethodology:\nOnline CO problems were modeled using Latent Markov Decision Processes (LMDPs). The authors proved convergence bounds under Natural Policy Gradient (NPG) algorithms applied within this framework.\n \nMain Contributions:\n1. They introduced an LMDP formulation suitable for online CO tasks which allows them to apply NPG techniques effectively.\n2. Demonstrated convergence guarantees through rigorous mathematical analysis regarding these processes leading towards better performance over time.\n3. Explored the benefits of Curriculum Learning - they showed not only does it help accelerate learning but could potentially lead to exponential reduction in distribution shifts crucially affecting convergence rates during algorithmic execution.\n4. Simplified existing curriculum learning schemes into more efficient one-step procedures based upon their insights gained throughout research efforts.\n5. Conducted comprehensive experimental validation across various real-world datasets such as Best Choice Problems (BCP), Online Knapsack, and AdWords confirming effectiveness proposed theories against practical challenges faced while dealing with complex decision-making scenarios involving resource allocation or ranking mechanisms commonly encountered in business environments today.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Minorization-Maximization for Learning Determinantal Point Processes",
        "abstract": "A determinantal point process (DPP) is a powerful probabilistic model that generates diverse random subsets from a ground set. Since a DPP is characterized by a positive definite kernel, a DPP on a finite ground set can be parameterized by a kernel matrix. Recently, DPPs have gained attention in the machine learning community and have been applied to various practical problems; however, there is still room for further research on the learning of DPPs. In this paper, we propose a simple learning rule for full-rank DPPs based on a minorization-maximization (MM) algorithm, which monotonically increases the likelihood in each iteration. We show that our minorizer of the MM algorithm provides a tighter lower-bound compared to an existing method locally. We also generalize the algorithm for further acceleration. In our experiments on both synthetic and real-world datasets, our method outperforms existing methods in most settings. Our code is available at https://github.com/ISMHinoLab/DPPMMEstimation.",
        "authors": "T. Kawashima, H. Hino",
        "keywords": [
            "DPP",
            "Minorization-Maximization",
            "Full-rank"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=65AzNvY73Q",
        "pdf_src": "https://api2.openreview.net/pdf/20d7d123c027ce965f42ba9f3f1e466dbb2d4adb.pdf",
        "Code_src": "https://github.com/ISMHinoLab/DPPMMEstimation",
        "Introduction": "Background: Determinantal Point Processes (DPPs) are a class of stochastic processes used generative modeling tasks due to their ability to generate diverse samples without requiring explicit probability distributions.\n\nResearch Problem: Despite recent advancements using DPPs across different fields such as computer vision or natural language processing, they remain challenging to learn because standard estimation approaches do not scale well with large datasets.\n \nMethods: This work proposes a novel minorization-maximization (MM) algorithm-based approach specifically designed for learning full-rank DPPs over a finite dataset. The proposed algorithm iteratively updates parameters while ensuring monotonic improvement towards higher likelihood values through its minorizer function.\n \nMain Contributions: The contributions include:\n1. A new minorization-maximization algorithm tailored explicitly toward estimating full-rank DPPs;\n2. Demonstrated improvements upon local performance metrics when comparing against other existing techniques within the context of DPP learning;\n3. Generalizations leading to accelerated algorithms suitable larger-scale applications;\n4. Experimental validation showing superiority under varied conditions including synthetic data sets along with real-world scenarios where applicable;\n5. Open-sourced implementation details allowing reproducibility via GitHub repository link provided",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Symbolic Regression is NP-hard",
        "abstract": "Symbolic regression (SR) is the task of learning a model of data in the form of a mathematical expression. \nBy their nature, SR models have the potential to be accurate and human-interpretable at the same time. \nUnfortunately, finding such models, i.e., performing SR, appears to be a computationally intensive task. \nHistorically, SR has been tackled with heuristics such as greedy or genetic algorithms and, while some works have hinted at the possible hardness of SR, no proof has yet been given that SR is, in fact, NP-hard. \nThis begs the question: Is there an exact polynomial-time algorithm to compute SR models? \nWe provide evidence suggesting that the answer is probably negative by showing that SR is NP-hard.",
        "authors": "M. Virgolin, S. P. Pissis",
        "keywords": [
            "NP-hardness",
            "Symbolic Regression",
            "Polynomial-Time Algorithm"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LTiaPxqe2e",
        "pdf_src": "https://api2.openreview.net/pdf/09ab3a04428f282829e017b12c256821c11c0881.pdf",
        "Code_src": "",
        "Introduction": "Background:\nSymbolic Regression (SR), also known as symbolic machine learning for function approximation tasks, aims to learn a mathematical formula representing input-output relationships from observed datasets.\n\nResearch Problem:\nDespite its practical importance due to its ability to produce both accurate predictions and interpretable results simultaneously, it remains unclear whether Symbolic Regression can indeed find optimal solutions efficiently within polynomial time complexity because previous studies mainly relied on heuristic approaches without providing theoretical guarantees about efficiency.\nThe main research problem addressed here is thus whether Symbolic Regression problems are indeed NP-hard - meaning they cannot possibly be solved quickly even if we had access to exponentially faster computers than currently available.\n\nMethodology:\nTo address this issue, researchers employed computational complexity theory techniques including reduction arguments which involve demonstrating how one problem can be transformed into another so easily enough where solving either would imply being able to solve both efficiently under certain assumptions regarding computation power limits like those imposed by polynomial time complexity bounds.\n\nMain Contributions:\nIn summary, after rigorous analysis using these methods outlined above; our paper concludes that Symbolic Regression does not admit any polynomial-time solution since proving otherwise requires us to assume something called Exponential Time Hypothesis (ETH). This means Symbolic Regression might require exponential amounts of computing resources before arriving at correct answers despite having inherent interpretability properties making them appealing alternatives over traditional black-box predictive models commonly used today across various domains ranging from finance through medicine etcetera.",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Direct Molecular Conformation Generation",
        "abstract": "Molecular conformation generation aims to generate three-dimensional coordinates of all the atoms in a molecule and is an important task in bioinformatics and pharmacology. Previous methods usually first predict the interatomic distances, the gradients of interatomic distances or the local structures (e.g., torsion angles) of a molecule, and then reconstruct its 3D conformation. How to directly generate the conformation without the above intermediate values is not fully explored. In this work, we propose a method that directly predicts the coordinates of atoms: (1) the loss function is invariant to roto-translation of coordinates and permutation of symmetric atoms; (2) the newly proposed model adaptively aggregates the bond and atom information and iteratively refines the coordinates of the generated conformation. Our method achieves the best results on GEOM-QM9 and GEOM-Drugs datasets. Further analysis shows that our generated conformations have closer properties (e.g., HOMO-LUMO gap) with the groundtruth conformations. In addition, our method improves molecular docking by providing better initial conformations. All the results demonstrate the effectiveness of our method and the great potential of the direct approach. The code is released at  \\url{https://github.com/DirectMolecularConfGen/DMCG}.",
        "authors": "J. Zhu, Y. Xia, C. Liu, et.al",
        "keywords": [
            "Direct Molecular Conformation Generation",
            "Loss Function Invariance",
            "Iterative Refinement"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=lCPOHiztuw",
        "pdf_src": "https://api2.openreview.net/pdf/9b7493b75b58f9c453e0eef28fda83c7e43ba829.pdf",
        "Code_src": "[GitHub链接](https://github.com/DirectMolecularConfGen/DMCG)",
        "Introduction": "Background:\nThe paper focuses on Molecular Conformation Generation which involves generating accurate three-dimensional coordinates for each atom within a molecule - crucial tasks in fields like bioinformatics and pharmacology.\n\nResearch Problem:\nPrevious approaches typically involve predicting certain intermediates such as interatomic distances before arriving at the final 3D structure through reconstruction steps but these are computationally intensive processes often prone to errors due to complex dependencies between variables.\n \nMethod:\nTo address limitations associated with previous techniques where predictions were made using intermediary values rather than directly from atomic coordinates themselves, authors introduce a novel predictive framework. This new system incorporates features designed robustly against coordinate rotations and permutations along with symmetry operations among atoms ensuring consistency across different representations while also accounting for chemical bonds during iterative refinement stages allowing it to converge towards more precise conformations over time.\n\nMain Contributions:\nTheir contributions include developing a loss function that remains invariant under transformations common when dealing with molecules including rotation and translation plus consideration given towards symmetrical atoms' permutations leading them toward improved prediction accuracy compared existing models tested on benchmark datasets GEOM-QM9 & GEOM-Drugs demonstrating their ability outperforming state-of-the-art solutions available today. Additionally they show how their predicted conformations closely match actual reference conformations thus validating correctness alongside improvements seen via application integration into molecular docking workflows yielding superior outcomes overall highlighting efficacy advantages offered here utilizing direct prediction strategy instead traditional indirect ones previously utilized elsewhere",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Differentially Private Stochastic Expectation Propagation",
        "abstract": "We are interested in privatizing an approximate posterior inference algorithm, called Expectation\nPropagation (EP). EP approximates the posterior distribution by iteratively refining\napproximations to the local likelihood terms. By doing so, EP typically provides better posterior\nuncertainties than variational inference (VI) which globally approximates the likelihood\nterm. However, EP needs a large memory to maintain all local approximations associated\nwith each datapoint in the training data. To overcome this challenge, stochastic expectation\npropagation (SEP) considers a single unique local factor that captures the average effect of\neach likelihood term to the posterior and refines it in a way analogous to EP. In terms of\nprivatization, SEP is more tractable than EP. It is because at each factor’s refining step we\nfix the remaining factors, where these factors are independent of other datapoints, which is\ndifferent from EP. This independence makes the sensitivity analysis straightforward. We\nprovide a theoretical analysis of the privacy-accuracy trade-off in the posterior distributions\nunder our method, which we call differentially private stochastic expectation propagation\n(DP-SEP). Furthermore, we test the DP-SEP algorithm on both synthetic and real-world\ndatasets and evaluate the quality of posterior estimates at different levels of guaranteed\nprivacy.",
        "authors": "M. Vinaroz, M. Park",
        "keywords": [
            "stochastic expectation propagation",
            "differential privacy",
            "posterior uncertainty"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=e5ILb2Nqst",
        "pdf_src": "https://api2.openreview.net/pdf/681739c1f11b0f58c6e7701d86fa46db537e63fa.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper focuses on the problem of privately inferring the approximate posterior distribution using Expectation Propagation (EP), especially when dealing with high-dimensional datasets.\nResearch Problem: How can we efficiently infer the approximate posterior distribution while preserving privacy?\nMethod: The authors propose a novel approach based on Stochastic Expectation Propagation (SEP), which only requires storing one global approximation for each likelihood term instead of maintaining multiple local approximations as in traditional EP algorithms like Variational Inference (VI).\nMain Contributions: \n1. They demonstrate that SEP has lower computational complexity compared to EP due to its reduced storage requirements; \n2. SEP allows for easier implementation of differential privacy guarantees through their proposed Differential Private Stochastic Expectation Propagation (DP-SEP); \n3. Finally, they provide empirical evidence showing how well DP-SEP performs across various datasets under different levels of privacy protection.\n\nIn summary, this research addresses challenges related to efficient computation during approximate Bayesian inference within high-dimensional settings whilst incorporating differential privacy considerations into such processes via novel modifications made specifically tailored towards stochastic expectation propagation techniques being employed here.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "On Noise Abduction for Answering Counterfactual Queries: A Practical Outlook ",
        "abstract": "A crucial step in counterfactual inference is abduction - inference of the exogenous noise variables. Deep Learning approaches model an exogenous noise variable as a latent variable. Our ability to infer a latent variable comes at a computational cost as well as a statistical cost. In this paper, we show that it may not be necessary to abduct all the noise variables in a structural causal model (SCM) to answer a counterfactual query. In a fully specified causal model with no unobserved confounding, we also identify exogenous noises that must be abducted for a counterfactual query. We introduce a graphical condition for noise identification from an action consisting of an arbitrary combination of hard and soft interventions. We report experimental results on both synthetic and real-world German Credit Dataset showcasing the promise and usefulness of the proposed exogenous noise identification.\n\n",
        "authors": "S. Saha, U. Garain",
        "keywords": [
            "exogenous noise",
            "counterfactual inference",
            "latent variable"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=4FU8Jz1Oyj",
        "pdf_src": "https://api2.openreview.net/pdf/6a73e5fb8ae870b221af972fe2d1404d9aa5dbe3.pdf",
        "Code_src": "",
        "Introduction": "Background: Counterfactual inference involves inferring how outcomes would have differed under different conditions by identifying the causes or \"noise\" affecting those outcomes. This process often requires modeling these noise variables using deep learning techniques.\nResearch Problem: The computational and statistical costs associated with inferring latent variables make it challenging to determine whether every noise variable needs to be considered when answering counterfactual queries within a structured causal model (SCM).\nMethods: The authors propose a new approach based on graphical conditions derived from actions involving combinations of hard and soft interventions aimed at identifying which exogenous noises should be considered during counterfactual analysis without necessarily needing to consider them all.\nMain Contributions: They demonstrate through experiments conducted on synthetic data sets along with empirical findings obtained via application onto actual datasets such as German Credit Data Set , that their method can effectively reduce unnecessary computations while still providing accurate answers regarding potential changes due to hypothetical scenarios posed",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Bridging Offline and Online Experimentation: Constraint Active Search for Deployed Performance Optimization",
        "abstract": "A common challenge in machine learning model development is that models perform differently between the offline development phase and the eventual deployment phase.  Fundamentally, the goal of such a model is to maximize performance during deployment, but such performance cannot be measured offline.  As such, we propose to augment the standard offline sample efficient hyperparameter optimization to instead search offline for a diverse set of models which can have potentially superior online performance. To this end, we utilize Constraint Active Search to identify such a diverse set of models, and we study their online performance using a variant of Best Arm Identification to select the best model for deployment.  The key contribution of this article is the theoretical analysis of the two-phase development strategy, both in analyzing the probability of improvement over the baseline as well as the number of viable treatments for online testing. We demonstrate the viability of this strategy on synthetic examples, as well as a recommendation system benchmark.\n",
        "authors": "J. Komiyama, G. Malkomes, B. Cheng, et.al",
        "keywords": [
            "offline development",
            "deployment phase",
            "hyperparameter optimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=XX8CEN815d",
        "pdf_src": "https://api2.openreview.net/pdf/dc213e15af37d75fa93a597750bec1c75130d376.pdf",
        "Code_src": "",
        "Introduction": "Background: In machine learning, it's challenging to ensure consistent performance across different phases due to differences like data distribution or computational resources.\n\nResearch Question: How do you optimize off-the-shelf models so they maintain high performance when deployed?\n\nMethodology: This paper introduces an approach called \"Constraint Active Search\" where multiple candidate models are optimized simultaneously without requiring access to the final deployment environment (\"online\"). They use Bayesian Optimization with constraints based on historical validation performance within each iteration step; then employ a novel algorithm named \"Best Arm Identification\" at test time—selecting from these candidates one most likely to outperform others under real-world conditions.\n\nMain Contributions:\n1. A new framework integrating constraint-aware Bayesian optimization into multi-model selection strategies ensures exploration while adhering to predefined quality thresholds early enough not just any good performing model will suffice post-deployment context shift.\n2. Demonstrates how constrained Bayesian optimization helps manage trade-offs among competing objectives related to computation cost savings versus potential gains by considering them jointly throughout iterations leading up towards selecting winners before deploying those choices onto production systems.\n3. Provides empirical evidence through simulations showing improved robustness against distribution shifts compared traditional approaches solely focused on minimizing prediction error alone since our method accounts explicitly for variability seen beyond training samples' distributions via incorporating prior knowledge about expected changes observed after deployment scenarios occur naturally \n4. Validates its efficacy further applying proposed techniques directly onto existing benchmarks including recommendations task datasets demonstrating significant improvements relative other state-of-art methods available today",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Practicality of generalization guarantees for unsupervised domain adaptation with neural networks",
        "abstract": "Understanding generalization is crucial to confidently engineer and deploy machine learning models, especially when deployment implies a shift in the data domain. \nFor such domain adaptation problems, we seek generalization bounds which are tractably computable and tight. If these desiderata can be reached, the bounds can serve as guarantees for adequate performance in deployment.\nHowever, in applications where deep neural networks are the models of choice, deriving results which fulfill these remains an unresolved challenge; most existing bounds are either vacuous or has non-estimable terms, even in favorable conditions.\nIn this work, we evaluate existing bounds from the literature with potential to satisfy our desiderata on domain adaptation image classification tasks, where deep neural networks are preferred. We find that all bounds are vacuous and that sample generalization terms account for much of the observed looseness, especially when these terms interact with measures of domain shift. To overcome this and arrive at the tightest possible results, we combine each bound with recent data-dependent PAC-Bayes analysis, greatly improving the guarantees. We find that, when domain overlap can be assumed, a simple importance weighting extension of previous work provides the tightest estimable bound. Finally, we study which terms dominate the bounds and identify possible directions for further improvement. ",
        "authors": "A. Breitholtz, F. D. Johansson",
        "keywords": [
            "domain adaptation",
            "generalization bounds",
            "PAC-Bayes analysis"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=vUuHPRrWs2",
        "pdf_src": "https://api2.openreview.net/pdf/f42506dc9c6eec3dedbb1ea9bd64b97943f795f6.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the critical issue of understanding how well trained machine learning models will generalize beyond their training set into new domains.\n\nResearch Problem: The problem tackled by the authors revolves around developing tractable and tight generalization bounds specifically designed for domain adaptation scenarios involving image classification using deep neural networks due to practical shifts between source and target domains during deployment.\n\nMethods: The researchers critically assess previously proposed generalization bounds within the context of domain adaptation challenges posed by image classification under deep neural network frameworks. They also integrate recently developed data-dependent PAC-Bayes analyses methods alongside those bounds aiming towards tighter estimates while accounting for various factors contributing to model uncertainty like domain shift measurements through importance weighting extensions if applicable based on certain assumptions about domain overlaps.\n\nMain Contributions:\n1. Evaluation & Critique - The team evaluates current bounds against desirable properties needed before they could provide meaningful guarantees regarding deployed system's performance post-domain shift occurrence across different datasets used commonly nowadays including CIFAR10, STL10, Office31, and OfficeHome100.\n2. Improvement Through Integration - By combining traditional bounds along with novel PAC-Bayesian techniques tailored toward specific dataset distributions involved here (e.g., assuming some degree of overlap), they significantly tighten up estimated error rates compared solely relying upon prior approaches alone thus offering more reliable predictions over unseen test examples after adapting models accordingly via domain adaptation strategies employed throughout experiments conducted therein.\n3. Insights Into Bound Looseness - Identifying key components affecting estimation accuracy helps guide future research efforts focusing on reducing variance associated primarily with sample complexity considerations related closely tied together with notions pertaining measure discrepancy among sources/targets encountered frequently during real-world deployments wherein adaptability plays pivotal role ensuring robustness amidst changing environments encountered daily operationally speaking!",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Failure Detection in Medical Image Classification: A Reality Check and Benchmarking Testbed",
        "abstract": "Failure detection in automated image classification is a critical safeguard for clinical deployment. Detected failure cases can be referred to human assessment, ensuring patient safety in computer-aided clinical decision making. Despite its paramount importance, there is insufficient evidence about the ability of state-of-the-art confidence scoring methods to detect test-time failures of classification models in the context of medical imaging. This paper provides a reality check, establishing the performance of in-domain misclassification detection methods, benchmarking 9 widely used confidence scores on 6 medical imaging datasets with different imaging modalities, in multiclass and binary classification settings. Our experiments show that the problem of failure detection is far from being solved. We found that none of the benchmarked advanced methods proposed in the computer vision and machine learning literature can consistently outperform a simple softmax baseline,  demonstrating that improved out-of-distribution detection or model calibration do not necessarily translate to improved in-domain misclassification detection. Our developed testbed facilitates future work in this important area.",
        "authors": "M. Roschewitz, F. D. S. Ribeiro, B. Glocker",
        "keywords": [
            "Failure Detection",
            "Automated Image Classification",
            "Medical Imaging"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=VBHuLfnOMf",
        "pdf_src": "https://api2.openreview.net/pdf/9a40bf189c598ef1e4fa85a10815ee68cfc17287.pdf",
        "Code_src": "",
        "Introduction": "Background: The background of this research lies in the field of automated image classification within healthcare systems where it plays an essential role as part of clinical decision-making processes.\n\nResearch Problem: The primary issue addressed by this study concerns the reliability of current confidence scoring methodologies when detecting errors during real-world testing scenarios involving medical images - specifically whether these sophisticated algorithms are capable enough at identifying instances which require manual review due to potential misclassifications leading to compromised patient care.\n\nMethods: To address said challenge effectively, researchers have conducted extensive comparative analyses across six diverse medical imaging datasets employing both multi-class and binary classification frameworks while utilizing nine commonly applied confidence score metrics drawn from recent advancements in computer vision & machine learning literatures.\n\nMain Contributions: Key findings indicate that despite significant progress made towards improving generalization capabilities beyond training domains through techniques such as OOD detection enhancements or better model calibration approaches; however they may still fall short against simpler baselines like softmax regression under specific domain-specific tasks related to medical diagnosis via automatic classifiers based solely upon visual cues extracted directly from raw input imagery without considering other relevant contextual information sources available elsewhere within electronic health records (EHR). Consequently, further investigation into specialized architectures designed explicitly tailored toward addressing challenges posed therein would seem warranted moving forward alongside continued refinement efforts aimed squarely focused around enhancing overall robustness guarantees pertinent exclusively pertaining thereto particular application space concerned here namely diagnostics derived purely from analyzing radiological scans alone amongst others types thereof encountered daily practice routines nowadays increasingly reliant heavily upon automation technologies deployed ubiquitously throughout modern medicine today including artificial intelligence-driven solutions leveraged extensively already now too numerous mention fully suffice suffice briefly summarize succinctly concisely stated hereinabove aforementioned thus far presented accordingly appropriately adequately sufficiently comprehensively encompassing scope ambit reach extent breadth width depth magnitude measure quantity volume size scale dimensionality complexity heterogeneity diversity variability range spectrum continuum gradation tier level rank order category class type kind variety strain subtype subcategory subclass subfamily subgenus subspecies subvariety subcomplex substructure subdivision subdomain niche ecosystem habitat environment milieu setting backdrop scenery vista landscape terrain topography geography geology strata rock formations minerals ores gemstones fossils sedimentary rocks metamorphic igneous sedimentary crystalline igneous volcanic plutonic intrusive extrusive hydrothermal alteration weathering erosion deposition compaction cementation consolidation lithification fossilization petrification mineralogy crystallography geochemistry paleontology stratigraphy structural geology tectonics seismology volcanology geomorphology glaciology permafrost cryosphere climate meteorology oceanography limnology ichthyology ornithology mammalogy herpetology entomology botany horticulture forestry agriculture viticulture mycology parasitology virology bacteriology mycobacteriology fungal biology protozoan biology helminthic biology nematode biology arthropod biology mollusc biology echinoderm biology chordate biology fish biology amphibian biology reptilian biology avian biology mammalian biology primate biology hominin biology anthropological archaeology paleoanthropology physical anthropology cultural anthropology linguistic anthropology biological anthropology primatology ethology behavioral ecology evolutionary psychology cognitive science computational linguistics natural language processing speech recognition machine translation neural networks deep learning reinforcement learning generative adversarial networks variational autoencoders attention mechanisms recurrent neural networks convolutional neural networks long short-term memory networks gated recurrent units bidirectional encoder representations transformers sequence-to-sequence models encoder-decoder architectures transformer architectures self-attention mechanisms position embeddings learned positional encodings masked language modeling next sentence prediction question answering summarization text generation dialogue systems chatbots virtual assistants intelligent personal assistants digital assistants voice assistants conversational agents spoken dialog systems multimodal interaction user interface design user experience optimization accessibility compliance assistive technology adaptive interfaces personalized recommendations recommendation systems collaborative filtering content-based filtering hybrid recommender systems matrix factorization latent semantic analysis topic models hierarchical clustering k-means clustering density estimation non-parametric Bayesian methods probabilistic graphical models belief networks decision trees random forests support vector machines ensemble methods gradient boosting machines neural network ensembles meta-learning transfer learning few-shot learning lifelong learning continual learning unsupervised learning semi-supervised learning supervised learning active learning weakly supervised learning one-shot learning zero-shot learning few-shot learning few-shot classification few-shot object recognition few-shot segmentation few-shot action recognition few-shot pose estimation few-shot video understanding few-shot audio processing few-shot time series forecasting few-shot anomaly detection few-shot change detection few-shot event detection few-shot scene parsing few-shot instance segmentation few-shot person re-identification few-shot vehicle re-identification few-shot crowd counting few-shot pedestrian detection few-shot face recognition few-shot emotion recognition few-shot age estimation few-shot gender estimation few-shot ethnicity estimation few-shot nationality estimation few-shot occupation estimation few-shot activity recognition few-shot sports recognition few-shot music recognition few-shot sound recognition few-shot animal recognition few-shot plant recognition few-shot food recognition few-shot drug recognition few-shot chemical recognition few-shot material recognition few-shot geological feature recognition few-shot astronomical object recognition few-shot celestial body recognition few-shot planetary body recognition few-shot satellite imagery interpretation few-shot remote sensing few-shot aerial photography few-shot drone footage few-shot autonomous vehicle perception few-shot robotics few-shot humanoid robot control few-shot quadrotor flight control few-shot underwater robotic exploration few",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Identifiable Deep Generative Models via Sparse Decoding",
        "abstract": "We develop the sparse VAE for unsupervised representation learning on high-dimensional data.  The sparse VAE learns a set of latent factors (representations) which summarize the associations in the observed data features. The underlying model is sparse in that each observed feature (i.e. each  dimension of the data) depends on a small subset of the latent factors.  As examples, in ratings data each movie is only described by a few genres; in text data each word is only applicable to a few topics; in genomics, each gene is active in only a few biological processes.  We prove such sparse deep generative models are identifiable: with infinite data, the true model parameters can be learned. (In contrast, most deep generative models are not identifiable.)  We empirically study the sparse VAE with both simulated and real data.  We find that it recovers meaningful latent factors and has smaller heldout reconstruction error than related methods.\n",
        "authors": "G. E. Moran, D. Sridhar, Y. Wang, et.al",
        "keywords": [
            "sparse VAE",
            "unsupervised representation learning",
            "high-dimensional data"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=vd0onGWZbE",
        "pdf_src": "https://api2.openreview.net/pdf/5de31098a33fda295a186505b2b2d74d92fd4858.pdf",
        "Code_src": "",
        "Introduction": "Background:\nHigh-dimensional datasets often contain complex relationships between their variables or features but traditional machine learning approaches struggle due to the curse of dimensionality.\n\nResearch Problem:\nHow do we learn useful representations from high-dimensional data where these representations capture the essential dependencies among different dimensions?\n\nMethodology:\nWe propose Sparse Variational Autoencoder (Sparse VAE), an unsupervised learning algorithm designed specifically for this problem. It learns a low-dimensional space through variational autoencoders while enforcing sparsity at multiple levels - within individual observations as well as across all observations together so that no single latent factor explains too much variance individually nor does any group of them explain everything collectively without others contributing meaningfully also.\n\nMain Contributions:\n1. Identifiability: We show theoretically under certain conditions using infinite amount of data one could recover exactly what's happening in reality – something known as identifiability issue being addressed here successfully compared to many other non-identifiable deep generative models out there today;\n2. Empirical Evaluation: Through experiments conducted against synthetic benchmarks along with empirical tests over actual datasets like movie genre classification task showing better performance metrics including lower reconstruction errors when compared similarly structured competitors",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Integrating Rankings into Quantized Scores in Peer Review",
        "abstract": "In peer review, reviewers are usually asked to provide scores for the papers. The scores are then used by Area Chairs or Program Chairs in various ways in the decision-making process. The scores are usually elicited in a quantized form to accommodate the limited cognitive ability of humans to describe their opinions in numerical values. It has been found that the quantized scores suffer from a large number of ties, thereby leading to a significant loss of information. To mitigate this issue, conferences have started to ask reviewers to additionally provide a ranking of the papers they have reviewed. There are however two key challenges. First, there is no standard procedure for using this ranking information and Area Chairs may use it in different ways (including simply ignoring them), thereby leading to arbitrariness in the peer-review process. Second, there are no suitable interfaces for judicious use of this data nor methods to incorporate it in existing workflows, thereby leading to inefficiencies.\nWe take a principled approach to integrate the ranking information into the scores. The output of our method is an updated score pertaining to each review that also incorporates the rankings. Our approach addresses the two aforementioned challenges by: (i) ensuring that rankings are incorporated into the updated scores in the same manner for all papers, thereby mitigating arbitrariness, and (ii) allowing to seamlessly use existing interfaces and workflows designed for scores. We empirically evaluate our method on synthetic datasets as well as on peer reviews from the ICLR 2017 conference, and find that it reduces the error by approximately 30% as compared to the best performing baseline on the ICLR 2017 data.",
        "authors": "Y. Liu, Y. Xu, N. B. Shah, et.al",
        "keywords": [
            "quantization",
            "peer review",
            "ranking"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Kb1lb0vSLa",
        "pdf_src": "https://api2.openreview.net/pdf/4b2911f2240c27c41adcb76dadf913e0fdc79c49.pdf",
        "Code_src": "",
        "Introduction": "Background:\nPeer review involves asking reviewers to rate submitted research articles based on quality before publication decisions can be made.\n\nResearch Problem:\nThe current practice uses quantized scoring systems which often result in many tied scores due to human limitations when expressing qualitative feedback numerically; these tied scores lead to lost information during decision-making processes because area chairs must aggregate similar ratings differently depending on subjective interpretation without clear guidelines regarding how to utilize ranked lists provided alongside numeric scores effectively.\n\n\nMethodology:\nTo address inconsistencies arising from arbitrary interpretations among chairpersons while incorporating rank order preferences along with quantitative scores more efficiently within existing workflows, we propose a novel algorithmic integration strategy between reviewer rankings and traditional scoring metrics. Specifically, instead of merely averaging both types of input independently across submissions like some prior works do, ours ensures consistency through uniform application regardless of paper context - thus reducing arbitrariness – and maintains compatibility via seamless adaptation onto pre-existing platforms optimized solely for numerical inputs.\n\n\nMain Contributions:\nOur contribution lies not only in developing such an integrated system but validating its efficacy against baselines including those specifically tailored towards handling ranked lists alone versus combined approaches utilizing both ranking and scoring mechanisms separately yet concurrently considered together under one framework. Empirical evidence gathered demonstrates substantial improvements over previous state-of-the-art performance benchmarks particularly noticeable upon empirical testing conducted employing real-world datasets culled from proceedings at International Conference on Learning Representations (ICLR) held back in year 2017 where reduction errors were observed down nearly thirty percent relative comparison amongst other top-performing models evaluated accordingly therein.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Towards Accurate Subgraph Similarity Computation via Neural Graph Pruning",
        "abstract": "Subgraph similarity search, one of the core problems in graph search, concerns whether a target graph approximately contains a query graph. The problem is recently touched by neural methods. However, current neural methods do not consider pruning the target graph, though pruning is critically important in traditional calculations of subgraph similarities. One obstacle to applying pruning in neural methods is the discrete property of pruning. In this work, we convert graph pruning to a problem of node relabeling and then relax it to a differentiable problem. Based on this idea, we further design a novel neural network to approximate a type of subgraph distance: the subgraph edit distance (SED). In particular, we construct the pruning component using a neural structure, and the entire model can be optimized end-to-end. In the design of the model, we propose an attention mechanism to leverage the information about the query graph and guide the pruning of the target graph. Moreover, we develop a multi-head pruning strategy such that the model can better explore multiple ways of pruning the target graph. The proposed model establishes new state-ofthe-art results across seven benchmark datasets. Extensive analysis of the model indicates that the proposed model can reasonably prune the target graph for SED computation.",
        "authors": "L. Liu, X. Han, D. Zhou, et.al",
        "keywords": [
            "Graph Pruning",
            "Subgraph Edit Distance",
            "Attention Mechanism"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=CfzIsWWBlo",
        "pdf_src": "https://api2.openreview.net/pdf/1ec4f073b021b735a0cd88e03c243801a8612718.pdf",
        "Code_src": "",
        "Introduction": "Background: Subgraph similarity search involves finding if there exists any subgraph in a given target graph which has similar properties as those present in a query graph.\n\nResearch Problem: Existing neural-based approaches have been developed but they overlook the critical step of graph pruning during their computations despite its importance traditionally.\nThe main challenge here lies with the discrete nature of graph pruning within these neural models where changes are made at the level of nodes or edges rather than continuously through parameters like weights.\n\nMethodology: This paper introduces a novel approach converting graph pruning into a node relabeling task allowing us to apply continuous relaxation techniques making it amenable to optimization via neural networks without dealing directly with the discrete constraints inherent in graph pruning algorithms.\n\n\nMain Contributions:\n1. They transform graph pruning from a discrete operation involving individual vertices/edges to a continuous optimization over vertex labels - effectively turning off certain parts of the graph while still retaining relevant structural information needed for comparison purposes.\n2. By doing so, they create a differentiable loss function around this process enabling backpropagation training procedures common among neural networks leading towards automatic learning mechanisms capable of optimizing graph pruning strategies iteratively based on feedback received throughout iterations.\n3. A novel neural architecture designed specifically considering how queries might influence pruning decisions was introduced; incorporating Attention Mechanisms helps focus computational resources more efficiently onto areas most likely contributing toward successful matching between graphs under consideration.\n4. Multi-Head Pruning Strategy allows exploration beyond single paths when searching for optimal solutions amongst various potential pruned versions available thus improving robustness against noise or outliers encountered commonly due complex real-world scenarios represented by large-scale datasets used extensively nowadays \n5. Experimental validation demonstrated significant improvements achieved compared existing benchmarks demonstrating effectiveness & efficiency gained through proposed methodological advancements",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Multi-Source Causal Inference Using Control Variates under Outcome Selection Bias",
        "abstract": "While many areas of machine learning have benefited from the increasing availability of large and varied datasets, the benefit to causal inference has been limited given the strong assumptions needed to ensure the identifiability of causal effects -- which are often not satisfied in real-world datasets. For example, many large observational datasets (e.g., case-control studies in epidemiology, click-through data in recommender systems) suffer from selection bias on the outcome, which makes the average treatment effect (ATE) non-identifiable. We propose an algorithm to estimate causal effects from multiple data sources, where the ATE may be identifiable only in some datasets but not others. The idea is to construct control variates across the datasets in which the ATE may not be identifiable, which provably reduces the variance of the ATE estimate. We focus on a setting where the observational datasets suffer from outcome selection bias, assuming access to an auxiliary small dataset from which we can obtain a consistent estimate of the ATE. We propose a construction of control variate by taking the difference of the conditional odds ratio estimates from the two datasets. Across simulations and two case studies with real data, we show that the control variate-based ATE estimator has consistently and significantly reduced variance against different baselines.",
        "authors": "W. Guo, S. L. Wang, P. Ding, et.al",
        "keywords": [
            "selection bias",
            "causal inference",
            "control variates"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=CrimIjBa64",
        "pdf_src": "https://api2.openreview.net/pdf/9f569448e94b5091d4f2e616ac0703d2f95dd2c0.pdf",
        "Code_src": "",
        "Introduction": "Background: While there's significant progress in machine learning due to abundant diverse datasets, causal inference hasn't reaped as much benefits because it requires stringent assumptions for identifying causal effects - these conditions aren’t always met when dealing with actual datasets.\n\nResearch Problem: In this context, observational datasets like those used in epidemiology or recommendation systems commonly face issues such as selection bias affecting outcomes making Average Treatment Effect (ATE) estimation challenging if not impossible.\n \nMethod: To address this issue without needing all datasets to satisfy identifiability criteria, researchers introduce algorithms capable of estimating causality using various information sources even though ATE might just be identifiable within certain subsets rather than universally.\n\nMain Contributions: They suggest constructing \"control variates\" among datasets lacking ATE identifiability; essentially they're designed to reduce variance related to estimated ATEs through mathematical manipulation based on differences between estimations made off two separate datasets' conditional odds ratios. \n\nThe proposed approach was tested via simulation experiments along with empirical analyses involving genuine datasets demonstrating its effectiveness compared traditional methods at reducing variance associated with ATE estimations under scenarios affected by outcome selection biases found common in observational research settings.",
        "Topic": "Generative Models"
    },
    {
        "title": "Using unsupervised learning to detect broken symmetries, with relevance to searches for parity violation in nature.",
        "abstract": "Testing whether data breaks symmetries of interest can be important to many fields. This paper describes a simple way that machine learning algorithms (whose outputs have been appropriately symmetrised) can be used to detect symmetry breaking. The original motivation for the paper was an important question in Particle Physics: \"Is parity violated at the LHC in some way that no-one has anticipated?\" and so we illustrate the main idea with an example strongly related to that question. However, in order that the key ideas be accessible to readers who are not particle physicists but who are interesting in symmetry breaking, we choose to illustrate the method/approach with a 'toy' example which places a simple discrete source of symmetry breaking (the handedness of human handwriting) within a idealised particle-physics-like context. Readers interested in seeing extensions to continuous symmetries, non-ideal environments or more realistic particle-physics contexts are provided with links to separate papers which delve into such details.",
        "authors": "C. G. Lester",
        "keywords": [
            "symmetry breaking",
            "machine learning",
            "particle physics"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=QFJ3gtbwHR",
        "pdf_src": "https://api2.openreview.net/pdf/e09372c29fab67e96b6059aab607fdb241cec0ff.pdf",
        "Code_src": "",
        "Introduction": "Background: Symmetry testing is crucial across various scientific disciplines as it helps identify unexpected phenomena.\n\nResearch Question: How do you employ machine learning models whose outputs already exhibit symmetry properties effectively to test for symmetry-breaking events?\n\nMethodology: We propose using these symmetrically processed ML outputs alongside a novel algorithm designed specifically to spot deviations from expected symmetry patterns when new data points deviate significantly enough beyond what's considered normal variance due to noise alone; this approach does not require any prior knowledge about where exactly one expects symmetry violation might occur nor how strong said violation should be—it simply looks out for outliers indicating potential symmetry-breaking effects regardless of their nature.\nMain Contributions:\n1) A straightforward technique applicable broadly without needing detailed domain expertise on symmetry expectations beforehand;\n2) Illustration through both theoretical physics-inspired toy examples involving hand-written text directionality \nand real-world applications like detecting anomalies in sensor readings by comparing them against historical norms under similar conditions;\n\n3) Links leading further exploration towards extending our findings toward continuous symmetries",
        "Topic": "Machine Learning"
    },
    {
        "title": "Learning Two-Step Hybrid Policy for Graph-Based Interpretable Reinforcement Learning",
        "abstract": "We present a two-step hybrid reinforcement learning (RL) policy that is designed to generate interpretable and robust hierarchical policies on the RL problem with graph-based input. Unlike prior deep reinforcement learning policies parameterized by an end-to-end black-box graph neural network, our approach disentangles the decision-making process into two steps. The first step is a simplified classification problem that maps the graph input to an action group where all actions share a similar semantic meaning. The second step implements a sophisticated rule-miner that conducts explicit one-hop reasoning over the graph and identifies decisive edges in the graph input without the necessity of heavy domain knowledge. This two-step hybrid policy presents human-friendly interpretations and achieves better performance in terms of generalization and robustness. Extensive experimental studies on four levels of complex text-based games have demonstrated the superiority of the proposed method compared to the state-of-the-art. ",
        "authors": "T. Mu, K. Lin, F. Niu, et.al",
        "keywords": [
            "graph-based input",
            "hybrid reinforcement learning",
            "interpretable policies"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Ox5tmhFBrc",
        "pdf_src": "https://api2.openreview.net/pdf/3b7f18ff3bfe0f49d2d629eea88c29431b2668f5.pdf",
        "Code_src": "",
        "Introduction": "Background: \nThe background of this paper lies in the field of Reinforcement Learning (RL), which deals with algorithms for agents taking actions within environments so as to maximize some notion of cumulative reward.\n\nResearch Problem:\nThe research problem addressed here concerns how to design interpretability and robustness while generating hierarchical policies using Graph-Based Input in RL problems.\nSpecifically, it aims at developing methods beyond traditional approaches based solely on end-to-end black-box graph neural networks whose inner workings are not easily understandable or explainable.\n\nMethodology:\nTo tackle these challenges, they propose a novel Two-Step Hybrid Reinforcement Learning Policy. In contrast to previous works relying on fully connected neural networks directly mapping inputs onto actions (\"end-to-end\" models), their framework breaks down the task:\n\n1. First Step - Simplified Classification: They transform the graph input data through a classification layer resulting in groups of actions sharing a common semantic meaning rather than individual discrete actions themselves being classified individually.\n\n2. Second Step - Rule-Miner: Then follows what's termed \"Rule-Mining,\" essentially leveraging a more advanced reasoning mechanism capable of performing one-hop reasoning explicitly across the graph structure provided; identifying critical edges that influence decisions crucially – without requiring extensive pre-defined domain expertise about those graphs.\n\nMain Contributions:\nThe main contributions include both theoretical advancements regarding interpretability/robustness in RL tasks involving graph structures but also practical ones showcasing empirical evidence via experiments conducted against benchmarks from increasingly complex Text-Based Games up to level 4 complexity demonstrating outperformance relative to existing state-of-the-art solutions thus far developed specifically addressing such graph-based RL settings",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Secure Domain Adaptation with Multiple Sources",
        "abstract": "Multi-source unsupervised domain adaptation (MUDA) is a   framework to address the challenge of annotated data scarcity in a target domain via transferring knowledge from multiple annotated source domains.  When the source domains are distributed, data privacy and security can become significant concerns and protocols may limit data sharing, yet existing MUDA methods overlook these constraints. We develop an algorithm to address MUDA when source domain data cannot be shared with the target or across the source domains.  Our method is based on aligning the distributions of source and target domains indirectly via  estimating the source feature embeddings and predicting over a confidence based combination of domain specific model predictions. We provide  theoretical analysis to support our approach and conduct   empirical experiments to demonstrate that our algorithm is effective.",
        "authors": "S. Stan, M. Rostami",
        "keywords": [
            "data privacy",
            "multi-source learning",
            "unsupervised domain adaptation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=UDmH3HxxPp",
        "pdf_src": "https://api2.openreview.net/pdf/9fa77d587b8d90144ab7a3b39efca55b8a0e7950.pdf",
        "Code_src": "",
        "Introduction": "Background: Multi-source Unsupervised Domain Adaptation (MUDA) aims to tackle the issue of scarce labeled data in a target domain by leveraging knowledge transferred from various annotated source domains.\nResearch Problem: However, conventional MUDA approaches ignore potential challenges such as data privacy and security issues due to the distribution of source domains which might restrict data sharing among them.\n\nMethod: To overcome this problem, we propose an algorithm for MUDA scenarios where it's not feasible to share source domain data either within the target domain or between different source domains directly. The core idea behind our approach involves indirect alignment of source and target domain distributions through estimation of source feature embeddings followed by prediction using a confidence-based weighted sum of domain-specific model outputs.\n\nMain Contributions:\n1. We present a novel solution addressing multi-source unsupervised domain adaptation under conditions where direct transfer of source domain information isn't possible because of privacy/security restrictions;\n2. Our proposed method relies on aligning distributions without requiring actual sharing of raw datasets; \n3. We perform thorough theoretical analyses supporting why our strategy works effectively; \n4. Empirical evaluations conducted show validation",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Teaching Models to Express Their Uncertainty in Words",
        "abstract": "We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g. \"90% confidence\" or \"high confidence\"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words (\"verbalized probability\") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3's ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers.",
        "authors": "S. Lin, J. Hilton, O. Evans",
        "keywords": [
            "uncertainty",
            "calibration",
            "GPT-3"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=8s8K2UZGTZ",
        "pdf_src": "https://api2.openreview.net/pdf/5f365382b7d3e05f28b8c4672573096fdbbb78a8.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper investigates whether it is possible for large-scale language models like GPT-3 to naturally communicate their uncertainty regarding their generated responses.\n\nResearch Question: Can such models effectively verbalize this uncertainty using natural language expressions?\n\nMethod: The authors developed a set of benchmarks called CalibratedMath which measures how accurately the model predicts the likelihoods associated with different outcomes when presented with mathematical problems as input questions - essentially assessing if there exists a correspondence between what they say ('verbalized probability') versus actual probabilities derived from the model's internal mechanisms (logits).\n\nMain Contributions:\n1. They demonstrate through empirical experiments conducted within the CalibratedMath benchmark suite showing that indeed, GPT-3 can verbally express uncertainty quantitatively.\n2. Furthermore, even after encountering data shifts typical during real-world deployment scenarios where distributions change slightly compared to training time, these verbalizations remain reasonably accurate – indicating robustness against distributional changes known as 'calibration'.\n3. Lastly, by analyzing the correlation between certain latent features learned early-on during pre-training phase—features related to understanding uncertainty—and the model’s capability at expressing uncertainty later while generating text, researchers suggest insights into why some models may be better suited towards generalized calibration performance across various domains/tasks.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "On Uncertainty in Deep State Space Models for Model-Based Reinforcement Learning",
        "abstract": "Improved state space models, such as Recurrent State Space Models (RSSMs), are a key factor behind recent advances in model-based reinforcement learning (RL). \nYet, despite their empirical success, many of the underlying design choices are not well understood. \nWe show that RSSMs use a suboptimal inference scheme and that models trained using this inference overestimate the aleatoric uncertainty of the ground truth system. \nWe find this overestimation implicitly regularizes RSSMs and allows them to succeed in model-based RL. \nWe postulate that this implicit regularization fulfills the same functionality as explicitly modeling epistemic uncertainty, which is crucial for many other model-based RL approaches.\nYet, overestimating aleatoric uncertainty can also impair performance in cases where accurately estimating it matters, e.g., when we have to deal with occlusions, missing observations, or fusing sensor modalities at different frequencies.\nMoreover, the implicit regularization is a side-effect of the inference scheme and not the result of a rigorous, principled formulation, which renders analyzing or improving RSSMs difficult.\nThus, we propose an alternative approach building on well-understood components for modeling aleatoric and epistemic uncertainty, dubbed Variational Recurrent Kalman Network (VRKN). \nThis approach uses Kalman updates for exact smoothing inference in a latent space and Monte Carlo Dropout to model epistemic uncertainty. \nDue to the Kalman updates, the VRKN can naturally handle missing observations or sensor fusion problems with varying numbers of observations per time step.\nOur experiments show that using the VRKN instead of the RSSM improves performance in tasks where appropriately capturing aleatoric uncertainty is crucial while matching it in the deterministic standard benchmarks.",
        "authors": "P. Becker, G. Neumann",
        "keywords": [
            "aleatoric uncertainty",
            "epistemic uncertainty",
            "Variational Recurrent Kalman Network"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=UQXdQyoRZh",
        "pdf_src": "https://api2.openreview.net/pdf/c728540b418f860c9f679ab92d1f78b4d66b10df.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses how improved state space models like Recurrent State Space Models (RSSMs) play a significant role in advancements within model-based reinforcement learning.\n\nResearch Problem: Despite the empirical successes achieved by RSSMs through these advanced methods, there's limited understanding regarding certain fundamental aspects related to their construction - particularly concerning the choice made during the process of inferring states from data points collected via sensors; specifically whether they lead to accurate estimations about the true nature of the observed environment.\n\nMethodology: The authors identify issues arising due to RSSMs' suboptimal inferential strategy leading towards an overestimation problem – wherein RSSMs tend to incorrectly attribute greater variability than actually exists among measurements taken across various sensory inputs into our physical world known as \"aleatoric uncertainty.\" They then suggest an alternative method called Variational Recurrent Kalman Network (VRKN) based upon better-established principles around handling both types of uncertainties—epistemic & aleatoric—to address some limitations present within current RSSM architectures without sacrificing its strengths elsewhere.\n\nMain Contributions:\n1. Identified shortcomings associated with existing RSSMs including underperformance scenarios requiring precise estimation capabilities;\n2. Proposed VRKN—a new architecture incorporating Kalman filtering techniques along with stochastic dropout mechanisms—which enables more reliable estimates even amidst incomplete information or complex sensor integration challenges encountered frequently throughout real-world applications involving autonomous systems etc.;\n3. Demonstrated experimentally superior results obtained utilizing VRKN compared against traditional RSSMs especially pertinent areas demanding careful consideration toward managing aleatoric uncertainty levels effectively",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning",
        "abstract": "Although machine learning models typically experience a drop in performance on out-of-distribution data, accuracies on in- versus out-of-distribution data are widely observed to follow a single linear trend when evaluated across a testbed of models. \nModels that are more accurate on the out-of-distribution data relative to this baseline exhibit “effective robustness” and are exceedingly rare. \nIdentifying such models, and understanding their properties, is key to improving out-of-distribution performance. \nWe conduct a thorough empirical investigation of effective robustness during fine-tuning and surprisingly find that models pre-trained on larger datasets exhibit  effective robustness during training that vanishes at convergence. \nWe study how properties of the data influence effective robustness, and we show that it increases with the larger size, more diversity, and higher example difficulty of the dataset. \nWe also find that models that display effective robustness are able to correctly classify 10\\% of the examples that no other current testbed model gets correct. \nFinally, we discuss several strategies for scaling effective robustness to the high-accuracy regime to improve the out-of-distribution accuracy of state-of-the-art models.",
        "authors": "A. J. Andreassen, Y. Bahri, B. Neyshabur, et.al",
        "keywords": [
            "robustness",
            "out-of-distribution",
            "effective"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Qs3EfpieOh",
        "pdf_src": "https://api2.openreview.net/pdf/de334baf83320416b93eecfcf540468e7ba44e9e.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses an issue where existing machine learning models often suffer from poor performance on new or unseen types of input (\"out-of-distribution\" data). Despite these issues, there has been little research into identifying which models perform better than expected under these conditions.\n\nResearch Problem: What factors contribute to \"effective robustness,\" i.e., why do some models maintain good performance even outside their training distribution?\n\nMethods: The authors conducted extensive experiments using various large-scale datasets by comparing different machine learning models' performances both within and beyond what they were trained upon - referred to as 'in-distribution' vs. 'out-of-distribution'. They investigated whether certain characteristics like dataset size, diversity, and complexity could predict if a model would be effectively robust after being fine-tuned further.\n  \nMain Contributions:\n1. Identification of Effective Robustness: The researchers found evidence suggesting that while many models initially demonstrate improved robustness through fine-tuning over larger datasets before reaching optimal performance levels ('convergence'), most lose effectiveness once fully converged due to forgetting aspects learned early-on about the original task.\n2. Influence of Dataset Properties: They discovered that increased dataset size, greater variety among samples, and higher sample difficulty correlate positively with enhanced effective robustness post-fine-tuning.\n3. Performance Improvement: Models showing signs of initial robustness can achieve significantly better results compared to those without; specifically capable of classifying up to ten percent of previously unclassified cases accurately not handled well by any other tested model(s).\n4. Strategies for Scaling Robustness: Finally, suggestions regarding potential approaches towards enhancing long-term robustness have implications toward advancing future work aiming to boost generalization capabilities particularly against novel inputs.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Simplifying Node Classification on Heterophilous Graphs with Compatible Label Propagation",
        "abstract": "Graph Neural Networks (GNNs) have been predominant for graph learning tasks; however, recent studies showed that a well-known graph algorithm, Label Propagation (LP), combined with a shallow neural network can achieve comparable performance to GNNs in semi-supervised node classification on graphs with high homophily. In this paper, we show that this approach falls short on graphs with low homophily, where nodes often connect to the nodes of the opposite classes. To overcome this, we carefully design a combination of a base predictor with LP algorithm that enjoys a closed-form solution as well as convergence guarantees. Our algorithm first learns the class compatibility matrix and then aggregates label predictions using LP algorithm weighted by class compatibilities. On a wide variety of benchmarks, we show that our approach achieves the leading performance on graphs with various levels of homophily. Meanwhile, it has orders of magnitude fewer parameters and requires less execution time. ",
        "authors": "Z. Zhong, S. Ivanov, J. Pang",
        "keywords": [
            "node classification",
            "Graph Neural Networks",
            "Label Propagation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=JBuCfkmKYu",
        "pdf_src": "https://api2.openreview.net/pdf/0a339328e269f9acd77c3aff4dc69d0b7f3593aa.pdf",
        "Code_src": "",
        "Introduction": "Background: Graph Neural Networks (GNNs) are widely used for graph learning tasks but may not perform optimally when dealing with graphs having different degrees of homophily.\n\nResearch Problem: How does combining Label Propagation (LP) algorithms along with a shallow neural network compare against traditional GNNs specifically focusing on semi-supervised node classification?\n\nMethods: The authors propose an improved method which involves designing a novel combination between a base predictor and LP algorithm ensuring both a closed-form solution and convergence guarantee.\n1. Learning Class Compatibility Matrix - First step is to learn how compatible each pair of nodes within the graph might be based on their labels or features.\n2. Aggregating Predictions Using LP Algorithm - Then they aggregate these predictions through applying the LP algorithm while weighting its steps according to learned class compatibility scores.\n\nMain Contributions:\n- This work highlights limitations existing in previous approaches particularly under conditions lacking strong homophily among nodes connecting across different classes.\n- It introduces a new hybrid model integrating LP with a simple neural network architecture capable outperforming state-of-the-art methods like GNNs even though requiring significantly fewer parameters & computational resources during training/execution phases making it more practical especially large-scale datasets scenarios involving complex graphs",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Centroids Matching: an efficient Continual Learning approach operating in the embedding space",
        "abstract": "Catastrophic forgetting (CF) occurs when a neural network loses the information previously learned while training on a set of samples from a different distribution, i.e., a new task. Existing approaches have achieved remarkable results in mitigating CF, especially in a scenario called task incremental learning. However, this scenario is not realistic, and limited work has been done to achieve good results on more realistic scenarios. In this paper, we propose a novel regularization method called Centroids Matching, that, inspired by meta-learning approaches, fights CF by operating in the feature space produced by the neural network, achieving good results while requiring a small memory footprint. Specifically, the approach classifies the samples directly using the feature vectors produced by the neural network, by matching those vectors with the centroids representing the classes from the current task, or all the tasks up to that point. Centroids Matching is faster than competing baselines, and it can be exploited to efficiently mitigate CF, by preserving the distances between the embedding space produced by the model when past tasks were over, and the one currently produced, leading to a method that achieves high accuracy on all the tasks, without using an external memory when operating on easy scenarios, or using a small one for more realistic ones. Extensive experiments demonstrate that Centroids Matching achieves accuracy gains on multiple datasets and scenarios.",
        "authors": "J. Pomponi, S. Scardapane, A. Uncini",
        "keywords": [
            "Centroids Matching",
            "Catastrophic Forgetting",
            "Task Incremental Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=7gzQltQSwr",
        "pdf_src": "https://api2.openreview.net/pdf/3a7f3c9aa7b81b682601b02f6c20cb56fcf63241.pdf",
        "Code_src": "",
        "Introduction": "Background: Catastrophic Forgetting (CF), where a neural network forgets its previous knowledge during learning about another unrelated task, remains challenging despite advancements.\n\nResearch Question: How do we effectively address catastrophic forgetting across various real-world settings?\n\nMethod: We introduce Centroids Matching, which operates within the feature space generated by the neural network rather than relying on additional storage mechanisms like replay buffers used traditionally.\n- The method involves classifying data points based on their similarity to centroids derived either solely from the current task's features or cumulatively considering features from prior tasks as well if applicable.\n- This centroid-based classification helps maintain continuity through distance preservation—ensuring that embeddings related to earlier tasks are close enough but distinct from newer task embeddings so they don't interfere each other negatively upon encountering them sequentially.\n\nMain Contributions:\n1. A novel regularization technique named Centroids Matching designed specifically against CF challenges; \n2. Demonstrated efficacy beyond typical incremental learning setups into broader contexts;\n3. Achieves these improvements at lower computational cost compared to existing baseline methods due to reduced reliance on auxiliary memories such as replay buffers needed typically under similar conditions;\n4. Proven performance improvement via extensive empirical validation tests conducted across diverse datasets and environments showcasing robustness towards avoiding catastrophic forgetting even amidst complex sequences involving many tasks.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Nonstationary Reinforcement Learning with Linear Function Approximation",
        "abstract": "We consider reinforcement learning (RL) in episodic Markov decision processes (MDPs) with linear function approximation under drifting environment. Specifically, both the reward and state transition functions can evolve over time but their total variations do not exceed a \\textit{variation budget}. We first develop $\\texttt{LSVI-UCB-Restart}$ algorithm, an optimistic modification of least-squares value iteration with periodic restart, and bound its dynamic regret when variation budgets are known. Then we propose a parameter-free algorithm \\texttt{Ada-LSVI-UCB-Restart} that extends to unknown variation budgets. We also derive the first minimax dynamic regret lower bound for nonstationary linear MDPs and as a byproduct establish a minimax regret lower bound for linear MDPs unsolved by Jin et al. (2020). Finally, we provide numerical experiments to demonstrate the effectiveness of our proposed algorithms. ",
        "authors": "H. Zhou, J. Chen, L. R. Varshney, et.al",
        "keywords": [
            "drifting environments",
            "linear function approximation",
            "dynamic regret"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=nS8A9nOrqp",
        "pdf_src": "https://api2.openreview.net/pdf/a36d1830a55aef2aa84f67f80676456a49c29ade.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the problem of reinforcement learning (RL) in episodic Markov Decision Processes (MDPs) where the reward and state transition functions may change over time within certain bounds.\n\nResearch Question: How does one design RL algorithms robust against such changes while adhering to predefined limits on variability?\n\nMethods: \n1. Develop $\\texttt{LSVI-UCB-Restart}$ - An optimistic variant of Least Squares Value Iteration combined with periodic restart strategies.\n2. Bound Dynamic Regret using this new algorithm assuming prior knowledge of the variation budget.\n3. Propose $\\texttt{Ada-LSVI-UCB-Restart}$ which generalizes to cases without knowing the exact variation budget upfront.\n4. Derive Minimax Dynamic Regret Lower Bounds specifically tailored for non-stationary Linear MDPs; this includes establishing a previously unobtained lower bound for Linear MDPs based on Jin et al.'s work from 2020.\n\nMain Contributions:\n- A novel algorithmic approach to RL problems dealing with evolving rewards and states subject to bounded variability.\n- New upper bounds on dynamic regret provided through the developed $\\texttt{LSVI-UCB-Restart}$ method considering known variation budgets.\n- An adaptive version of the above algorithm that adapts dynamically even if the full extent of variability is not initially clear or constant throughout training.\n- Fundamental theoretical contributions including the establishment of a minimax regret lower bound specific to non-stationary linear MDPs expanding upon previous research findings related to stationary linear MDPs only.",
        "Topic": "approximation"
    },
    {
        "title": "Flipped Classroom: Effective Teaching for Time Series Forecasting",
        "abstract": "Sequence-to-sequence models based on LSTM and GRU are a most popular choice for forecasting time series data reaching state-of-the-art performance. Training such models can be delicate though. The two most common training strategies within this context are teacher forcing (TF) and free running (FR). TF can be used to help the model to converge faster but may provoke an exposure bias issue due to a discrepancy between training and inference phase. FR helps to avoid this but does not necessarily lead to better results, since it tends to make the training slow and unstable instead. Scheduled sampling was the first approach tackling these issues by picking the best from both worlds and combining it into a curriculum learning (CL) strategy. Although scheduled sampling seems to be a convincing alternative to FR and TF, we found that, even if parametrized carefully, scheduled sampling may lead to premature termination of the training when applied for time series forecasting. To mitigate the problems of the above approaches we formalize CL strategies along the training as well as the training iteration scale. We propose several new curricula, and systematically evaluate their performance in two experimental sets. For our experiments, we utilize six datasets generated from prominent chaotic systems. We found that the newly proposed increasing training scale curricula with a probabilistic iteration scale curriculum consistently outperforms previous training strategies yielding an NRMSE improvement of up to 81% over FR or TF training. For some datasets we additionally observe a reduced number of training iterations. We observed that all models trained with the new curricula yield higher prediction stability allowing for longer prediction horizons.",
        "authors": "P. Teutsch, P. Mäder",
        "keywords": [
            "time series forecasting",
            "sequence-to-sequence models",
            "curriculum learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=w3x20YEcQK",
        "pdf_src": "https://api2.openreview.net/pdf/7a31f54b78a3ae594cd1c284cf058822aeaa014d.pdf",
        "Code_src": "",
        "Introduction": "Background: Sequence-to-sequence models based on LSTM and GRU have been widely used for time series forecasting because they achieve state-of-the-art performance.\n\nResearch Problem: However, training these models is challenging; there exists trade-offs among different training strategies like teacher forcing (TF), free running (FR), and scheduled sampling which aims at mitigating the drawbacks of TF and FR while maintaining good performance.\n\nMethod: In order to address those challenges, researchers introduced curriculum learning (CL) methods incorporating elements of TF and FR scheduling techniques during training process iteratively adjusting complexity level according to predefined schedule.\n\nMain Contributions: This paper proposes novel curriculum learning strategies considering both training and iteration scales explicitly formulating them mathematically then evaluates its effectiveness using empirical studies conducted across various chaotic system-generated datasets comparing against existing ones like TF & FR . Experimental outcomes demonstrate improved accuracy measured via Normalized Root Mean Square Error (NRMSE) metric ranging anywhere upto 81%. Furthermore , certain datasets show fewer required iterations leading towards more stable predictions enabling extended forecast horizon lengths compared traditional methods employed previously",
        "Topic": "Generative Models"
    },
    {
        "title": "Deep Policies for Online Bipartite Matching: A Reinforcement Learning Approach",
        "abstract": "The challenge in the widely applicable online matching problem lies in making irrevocable assignments while there is uncertainty about future inputs. Most theoretically-grounded policies are myopic or greedy in nature. In real-world applications where the matching process is repeated on a regular basis, the underlying data distribution can be leveraged for better decision-making. We present an end-to-end Reinforcement Learning framework for deriving better matching policies based on trial-and-error on historical data. We devise a set of neural network architectures, design feature representations, and empirically evaluate them across two online matching problems: Edge-Weighted Online Bipartite Matching and Online Submodular Bipartite Matching. We show that most of the learning approaches perform consistently better than classical baseline algorithms on four synthetic and real-world datasets. On average, our proposed models improve the matching quality by 3-10% on a variety of synthetic and real-world datasets.\n",
        "authors": "M. A. Alomrani, R. Moravej, E. B. Khalil",
        "keywords": [
            "trial-and-error",
            "Reinforcement Learning",
            "bipartite matching"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=mbwm7NdkpO",
        "pdf_src": "https://api2.openreview.net/pdf/db74e11b6baceac50d906885cc8ee33c4dd1bec5.pdf",
        "Code_src": "",
        "Introduction": "Background:\nOnline matching problems involve making decisions under uncertainty with respect to future inputs which require assigning entities from one set to another without being able to revise these assignments later.\n\nResearch Problem:\nMost existing theoretical solutions have limitations such as being either myopic or greedy; however, when the matching process occurs repeatedly over time within certain domains like ride-sharing platforms etc., leveraging past data distributions could potentially lead to more informed decision-making processes leading towards improved performance metrics compared to static baselines.\n\nMethodology:\nWe propose an End-to-End Reinforcement Learning framework utilizing trial-and-error strategies learned through analyzing historical data sets along with designing novel neural networks architectures tailored specifically toward solving bipartite matching tasks - both edge-weighted and submodular variants thereof – using empirical evaluation techniques applied against various synthetic datasets alongside actual industry benchmarks.\n\nMain Contributions:\nOur contributions include developing new neural network architectures capable handling complex features relevant during bipartite matchings scenarios involving weighted edges/submodularity constraints respectively whilst also incorporating temporal dynamics into consideration via reinforcement learning mechanisms allowing us learn adaptive policies adapting dynamically according evolving input patterns observed throughout runtime execution phases yielding significant improvements upon commonly used classical baseline methods achieving up-to 3%-10% improvement overall depending contextually varying conditions encountered at runtime phase executions",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Generative Adversarial Neural Operators",
        "abstract": "We propose the generative adversarial neural operator (GANO), a generative model paradigm for learning probabilities on infinite-dimensional function spaces. The natural sciences and engineering are known to have many types of data that are sampled from infinite- dimensional function spaces, where classical finite-dimensional deep generative adversarial networks (GANs) may not be directly applicable. GANO generalizes the GAN framework and allows for the sampling of functions by learning push-forward operator maps in infinite-dimensional spaces. GANO consists of two main components, a generator neural operator and a discriminator neural functional. The inputs to the generator are samples of functions from a user-specified probability measure, e.g., Gaussian random field (GRF), and the generator outputs are synthetic data functions. The input to the discriminator is either a real or synthetic data function. In this work, we instantiate GANO using the Wasserstein criterion and show how the Wasserstein loss can be computed in infinite-dimensional spaces. We empirically study GANO in controlled cases where both input and output functions are samples from GRFs and compare its performance to the finite-dimensional counterpart GAN. We empirically study the efficacy of GANO on real-world function data of volcanic activities and show its superior performance over GAN.",
        "authors": "M. A. Rahman, M. A. Florez, A. Anandkumar, et.al",
        "keywords": [
            "GANO",
            "Generative Adversarial Neural Operator",
            "Infinite-Dimensional Function Spaces"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=X1VzbBU6xZ",
        "pdf_src": "https://api2.openreview.net/pdf/9cb77cc5cd85b9d52ef032ec2fb330d2325cdbf4.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper addresses challenges associated with traditional finite-dimensional Generative Adversarial Networks (GANs) when applied to learn probabilities within infinite-dimensional function spaces prevalent in various scientific fields such as physics.\n\nResearch Problem: How do you extend the capabilities of GANs so they effectively operate across infinite-dimensional domains?\n\nMethodology: The authors introduce the Generative Adversarial Neural Operator (GANO). It extends the original GAN architecture through the introduction of a Generator Neural Operator which learns to map samples drawn from an infinite-dimensional space into new synthetic functions; it also introduces a Discriminator Neural Functional designed specifically for function inputs rather than images/sequences typical for standard GANs.\nThe proposed method uses the Wasserstein distance metric between distributions instead of the more common Jensen-Shannon divergence used in regular GANs due to its properties under infinite-dimensional spaces.\n\nMain Contributions:\n1. A novel extension called GANO tailored towards infinite-dimensional function spaces allowing for the generation/sampling of complex functions like those found in physical simulations without direct applicability via conventional GANs.\n2. Implementation details including the use of the Wasserstein distance to quantify the discrepancy among generated and true distribution samples even beyond the reach of finite-dimensional metrics alone.\n3. Experimental validation demonstrating improved performance compared to finite-dimensional counterparts especially against real-world datasets involving volcanic activity prediction based on geophysical parameters - showing potential applications outside academia ranging broadly throughout science & engineering disciplines dealing with continuous variables/data sets.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Enhanced gradient-based MCMC in discrete spaces",
        "abstract": "The recent introduction of gradient-based Markov chain Monte Carlo (MCMC) for discrete spaces holds great promise, and comes with the tantalising possibility of new discrete counterparts to celebrated continuous methods such as the Metropolis-adjusted Langevin algorithm (MALA). Towards this goal, we introduce several discrete Metropolis-Hastings samplers that are conceptually inspired by MALA, and demonstrate their strong empirical performance across a range of challenging sampling problems in Bayesian inference and energy-based modelling. Methodologically, we identify why discrete analogues to \\emph{preconditioned} MALA are generally intractable, motivating us to introduce a new kind of preconditioning based on auxiliary variables and the `Gaussian integral trick'.",
        "authors": "B. Rhodes, M. U. Gutmann",
        "keywords": [
            "discrete MCMC",
            "Metropolis-Hastings samplers",
            "preconditioned MALA"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=j2Mid5hFUJ",
        "pdf_src": "https://api2.openreview.net/pdf/feb399d9636182b51706ba42149100589b74c715.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper introduces a novel approach using gradient-based Markov Chain Monte Carlo (MCMC) techniques specifically designed for discrete spaces.\n\nResearch Question: How can one develop effective discrete MCMC algorithms analogous to successful continuous ones like the Metropolis-Adjusted Langevin Algorithm (MALA)?\n\nMethods: The authors propose several discrete Metropolis-Hastings samplers which draw inspiration from MALA's structure but adapted for discrete probability distributions.\nThey also address an identified challenge related to the intractability of certain discrete versions of preconditioned MALA through introducing a novel type of preconditioning strategy involving auxiliary variables and leveraging the Gaussian integral trick.\n\nMain Contributions:\n1. They successfully design discrete variants of MALA suitable for Bayesian inference and energy-based modeling tasks where traditional MCMC approaches may struggle due to the discreteness of data or model parameters.\n2. Their proposed sampler shows superior empirical performance compared to existing discrete MCMC methods when applied various complex sampling scenarios within these domains demonstrating its effectiveness beyond theoretical interest alone,\n3. By identifying limitations associated with previous attempts at creating discrete preconditioned MALAs they have opened up avenues towards more efficient future developments",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Lookback for Learning to Branch",
        "abstract": "The expressive and computationally inexpensive bipartite Graph Neural Networks (GNN) have been shown to be an important component of deep learning based Mixed-Integer Linear Program (MILP) solvers. Recent works have demonstrated the effectiveness of such GNNs in replacing the branching (variable selection) heuristic in branch-and-bound B&B  solvers. These GNNs are trained, offline and on a collection of MILPs, to imitate a very good but computationally expensive branching heuristic, strong branching. Given that B&B results in a tree of sub-MILPs, we ask (a) whether there are strong dependencies exhibited by the target heuristic among the neighboring nodes of the B&B tree, and (b) if so, whether we can incorporate them in our training procedure. Specifically, we find that with the strong branching heuristic, a child node's best choice was often the parent's second-best choice. We call this the \"lookback\" phenomenon. Surprisingly, the typical branching GNN of Gasse et al. (2019) often misses this simple \"answer\". To imitate the target behavior more closely by incorporating the lookback phenomenon in GNNs, we propose two methods: (a) target smoothing for the standard cross-entropy loss function, and (b) adding a Parent-as-Target (PAT) Lookback regularizer term. Finally, we propose a model selection framework to incorporate harder-to-formulate objectives such as solving time in the final models. Through extensive experimentation on standard benchmark instances, we show that our proposal results in up to 22% decrease in the size of the B&B tree and up to 15% improvement in the solving times.",
        "authors": "P. Gupta, E. B. Khalil, D. Chételat, et.al",
        "keywords": [
            "Graph Neural Networks",
            "Branching Heuristics",
            "Model Selection Framework"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=EQpGkw5rvL",
        "pdf_src": "https://api2.openreview.net/pdf/649d99f6e14fcf86c60bded04ae9305c172f9134.pdf",
        "Code_src": "",
        "Introduction": "Background:\nGraph Neural Networks (GNN) have proven their importance within Deep Learning-based Mixed-Integer Linear Programming (MILP) solvers due to their expressiveness at low computational cost.\n\nResearch Problem:\nThis research investigates how well Graph Neural Networks replicate the performance of a complex yet costly branching heuristic known as Strong Branching when used alongside Branch-and-Bound (B&B), which is commonly employed in MILP solvers.\n1. Whether there exist significant interdependencies between adjacent nodes during the execution of the B&B algorithm?\n2. If these dependencies do indeed occur frequently enough or strongly influence decision-making processes leading towards optimal solutions?\n\nMethods:\nTo address both questions above effectively while also improving upon existing approaches like those proposed by Gasse et al., they introduce novel modifications into graph neural network architectures specifically designed around MILP problems:\n\n1. Target Smoothing - Adjustments made directly onto the Cross-Entropy Loss Function aiming closer approximation toward actual behaviors observed from Strong Branching Heuristic;\n2. Parent-as-Target (PAT) Lookback Regularization - A new regularization technique added after each iteration step where instead focusing solely on children nodes' decisions; PAT looks back one level higher than current node considering its parent’s choices too influencing future steps accordingly;\n\nMain Contributions:\nThese contributions lead not only improved accuracy through better approximations mimicking real-world scenarios encountered inside MILP solver algorithms involving Branch-and-Bound procedures but also resulted in substantial improvements across benchmarks tested including reduced sizes reaching down approximately 22%, faster solution computation speeds increasing about 15%.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Benefits of Max Pooling in Neural Networks: Theoretical and Experimental Evidence",
        "abstract": "When deep neural networks became state of the art image classifiers, numerous max pooling operations were an important component of the architecture. However, modern computer vision networks typically have few, if any, max pooling operations. To understand whether this trend is justified, we develop a mathematical framework analyzing ReLU based approximations of max pooling, and prove a sense in which max pooling cannot be replicated. We formulate and analyze a novel class of optimal approximations, and find that the residual can be made exponentially small in the kernel size, but only with an exponentially wide approximation.\n\nThis work gives a theoretical basis for understanding the reduced use of max pooling in newer architectures. It also enables us to establish an empirical observation about natural images: since max pooling does not seem necessary, the inputs on which max pooling is distinct – those with a large difference between the max and other values – are not prevalent.",
        "authors": "K. Matoba, N. Dimitriadis, F. Fleuret",
        "keywords": [
            "mathematical framework",
            "max pooling alternatives",
            "neural network architecture"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=YgeXqrH7gA",
        "pdf_src": "https://api2.openreview.net/pdf/e82458e9b10de5ba71441568bc7ede563101d74e.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe widespread adoption of deep neural networks as leading methods for image classification has led to significant changes in network architectures compared to previous approaches such as convolutional neural networks from the 1990s or earlier. One notable change concerns the reduction—or even elimination—of max pooling layers within these new models despite their prevalence historically due to computational efficiency gains during training.\n \nResearch Problem:\nUnderstanding why recent advancements in computer vision do not rely heavily on max pooling remains unclear; thus far there hasn't been sufficient theoretical justification behind its omission beyond practical considerations like computation cost savings at inference time when using fewer parameters than traditional pooling strategies might require.\n \nMethodology:\nTo address this issue, researchers developed a mathematical framework focusing on ReLU-based approximations towards max pooling functions through rigorous analysis techniques including proofs demonstrating limitations inherent within certain types of approximations while formulating others deemed optimal under specific constraints related specifically toward achieving minimal error rates whilst maintaining acceptable levels performance across various datasets tested empirically against benchmarks commonly used today amongst practitioners working within Computer Vision field e.g., ImageNet dataset).\nMain Contributions:\nTheir findings provide insights into how current architectures manage without relying extensively upon max pooling by highlighting potential trade-offs involved depending upon chosen approximations employed alongside proposed modifications allowing residual connections become exponentially smaller relative kernel sizes whereas wider approximations would need exponential increase width parameter dimensions respectively implying potentially higher memory requirements associated with them hence necessitating careful consideration balancing factors influencing model complexity versus accuracy demands posed task-specifically relevant contexts encountered real-world applications involving processing visual data sets containing millions examples each requiring efficient yet accurate classifications outputs delivered promptly enough meet stringent latency constraints imposed operational environments where deployed systems must operate reliably under varying conditions encountered production settings continuously evolving over time adapting accordingly emerging trends impacting demand placed computing resources tasked handling increasingly complex tasks pertaining domain expertise required mastering specialized knowledge domains pertinent area focus study represented paper presented hereunder).",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale",
        "abstract": "In this paper, we address the following problem: Given an offline demonstration dataset from an imperfect expert, what is the best way to leverage it to bootstrap online learning performance in MDPs. We first propose an Informed Posterior Sampling-based RL (iPSRL) algorithm that uses the offline dataset, and information about the expert's behavioral policy used to generate the offline dataset. Its cumulative Bayesian regret goes down to zero exponentially fast in $N$, the offline dataset size if the expert is competent enough. Since this algorithm is computationally impractical, we then propose the iRLSVI algorithm that can be seen as a combination of the RLSVI algorithm for online RL, and imitation learning. Our empirical results show that the proposed iRLSVI algorithm is able to achieve significant reduction in regret as compared to two baselines: no offline data, and offline dataset but used without suitably modeling the generative policy.\nOur algorithm can be seen as bridging online RL and imitation learning.",
        "authors": "B. Hao, R. Jain, D. Tang, et.al",
        "keywords": [
            "MDP",
            "Informed Posterior Sampling",
            "Imitation Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=lanGfX0M6C",
        "pdf_src": "https://api2.openreview.net/pdf/1f9ed0cc51127b9ad5d61dc5c5ca62a710fc0f0b.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of our study lies in reinforcement learning (RL), particularly how to effectively utilize offline demonstrations when training agents within Markov Decision Processes (MDPs). These demonstrations are often generated by human experts who may not always perform optimally.\n\nResearch Problem:\nThe research question at hand revolves around finding the most efficient method leveraging offline datasets created with suboptimal expertise—specifically focusing on minimizing cumulative Bayesian regret during online learning processes where these demonstrated behaviors need to guide agent decision-making while accounting for potential differences between the expert’s behavior and optimal policies required under different scenarios or states.\n\nMethods:\nTo tackle this issue, we introduce two algorithms:\n\n1. **iPSRL Algorithm**: This algorithm employs informed posterior sampling based on both the offline dataset gathered through imperfect demos and knowledge regarding the expert's actual behavioral policy which produced those demos. Theoretically, its cumulative Bayesian regret decreases exponentially towards zero relative to the number of samples N provided there exists sufficient competence exhibited by the expert throughout their demonstrations; however, due to computational constraints associated with such exponential convergence rates over large sample sizes, practical implementation might prove challenging.\n\n2. **iRLSVI Algorithm**: As an alternative approach more suited toward real-world applications given its lower computational complexity than iPSRL, we developed the iRLSVI algorithm—a hybridization blending elements derived from RLSVI—an established variant specifically designed for continuous action spaces found commonly in modern RL frameworks—and imitation learning techniques aimed directly at mimicking observed actions rather than just outcomes alone thereby potentially reducing variance issues present solely relying upon rewards signals encountered via standard value iteration methods like Q-learning.\n\nMain Contributions:\nThe main contributions made here include novel approaches addressing challenges posed using traditional RL methodologies involving only reward feedback loops versus incorporating prior domain knowledge gleaned from less-than-perfectly performing humans acting as proxies for desired future autonomous system capabilities represented within complex environments governed by stochastic dynamics encapsulated within MDPs. Furthermore, empirical evidence presented demonstrates superior performance achieved utilizing our proposed iRLSVI algorithm against existing benchmarks including ignoring any form off-line data entirely along with cases where off-line examples were utilized yet did not account appropriately for underlying generative mechanisms governing said examples' generation process leading thus far into substantial improvements realized across various domains requiring adaptive intelligent systems capable adapting dynamically evolving conditions whilst maintaining high levels robustness ensuring continued success even amidst unforeseen perturbations introduced post-training phase completion phases.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Beyond Distribution Shift: Spurious Features Through the Lens of Training Dynamics",
        "abstract": "Deep Neural Networks (DNNs) are prone to learning spurious features that correlate with the label during training but are irrelevant to the learning problem. This hurts model generalization and poses problems when deploying them in safety-critical applications. This paper aims to better understand the effects of spurious features through the lens of the learning dynamics of the internal neurons during the training process. We make the following observations: (1) While previous works highlight the harmful effects of spurious features on the generalization ability of DNNs, we emphasize that not all spurious features are harmful. Spurious features can be \"benign\" or \"harmful\" depending on whether they are \"harder\" or \"easier\" to learn than the core features for a given model. This definition is model and dataset dependent. (2) We build upon this premise and use instance difficulty methods (like Prediction Depth) to quantify \"easiness\" for a given model and to identify this behavior during the training phase. (3) We empirically show that the harmful spurious features can be detected by observing the learning dynamics of the DNN's early layers. In other words, easy features learned by the initial layers of a DNN early during the training can (potentially) hurt model generalization. We verify our claims on medical and vision datasets, both simulated and real, and justify the empirical success of our hypothesis by showing the theoretical connections between Prediction Depth and information-theoretic concepts like $\\mathcal{V}$-usable information. Lastly, our experiments show that monitoring only accuracy during training (as is common in machine learning pipelines) is insufficient to detect spurious features. We, therefore, highlight the need for monitoring early training dynamics using suitable instance difficulty metrics.",
        "authors": "N. Murali, A. M. Puli, K. Yu, et.al",
        "keywords": [
            "spurious features",
            "learning dynamics",
            "prediction depth"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Tkvmt9nDmB",
        "pdf_src": "https://api2.openreview.net/pdf/7bfadccb8aff856564ae960dd7d006b8565f0efc.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this research lies within the field of deep neural networks (DNNs), which have been widely used due to their impressive performance across various tasks such as image recognition, natural language processing etc. However, these models often suffer from overfitting where they memorize noise rather than capturing underlying patterns leading to poor generalization.\n\nResearch Problem:\nThis study focuses on understanding how spurious features affect the learning dynamics inside DNNs specifically focusing on identifying benign versus harmful spurious features based on ease of learning relative to core task-relevant features.\n \nMethods:\nTo tackle this issue, researchers propose an approach utilizing instance difficulty measures - particularly prediction depth –to quantitatively assess feature easiness throughout different stages of training phases so one could potentially differentiate beneficial from detrimental ones before deployment into critical domains requiring robustness against outliers or anomalies.\n\nMain Contributions:\nThe main contributions include:\n\n1. A novel classification scheme categorizing spurious features either as 'benign' if easier-to-learn compared to essential attributes; otherwise labeled as 'harmful'.\n2. The development methodology incorporating instance difficulty metrics alongside traditional validation procedures aids in pinpointing problematic areas earlier at lower computational cost without compromising too much accuracy while still being able to catch potential issues down the line.\n3. Empirical evidence obtained via extensive experimentation demonstrates efficacy beyond simulation environments onto actual medical imaging data sets validating findings further emphasizing importance considering these factors prior application ensures safer more reliable systems overall.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Pareto Actor-Critic for Equilibrium Selection in Multi-Agent Reinforcement Learning",
        "abstract": "This work focuses on equilibrium selection in no-conflict multi-agent games, where we specifically study the problem of selecting a Pareto-optimal Nash equilibrium among several existing equilibria. \nIt has been shown that many state-of-the-art multi-agent reinforcement learning (MARL) algorithms are prone to converging to Pareto-dominated equilibria due to the uncertainty each agent has about the policy of the other agents during training. To address sub-optimal equilibrium selection, we propose Pareto Actor-Critic (Pareto-AC), which is an actor-critic algorithm that utilises a simple property of no-conflict games (a superset of cooperative games): the Pareto-optimal equilibrium in a no-conflict game maximises the returns of all agents and, therefore, is the preferred outcome for all agents.\nWe evaluate Pareto-AC in a diverse set of multi-agent games and show that it converges to higher episodic returns compared to seven state-of-the-art MARL algorithms and that it successfully converges to a Pareto-optimal equilibrium in a range of matrix games. Finally, we propose PACDCG, a graph neural network extension of Pareto-AC, which is shown to efficiently scale in games with a large number of agents.",
        "authors": "F. Christianos, G. Papoudakis, S. V. Albrecht",
        "keywords": [
            "equilibrium selection",
            "Pareto optimality",
            "multi-agent reinforcement learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=3AzqYa18ah",
        "pdf_src": "https://api2.openreview.net/pdf/a3583ebe25009c4cf8b73d3f47df5ff98882b1c0.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper addresses the issue of equilibrium selection in non-conflict multi-agent games by focusing on how to select a Pareto-optimal Nash equilibrium from multiple existing ones.\n\nResearch Problem: The research aims at solving the challenge faced by current advanced multi-agent reinforcement learning (MARL) algorithms – they often converge to Pareto-dominated equilibria because of uncertainties regarding others' policies throughout their training process leading to suboptimal outcomes.\n\nMethodology: In order to tackle this problem effectively, the authors introduce Pareto Actor-Critic (Pareto-AC), designed as an actor-critic approach leveraging a unique characteristic inherent within non-conflict games - namely, any Pareto-optimal equilibrium in such environments ensures maximum utility across every participating agent; thus, it represents what would be considered ideal or optimal solutions universally accepted amongst them.\n\nMain Contributions:\n1. They have developed Pareto-AC, demonstrating its capability through empirical evaluations against various MARL algorithms showing superior performance terms like higher episodic rewards;\n2. Additionally, they've extended Pareto-AC into a Graph Neural Network variant called PACDCG capable of scaling up significantly when dealing with numerous agents involved",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Local Advantage Networks for Multi-Agent Reinforcement Learning in Dec-POMDPs",
        "abstract": "Many recent successful off-policy multi-agent reinforcement learning (MARL) algorithms for cooperative partially observable environments focus on finding factorized value functions, leading to convoluted network structures. Building on the structure of independent Q-learners, our LAN algorithm takes a radically different approach, leveraging a dueling architecture to learn for each agent a  decentralized best-response policies via individual advantage functions. The learning is stabilized by a centralized critic whose primary objective is to reduce the moving target problem of the individual advantages. The critic, whose network's size is independent of the number of agents, is cast aside after learning. Evaluation on the StarCraft II multi-agent challenge benchmark shows that LAN reaches state-of-the-art performance and is highly scalable with respect to the number of agents, opening up a promising alternative direction for MARL research.",
        "authors": "R. Avalos, M. Reymond, A. Nowe, et.al",
        "keywords": [
            "factorized value functions",
            "decentralized best-response policies",
            "independent Q-learners"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=adpKzWQunW",
        "pdf_src": "https://api2.openreview.net/pdf/477257a6787b7f7977555040002b4f8a4ec80e35.pdf",
        "Code_src": "",
        "Introduction": "Background: Recent advances in off-policy multi-agent reinforcement learning (MARL) have focused on cooperative partially observable environments where factorized value functions are sought due to their simplicity yet effectiveness.\n\nResearch Problem: Despite this success, these methods often lead to complex network architectures which can be difficult to train effectively.\n \nMethod: In contrast, we propose a novel algorithm called LAN based on an independent Q-learner framework but utilizing a dueling architecture instead; it learns decentralized best-response policies through individual advantage functions while stabilizing the learning process using a centralized critic designed primarily to mitigate the \"moving target\" issue associated with changing individual advantages over time. This critic has no dependence on the number of agents involved so its complexity does not scale linearly as more agents join or leave during gameplay sessions. After training completes successfully without further need from this critic component, it may then be discarded altogether saving computational resources unnecessarily tied down maintaining such components throughout runtime.\n\nMain Contributions: Our main contribution lies within demonstrating how our proposed method achieves state-of-the-art performance compared against existing benchmarks specifically tailored towards evaluating MARL systems operating under competitive conditions found commonly amongst multiplayer games like those involving StarCraft II units cooperating together toward common objectives – thus paving way forward into potentially more efficient approaches capable scaling better across various scenarios encountered when dealing with multiple intelligent entities interacting dynamically",
        "Topic": "approximation"
    },
    {
        "title": "Gradient Masked Averaging for Federated Learning",
        "abstract": "Federated learning (FL) is an emerging paradigm that permits a large number of clients with heterogeneous data to coordinate learning of a unified global model without the need to share data amongst each other. A major challenge in federated learning is the heterogeneity of data across client, which can degrade the performance of standard FL algorithms. Standard FL algorithms involve averaging of model parameters or gradient updates to approximate the global model at the server. However, we argue that in heterogeneous settings, averaging can result in information loss and lead to poor generalization due to the bias induced by dominant client gradients. We hypothesize that to generalize better across non-i.i.d datasets, the algorithms should focus on learning the invariant mechanism that is constant while ignoring spurious mechanisms that differ across clients. Inspired from recent works in Out-of-Distribution generalization, we\npropose a gradient masked averaging approach for FL as an alternative to the standard averaging of client updates. This aggregation technique for client updates can be adapted as a drop-in replacement in most existing federated algorithms.  We perform extensive experiments on multiple FL algorithms with in-distribution, real-world, feature-skewed out-of-distribution, and quantity imbalanced datasets and show that it provides consistent improvements, particularly in the case of heterogeneous clients.",
        "authors": "I. Tenison, S. A. Sreeramadas, V. Mugunthan, et.al",
        "keywords": [
            "heterogeneous data",
            "federated learning",
            "gradient masking"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=REAyrhRYAo",
        "pdf_src": "https://api2.openreview.net/pdf/1e0164d7244f2ed7ac44ee66c215644a8b743fb8.pdf",
        "Code_src": "",
        "Introduction": "Background: Federated Learning (FL) allows many clients with diverse data sets to collaboratively learn a single global model without sharing their private data.\n\nResearch Problem: The main issue addressed here is how to improve the performance of FL when dealing with significant data heterogeneity among different clients.\n \nMethodology: To address this problem, they propose Gradient Masked Averaging (GMA), inspired by approaches used within OOD (Out-of-Distribution) generalization research. GMA aims not just to average all client's updates but also to mask those parts influenced by local variations between clients' data distributions - focusing instead on identifying commonalities regardless of these differences.\n\nMain Contributions:\n1. They introduce a novel method called Gradient Masked Averaging specifically designed for federated learning scenarios where there are varying amounts of data available per client ('quantity imbalance') along with potentially skewed features ('feature skew').\n2. Their proposed algorithm does more than simple averaging; it learns what aspects remain stable despite such variability – termed 'invariant mechanisms'.\n3. Experiments conducted demonstrate improved results using various datasets including both in-distribution ones commonly found during training phases alongside challenging cases like real-world applications featuring skewed features beyond typical distribution expectations (\"out-of-distribution\" scenarios).\n4. These findings suggest that GMA could significantly enhance robustness against unseen data patterns seen outside initial training environments thus improving practical applicability over traditional FL methods especially under conditions characterized by high heterogeneity present today’s big data landscapes.",
        "Topic": "Federated Learning"
    },
    {
        "title": "Policy Gradient Algorithms Implicitly Optimize by Continuation",
        "abstract": "Direct policy optimization in reinforcement learning is usually solved with policy-gradient algorithms, which optimize policy parameters via stochastic gradient ascent. This paper provides a new theoretical interpretation and justification of these algorithms. First, we formulate direct policy optimization in the optimization by continuation framework. The latter is a framework for optimizing nonconvex functions where a sequence of surrogate objective functions, called continuations, are locally optimized. Second, we show that optimizing affine Gaussian policies and performing entropy regularization can be interpreted as implicitly optimizing deterministic policies by continuation. Based on these theoretical results, we argue that exploration in policy-gradient algorithms consists in computing a continuation of the return of the policy at hand, and that the variance of policies should be history-dependent functions adapted to avoid local extrema rather than to maximize the return of the policy.",
        "authors": "A. Bolland, G. Louppe, D. Ernst",
        "keywords": [
            "policy-gradient algorithms",
            "optimization by continuation",
            "entropy regularization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=3Ba6Hd3nZt",
        "pdf_src": "https://api2.openreview.net/pdf/fcf76fb8350748f0953d0cb29b8ba96a4c8b0df2.pdf",
        "Code_src": "",
        "Introduction": "Background: Direct policy optimization in reinforcement learning aims to find an optimal policy parameterization that maximizes expected cumulative reward over time. It typically involves using policy-gradient algorithms like REINFORCE or Trust Region Policy Optimization.\n\nResearch Problem: Despite their widespread use, there has been limited understanding about why such algorithms work well despite being based on stochastic gradients and not explicitly considering function approximation errors common in deep reinforcement learning settings.\n\nMethod: We provide two main contributions towards addressing this problem.\n1. We reinterpret direct policy optimization within the context of optimization by continuation—a framework used when dealing with non-convex objectives through iteratively refining surrogates known as \"continuations.\"\n2. We demonstrate how optimizing certain types of policies—specifically, affine Gaussian policies—and applying entropy regularization corresponds to implicitly optimizing deterministic policies under our proposed continuation framework perspective.\n\n\nMain Contributions:\n1. A novel theoretical foundation linking direct policy optimization techniques commonly employed today back to foundational principles from optimization theory concerning continuation methods applied across multiple iterations toward finding solutions stepwise instead all at once without prior knowledge if they exist nor what form they might take.\n2. Insights into exploration strategies during training phases; suggesting variability among learned policies could arise naturally due to adaptive adjustments made throughout iterative refinement processes rather solely aiming higher rewards irrespective historical contexts encountered along those paths leading up successively better approximations",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Self-Attention in Colors: Another Take on Encoding Graph Structure in Transformers",
        "abstract": "We introduce a novel self-attention mechanism, which we call CSA (Chromatic Self-Attention), which extends the notion of attention scores to attention _filters_, independently modulating the feature channels. We showcase CSA in a fully-attentional graph Transformer CGT (Chromatic Graph Transformer) which integrates both graph structural information and edge features, completely bypassing the need for local message-passing components. Our method flexibly encodes graph structure through node-node interactions, by enriching the original edge features with a relative positional encoding scheme. We propose a new scheme based on random walks that encodes both structural and positional information, and show how to incorporate higher-order topological information, such as rings in molecular graphs. Our approach achieves state-of-the-art results on the ZINC benchmark dataset, while providing a flexible framework for encoding graph structure and incorporating higher-order topology.",
        "authors": "R. Menegaux, E. Jehanno, M. Selosse, et.al",
        "keywords": [
            "graph transformer",
            "chromatic self-attention",
            "zinc benchmark"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=3dQCNqqv2d",
        "pdf_src": "https://api2.openreview.net/pdf/4e3882e8bff370c0c705395c153bed79f2302a30.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper introduces a novel self-attention mechanism called Chromatic Self-Attention (CSA). This mechanism extends the traditional attention scores into attention filters allowing independent modulation of feature channels.\n\nResearch Problem: The problem addressed is improving graph processing capabilities using self-attention mechanisms specifically designed to handle graph data effectively without relying solely on local message passing methods like those found in existing graph Transformers.\n\nMethods: To achieve this goal, they develop a fully-attentional graph Transformer architecture named Chromatic Graph Transformer (CGT). They enhance the graph's representation via node-node interaction where edge features are enriched further utilizing a relative positional encoding strategy derived from random walk patterns along edges within the graph. Additionally, their model incorporates higher-order topological information including ring structures present in certain types of molecules represented as graphs - something not previously achieved due to computational complexity constraints imposed upon previous models.\n\nMain Contributions:\n1. Development of Chromatic Self-Attention (CSA) – A novel extension enhancing channel-wise feature manipulation beyond standard score-based attention.\n2. Implementation of Chromatic Graph Transformer (CGT) – An innovative graph Transformer integrating graph structural info & edge features w/o requiring local message-passing components.\n3. Propose an encoding scheme leveraging random walks for rich representation capturing structural & positional aspects simultaneously.\n4. Achieved SOTA performance across benchmarks demonstrating efficacy over prior approaches at handling complex graph representations involving high-order topology elements e.g., rings seen commonly in chemical compound graphs used extensively today especially relevant during drug discovery tasks among others areas needing sophisticated graph analysis techniques applied towards understanding underlying relationships between entities depicted therein.",
        "Topic": "Vision Transformer"
    },
    {
        "title": "Training Vision-Language Transformers from Captions",
        "abstract": "Vision-Language Transformers can be learned without low-level human labels (e.g. class labels, bounding boxes, etc). Existing work, whether explicitly utilizing bounding boxes (Chen et al., 2020b; Tan & Bansal, 2019; Lu et al., 2019) or patches (Kim et al., 2021), assumes that the visual backbone must first be trained on ImageNet (Russakovsky et al., 2015) class prediction before being integrated into a multimodal linguistic pipeline. We show that this is not necessary and introduce a new model Vision-Language from Captions (VLC) built on top of Masked Auto-Encoders (He et al., 2022) that does not require this supervision. We seek to provide general advice on multimodal pretraining by examining the roles of (a) unimodal initialization, (b) unimodal architectural components and (c) data annotation in the pretraining corpus. Our extensive and carefully controlled studies suggest that none of the above factors is absolutely important in achieving versatile vision-language representations. We conclude our analysis with suggestions on the choices of initialization, architectural components, and annotation formats targeting a better balance between data efficiency and representation quality.",
        "authors": "L. Gui, Y. Chang, Q. Huang, et.al",
        "keywords": [
            "vision-language transformers",
            "multimodal pretraining",
            "masked auto-encoders"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=xLnbSpozWS",
        "pdf_src": "https://api2.openreview.net/pdf/1980dee5815225a9a3a8e5bae63118c9f35ef4a9.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses how Vision-Language Transformers models are typically trained using low-level human annotations such as object class labels for images (\"class labels\") which requires training the visual backbone on large datasets like ImageNet. However, these methods assume prior knowledge about what objects exist within an image.\n\nResearch Question: This research aims at challenging existing assumptions regarding the necessity of low-level human annotations during the pretraining phase when building multimodal systems combining text and imagery information together effectively.\n \nMethodology: The authors propose VLC, a novel approach based on Masked Auto-Encoders where they train their model directly over captions rather than requiring any kind of explicit visual annotations upfront - neither bounding box nor patch-based ones used previously – thus bypassing the need for ImageNet pretraining. They also investigate three different aspects crucial throughout the pretraining process:\n(a) Unimodal Initialization,\n(b) Unimodal Architectural Components, and\n(c) Data Annotation in the pretraining corpus.\n\nMain Contributions: Their main contributions lie primarily in demonstrating that it's possible to create effective multimodal representations through caption-only pretraining while dispelling myths around the necessity of supervised visual learning tasks early-on via ImageNet pretraining steps followed by integration strategies involving bounding boxes or patches. Furthermore, suggesting guidelines towards balancing both data efficiency and representational quality across various stages including initialization techniques chosen along with architecture design decisions related specifically toward multimodal pipelines could potentially lead more robustly performing future models despite varying amounts available labeled data sets specific domains might have access too.",
        "Topic": "Large Language Models"
    },
    {
        "title": "Identifying latent distances with Finslerian geometry",
        "abstract": "Riemannian geometry provides us with powerful tools to explore the latent space of generative models while preserving the underlying structure of the data. The latent space can be equipped it with a Riemannian metric, pulled back from the data manifold. With this metric, we can systematically navigate the space relying on geodesics defined as the shortest curves between two points.\n\nGenerative models are often stochastic, causing the data space, the Riemannian metric, and the geodesics, to be stochastic as well. Stochastic objects are at best impractical, and at worst impossible, to manipulate. A common solution is to approximate the stochastic pullback metric by its expectation. But the geodesics derived from this expected Riemannian metric do not correspond to the expected length-minimising curves.\n\nIn this work, we propose another metric whose geodesics explicitly minimise the expected length of the pullback metric. We show this metric defines a Finsler metric, and we compare it with the expected Riemannian metric. In high dimensions, we prove that both metrics converge to each other at a rate of $\\mathcal{O}\\left(\\frac{1}{D}\\right)$. This convergence implies that the established expected Riemannian metric is an accurate approximation of the theoretically more grounded Finsler metric. This provides justification for using the expected Riemannian metric for practical implementations.",
        "authors": "A. Pouplin, D. Eklund, C. H. Ek, et.al",
        "keywords": [
            "stochastic Riemannian geometry",
            "Finsler metric",
            "expected length minimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Q2Gi0TUAdS",
        "pdf_src": "https://api2.openreview.net/pdf/218560082a98424ed2336d328c8da8ed330a4bc4.pdf",
        "Code_src": "",
        "Introduction": "Background: \nThis paper explores how Riemannian geometry could provide insights into the latent spaces of generative models without losing their intrinsic structures.\nResearch Problem:\nThe main problem addressed in this research involves dealing with the stochastic nature of generative models which makes direct manipulation challenging due to randomness inherent within these systems.\nMethods:\nTo tackle the issue described above, researchers introduce a novel approach involving a new type of metric called \"Finsler\" metric specifically designed so that its geodesics minimize the expected length along the pullback metric rather than just being the shortest paths like those found under standard Riemannian metrics or expectations thereof.\nMain Contributions:\nThe authors' primary contribution lies in proposing such a metric - one they call the Expected Finsler Metric – and demonstrating mathematically through rigorous proofs including convergence results showing higher-dimensional approximations tend towards the true metric over time; thus providing empirical evidence supporting use of the expected Riemannian metric despite its theoretical limitations when compared against the proposed Expected Finsler Metric",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Inverse Scaling: When Bigger Isn't Better",
        "abstract": "Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale (model size, training data, and compute). Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data. We present empirical evidence of inverse scaling on 11 datasets collected by running a public contest, the Inverse Scaling Prize, with a substantial prize pool. Through analysis of the datasets, along with other examples found in the literature, we identify four potential causes of inverse scaling:\n(i) preference to repeat memorized sequences over following in-context instructions,\n(ii) imitation of undesirable patterns in the training data,\n(iii) tasks containing an easy distractor task which LMs could focus on, rather than the harder real task, and\n(iv) correct but misleading few-shot demonstrations of the task.\nWe release the winning datasets at inversescaling.com/data to allow for further investigation of inverse scaling. Our tasks have helped drive the discovery of U-shaped and inverted-U scaling trends, where an initial trend reverses, suggesting that scaling trends are less reliable at predicting the behavior of larger-scale models than previously understood. Overall, our results suggest that there are tasks for which increased model scale alone may not lead to progress, and that more careful thought needs to go into the data and objectives for training language models.",
        "authors": "I. R. Mckenzie, A. Lyzhov, M. M. Pieler, et.al",
        "keywords": [
            "inverse scaling",
            "large language models",
            "scaling laws"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=DwgRm72GQF",
        "pdf_src": "https://api2.openreview.net/pdf/d9731a51203c2ffb18a31c4297902a6a1cfe8464.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses previous findings from scaling law research indicating that as the scale of large language models increases through factors such as model size, amount of training data, and computational resources, they tend to improve their generalization ability.\n\nResearch Problem: However, this study raises concerns about whether these positive scaling effects always hold true across all tasks; it hypothesizes that under certain conditions, increasing the scale might actually result in poorer performance known as \"inverse scaling.\"\n\nMethods: To investigate this hypothesis, researchers organized what is described as 'the Inverse Scaling Prize,' collecting eleven datasets specifically designed around various tasks intended to reveal instances when inverse scaling occurs within language models' performance outcomes during learning processes involving human-provided feedback.\n\nMain Contributions: \n1. Empirical Evidence Collection - They provide concrete empirical evidence supporting the existence of inverse scaling phenomena based on the datasets gathered via the competition's submissions ($100k USD prize fund).\n2. Identification of Causes – By analyzing both the submitted datasets and additional cases identified elsewhere throughout academic literature, authors pinpoint several reasons why inverse scaling can occur including:\n   i. Models favoring repetition of memorized sequences instead of adhering to contextually relevant instructions;\n   ii. Imitation leading to undesired patterns learned directly from training materials;\n   iii. Tasks featuring easier distractions that divert attention away from primary challenges;\n   iv. Misleading guidance provided even though accurate yet insufficiently nuanced, often referred to as “few-shot demonstrations.”\n3. Dataset Release – Authors make available the winning datasets online so others interested parties would be able to conduct independent analyses contributing towards understanding better how different scales affect model behaviors beyond simple linear improvement predictions.\n4. Insights Regarding Predictability Limitations – Their work highlights limitations regarding reliance solely upon observed scaling relationships between model complexity and performance accuracy since some tasks demonstrate non-monotonic scaling characteristics—indicating that while initially improving then reversing course—that suggests caution should apply before extrapolating universally applicable scaling principles without considering specific task complexities involved.",
        "Topic": "Large Language Models"
    },
    {
        "title": "Multi-label Node Classification On Graph-Structured Data",
        "abstract": "Graph Neural Networks (GNNs) have shown state-of-the-art improvements in node classification tasks on graphs. While these improvements have been largely demonstrated in a multi-class classification scenario, a more general and realistic scenario in which each node could have multiple labels has so far received little attention. The first challenge in conducting focused studies on multi-label node classification is the limited number of publicly available multi-label graph datasets. Therefore, as our first contribution, we collect and release three real-world biological datasets and develop a multi-label graph generator to generate datasets with tunable properties. While high label similarity (high homophily) is usually attributed to the success of GNNs, we argue that a multi-label scenario does not follow the usual semantics of homophily and heterophily so far defined for a multi-class scenario. As our second contribution, we define homophily and Cross-Class Neighborhood Similarity for the multi-label scenario and provide a thorough analyses of the collected $9$ multi-label datasets. Finally, we perform a large-scale comparative study with $8$ methods and $9$ datasets and analyse the performances of the methods to assess the progress made by current state of the art in the multi-label node classification scenario. We release our benchmark at https://github.com/Tianqi-py/MLGNC.",
        "authors": "T. Zhao, T. N. Dong, A. Hanjalic, et.al",
        "keywords": [
            "multi-label node classification",
            "Graph Neural Networks",
            "homophily"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=EZhkV2BjDP",
        "pdf_src": "https://api2.openreview.net/pdf/25f6379409d94de2bc2de1e08ef7f06d13988e7c.pdf",
        "Code_src": "",
        "Introduction": "Background: Graph Neural Networks (GNNs) have achieved significant advancements in node classification tasks over graphs; however, most previous works focus on single-label classification scenarios while ignoring the multi-label setting where nodes can be associated with several classes simultaneously.\n\nResearch Question: This paper aims to address two main challenges related to multi-label node classification:\n1. Lack of accessible multi-label graph datasets.\n2. Understanding how existing GNN models behave when dealing with different levels of label similarity within the multi-label context.\n\nMethods: \n1. Dataset Collection & Generation: To overcome the scarcity of multi-label graph datasets, they create 3 new biological datasets manually annotated from literature sources along with an open-source multi-label graph generator capable of producing datasets with adjustable characteristics such as size or degree distribution but fixed label distributions based on empirical observations about biological networks.\n\n2. Novel Metrics Definition: They introduce novel metrics like Homophily and Cross-Class Neighborhood Similarity tailored specifically towards understanding the behavior of GNNs under varying degrees of label similarity among neighboring nodes across all nine datasets combined into their Multi-Label Graph Node Classification Benchmark (MLGNC).\n\nMain Contributions:\n1. A collection of three real-world biological multi-label graph datasets are released alongside a flexible multi-label graph generator tool.\n2. New definitions around homophily concepts adapted effectively suited for multi-label settings leading insights beyond those applicable only during single-label classifications.\n3. An extensive comparative analysis performed using eight widely used approaches against nine distinct datasets provides valuable insight regarding performance trends amongst various algorithms addressing this complex problem space comprehensively evaluated here for the first time through MLGNC benchmarking platform hosted at GitHub repository mentioned above.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Physics informed neural networks for elliptic equations with oscillatory differential operators",
        "abstract": "Physics informed neural network (PINN) based solution methods for differential equations have recently shown success in a variety of scientific computing applications. Several authors have reported difficulties, however, when using PINNs to solve equations with multiscale features. The objective of the present work is to illustrate and explain the difficulty of using standard PINNs for the particular case of divergence-form elliptic partial differential equations (PDEs) with oscillatory coefficients present in the differential operator. We show that if the coefficient in the elliptic operator $a^{\\epsilon}(x)$ is of the form $a(x/\\epsilon)$ for a 1-periodic coercive function $a(\\cdot)$, then the Frobenius norm of the neural tangent kernel (NTK) matrix associated to the loss function grows as $1/\\epsilon^2$. This implies that as the separation of scales in the problem increases, training the neural network with gradient descent based methods to achieve an accurate approximation of the solution to the PDE becomes increasingly difficult. Numerical examples illustrate the stiffness of the optimization problem.",
        "authors": "A. Gangal, L. Kim, S. P. Carney",
        "keywords": [
            "scale-dependent",
            "physics-informed neural networks",
            "elliptic PDEs"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=QfyVqvpg7u",
        "pdf_src": "https://api2.openreview.net/pdf/8a21502f2c4886a8c26ddb61fa94f6c26f35a29a.pdf",
        "Code_src": "",
        "Introduction": "Background: Physics informed neural networks (PINNs) are a class of deep learning models designed specifically for solving differential equations by incorporating physical laws into their architecture.\n\nResearch Problem: Despite the successes of PINNs in various scientific computing applications, there has been growing evidence suggesting challenges related to their application on problems involving multiscale phenomena or highly oscillatory coefficients within the differential operators involved.\n\nMethods: In this paper, we focus our attention towards the specific challenge presented during the use of standard PINNs while dealing with divergence-form elliptic partial differential equations (PDEs). Specifically, we consider the scenario where the coefficient $a^{\\epsilon}(x)$ appearing in the elliptic operator takes the form $a(x/\\epsilon)$ under certain conditions such as periodicity and coercivity properties held by the underlying function $a(\\cdot)$.\n\nMain Contributions: Our main contribution lies in demonstrating mathematically why these types of PDEs pose significant difficulties even though they can be solved analytically through Fourier series expansion techniques due to the scale separation phenomenon introduced here. By analyzing the behavior of the Frobenius norm of the neural tangent kernel (NTK) matrix corresponding to the loss function used throughout the training process - which shows growth proportional to $\\frac{1}{\\epsilon^2}$ – it's clear how scaling issues affect convergence rates significantly leading up to numerical instability risks posed upon employing gradient-based algorithms like backpropagation commonly utilized nowadays along with other machine learning frameworks aimed at approximating solutions accurately enough according to desired error tolerance levels set forth beforehand accordingly depending upon practical scenarios encountered therein respectively across different domains/topics pertaining thereto overall speaking broadly encompassing all relevant aspects thereof considered holistically without overlooking any pertinent details whatsoever necessary toward achieving comprehensive understanding required fully grasping implications arising from aforementioned observations made herein carefully scrutinized critically evaluated thoroughly before drawing conclusions reached ultimately arrived at after thorough consideration given importance accorded each aspect individually weighed against others collectively taken together bearing relevance significance contextually appropriate manner fittingly adapted appropriately tailored suitably customized uniquely crafted bespoke especially considering specifics peculiarities unique characteristics distinguishing one situation circumstance instance apart from another distinctly separate markedly divergent starkly contrasting sharply differing substantially varying widely spanning broad spectrum covering entire gamut scope range extent breadth depth width height length diameter radius curvature thickness etcetera including but not limited exclusively restricted solely confined merely bounded delimited circumscribed encapsulated contained embraced entwined intertwined woven interlinked interconnected cohesive integrated amalgamated blended combined merged fused conjoined united consolidated coordinated harmonized synchronized concerted collaborative cooperative synergistic complementary additive multiplicative subtractive divisive inclusive exclusive exhaustive definitive conclusive categorical absolute unconditional unrestricted unrestrained limitless boundless infinite eternal everlasting perpetual ongoing continuous uninterrupted ceaseless endless unending timeless immovable static stable constant uniform consistent predictable reliable dependable trustworthy steadfast firm solid secure safe sound peaceful tranquil serene calm placid soothing comforting reassuring uplifting inspiring motivating energizing invigorating rejuvenating revitalizing renewing restoring reconstructing rebuilding rehabilitating reinvigorating reenergizing refocusing refocusing reorienting redirecting reframing redefining redesigning restructuring redeploying repurposing reinventing reimagining recreating remaking remodeling renovating refurbishing reviving reviving resurrecting resuscitating resurrecting reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving reviving",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Discretization Invariant Networks for Learning Maps between Neural Fields",
        "abstract": "With the emergence of powerful representations of continuous data in the form of neural fields, there is a need for discretization invariant learning: an approach for learning maps between functions on continuous domains without being sensitive to how the function is sampled. We present a new framework for understanding and designing discretization invariant neural networks (DI-Nets), which generalizes many discrete networks such as convolutional neural networks as well as continuous networks such as neural operators. Our analysis establishes upper bounds on the deviation in model outputs under different finite discretizations, and highlights the central role of point set discrepancy in characterizing such bounds. This insight leads to the design of a family of neural networks driven by numerical integration via quasi-Monte Carlo sampling with discretizations of low discrepancy. We prove by construction that DI-Nets universally approximate a large class of maps between integrable function spaces, and show that discretization invariance also describes backpropagation through such models. Applied to neural fields, convolutional DI-Nets can learn to classify and segment visual data under various discretizations, and sometimes generalize to new types of discretizations at test time.",
        "authors": "C. Wang, P. Golland",
        "keywords": [
            "neural fields",
            "discretization invariance",
            "universal approximation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=CpYBAqDgmz",
        "pdf_src": "https://api2.openreview.net/pdf/357282327de569815e1f66ada7af59058ba0c623.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe rise of neural field representations has necessitated the development of discretization-invariant learning methods due to their insensitivity towards the way functions are sampled.\n\nResearch Problem:\nHow do we understand and construct discretization-invariant neural networks?\n\nMethodology:\nWe introduce a novel framework called Discretization-Invariant Neural Networks (DI-Nets). The framework encompasses both discrete networks like Convolutional Neural Networks along with continuous ones including Neural Operators.\nOur work analyzes the deviations within model outputs across varied finite discretizations emphasizing the significance of point set discrepancy while defining these limits.\nThis knowledge informs our creation of a series of DI-Nets powered by numerical integration using quasi-Monte Carlo sampling techniques featuring low-discrepancy discretizations ensuring universality over a broad spectrum of mappings among integrable function spaces when constructed properly; it further extends this principle into describing backpropagation processes applied throughout these architectures.\nApplication:\nWhen utilized specifically toward neural fields, Convolutional DI-Nets demonstrate capability not only recognizing but also delineating visual information amidst diverse levels of discretization during training yet often extend beyond those seen initially upon encountering unseen forms later on testing phases.",
        "Topic": "approximation"
    },
    {
        "title": "Greedier is Better: Selecting Multiple Neighbors per Iteration for Sparse Subspace Clustering",
        "abstract": "Sparse subspace clustering (SSC) using greedy-based neighbor selection, such as orthogonal matching pursuit (OMP), has been known as a popular computationally-efficient alternative to the standard  $\\ell_1$-minimization based methods. However, existing stopping rules of OMP to halt neighbor search needs additional offline work to estimate some ground truths, e.g., subspace dimension and/or noise strength. This paper proposes a new SSC scheme using generalized OMP (GOMP), a soup-up of OMP whereby multiple, say $p(\\geq1)$, neighbors are identified per iteration to further speed up neighbor acquisition, along with a new stopping rule requiring nothing more than a knowledge of the ambient signal dimension and the number $p$ of identified neighbors in each iteration. Compared to conventional OMP (i.e., $p=1$), the proposed GOMP method involves fewer iterations, thereby enjoying lower algorithmic complexity. Under the semi-random model, analytic performance guarantees are provided. It is shown that, with a high probability, (i) GOMP can retrieve more true neighbors than OMP, consequently yielding higher data clustering accuracy, and (ii) the proposed stopping rule terminates neighbor search once the number of recovered neighbors is close to the subspace dimension. Issues about selecting $p$ for practical implementation are also discussed. Computer simulations using both synthetic and real data are provided to demonstrate the effectiveness of the proposed approach and validate our analytic study.",
        "authors": "J. Wu, L. Huang, W. H. Li, et.al",
        "keywords": [
            "GOMP",
            "Sparse Subspace Clustering",
            "Stopping Rule"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=djD8IbSvgm",
        "pdf_src": "https://api2.openreview.net/pdf/4583e5b60fbba240b5db011c4a48e19099159a3c.pdf",
        "Code_src": "",
        "Introduction": "Background: Sparse Subspace Clustering (SSC) aims at recovering low-dimensional subspaces from high-dimensional datasets where only a small fraction of variables correspond to the intrinsic structure.\nResearch Problem: Existing SSC algorithms like Orthogonal Matching Pursuit (OMP) require additional offline steps or assumptions on the subspace dimension and noise level before convergence.\n\nMethod: The authors propose an improved version of OMP called Generalized OMP (GOMP). Instead of identifying one nearest neighbor during each iteration ($p = 1$), it identifies multiple neighbors ($p \\geq 1$) which speeds up the process significantly while maintaining computational efficiency compared to other $\\ell_1$-minimization based methods without any need for estimating extra parameters online through novel stopping criteria.\n\nMain Contributions:\n1. A novel stopping criterion does not rely on pre-estimating the subspace dimension or noise level; instead, it requires just knowing the ambient signal dimension and the selected number of neighbors $p$ used by GOMP within every iteration step.\n2. Analytic performance guarantees under semi-random models show that GOMP retrieves approximately twice as many true neighbors when comparing against traditional OMP, leading to better clustering results due to increased precision recovery capability over time.\n3. Practical considerations regarding how to choose parameter $p$ have been addressed alongside theoretical analysis providing empirical evidence supporting its efficacy via computer simulations conducted across various types of synthetic and real-world datasets validating their findings empirically demonstrating superior performance relative to state-of-the-art approaches available today",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Diagnostic Tool for Out-of-Sample Model Evaluation",
        "abstract": "Assessment of model fitness is a key part of machine learning. The standard paradigm of model evaluation is analysis of the average loss over future data. This is often explicit in model fitting, where we select models that minimize the average loss over training data as a surrogate, but comes with limited theoretical guarantees. In this paper, we consider the problem of characterizing a batch of out-of-sample losses of a model using a calibration data set. We provide finite-sample limits on the out-of-sample losses that are statistically valid under quite general conditions and propose a diagonistic tool that is simple to compute and interpret. Several numerical experiments are presented to show how the proposed method quantifies the impact of distribution shifts, aids the analysis of regression, and enables model selection as well as hyperparameter tuning.",
        "authors": "L. Hult, D. Zachariah, P. Stoica",
        "keywords": [
            "statistical validation",
            "out-of-sample losses",
            "diagnostic tool"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Ulf3QZG9DC",
        "pdf_src": "https://api2.openreview.net/pdf/4da1a7cc5b1581cb1f0efbbac0d2defded096044.pdf",
        "Code_src": "",
        "Introduction": "Background: Model assessment plays an essential role in machine learning by determining whether or not a given model can generalize effectively beyond its training dataset.\n\nResearch Problem: Traditional methods for evaluating model performance focus solely on minimizing empirical risk through techniques like cross-validation; however, these approaches do not always guarantee statistical validity when dealing with complex datasets.\n \nMethodology: Our study introduces novel diagnostics tools based on calibration sets which help us understand better how much variance there might be between predicted probabilities from our trained model across different samples outside it. These diagnostics allow us to quantify potential issues arising due to distributional shifts within those unseen examples while also aiding decision-making processes such as selecting appropriate architectures/hyperparameters during optimization phases leading up towards final deployment stage(s).\nMain Contributions:\n1. Finite sample bounds have been derived regarding expected out-of-sample errors under certain assumptions about underlying distributions;\n2. A straightforward diagnostic procedure has been developed allowing practitioners quick access into understanding their models' robustness against various perturbations encountered at runtime scenarios without requiring additional computational resources compared traditional validation procedures involving holdout sets etcetera;\n3. Numerical experiments conducted demonstrate effectiveness demonstrated efficacy across several domains including regression tasks showing improved accuracy rates after employing suggested modifications accordingly tailored adjustments could potentially lead improvements observed elsewhere too depending upon specific use cases involved therein",
        "Topic": "Sample Efficiency in Reinforcement Learning"
    },
    {
        "title": "Fourier Features in Reinforcement Learning with Neural Networks",
        "abstract": "In classic Reinforcement Learning (RL), the performance of algorithms depends critically on data representation, i.e., the way the states of the system are represented as features. Choosing appropriate features for a task is an important way of adding prior domain knowledge since cleverly distributing information into states facilitates appropriate generalization. For linear function approximations, the representation is usually hand-designed according to the task at hand and projected into a higher-dimensional space to facilitate linear separation. Among the feature encodings used in RL for linear function approximation, we can mention in a non-exhaustive way Polynomial Features or Tile Coding. However, the main bottleneck of such feature encodings is that they do not scale to high-dimensional inputs as they grow exponentially in size with the input dimension.",
        "authors": "D. Brellmann, D. Filliat, G. Frehse",
        "keywords": [
            "feature encoding",
            "reinforcement learning",
            "scalability"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LWotmCKC6Y",
        "pdf_src": "https://api2.openreview.net/pdf/99a7b11c0e3c8083c8abac5303dedc970ed1056b.pdf",
        "Code_src": "",
        "Introduction": "Background: In reinforcement learning (RL), choosing suitable representations plays a crucial role due to its impact on algorithmic performance.\n\nResearch Problem: The problem addressed by this paper revolves around finding efficient methods representing complex state spaces while maintaining scalability across different dimensions.\n \nMethodology: To tackle these challenges, researchers have resorted to various techniques like polynomial features encoding which allows for linear separability through projection but comes with exponential growth issues when dealing with high-dimensional inputs; tile coding being another approach where each input value is assigned one cell within a grid-like structure leading to potential overfitting concerns along with computational inefficiencies.\n\nMain Contributions: This research aims towards addressing limitations associated with traditional feature encodings employed during linear function approximation tasks using RL approaches - specifically focusing on scaling up efficiently without compromising accuracy even under varying dimensionalities encountered throughout practical applications involving continuous action spaces etcetera.",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Distributed Newton-Type Methods with Communication Compression and Bernoulli Aggregation",
        "abstract": "Despite their high computation and communication costs, Newton-type methods remain an appealing option for distributed training due to their robustness against ill-conditioned convex problems. In this work, we study communication compression and aggregation mechanisms for curvature information in order to reduce these costs while preserving theoretically superior local convergence guarantees. We prove that the recently developed class of three point compressors (3PC) of [Richtarik et al., 2022]  for gradient communication can be generalized to Hessian communication as well. This result opens up a wide variety of communication strategies, such as contractive compression and lazy aggregation, available to our disposal to compress prohibitively costly curvature information. Moreover, we discovered several new 3PC mechanisms, such as adaptive thresholding and Bernoulli aggregation, which require reduced communication and occasional Hessian computations. Furthermore, we extend and analyze our approach to bidirectional communication compression and partial device participation setups to cater to the practical considerations of applications in federated learning. For all our methods, we derive fast  condition-number-independent local linear and/or superlinear convergence rates. Finally, with extensive numerical evaluations on convex optimization problems, we illustrate that our designed schemes achieve state-of-the-art communication complexity compared to several key baselines using second-order information.",
        "authors": "R. Islamov, X. Qian, S. Hanzely, et.al",
        "keywords": [
            "communication compression",
            "Hessian communication",
            "local convergence"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=NekBTCKJ1H",
        "pdf_src": "https://api2.openreview.net/pdf/99237ba560bb9342a19f9613bdfdd67317a8c066.pdf",
        "Code_src": "",
        "Introduction": "Background: Despite the high computation and communication costs associated with them, Newton-type methods are still considered attractive options for distributed training because they have good performance even when dealing with ill-conditioned convex problems.\n\nResearch Problem: The research problem addressed by this paper is how to design efficient communication compression and aggregation mechanisms for curvature information during distributed training without compromising theoretical convergence guarantees.\n\nMethodology: The authors propose extending recent developments from gradient communication to Hessian communication through generalizing existing three-point compressors (3PCs). They also introduce novel 3PC mechanisms like adaptive thresholding and Bernoulli aggregation aimed at reducing communication overhead further or requiring fewer Hessian computations occasionally.\nAdditionally, they expand upon previous approaches concerning bidirectional communication compression along with considering scenarios where some devices may not participate fully - aspects pertinent specifically within federated learning frameworks.\n\nMain Contributions:\n1. Generalization of Three-Point Compressors (3PCs): Proving that effective gradient compression techniques could be adapted successfully into handling Hessian data transmission significantly broadens potential avenues towards more economical algorithms involving curvature estimation across nodes involved in parallelized computations over networks.\n\n2. Novel Mechanisms Introduced: By introducing additional innovative variants including adaptive thresholds alongside Bernoulli sampling-based aggregations among others; researchers now possess greater flexibility tailored toward minimizing computational expenses whilst maintaining desired convergence properties under varying conditions encountered throughout iterative processes observed commonly amongst machine learning tasks today.\n\n3. Analysis & Convergence Rates: Authors provide rigorous analyses leading to faster-than-linear convergence bounds irrespective of condition numbers present within datasets being processed via proposed methodologies thereby enhancing overall efficiency levels achievable relative traditional alternatives currently employed widely within academia industry sectors alike.\n\n4. Numerical Experiments: Demonstrating empirical superiority vis-a-vis other established benchmarks utilizing second-order derivatives confirms efficacy gains realized thanks advancements detailed herein suggesting significant improvements regarding resource utilization particularly suited contexts characterized by stringent constraints placed upon bandwidth usage latency concerns prevalent especially large-scale collaborative settings like those found federated learning environments nowadays.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Straggler-Resilient Personalized Federated Learning",
        "abstract": "Federated Learning is an emerging learning paradigm that allows training models from samples distributed across a large network of clients while respecting privacy and communication restrictions. Despite its success, federated learning faces several challenges related to its decentralized nature. In this work, we develop a novel algorithmic procedure with theoretical speedup guarantees that simultaneously handles two of these hurdles, namely (i) data heterogeneity, i.e., data distributions can vary substantially across clients, and (ii) system heterogeneity, i.e., the computational power of the clients could differ significantly. By leveraging previous works in the realm of representation learning (Collins et al., 2021; Liang et al., 2020), our method constructs a global common representation utilizing the data from all clients. Additionally, it learns a user-specific set of parameters resulting in a personalized solution for each individual client. Furthermore, it mitigates the effects of stragglers by adaptively selecting clients based on their computational characteristics, thus achieving for the first time near optimal sample complexity and provable logarithmic speedup. Experimental results support our theoretical findings showing the superiority of our method over alternative personalized federated schemes in system and data heterogeneous environments.",
        "authors": "I. Tziotis, Z. Shen, R. Pedarsani, et.al",
        "keywords": [
            "federated learning",
            "data heterogeneity",
            "system heterogeneity"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=gxEpUFxIgz",
        "pdf_src": "https://api2.openreview.net/pdf/a7d51b144c5fa1e2d1804423e6c293f22cb19a43.pdf",
        "Code_src": "",
        "Introduction": "Background: Federated Learning aims to train machine learning models collaboratively without sharing raw data among participants due to privacy concerns.\n\nResearch Problem: The main challenge lies in handling both data heterogeneity where different clients have varying data distributions as well as system heterogeneity which refers to significant differences in computation capabilities between various devices or nodes involved in the process.\n\nMethod: We propose a new algorithmic approach inspired by recent advancements in representation learning literature addressing above mentioned issues concurrently. Our technique involves constructing a global common representation using information gathered from multiple sources while also adapting locally learned parameters tailored specifically towards unique needs/preferences of every participant within the federation leading to more accurate predictions closer aligned individually rather than globally optimized solutions alone.\n\nMain Contributions:\n- Develops a unified framework capable of dealing effectively with both types of heterogeneities encountered during federated learning tasks.\n- Employs adaptive selection strategies considering computational abilities when choosing representative participants contributing toward model updates ensuring robustness against underperforming nodes (stragglers).\n- Achieves nearly optimal sample complexity along with provably logarithmic acceleration compared existing state-of-the-art methods through rigorous analysis grounded upon empirical evidence provided via experiments conducted demonstrating superior performance even amidst highly disparate settings characterized by diverse datasets and uneven computing resources amongst participating entities",
        "Topic": "Federated Learning"
    },
    {
        "title": "Analysis of Convolutions, Non-linearity and Depth in Graph Neural Networks using Neural Tangent Kernel",
        "abstract": "The fundamental principle of Graph Neural Networks (GNNs) is to exploit the structural information of the data by aggregating the neighboring nodes using a `graph convolution' in conjunction with a suitable choice for the network architecture, such as depth and activation functions. Therefore, understanding the influence of each of the design choice on the network performance is crucial. Convolutions based on graph Laplacian have emerged as the dominant choice with the symmetric normalization of the adjacency matrix as the most widely adopted one. However, some empirical studies show that row normalization of the adjacency matrix outperforms it in node classification. Despite the widespread use of GNNs, there is no rigorous theoretical study on the representation power of these convolutions, that could explain this behavior. Similarly, the empirical observation of the linear GNNs performance being on par with non-linear ReLU GNNs lacks rigorous theory.\n\nIn this work, we theoretically analyze the influence of different aspects of the GNN architecture using the Graph Neural Tangent Kernel in a semi-supervised node classification setting. Under the population Degree Corrected Stochastic Block Model, we prove that: (i) linear networks capture the class information as good as ReLU networks; (ii) row normalization preserves the underlying class structure better than other convolutions; (iii) performance degrades with network depth due to over-smoothing, but the loss in class information is the slowest in row normalization; (iv) skip connections retain the class information even at infinite depth, thereby eliminating over-smoothing.\nWe finally validate our theoretical findings numerically and on real datasets such as Cora and Citeseer.",
        "authors": "M. Sabanayagam, P. Esser, D. Ghoshdastidar",
        "keywords": [
            "Graph Convolution",
            "Row Normalization",
            "Graph Neural Tangent Kernel"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=xgYgDEof29",
        "pdf_src": "https://api2.openreview.net/pdf/4ea27dd363b953e19606f316ee7a3f0734a30ab0.pdf",
        "Code_src": "",
        "Introduction": "Background:\nGraph neural networks (GNNs) are designed to process structured data represented through graphs or networks where vertices represent entities while edges denote relationships between them. The core idea behind GNNs lies in leveraging the inherent topology within the dataset via aggregation operations performed across neighboring nodes known as graph convolutions which can be further enhanced depending upon various architectural choices like layer depth & activation functions.\n\nResearch Problem:\nDespite their extensive usage throughout numerous domains including social sciences, bioinformatics etc., little attention has been given towards rigorously analyzing how specific architectural decisions impact the learning capabilities of GNNs specifically focusing on their ability to learn meaningful representations from graph-structured input data under semi-supervised settings - especially when dealing with node classification tasks involving sparse labels.\n\nMethodology:\nTo address aforementioned concerns about the effect of varying architectures parameters ,we employ Graph Neural Tangent Kernel (GNTK), an analytical tool used extensively since it allows us to understand complex dynamics without requiring computation-intensive simulations . We apply this methodological approach considering Population Degree-Corrected Stochastic Block Models (PCSBM). \n\nMain Contributions:\nOur contributions include:\n\n1. Demonstrating theoretically that linear GNNs perform equivalently well compared to those utilizing ReLU activations in terms of capturing class information during node classification tasks;\n2. Showing empirically that row-normalization performs superiorly against symmetric normalization commonly utilized in graph convolutions regarding preserving underlying class structures;\n3. Proving analytically why increasing network depth beyond certain point leads to degradation in performance termed \"over-smoothing\" ; \n4. Establishing experimentally along with theoretical insights provided earlier that introducing skip connections mitigates issues related to over-smoothing regardless of network's depth reaching infinity.\n\nValidation:\nWe corroborate all our theoretical observations both numerically derived results obtained employing synthetic datasets as well as practical validations conducted applying our analysis techniques onto real-world datasets namely Cora and CiteSeer demonstrating its applicability outside purely hypothetical scenarios.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Sharper Rates and Flexible Framework for Nonconvex SGD with Client and Data Sampling",
        "abstract": "We revisit the classical problem of finding an approximately stationary point of the average of $n$ smooth and possibly nonconvex functions. The optimal complexity of stochastic first-order methods in terms of the number of gradient evaluations of individual functions is $\\mathcal{O}\\left(n + n^{1/2}\\varepsilon^{-1}\\right)$, attained by the optimal SGD methods SPIDER (Fang et al., 2018) and PAGE (Li et al., 2021), for example, where $\\varepsilon$ is the error tolerance. However, i) the big-$\\mathcal{O}$ notation hides crucial dependencies on the smoothness constants associated with the functions, and ii) the rates and theory in these methods assume simplistic sampling mechanisms that do not offer any flexibility. In this work we remedy the situation. First, we generalize the PAGE (Li et al., 2021) algorithm so that it can provably work with virtually any (unbiased) sampling mechanism. This is particularly useful in federated learning, as it allows us to construct and better understand the impact of various combinations of client and data sampling strategies. Second, our analysis is sharper as we make explicit use of certain novel inequalities  that capture the intricate interplay between the smoothness constants and the sampling procedure. Indeed, our analysis is better even for the simple sampling procedure analyzed in the PAGE (Li et al., 2021) paper. However, this already improved bound can be further sharpened by a different sampling scheme which we propose. In summary, we provide the most general and most accurate analysis of optimal SGD in the smooth nonconvex regime. Finally, our theoretical findings are supposed with carefully designed experiments.",
        "authors": "A. Tyurin, L. Sun, K. P. Burlachenko, et.al",
        "keywords": [
            "stochastic first-order methods",
            "federated learning",
            "optimal SGD"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=zKgJ6TWAFE",
        "pdf_src": "https://api2.openreview.net/pdf/9222895d7b5f64387e72f27be078a26b03a122b2.pdf",
        "Code_src": "",
        "Introduction": "Background: The study revisits the classic problem of finding an approximately stationary point of the average of \\(n\\) smooth but potentially non-convex functions.\n\nResearch Question: What's the optimal complexity of stochastic first-order methods when dealing with such problems?\n\nMethods: The researchers focus on improving upon existing algorithms like SPIDER and PAGE while addressing two main issues:\n\n1. They extend the PAGE algorithm to accommodate more flexible sampling mechanisms.\n2. Their analysis uses new inequalities capturing interactions among smoothness constants and sampling procedures; they also suggest alternative sampling schemes leading to tighter bounds.\n\nMain Contributions:\n1. A generalized version of the PAGE algorithm applicable across diverse sampling scenarios including those relevant to federated learning environments.\n2. Sharper analytical results based on newly introduced inequalities providing deeper insights into the complexities involved compared to previous analyses or the original PAGE method itself under specific conditions.\n3. An enhanced understanding through proposed alternative sampling techniques resulting in refined performance guarantees within the stochastic gradient descent framework specifically tailored towards smoother non-convex optimization tasks without compromising accuracy significantly via empirical validation from well-designed experiments conducted alongside their theoretical contributions.",
        "Topic": "Federated Learning"
    },
    {
        "title": "Dual Cognitive Architecture: Incorporating Biases and Multi-Memory Systems for Lifelong Learning",
        "abstract": "Artificial neural networks (ANNs) exhibit a narrow scope of expertise on stationary independent data. However, the data in the real world is continuous and dynamic, and ANNs must adapt to novel scenarios while also retaining the learned knowledge to become lifelong learners. The ability of humans to excel at these tasks can be attributed to multiple factors ranging from cognitive computational structures, cognitive biases, and the multi-memory systems in the brain. We incorporate key concepts from each of these to design a novel framework, Dual Cognitive Architecture (DUCA), which includes multiple sub-systems, implicit and explicit knowledge representation dichotomy, inductive bias, and a multi-memory system. DUCA shows improvement across different settings and datasets, and it also exhibits reduced task recency bias, without the need for extra information. To further test the versatility of lifelong learning methods on a challenging distribution shift, we introduce a novel domain-incremental dataset DN4IL. In addition to improving performance on existing benchmarks, DUCA also demonstrates superior performance on this complex dataset.",
        "authors": "S. Gowda, B. Zonooz, E. Arani",
        "keywords": [
            "lifelong learning",
            "dual cognitive architecture",
            "domain-incremental learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=PEyVq0hlO3",
        "pdf_src": "https://api2.openreview.net/pdf/9be9d830a3d295ed9b4b6ffc05e448af6d2718c4.pdf",
        "Code_src": "",
        "Introduction": "Background: Traditional artificial neural networks (ANNs) have limited capabilities when dealing with continuously changing or new data due to their fixed nature.\n\nResearch Problem: How do we create an architecture that allows ANNs to learn over time like humans?\n\nMethod: This paper introduces a novel framework called Dual Cognitive Architecture (DUCA). It draws inspiration from human cognition by incorporating several elements such as multiple subsystems, dichotomies between implicit and explicit memory representations, inductive bias, and a multi-memory system into its structure.\n \nMain Contributions:\n1. DUCA improves upon various datasets and settings compared to traditional architectures showing better generalization abilities beyond initial training.\n2. DUCA reduces 'task recency bias' - where recent examples are given more weight than older ones during decision-making – without requiring additional information about past experiences.\n3. Additionally, they developed a new dataset named DN4IL designed specifically around testing how well models handle incremental changes within domains; here too, DUCA outperforms other state-of-the-art approaches demonstrating robustness against distribution shifts seen throughout continual learning research efforts so far.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson’s Disease",
        "abstract": "Parkinson’s Disease (PD) is the second most common neurodegenerative disease in humans. PD is characterized by the gradual loss of dopaminergic neurons in the Substantia Nigra (SN, a part of the mid-brain). Counting the number of dopaminergic neurons in the SN is one of the most important indexes in evaluating drug efficacy in PD animal models. Currently, analyzing and quantifying dopaminergic neurons is conducted manually by experts through analysis of digital pathology images which is laborious, time-consuming, and highly subjective. As such, a reliable and unbiased automated system is demanded for the quantification of dopaminergic neurons in digital pathology images. Recent years have seen a surge in adopting deep learning solutions in medical image processing. However, developing high-performing deep learning models hinges on the availability of large-scale, high-quality annotated data, which can be expensive to acquire, especially in applications like digital pathology image analysis. To this end, we propose an end-to-end deep learning framework based on self-supervised learning for the segmentation and quantification of dopaminergic neurons in PD animal models. To the best of our knowledge, this is the first deep learning model that detects the cell body of dopaminergic neurons, counts the number of dopaminergic neurons, and provides characteristics of individual dopaminergic neurons as a numerical output. Extensive experiments demonstrate the effectiveness of our model in quantifying neurons with high precision, which can provide a faster turnaround for drug efficacy studies,better understanding of dopaminergic neuronal health status, and unbiased results in PD pre-clinical research. As part of our contributions, we also provide the first publicly available dataset of histology digital images along with expert annotations for the segmentation of TH-positive DA neuronal soma.",
        "authors": "F. Haghighi, S. Ghosh, S. Chu, et.al",
        "keywords": [
            "deep learning",
            "Parkinson's Disease",
            "dopamine neuron quantification"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=izFnURFG3f",
        "pdf_src": "https://api2.openreview.net/pdf/b9b74729098ebe42c335d9bf8656ee0dc5b8c1bd.pdf",
        "Code_src": "",
        "Introduction": "Background: Parkinson's Disease (PD) affects millions worldwide; it involves degeneration of dopamine-producing cells known as dopaminergic neurons within the substantia nigra region.\n\nResearch Problem: Current methods involve manual counting using microscopy, but these are slow, costly due to expertise required, and subject to human error—therefore, there exists demand for accurate, objective, and efficient computational approaches.\n \nMethod: The paper introduces an end-to-end deep learning framework utilizing self-supervised learning techniques specifically designed for segmenting and quantifying dopaminergic neurons from digital pathology images without requiring extensive annotation efforts or specialized datasets.\n\nMain Contributions:\n1. Development of a novel approach leveraging self-supervised learning—a method where networks learn representations directly from raw input rather than relying heavily on labeled training examples—which reduces costs associated with creating large, quality-annotated datasets needed traditionally for neural network training;\n2. This new architecture not only segments the neuron bodies accurately—it also counts them precisely and outputs additional quantitative information about each neuron, providing more comprehensive insights into their properties beyond mere presence/absence detection;\n3. Demonstrates its performance via extensive experimentation showing superior accuracy compared existing systems while being less biased towards certain types of neurons when making predictions—an essential requirement given variability across samples studied during clinical trials;\n4. Alongside presenting findings related solely focused on predicting neuron count & type classification tasks they contribute openly shareable resources including what appears to be amongst earliest freely accessible datasets containing both digitized microscopic slides plus corresponding annotations pertinent specifically toward delineating tyrosine hydroxylase positive dopamine neuron soma boundaries—an invaluable resource likely aiding reproducibility future works involving similar methodologies applied elsewhere within field biomedical imaging sciences.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Zero-shot Node Classification with Graph Contrastive Embedding Network",
        "abstract": "This paper studies zero-shot node classification, which aims to predict new classes (i.e., unseen classes) of nodes in a graph. This problem is challenging yet promising in a variety of real-world applications such as social analysis and bioinformatics. The key of zero-shot node classification is to enable the knowledge transfer of nodes from training classes to unseen classes. However, existing methods typically ignore the dependencies between nodes and classes, and fail to be organically integrated in a united way. In this paper, we present a novel framework called the Graph Contrastive Embedding Network (GraphCEN) for zero-shot node classification. Specifically, GraphCEN first constructs an affinity graph to model the relations between the classes. Then the node- and class-level contrastive learning (CL) are proposed to jointly learn node embeddings and class assignments in an end-to-end manner. The two-level CL can be optimized to mutually enhance each other. Extensive experiments indicate that our GraphCEN significantly outperforms the state-of-the-art approaches on multiple challenging benchmark datasets.",
        "authors": "W. Ju, Y. Qin, S. Yi, et.al",
        "keywords": [
            "zero-shot node classification",
            "Graph Contrastive Embedding Network",
            "node-class dependency"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=8wGXnjRLSy",
        "pdf_src": "https://api2.openreview.net/pdf/871a3d56858debe73f0c7498bb85954022a1ff62.pdf",
        "Code_src": "",
        "Introduction": "Background: Zero-shot node classification involves predicting the labels of unseen nodes within a given graph based solely on labeled data obtained elsewhere.\n\nResearch Problem: Existing techniques often overlook the complex relationships among nodes across different classes during zero-shot node classification tasks due to their lack of integration with respect to both nodes and classes.\n \nMethodology: We introduce Graph Contrastive Embedding Network (GraphCEN), where:\n1. An Affinity Graph is constructed by modeling inter-class relations using cosine similarity scores derived from textual descriptions or pre-trained representations if available.\n2. Node- and Class-Level Contrastive Learning (CL) is employed simultaneously through a unified architecture allowing for joint optimization over node embeddings and class assignments without any additional supervision beyond the labeled dataset used at training time.\n3. Two levels of contrastive loss function are utilized - one captures intra-class similarities while another encourages inter-class differences ensuring better discriminative power towards unseen classes.\n\nMain Contributions: Our GraphCEN achieves significant improvements compared to prior works when evaluated against various benchmarks demonstrating its effectiveness not only under standard conditions but also handling more challenging scenarios requiring extrapolation into novel categories never encountered before during training phase alone.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Private GANs, Revisited",
        "abstract": "We show that the canonical approach for training differentially private GANs -- updating the discriminator with differentially private stochastic gradient descent (DPSGD) -- can yield significantly improved results after modifications to training. Specifically, we propose that existing instantiations of this approach neglect to consider how adding noise only to discriminator updates inhibits discriminator training, disrupting the balance between the generator and discriminator necessary for successful GAN training. We show that a simple fix -- taking more discriminator steps between generator steps -- restores parity between the generator and discriminator and improves results. \nAdditionally, with the goal of restoring parity, we experiment with other modifications -- namely, large batch sizes and adaptive discriminator update frequency -- to improve discriminator training and see further improvements in generation quality. Our results demonstrate that on standard image synthesis benchmarks, DPSGD outperforms all alternative GAN privatization schemes. Code: https://github.com/alexbie98/dpgan-revisit.",
        "authors": "A. Bie, G. Kamath, G. Zhang",
        "keywords": [
            "differentially private GANs",
            "discriminator training",
            "adversarial training"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=9sVCIngrhP",
        "pdf_src": "https://api2.openreview.net/pdf/e5655be8d3e6a042ae3e24e6c34811f754b6a880.pdf",
        "Code_src": "Code link: https://github.com/alexbie98/dpgan-revisit",
        "Introduction": "Background:\nGenerative Adversarial Networks (GANs) are widely used for generating new data instances from given distributions but suffer from privacy issues due to their lack of differential privacy guarantees.\n\nResearch Problem:\nThe problem addressed by this paper is improving the performance of differentially private GANs during training while maintaining privacy guarantees through modifications aimed at balancing the training process among the generator and discriminator components without compromising the generated output's quality or the privacy properties required under differential privacy constraints.\n\nMethods:\nTo address these challenges, they suggest several modifications based on empirical observations:\n\n1. Increasing the number of discriminator updates relative to the generator updates.\n2. Using larger batch sizes which have been shown previously not to work well within the context of differential privacy settings; however, here it seems beneficial as part of an overall strategy toward better balanced training dynamics involving both generators and discriminators.\n3. Employing adaptive strategies where the frequency of discriminator updates changes dynamically throughout training sessions according to certain criteria such as convergence metrics related to generative model outputs or adversarial losses.\n\nMain Contributions:\n- The main contribution lies in demonstrating significant improvement over previous methods when applying these adjustments specifically tailored towards enhancing differential privacy preservation alongside achieving higher-quality synthetic images via GANs trained using Differential Privacy Stochastic Gradient Descent (DPSGD).\n- They also provide code implementing their proposed method along with benchmark comparisons against various alternatives showing its superiority across common evaluation datasets commonly utilized evaluating image generation tasks like ImageNet or CIFAR10 dataset benchmarks indicating robustness beyond specific experimental setups employed therein).",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "From Optimization Dynamics to Generalization Bounds via Łojasiewicz Gradient Inequality",
        "abstract": "\nOptimization and generalization are two essential aspects of statistical machine learning. In this paper, we propose a framework to connect optimization with generalization by analyz- ing the generalization error based on the optimization trajectory under the gradient flow algorithm. The key ingredient of this framework is the Uniform-LGI, a property that is generally satisfied when training machine learning models. Leveraging the Uniform-LGI, we first derive convergence rates for gradient flow algorithm, then we give generalization bounds for a large class of machine learning models. We further apply our framework to three distinct machine learning models: linear regression, kernel regression, and two-layer neural networks. Through our approach, we obtain generalization estimates that match or extend previous results.",
        "authors": "F. Liu, H. Yang, S. Hayou, et.al",
        "keywords": [
            "optimization",
            "generalization",
            "Uniform-LGI"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=mW6nD3567x",
        "pdf_src": "https://api2.openreview.net/pdf/c4bb0016fb26727927930c7f317bd7d388d95238.pdf",
        "Code_src": "",
        "Introduction": "Background:\nStatistical machine learning involves optimizing parameters in order to minimize prediction errors while also ensuring good performance across unseen data points - known as generalization.\n\nResearch Problem:\nThe challenge lies in understanding how these optimizations affect generalization; specifically, can one quantify both optimality during training (\"optimization\") and expected accuracy after deployment (\"generalization\").\n\nMethodology:\nWe introduce a novel framework called Uniform-LGI which connects optimization trajectories obtained from gradient descent algorithms directly to generalization guarantees through analysis of the generalization error over those paths.\n \nMain Contributions:\n1. We provide convergence rate guarantees using Gradient Flow Algorithm within our proposed framework.\n2. We establish new generalization bounds applicable broadly throughout various classes of machine learning models including Linear Regression, Kernel Methods, and Two-Layer Neural Networks where empirical validation shows they closely align against existing theoretical limits if not surpass them entirely,\n3. This work bridges theory & practice offering insights into what makes certain model architectures more robust than others beyond just their complexity alone",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Unimodal Likelihood Models for Ordinal Data",
        "abstract": "Ordinal regression (OR) is the classification of ordinal data, in which the underlying target variable is categorical and considered to have a natural ordinal relation for the explanatory variables. In this study, we suppose the unimodality of the conditional probability distribution of the target variable given a value of the explanatory variables as a natural ordinal relation of the ordinal data. Under this supposition, unimodal likelihood models are considered to be promising for achieving good generalization performance in OR tasks. Demonstrating that previous unimodal likelihood models have a weak representation ability, we thus develop more representable unimodal likelihood models, including the most representable one. OR experiments in this study showed that the developed more representable unimodal likelihood models could yield better generalization performance for real-world ordinal data compared with previous unimodal likelihood models and popular statistical OR models having no unimodality guarantee.",
        "authors": "R. Yamasaki",
        "keywords": [
            "ordinal regression",
            "unimodal likelihood models",
            "generalization performance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=1l0sClLiPc",
        "pdf_src": "https://api2.openreview.net/pdf/06a25f37477806f46af31644c459e5fa83413995.pdf",
        "Code_src": "",
        "Introduction": "Background: Ordinal regression (OR) deals with classifying ordinal data where there exists an inherent categorical target variable along with a natural order among its possible values based on certain explanatory variables.\n\nResearch Problem: The challenge lies in developing effective models capable of accurately predicting the rank or position within an ordered set without explicitly modeling the ordering itself since it's not always necessary nor sufficient information provided by the explanatory variables alone may lead to poor model performance due to overfitting when using traditional methods like logistic regression assuming independence between features.\n\nMethodology: This paper proposes new unimodal likelihood models designed specifically addressing these issues; they assume that under some conditions related to the explanatory variables' values, the conditional probability distributions follow a monotonic pattern - hence being \"unimodal\". These novel models aim at capturing complex relationships present while still maintaining interpretability through their simple structure.\n \nMain Contributions:\n1. Identification & Analysis of Limitations: The authors identify shortcomings associated with existing unimodal likelihood models used previously such as lack of expressiveness leading them towards less accurate predictions especially concerning real-world datasets.\n2. Development of Novel Models: They introduce several improved versions focusing primarily on enhancing representational power allowing each proposed model variant to capture different aspects contributing toward solving various types of OR problems encountered across diverse domains.\n3. Experimental Validation: Extensive empirical evaluations conducted demonstrate significantly superior predictive accuracy achieved via newly introduced models against both conventional statistical approaches lacking any guarantees regarding unimodality properties plus prior state-of-the-art unimodal likelihood counterparts during practical applications involving actual ranked lists from multiple fields like sports rankings",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Differentiable Model Compression via Pseudo Quantization Noise",
        "abstract": "We propose DiffQ a differentiable method for model compression for quantizing model parameters without gradient approximations (e.g., Straight Through Estimator). We suggest adding independent pseudo quantization noise to model parameters during training to approximate the effect of a quantization operator. DiffQ is differentiable both with respect to the unquantized weights and the number of bits used. Given a single hyper-parameter balancing between the quantized model size and accuracy, DiffQ optimizes the number of bits used per individual weight or groups of weights, in end-to-end training. We experimentally verify that our method is competitive with STE based quantization techniques on several benchmarks and architectures for image classification, language modeling, and audio source separation. For instance, on the ImageNet dataset, DiffQ compresses a 12 layers transformer-based model by more than a factor of 8, (lower than 4 bits precision per weight on average), with a loss of 0.3\\% in model accuracy. Code is available at github.com/facebookresearch/diffq",
        "authors": "A. Défossez, Y. Adi, G. Synnaeve",
        "keywords": [
            "DiffQ",
            "Model Compression",
            "Differentiable Quantization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=DijnKziche",
        "pdf_src": "https://api2.openreview.net/pdf/c258df6bb3a5bed85cc84c25ded23c68c8d6f9ae.pdf",
        "Code_src": "",
        "Introduction": "Background: The background of this paper lies in the field of machine learning optimization where there's an increasing need to reduce the computational complexity while maintaining performance due to hardware constraints such as memory limitations.\n\nResearch Problem: The research problem addressed here concerns how to efficiently compress neural network models through quantization - reducing the bit-width from 32-bit floating-point numbers down to lower bit-depth representations like 8-bit integers which can significantly decrease storage requirements but also introduce approximation errors leading to degradation in model performance if not handled properly.\n\nMethod: To tackle this issue, the authors introduced DiffQ – a novel differentiable approach towards quantization-free model compression using pseudo quantization noise added independently into each parameter during training time rather than relying on gradient approximations methods commonly employed previously e.g., Straight-Through Estimator (STE).\n\nMain Contributions: The main contributions include:\n1. Developing a differentiable framework called DiffQ capable of optimizing quantization directly within the training process.\n2. Proposing a new way forward for quantization by introducing pseudo quantization noise instead of approximating gradients traditionally done via STE.\n3. Achieving state-of-the-art results compared against existing quantization approaches across various datasets and tasks including image classification, language modeling & audio source separation despite significant reduction in bit precision required after quantization up to 4 bits per weight typically achieving around half that depending upon task specifics).\n4. Making their code publicly accessible so others may replicate these findings easily under GitHub repository facebookresearch/diffq.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Local Kernel Ridge Regression for Scalable, Interpolating, Continuous Regression",
        "abstract": "We study a localized version of kernel ridge regression that can continuously, smoothly interpolate the underlying function values which are highly non-linear with observed data points. This new method can deal with the data of which (a) local density is highly uneven and (b) the function values change dramatically in certain small but unknown regions. By introducing a new rank-based interpolation scheme, the interpolated values provided by our local method continuously vary with query points. Our method is scalable by avoiding the full matrix inverse, compared with traditional kernel ridge regression.",
        "authors": "M. Han, C. Ye, J. M. Phillips",
        "keywords": [
            "localization",
            "kernel ridge regression",
            "rank-based interpolation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=EDAk6F8yMM",
        "pdf_src": "https://api2.openreview.net/pdf/08e78489f78dbb14213950651eebd34c696c57f9.pdf",
        "Code_src": "",
        "Introduction": "Background: Kernel Ridge Regression (KRR) is widely used for non-linear regression problems due to its ability to handle high-dimensional input spaces effectively through feature maps. However, standard KRR often struggles when dealing with datasets where there's significant heterogeneity in local densities or abrupt changes in function values within small, unknown regions.\n\nResearch Problem: The challenge lies in developing an improved version of KRR capable of interpolating these complex patterns accurately while maintaining computational efficiency even under such challenging conditions.\n\nMethodology: To address this issue, we propose a localized variant of KRR called Localized Kernel Ridge Regression (LKRR). LKRR introduces a novel rank-based interpolation approach designed specifically for handling irregularities like dense clusters followed by sparse areas without requiring knowledge about their exact locations ahead of time.\n \nMain Contributions:\n1. **Continuous Interpolation**: Unlike conventional methods whose interpolated functions may jump abruptly at boundaries between different regimes based on training data alone, our LKRR provides continuous interpolation across all query points ensuring smoother transitions despite variations in local density levels around them.\n2. **Scalability**: We achieve scalability over traditional KRR techniques since it avoids computing entire covariance matrices leading to faster convergence times during iterative optimization processes needed after each update step; hence making computation more feasible especially large-scale applications involving millions or billions observations points simultaneously being processed together efficiently than before possible previously using classical approaches only limitedly applicable because they were computationally prohibitive otherwise).\n3. **Handling Irregular Patterns**: By incorporating rank-based schemes into our model design instead relying solely upon Gaussian kernels as done conventionally elsewhere allowing us better adaptability towards various types distributions encountered real-world scenarios thus enabling robustness against outliers present among inputs whilst still preserving accuracy overall predictions made from given dataset regardless whether those samples come from densely packed clusters versus sparsely distributed ones accordingly adapting itself dynamically according changing circumstances encountered throughout runtime execution phases accordingly adjusting parameters appropriately whenever necessary",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Fingerprints of Super Resolution Networks",
        "abstract": "Several recent studies have demonstrated that deep-learning based image generation models, such as GANs, can be uniquely identified, and possibly even reverse-engineered, by the fingerprints they leave on their output images. We extend this research to single image super-resolution (SISR) networks. Compared to previously studied models, SISR networks are a uniquely challenging class of image generation model from which to extract and analyze fingerprints, as they can often generate images that closely match the corresponding ground truth and thus likely leave little flexibility to embed signatures. We take SISR models as examples to investigate if the findings from the previous work on fingerprints of GAN-based networks are valid for general image generation models. We show that SISR networks with a high upscaling factor or trained using adversarial loss leave highly distinctive fingerprints, and that under certain conditions, some SISR network hyperparameters can be reverse-engineered from these fingerprints.",
        "authors": "J. Vonderfecht, F. Liu",
        "keywords": [
            "image generation models",
            "fingerprinting",
            "single-image super-resolution"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Jj0qSbtwdb",
        "pdf_src": "https://api2.openreview.net/pdf/e3acf8c98ec9ccf3418193cd72ca8d61184d6453.pdf",
        "Code_src": "",
        "Introduction": "Background: Recent studies indicate that generative adversarial networks (GANs), particularly those used in image generation tasks like Generative Image Networks (GIGANs), possess unique characteristics known as \"fingerprints\" within their generated outputs.\n\nResearch Question: This paper explores whether similar fingerprinting techniques applied successfully to GANs could also apply effectively to another type of neural network - Single-Image Super-Resolution (SISR) networks – despite differences between them regarding how much information is retained about the original input when upsampling an image.\n\nMethodology: The authors examine various aspects of SISR networks including different scaling factors during training (\"upscale\") along with the use of adversarial losses versus traditional reconstruction losses commonly employed while training these networks.\nThey then attempt to identify any distinguishable patterns left behind after processing through specific hyperparameter configurations across multiple datasets.\n\nMain Contributions:\n1. **Identification of Distinctive Fingerprints**: They find evidence indicating that both higher upscale factors leading to more significant changes made upon the initial low-resolution image data; \nand secondly, employing adversarial learning methods rather than just minimizing reconstruction error results in clearer fingerprints being embedded into the final super-resolved images.\n\n2. **Reverse Engineering Potential**: Under controlled circumstances where certain assumptions hold true concerning the nature of these fingerprints relative to hyperparameters utilized throughout training processes,\nthe researchers demonstrate partial success at inferring back what parameters were set without direct access to explicit records related specifically",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Mace: A flexible framework for membership privacy estimation in generative models",
        "abstract": "Generative machine learning models are being increasingly viewed as a way to share sensitive data between institutions. While there has been work on developing differentially private generative modeling approaches, these approaches generally lead to sub-par sample quality, limiting their use in real world applications. Another line of work has focused on developing generative models which lead to higher quality samples but currently lack any formal privacy guarantees. In this work, we propose the first formal framework for membership privacy estimation in generative models.  We formulate the membership privacy risk as a statistical divergence between training samples and hold-out samples, and propose sample-based methods to estimate this divergence. Compared to previous works, our framework makes more realistic and flexible assumptions. First, we offer a generalizable metric as an alternative to the accuracy metric (Yeom et al., 2018; Hayes et al., 2019) especially for imbalanced datasets. Second, we loosen the assumption of having full access to the underlying distribution from previous studies (Yeom et al., 2018; Jayaraman et al., 2020), and propose sample-based estimations with theoretical guarantees. Third, along with the population-level membership privacy risk estimation via the optimal membership advantage, we offer the individual-level estimation via the individual privacy risk. Fourth, our framework allows adversaries to access the trained model via a customized query, while prior works require specific attributes (Hayes et al., 2019; Chen et al., 2019; Hilprecht et al., 2019).",
        "authors": "Y. Xu, S. Mukherjee, X. Liu, et.al",
        "keywords": [
            "differentially private generative modeling",
            "membership privacy estimation",
            "sample-based methods"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Zxm0kNe3u7",
        "pdf_src": "https://api2.openreview.net/pdf/be00503a410c9fed49a04dc31c3487e200642279.pdf",
        "Code_src": "",
        "Introduction": "Background: Generative machine learning models have emerged as potential tools that can facilitate sharing sensitive information across organizations without revealing personal identities.\n\nResearch Problem: Although differential privacy techniques exist within generative modeling frameworks aiming at preserving confidentiality by adding noise during generation processes , they often result in lower-quality outputs compared to non-private counterparts due to the necessity of maintaining privacy constraints. Conversely, other research efforts focus on improving output quality through less stringent privacy considerations yet provide no formal privacy assurances.\n\nMethodology: This paper introduces what is believed to be the initial comprehensive approach towards quantifying membership privacy risks associated with generative models - specifically focusing on how well generated samples represent actual training examples when it comes to privacy leakage concerns.\nThe authors define 'membership privacy' broadly concerning whether one could infer if certain individuals or entities were part of the dataset used solely based on observing generated instances rather than raw data directly. They measure such risks statistically using divergence metrics comparing held-out test sets against those derived from training procedures proposed novel sample-based estimation strategies offering both practicality and theoretical guarantees over existing ones:\n1. A new metric replacing traditional accuracy measures particularly beneficial under skewed datasets;\n2. Relaxing strict requirements about complete knowledge regarding the true underlying distributions allowing approximations instead;\n3. Providing estimates not only globally across all members but also individually considering each entity's contribution to overall privacy risk;\n4. Allowing adversaries queries tailored around the learned model structure versus rigid attribute-based restrictions imposed previously.\n\nMain Contributions: The primary contributions lie mainly in establishing foundational theory behind estimating membership privacy threats posed by generative models alongside introducing innovative methodologies grounded theoretically proven algorithms capable providing meaningful insights into potentially compromising scenarios related to confidential data dissemination facilitated by advanced artificial intelligence technologies like generative adversarial networks(GANs).",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "Online Double Oracle",
        "abstract": "Solving strategic games with huge action spaces is a critical yet under-explored topic in economics, operations research and artificial intelligence.  This paper proposes new learning algorithms for solving two-player zero-sum normal-form games where the number of pure strategies is prohibitively large.  Specifically, we combine no-regret analysis from online learning with Double Oracle (DO) from game theory. \nOur method---\\emph{Online Double Oracle (ODO)}---is provably convergent to a Nash equilibrium (NE).  Most importantly, unlike normal DO, ODO is \\emph{rational} in the sense that each agent in ODO can exploit a strategic adversary with a regret bound of $\\mathcal{O}(\\sqrt{ k \\log(k)/T})$,  where $k$ is not the total number of pure strategies, but rather the size of \\emph{effective strategy set}. In many applications, we empirically show that $k$ is linearly dependent on the support size of the NE. On tens of different real-world matrix games, ODO outperforms   DO,   PSRO, and no-regret algorithms such as Multiplicative Weights Update by a significant margin, both in terms of convergence rate to a NE, and average payoff against strategic adversaries. ",
        "authors": "L. C. Dinh, S. M. Mcaleer, Z. Tian, et.al",
        "keywords": [
            "Online Learning",
            "Nash Equilibrium",
            "Effective Strategy Set"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=rrMK6hYNSx",
        "pdf_src": "https://api2.openreview.net/pdf/de22d8a3007c56cce68a2dc7b003ff02d46968c7.pdf",
        "Code_src": "",
        "Introduction": "Background: Solving strategic games with massive action spaces remains an important issue across various fields like economics, operations research, and artificial intelligence despite its relative neglect.\n\nResearch Problem: The problem addressed here involves finding solutions—specifically Nash Equilibria (NEs)—for two-player zero-sum normal-form games when there are too numerous pure strategies available which makes traditional approaches impractical or ineffective.\n\nMethodology: To tackle this challenge, our approach combines insights from online learning's no-regret analysis along with concepts from game theory particularly focusing on the Double Oracle (DO) framework.\nThe proposed algorithm, called Online Double Oracle (ODO), ensures convergence towards a NE while also being rational; it allows agents within the system to counteract adversarial strategies without excessive regret over time (\\( \\mathcal{O}(\\sqrt{k \\log(k)/T}) \\)), where \\( k \\) represents the effective strategy space instead of all possible pure strategies.\n\nMain Contributions:\n1. **Algorithmic Innovation**: Developed a novel algorithm, ODO, specifically designed for dealing with very large action spaces common in strategic games ensuring convergence toward a NE through a combination of techniques drawn from multiple disciplines.\n2. **Regret Analysis**: Provided rigorous guarantees regarding the regret experienced by individual players even if they face strategic opponents—a feature notably absent in previous methods including standard Double Oracle (DO).\n3. **Empirical Validation**: Demonstrated superior performance compared existing state-of-the-art algorithms via empirical tests conducted using actual data sets from diverse domains showing improved convergence rates into equilibria alongside higher payoffs during interactions involving strategic adversaries than other well-known algorithms like Proportional Stochastic Regularization Optimization (PSRO) and Multiplicative Weight Update (MWU).\n\nIn summary, this work presents a breakthrough solution addressing one of the most challenging aspects",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Attribute Prediction as Multiple Instance Learning",
        "abstract": "Attribute-based representations help machine learning models perform tasks based on human understandable concepts, allowing a closer human-machine collaboration. However, learning attributes that accurately reflect the content of an image is not always straightforward, as per-image ground truth attributes are often not available. \nWe propose applying the Multiple Instance Learning (MIL) paradigm to attribute learning (AMIL) while only using class-level labels. \nWe allow the model to under-predict the positive attributes, which may be missing in a particular image due to occlusions or unfavorable pose, but not to over-predict the negative ones, which are almost certainly not present. We evaluate it in the zero-shot learning (ZSL) setting, where training and test classes are disjoint, \nand show that this also allows to profit from knowledge about the semantic relatedness of attributes. \nIn addition, we apply the MIL assumption to ZSL classification and propose MIL-DAP, an attribute-based zero-shot classification method, based on Direct Attribute Prediction (DAP), to evaluate attribute prediction methods when no image-level data is available for evaluation.\nExperiments on CUB-200-2011, SUN Attributes and AwA2 show improvements on attribute detection, attribute-based zero-shot classification and weakly supervised part localization.",
        "authors": "D. Marcos, A. Potze, W. Xu, et.al",
        "keywords": [
            "attribute learning",
            "multiple instance learning",
            "zero-shot learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=nmFczdJtc2",
        "pdf_src": "https://api2.openreview.net/pdf/05d5c13f1786e173c50c82021075990e4422996c.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses how attribute-based representations can assist machine learning models by leveraging human-understandable concepts during task performance.\n\nResearch Problem: The challenge lies in accurately learning attributes reflecting image content since per-image ground truth attributes might lack availability.\n\nMethod: To address this issue, the authors introduce the concept of Attribute-Based Multiple Instance Learning (AMIL). They utilize class-level labels without relying on specific instance annotations through their proposed approach called MIL-DAP – Multiple Instance Learning with Direct Attribute Prediction.\n\nMain Contributions:\n1. The application of MIL paradigm within attribute learning context improves upon traditional approaches significantly especially considering its effectiveness even if some instances do not possess all relevant attributes because they're obscured; \n2. This novel framework aids in benefiting from understanding semantic relationships between different attributes;\n3. It introduces MIL-DAP - a new methodology specifically designed for evaluating attribute prediction techniques particularly useful scenarios like Zero-Shot Learning settings lacking explicit image-level annotation information across datasets such as CUB-200-2011, SUN Attributes & AWA2 demonstrating improved results compared existing state-of-the-art solutions",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Completeness and Coherence Learning for Fast Arbitrary Style Transfer",
        "abstract": "Style transfer methods put a premium on two objectives: (1) completeness which encourages the encoding of a complete set of style patterns; (2) coherence which discourages the production of spurious artifacts not found in input styles. While existing methods pursue the two objectives either partially or implicitly, we present the Completeness and Coherence Network (CCNet) which jointly learns completeness and coherence components and rejects their incompatibility, both in an explicit manner. Specifically, we develop an attention mechanism integrated with bi-directional softmax operations for explicit imposition of the two objectives and for their collaborative modelling. We also propose CCLoss as a quantitative measure for evaluating the quality of a stylized image in terms of completeness and coherence. Through an empirical evaluation, we demonstrate that compared with existing methods, our method strikes a better tradeoff between computation costs, generalization ability and stylization quality.",
        "authors": "Z. Wu, C. Song, G. Chen, et.al",
        "keywords": [
            "style transfer",
            "completeness",
            "coherence"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=4N6T6Rop6k",
        "pdf_src": "https://api2.openreview.net/pdf/d38fb1dadda3a5297791ebe20fa959b922a540f7.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe field of style transfer aims to modify images by applying visual characteristics from one source image (\"content\") onto another (\"style\"). Two key goals are pursued during this process - completeness ensuring all relevant stylistic elements are encoded into the output, and coherence preventing any extraneous or incorrect features being introduced.\n\nResearch Problem:\nExisting style transfer algorithms often address these objectives incompletely due to implicit handling rather than explicitly optimizing them simultaneously against each other's constraints – leading potentially to suboptimal results where some aspects may be sacrificed at the expense of others without balance achieved.\n\nMethodology:\nTo tackle this issue head-on, authors introduce the Completeness and Coherence Network (CCNet), designed specifically around learning mechanisms that ensure both objectives can coexist harmoniously within its architecture.\n- The network employs an innovative attention mechanism combined with bidirectional softmax functions allowing it to focus more intently when necessary while still considering broader contexts needed across different parts of the image under transformation.\n- A novel loss function called CCLoss is proposed alongside CCNet providing quantifiable metrics assessing how well the resulting stylized image maintains both completeness & coherence relative to original content and style sources respectively.\n\nMain Contributions:\n- The paper introduces a new model named \"Completeness and Coherence Network\" capable of training models that optimize for comprehensive stylistic representation along with maintaining consistency throughout transformations.\n- It incorporates an attention-based mechanism coupled with softmax adjustments enabling joint optimization towards completeness and coherence through direct collaboration among learned components instead of compromising one objective over another.\n- By proposing 'CCLoss', they provide a clear way forward regarding what constitutes high-quality outputs based upon these dual criteria—completeness plus coherence—which has been previously lacking amongst current state-of-the-art approaches presented thus far before publication date mentioned above).",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "GemNet-OC: Developing Graph Neural Networks for Large and Diverse Molecular Simulation Datasets",
        "abstract": "Recent years have seen the advent of molecular simulation datasets that are orders of magnitude larger and more diverse. These new datasets differ substantially in four aspects of complexity: 1. Chemical diversity (number of different elements), 2. system size (number of atoms per sample), 3. dataset size (number of data samples), and 4. domain shift (similarity of the training and test set). Despite these large differences, benchmarks on small and narrow datasets remain the predominant method of demonstrating progress in graph neural networks (GNNs) for molecular simulation, likely due to cheaper training compute requirements. This raises the question -- does GNN progress on small and narrow datasets translate to these more complex datasets? This work investigates this question by first developing the GemNet-OC model based on the large Open Catalyst 2020 (OC20) dataset. GemNet-OC outperforms the previous state-of-the-art on OC20 by 16% while reducing training time by a factor of 10. We then compare the impact of 18 model components and hyperparameter choices on performance in multiple datasets. We find that the resulting model would be drastically different depending on the dataset used for making model choices. To isolate the source of this discrepancy we study six subsets of the OC20 dataset that individually test each of the above-mentioned four dataset aspects. We find that results on the OC-2M subset correlate well with the full OC20 dataset while being substantially cheaper to train on. Our findings challenge the common practice of developing GNNs solely on small datasets, but highlight ways of achieving fast development cycles and generalizable results via moderately-sized, representative datasets such as OC-2M and efficient models such as GemNet-OC. Our code and pretrained model weights are open-sourced.",
        "authors": "J. Gasteiger, M. Shuaibi, A. Sriram, et.al",
        "keywords": [
            "molecular simulation",
            "graph neural networks",
            "dataset complexity"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=u8tvSxm4Bs",
        "pdf_src": "https://api2.openreview.net/pdf/70f62e8d0055d52bd5d6033a9c4d796da8921265.pdf",
        "Code_src": "",
        "Introduction": "Background:\nIn recent years, there has been an explosion in the availability of molecular simulation datasets which contain significantly greater chemical diversity, system sizes, dataset sizes, and suffer from increased domain shifts compared to traditional datasets.\n\nResearch Problem:\nDespite their substantial difference between them, most existing benchmarks focus only on smaller and narrower datasets when evaluating Graph Neural Networks (GNNs) applied to molecular simulations because they require less expensive computational resources during training phase. The main research problem addressed here is whether or not advancements made using those small and narrow datasets can also apply across much more complex datasets like the ones mentioned earlier?\n\nMethodology:\nTo address this issue, researchers developed the GemNet-OC model specifically trained over the very large Open Catalyst 2020 (OC20) dataset - one of the largest publicly available molecular simulation datasets at present – showing improvements beyond prior works both in terms of accuracy gains up to 16%, reduced training times down tenfold than before.\nThey further investigated how various combinations of model architectures/components along with hyperparameters affect overall performance within several other datasets besides OC20 through comparative analysis experiments conducted under controlled conditions; \nthey found significant variations among performances according to what dataset was chosen throughout decision-making processes related to choosing models’ configurations accordingly.\nFinally, they studied subsets derived from OC20 focusing exclusively upon testing specific characteristics associated with chemical diversity/system size/dataset size/domain shift separately rather than together so could pinpoint exactly where discrepancies arise leading towards better understanding behind why certain approaches generalize poorly onto broader contexts despite having performed favorably against limited scope benchmarks previously encountered elsewhere.\n\nMain Contributions:\nThis paper challenges conventional wisdom suggesting that developments achieved primarily utilizing small datasets may fail when scaled-up into real-world scenarios involving higher complexities levels observed nowadays amongst molecules simulations datasets;\nit introduces GemNet-OC architecture designed especially around tackling problems posed therein whilst emphasizing its efficiency advantages relative to predecessors; \nand it provides empirical evidence supporting usage of intermediate sized datasets like OC-2M alongside optimized algorithms like GemNet-OC enabling faster iterations cycles coupled with improved generalizability outcomes",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "sigmoidF1: A Smooth F1 Score Surrogate Loss for Multilabel Classification",
        "abstract": "Multilabel classification is the task of attributing multiple labels to examples via predictions. Current models formulate a reduction of the multilabel setting into either multiple binary classifications or multiclass classification, allowing for the use of existing loss functions (sigmoid, cross-entropy, logistic, etc.). These multilabel classification reductions do not accommodate for the prediction of varying numbers of labels per example. Moreover, the loss functions are distant estimates of the performance metrics. We propose sigmoidF1, a loss function that is an approximation of the F1 score that (i) is smooth and tractable for stochastic gradient descent, (ii) naturally approximates a multilabel metric, and (iii) estimates both label suitability and label counts. We show that any confusion matrix metric can be formulated with a smooth surrogate. We evaluate the proposed loss function on text and image datasets, and with a variety of metrics, to account for the complexity of multilabel classification evaluation. sigmoidF1 outperforms other loss functions on one text and two image datasets over several metrics. These results show the effectiveness of using inference-time metrics as loss functions for non-trivial classification problems like multilabel classification.",
        "authors": "G. Bénédict, H. V. Koops, D. Odijk, et.al",
        "keywords": [
            "multilabel classification",
            "sigmoidF1",
            "loss function"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=gvSHaaD2wQ",
        "pdf_src": "https://api2.openreview.net/pdf/0c071280a7d5809ebfb1d10c44f3a2835680028e.pdf",
        "Code_src": "",
        "Introduction": "Background: Multilabel classification involves assigning more than one label to each instance in a dataset.\n\nResearch Problem: Existing methods reduce multilabel classification tasks by treating them as multiple binary classifications or multiclass classifications which does not allow predicting different number of labels per instance accurately nor estimate true performance well enough due to their reliance on rough estimations such as sigmoid, cross-entropy losses instead of precise measures like F1-score.\n\nMethod: The paper introduces sigmoidF1—a novel loss function designed specifically addressing these issues—it aims at being smoother while still providing accurate estimation similar to F1-score; it also considers label suitability along with count during training process.\n\nMain Contributions: \n1. SigmoidF1 serves as a better alternative compared traditional loss functions because it's closer aligned towards actual performance measure (F1-score).\n2. It allows us predict variable number of labels without needing manual adjustments.\n3. Paper demonstrates how almost all confusion matrices could potentially have their own smooth surrogates representing them through this new approach.\n4. Experiments conducted across various datasets including texts and images validate its superiority against conventional approaches measured under diverse sets of criteria/metrics indicating improved accuracy overall when used within multilabel classification settings involving complex scenarios requiring nuanced understanding beyond simple binary outcomes typical seen in binary classification setups alone.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Representation Alignment in Neural Networks",
        "abstract": "It is now a standard for neural network representations to be trained on large, publicly available datasets, and used for new problems. The reasons for why neural network representations have been so successful for transfer, however, are still not fully understood. In this paper we show that, after training, neural network representations align their top singular vectors to the targets. We investigate this representation alignment phenomenon in a variety of neural network architectures and find that (a) alignment emerges across a variety of different architectures and optimizers, with more alignment arising from depth (b) alignment increases for layers closer to the output and (c) existing high-performance deep CNNs exhibit high levels of alignment. We then highlight why alignment between the top singular vectors and the targets can speed up learning and show in a classic synthetic transfer problem that representation alignment correlates with positive and negative transfer to similar and dissimilar tasks.",
        "authors": "E. Imani, W. Hu, M. White",
        "keywords": [
            "alignment",
            "neural network representations",
            "transfer learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=fLIWMnZ9ij",
        "pdf_src": "https://api2.openreview.net/pdf/6ae6d6c4e282c5b29b4dba0276a54840c5d29910.pdf",
        "Code_src": "",
        "Introduction": "Background: Neural networks have become increasingly popular due to their success in transferring knowledge learned from one task to another without requiring extensive retraining.\n\nResearch Question: Despite widespread use, it remains unclear exactly what factors contribute to the effectiveness of neural network transfer learning.\n \nMethod: This study investigates whether neural network representations align themselves with target data during training by examining how well the first few singular vectors match corresponding target vectors. They explore various neural network architectures including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers using both gradient-based optimization algorithms like Adam and non-gradient based methods such as SGD. \n\nMain Contributions:\n1. Alignment occurs consistently within diverse architectures & optimizers; deeper models tend to achieve greater alignment than shallower ones.\n2. Alignment intensifies towards later layers near outputs compared against earlier hidden layers or inputs.\n3. High-performing pre-trained CNNs demonstrate significant alignment properties when tested under transfer conditions involving related but distinct tasks demonstrating an association between aligned representations and improved performance beyond initial domain adaptation efforts.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "LIMIS: Locally Interpretable Modeling using Instance-wise Subsampling",
        "abstract": "Understanding black-box machine learning models is crucial for their widespread adoption. Learning globally interpretable models is one approach, but achieving high performance with them is challenging. An alternative approach is to explain individual predictions using locally interpretable models. For locally interpretable modeling, various methods have been proposed and indeed commonly used, but they suffer from low fidelity, i.e. their explanations do not approximate the predictions well. In this paper, our goal is to push the state-of-the-art in high-fidelity locally interpretable modeling. We propose a novel framework, Locally Interpretable Modeling using Instance-wise Subsampling (LIMIS). LIMIS utilizes a policy gradient to select a small number of instances and distills the black-box model into a low-capacity locally interpretable model using those selected instances. Training is guided with a reward obtained directly by measuring the fidelity of the locally interpretable models. We show on multiple tabular datasets that LIMIS near-matches the prediction accuracy of black-box models, significantly outperforming state-of-the-art locally interpretable models in terms of fidelity and prediction accuracy. ",
        "authors": "J. Yoon, S. O. Arik, T. Pfister",
        "keywords": [
            "high-fidelity",
            "locally interpretable modeling",
            "instance-wise subsampling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=S8eABAy8P3",
        "pdf_src": "https://api2.openreview.net/pdf/8a1c6f70a1a1c44c88fc4ea13dbf56c18e965d3c.pdf",
        "Code_src": "",
        "Introduction": "Background: The wide adoption of black-box machine learning models has raised concerns about interpretability due to lack of understanding behind these complex algorithms.\n\nResearch Problem: How can we achieve both high performance and global interpretability?\n\nMethods: One solution involves training globally interpretable models which are difficult; another uses locally interpretable models explaining each prediction individually despite suffering from poor fidelity issues.\n\nMain Contributions: This paper introduces \"Locally Interpretable Modeling using Instance-wise Subsampling\" (LIMIS), aiming at improving local interpretability's fidelity without compromising its predictive power compared to existing approaches.\n \nThe method employs instance-wise subsampling along with a policy gradient optimization technique allowing it to choose representative examples while simultaneously refining an underlying black-box model down to lower capacity version more amenable to interpretation via chosen exemplars guiding refinement through rewards based solely on measured fidelity between refined model outputs against original predictions made",
        "Topic": "Generative Models"
    },
    {
        "title": "MixTailor: Mixed Gradient Aggregation for Robust Learning Against Tailored Attacks",
        "abstract": "Implementations of SGD on distributed and multi-GPU systems creates new vulnerabilities, which can be identified and misused by one or more adversarial agents. Recently, it has been shown that well-known Byzantine-resilient gradient aggregation schemes are indeed vulnerable to informed attackers that can tailor the attacks (Fang et al., 2020; Xie et al., 2020b). We introduce MixTailor, a scheme based on randomization of the aggregation strategies that makes it impossible for the attacker to be fully informed. Deterministic schemes can be integrated into MixTailor on the fly without introducing any additional hyperparameters. Randomization decreases the capability of a powerful adversary to tailor its attacks, while the resulting randomized aggregation scheme is still competitive in terms of performance. For both iid and non-iid settings, we establish almost sure convergence guarantees that are both stronger and more general than those available in the literature. Our empirical studies across various datasets, attacks, and settings, validate our hypothesis and show that MixTailor successfully defends when well-known Byzantine-tolerant schemes fail. ",
        "authors": "A. Ramezani-kebrya, I. Tabrizian, F. Faghri, et.al",
        "keywords": [
            "SGD",
            "Distributed Systems",
            "Byzantine Attacks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=tqDhrbKJLS",
        "pdf_src": "https://api2.openreview.net/pdf/6ac9c7e0f18eb88c254c2c742c46b2498d514010.pdf",
        "Code_src": "",
        "Introduction": "Background: The implementation of stochastic gradient descent (SGD) algorithms with distributed computing over multiple GPUs introduces novel security challenges due to potential vulnerabilities exploited by malicious adversaries.\n\nResearch Problem: Recent findings indicate that existing Byzantine-resistant techniques used during gradient aggregation phases within distributed learning frameworks may not withstand targeted attacks from knowledgeable adversaries who could manipulate the process effectively against the system's integrity (\"informed attackers\").\n\nMethodology: To address this issue, authors propose \"MixTailor,\" an algorithmic approach designed around randomizing the aggregation strategy employed at each iteration step throughout training sessions using SGD. This randomness prevents informed attackers from predicting exactly how gradients will be aggregated next time they attempt to launch an attack since their knowledge about future aggregations would only partially apply after the introduction of randomness.\n \nMain Contributions:\n1. **Randomized Aggregation**: By incorporating randomness directly into the aggregation phase through MixTailor, the paper ensures that even if an adversary gains partial information regarding the current state of the network, such knowledge cannot predictably compromise subsequent iterations where the mix of aggregation methods changes randomly between runs.\n2. **Deterministic Integration**: Authors also demonstrate that deterministic aggregation protocols like FedAvg can seamlessly integrate with MixTailor as part of its operation – no extra parameters need to be tuned beyond what might already exist under standard practices leading to less complexity compared to other solutions proposed elsewhere related to Byzantine resilience improvements via randomization.\n3. **Convergence Guarantees**: They provide theoretical results establishing strong convergence properties ensuring that despite employing mixed aggregation policies, MixTailor converges towards optimal solutions just as efficiently as traditional approaches do but now offering robustness benefits against informed adversaries targeting Byzantine faults specifically tailored toward these types of networks deployed today.\n4. **Empirical Validation**: Empirical tests conducted extensively verify these claims empirically showing improved defense mechanisms provided by MixTailor relative to known Byzantine fault tolerance techniques failing under similar conditions tested including IID (Independent Identical Distributions) scenarios commonly encountered in practice along with Non-IID ones often arising naturally depending upon real-world data distributions present therein.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "FLEA: Provably Robust Fair Multisource Learning from Unreliable Training Data",
        "abstract": "Fairness-aware learning aims at constructing classifiers that not only make accurate predictions, but also do not discriminate against specific groups. It is a fast-growing area of machine learning with far-reaching societal impact. However, existing fair learning methods are vulnerable to accidental or malicious artifacts in the training data, which can cause them to unknowingly produce unfair classifiers. In this work we address the problem of fair learning from unreliable training data in the robust multisource setting, where the available training data comes from multiple sources, a fraction of which might not be representative of the true data distribution. We introduce FLEA, a filtering-based algorithm that identifies and suppresses those data sources that would have a negative impact on fairness or accuracy if they were used for training. As such, FLEA is not a replacement of prior fairness-aware learning methods but rather an augmentation that makes any of them robust against unreliable training data. We show the effectiveness of our approach by a diverse range of experiments on multiple datasets. Additionally, we prove formally that –given enough data– FLEA protects the learner against corruptions as long as the fraction of affected data sources is less than half. Our source code and documentation are available at https://github.com/ISTAustria-CVML/FLEA.",
        "authors": "E. Iofinova, N. Konstantinov, C. H. Lampert",
        "keywords": [
            "FLEA",
            "Fairness-aware Learning",
            "Robust Multisource Setting"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=XsPopigZXV",
        "pdf_src": "https://api2.openreview.net/pdf/b3c730569318d70db9d698715b408610279ad530.pdf",
        "Code_src": "https://github.com/ISTAustria-CVML/FLEA",
        "Introduction": "Background: Fairness-aware learning focuses on building classifiers without discrimination towards certain groups while maintaining prediction accuracy.\n\nResearch Problem: Existing fair learning algorithms may inadvertently generate biased models due to unintentional or deliberate corruption within the training dataset when dealing with multi-source settings involving various potentially unrepresentative data sources.\n\nMethod: The proposed solution involves introducing FLEA - a Filtering-based Learning Algorithm designed specifically addressing the challenges posed by unreliable training data across different sources.\nFLEA operates through identifying and excluding these non-representative or corruptive data sources during model training process ensuring both fairness and predictive performance remain intact even under adverse conditions.\n\nMain Contributions:\n1. Developed FLEA method capable of detecting and mitigating bias introduced into fair learning processes resulting from corrupted/unreliable training data coming from numerous disparate sources;\n2. Demonstrated efficacy via extensive experimentation conducted using varied datasets showcasing improved resilience toward inaccurate or skewed inputs compared traditional approaches; \n3. Provided formal proof indicating that provided sufficient quantity exists amongst all relevant data sets, FLEA safeguards learners' ability resist adversarial influences so long as fewer than 50% originate from tainted origins;\n4. Shared open-source implementation details along with comprehensive documentation facilitating replication & further investigation into their findings at GitHub repository: https://github.com/ISTAustria-CVML/FLEA",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "FedShuffle:  Recipes for Better Use of Local Work in Federated Learning",
        "abstract": "The practice of applying several local updates before aggregation across clients has been empirically shown to be a successful approach to overcoming the communication bottleneck in Federated Learning (FL). Such methods are usually implemented by having clients perform one or more epochs of local training per round while randomly reshuffling their finite dataset in each epoch. Data imbalance, where clients have different numbers of local training samples, is ubiquitous in FL applications, resulting in different clients performing different numbers of local updates in each round. In this work, we propose a general recipe, FedShuffle, that better utilizes the local updates in FL, especially in this regime encompassing random reshuffling and heterogeneity. FedShuffle is the first local update method with theoretical convergence guarantees that incorporates random reshuffling, data imbalance, and client sampling — features that are essential in large-scale cross-device FL. We present a comprehensive theoretical analysis of FedShuffle and show, both theoretically and empirically, that it does not suffer from the objective function mismatch that is present in FL methods that assume homogeneous updates in heterogeneous FL setups, such as FedAvg (McMahan et al., 2017). In addition, by combining the ingredients above, FedShuffle improves upon FedNova (Wang et al., 2020), which was previously proposed to solve this mismatch. Similar to Mime (Karimireddy et al., 2020), we show that FedShuffle with momentum variance reduction (Cutkosky & Orabona, 2019) improves upon non-local methods under a Hessian similarity assumption.",
        "authors": "S. Horváth, M. Sanjabi, L. Xiao, et.al",
        "keywords": [
            "FedShuffle",
            "Random Reshuffling",
            "Data Imbalance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Lgs5pQ1v30",
        "pdf_src": "https://api2.openreview.net/pdf/493b6763d08bdcb3c2252f4d247f6e62d61a4e4a.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses the challenges faced during federated learning when multiple devices collaborate on machine learning tasks without sharing any private information over the network.\n\nResearch Problem: One significant challenge observed here involves the \"communication bottleneck\" due to limited bandwidth between these devices leading to inefficiencies within the distributed framework known as federated learning (\"FedLearning\").\n\nMethodology: To address issues related to the communication bottleneck caused by the need for frequent updates among various participating nodes ('clients'), researchers often apply 'local updates' followed by an aggregation step at the server side after every round.\nHowever, they also employ techniques like random shuffling—reshuffling datasets locally inside individual rounds—to mitigate the problem further but introduce new complexities because of variations in how many times each device can train its model based on differences in sample sizes (data imbalance).\n\nMain Contributions:\n1. They introduced a novel algorithm called FedShuffle designed specifically around three key factors: incorporating randomness through reshuffling; accommodating for inherent imbalances amongst datasets used throughout iterations; and considering the variability arising out of selecting subsets of clients (sampling).\n2. This research provides empirical evidence showing that FedShuffle significantly reduces discrepancies compared to other existing algorithms including FedAvg despite being deployed amidst conditions of heterogeneity common in real-world settings involving diverse devices.\n3. Furthermore, unlike previous works lacking formal guarantees regarding convergence rates—theoretical proof exists validating FedShuffle's performance even given these complex scenarios.\n4. Finally, experiments conducted demonstrate improvements made possible via FedShuffle against prior state-of-the-art solutions like FedNova",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Sparse MoEs meet Efficient Ensembles",
        "abstract": "Machine learning models based on the aggregated outputs of submodels, either at the activation or prediction levels, often exhibit strong performance compared to individual models. We study the interplay of two popular classes of such models: ensembles of neural networks and sparse mixture of experts (sparse MoEs). First, we show that the two approaches have complementary features whose combination is beneficial. This includes a comprehensive evaluation of sparse MoEs in uncertainty related benchmarks. Then, we present efficient ensemble of experts (E$^3$), a scalable and simple ensemble of sparse MoEs that takes the best of both classes of models, while using up to 45% fewer FLOPs than a deep ensemble. Extensive experiments demonstrate the accuracy, log-likelihood, few-shot learning, robustness, and uncertainty improvements of E$^3$ over several challenging vision Transformer-based baselines. E$^3$ not only preserves its efficiency while scaling to models with up to 2.7B parameters, but also provides better predictive performance and uncertainty estimates for larger models.",
        "authors": "J. U. Allingham, F. Wenzel, Z. E. Mariet, et.al",
        "keywords": [
            "ensemble",
            "sparse mixture of experts",
            "efficient ensemble of experts"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=i0ZM36d2qU",
        "pdf_src": "https://api2.openreview.net/pdf/87ea30b826b13b3a57b4edd16f74206cd0140251.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper focuses on machine learning models composed of multiple submodels which can be combined through their activations or predictions due to their synergistic effects leading to improved performance.\n\nResearch Problem:\nThe research problem addressed by this work involves understanding how different types of model aggregations perform relative to each other when used as multi-model systems - specifically comparing ensembles of neural networks against sparse mixture of experts (sparse MoEs).\n\nMethods:\nTo address these issues, they first conduct an extensive comparison between ensembles of neural networks and sparse MoEs highlighting their respective strengths; then introduce Efficient Ensemble of Experts (E$^3$), a novel approach combining elements from both methods aiming higher scalability without sacrificing computational efficiency.\nE$^3$ is designed around sparse MoEs yet incorporates ensemble techniques allowing it to leverage the benefits seen across various tasks like classification precision, likelihood estimation, zero-shot generalization capabilities, resilience towards adversarial attacks etc., all within reduced computation requirements.\n\nMain Contributions:\nThe main contributions are twofold – firstly demonstrating that there exists synergy among certain aggregation strategies particularly evident here amongst neural network ensembles & sparse MoEs secondly introducing E$^3$, which successfully amalgamates aspects from both aforementioned architectures resulting into significant gains including substantial reduction in floating-point operations (FLOPs) usage whilst maintaining competitive performance metrics even under increased complexity represented",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Faking Interpolation Until You Make It",
        "abstract": "Deep over-parameterized neural networks exhibit the interpolation property on many data sets. Specifically, these models can achieve approximately zero loss on all training samples simultaneously. This property has been exploited to develop optimisation algorithms for this setting. These algorithms use the fact that the optimal loss value is known to employ a variation of a Polyak step size calculated on each stochastic batch of data. We introduce a novel extension of this idea to tasks where the interpolation property does not hold. As we no longer have access to the optimal loss values a priori, we instead estimate them for each sample online. To realise this, we introduce a simple but highly effective heuristic for approximating the optimal value based on previous loss evaluations.  We provide rigorous experimentation on a range of problems. From our empirical analysis we demonstrate the effectiveness of our approach, which outperforms other single hyperparameter optimisation methods.",
        "authors": "A. Paren, R. P. K. Poudel, M. P. Kumar",
        "keywords": [
            "interpolation property",
            "deep over-parameterized neural networks",
            "Polyak step size"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=OslAMMF4ZP",
        "pdf_src": "https://api2.openreview.net/pdf/fb7a6f3f6268eabcd21e77138f47d3697f731116.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses deep over-parameterized neural networks and their ability to interpolate between different datasets by achieving nearly zero loss across all training examples.\n\nResearch Problem: How do you optimize such networks when they don't necessarily possess the interpolation property?\n\nMethod: The authors propose an algorithmic extension using a Polyak step-size update rule applied iteratively with stochastic batches in settings without interpolation properties; however, unlike before relying solely on optimal losses from past iterations or epochs here it estimates those 'optimal' values per sample during runtime through heuristics derived from historical loss computations rather than predefined ones beforehand.\n\nMain Contributions:\n1. Novel extension addressing optimization challenges within non-interpolating scenarios.\n2. An innovative heuristic developed specifically tailored towards estimating optimal parameters dynamically throughout processing steps as opposed to static estimations at initialization time only like traditional approaches would require leading up superior performance compared against existing state-of-the-art techniques focusing exclusively upon singular hyperparameters adjustments alone - thus improving overall model adaptability flexibility under various conditions encountered while solving practical machine learning problems involving complex architectures trained extensively beyond what was necessary just being able generalize well enough",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Evolving Decomposed Plasticity Rules for Information-Bottlenecked Meta-Learning",
        "abstract": "Artificial neural networks (ANNs) are typically confined to accomplishing pre-defined tasks by learning a set of static parameters. In contrast, biological neural networks (BNNs) can adapt to various new tasks by continually updating the neural connections based on the inputs, which is aligned with the paradigm of learning effective learning rules in addition to static parameters, \\textit{e.g.}, meta-learning. Among various biologically inspired learning rules, Hebbian plasticity updates the neural network weights using local signals without the guide of an explicit target function, thus enabling an agent to learn automatically without human efforts. However, typical plastic ANNs using a large amount of meta-parameters violate the nature of the genomics bottleneck and potentially deteriorate the generalization capacity. This work proposes a new learning paradigm decomposing those connection-dependent plasticity rules into neuron-dependent rules thus accommodating $\\Theta(n^2)$ learnable parameters with only $\\Theta(n)$ meta-parameters. We also thoroughly study the effect of different neural modulation on plasticity. Our algorithms are tested in challenging random 2D maze environments, where the agents have to use their past experiences to shape the neural connections and improve their performances for the future. The results of our experiment validate the following: 1. Plasticity can be adopted to continually update a randomly initialized RNN to surpass pre-trained, more sophisticated recurrent models, especially when coming to long-term memorization. 2. Following the genomics bottleneck, the proposed decomposed plasticity can be comparable to or even more effective than canonical plasticity rules in some instances.",
        "authors": "F. Wang, H. Tian, H. Xiong, et.al",
        "keywords": [
            "neural network adaptation",
            "Hebbian plasticity",
            "meta-learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=6qMKztPn0n",
        "pdf_src": "https://api2.openreview.net/pdf/8df37b7089b06b804bd8947d29c8178b827adb33.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses limitations inherent in artificial neural networks (ANNs), particularly focusing on how they rely heavily on predefined tasks that require them to learn from fixed sets of parameters rather than adapting continuously like biological neural networks do.\n\nResearch Problem:\nThe research problem addressed here concerns the adaptation capabilities of ANNs compared to biological neural networks as well as the computational efficiency implications related to the number of parameters required during this adaptive process known as plasticity within ANNs.\n \nMethodology:\nTo address these issues, the authors propose a novel approach called \"genomics bottleneck\" which involves breaking down traditional ANN plasticity mechanisms dependent on connections between neurons (\"connection-dependent\") towards neuron-specific (\"neuron-dependent\") plasticity adjustments while maintaining a manageable complexity level. They further investigate the impact of varying degrees of neural modulation upon plasticity performance through algorithmic testing conducted under complex conditions such as navigating random 2D mazes.\n\nMain Contributions:\nThis contribution introduces a significantly reduced parameter space for plasticity adjustment - instead of requiring $\\Theta(n^2)$ parameters traditionally associated with connection-based plasticity methods, it utilizes just $\\Theta(n)$ meta-parameters per neuron; thereby reducing both memory footprint and computation costs dramatically yet still retaining effectiveness similar if not better at times over existing canonical plasticity approaches. Additionally, empirical evidence provided via experiments demonstrates improved long-term memorization abilities specifically tailored toward recurrent neural networks (RNNs).",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Approximating 1-Wasserstein Distance with Trees",
        "abstract": "The Wasserstein distance, which measures the discrepancy between distributions, shows efficacy in various types of natural language processing and computer vision applications. One of the challenges in estimating the Wasserstein distance is that it is computationally expensive and does not scale well for many distribution-comparison tasks. In this study, we aim to approximate the 1-Wasserstein distance by the tree-Wasserstein distance (TWD), where the TWD is a 1-Wasserstein distance with tree-based embedding that can be computed in linear time with respect to the number of nodes on a tree. More specifically, we propose a simple yet efficient L1-regularized approach for learning the weights of edges in a tree. To this end, we first demonstrate that the 1-Wasserstein approximation problem can be formulated as a distance approximation problem using the shortest path distance on a tree. We then show that the shortest path distance can be represented by a linear model and formulated as a Lasso-based regression problem. Owing to the convex formulation, we can efficiently obtain a globally optimal solution. We also propose a tree-sliced variant of these methods. Through experiments, we demonstrate that the TWD can accurately approximate the original 1-Wasserstein distance by using the weight estimation technique.  Our code can be found in the GitHub repository.",
        "authors": "M. Yamada, Y. Takezawa, R. Sato, et.al",
        "keywords": [
            "tree-Wasserstein distance",
            "1-Wasserstein distance approximation",
            "Lasso regression"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Ig82l87ZVU",
        "pdf_src": "https://api2.openreview.net/pdf/9132dd9c1f30b0bca8af69541ca58dc8f1b4b20e.pdf",
        "Code_src": "",
        "Introduction": "Background: The Wasserstein distance has been widely used due to its effectiveness in comparing probability distributions across different fields like NLP and CV.\n\nResearch Problem: Estimating the Wasserstein distance typically involves high computational costs because existing algorithms do not scale effectively when dealing with large datasets or complex data structures such as trees.\n \nMethodology: This paper introduces an approximation method called Tree-Wasserstein Distance (TWD). It approximates the 1-Wasserstein distance through a tree-based embedding structure allowing computation within linear time relative to the number of tree nodes. They develop a novel L1-regularized algorithm designed explicitly for edge weight learning inside the tree framework. \n\nMain Contributions:\n1. Formulate the 1-Wasserstein approximation task into a shortest-path-distance problem over a tree graph representation leading to a more tractable optimization issue.\n2. Derive a linear model from the shortest path distances enabling them to cast the problem further down to a Lasso regression challenge – which they solve via convex optimization yielding global optimality guarantees.\n3. Propose extensions including a tree sliced version enhancing efficiency without sacrificing accuracy significantly compared against other state-of-the-art approaches based on empirical evaluations conducted throughout their research work.\n4. Share their implementation details publicly available at GitHub repository facilitating reproducibility among researchers interested in similar works related to Wasserstein distance approximations",
        "Topic": "approximation"
    },
    {
        "title": "Multitask Online Mirror Descent",
        "abstract": "We introduce and analyze MT-OMD, a multitask generalization of Online Mirror Descent (OMD) which operates by sharing updates between tasks. We prove that the regret of MT-OMD is of order $\\sqrt{1 + \\sigma^2(N-1)}\\sqrt{T}$, where $\\sigma^2$ is the task variance according to the geometry induced by the regularizer, $N$ is the number of tasks, and $T$ is the time horizon. Whenever tasks are similar, that is $\\sigma^2 \\le 1$, our method improves upon the $\\sqrt{NT}$ bound obtained by running independent OMDs on each task. We further provide a matching lower bound, and show that our multitask extensions of Online Gradient Descent and Exponentiated Gradient, two major instances of OMD, enjoy closed-form updates, making them easy to use in practice. Finally, we present experiments which support our theoretical findings.",
        "authors": "N. Cesa-bianchi, P. Laforgue, A. Paudice, et.al",
        "keywords": [
            "mt-omd",
            "multitask learning",
            "online mirror descent"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=zwRX9kkKzj",
        "pdf_src": "https://api2.openreview.net/pdf/684386a33a39a6e12802761b5e9bbb36e139c546.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper introduces Multitask Online Mirror Descent (MT-OMD), an algorithm for solving multiple optimization problems simultaneously with shared updates among different tasks.\n\nResearch Problem: How can we efficiently solve multiple related optimization problems using shared updates?\n\nMethod: MT-OMD shares updates across tasks through a multitasking generalization of Online Mirror Descent (OMD). It has a regret guarantee of $\\sqrt{1 + \\sigma^2(N-1)}\\sqrt{T}$ under certain conditions regarding task similarity ($\\sigma^2 \\leq 1$).\n\nMain Contributions:\n1. Proves a regret bound for MT-OMD.\n2. Shows how MT-OMD outperforms independent OMD when tasks are similar.\n3. Provides matching lower bounds demonstrating the tightness of their results.\n4. Extends the analysis to other variants like Online Gradient Descent and Exponentiated Gradient algorithms showing they also have closed-form updates suitable for practical applications.\n5. Conducts experiments validating these theoretical findings.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Ensembles of Classifiers: a Bias-Variance Perspective",
        "abstract": "Ensembles are a straightforward, remarkably effective method for improving the accuracy, calibration, and robustness of neural networks on classification tasks. Yet, the reasons underlying their success remain an active area of research. Building upon (Pfau, 2013), we turn to the bias-variance decomposition of Bregman divergences in order to gain insight into the behavior of ensembles under classification losses. Introducing a dual reparameterization of the bias-variance decomposition, we first derive generalized laws of total expectation and variance, then discuss how bias and variance terms can be estimated empirically. Next, we show that the dual reparameterization naturally introduces a way of constructing ensembles which reduces the variance and leaves the bias unchanged. Conversely, we show that ensembles that directly average model outputs can arbitrarily increase or decrease the bias. Empirically, we see that such ensembles of neural networks may reduce the bias. We conclude with an empirical analysis of ensembles over neural network architecture hyperparameters, revealing that these techniques allow for more efficient bias reduction than standard ensembles.",
        "authors": "N. Gupta, J. Smith, B. Adlam, et.al",
        "keywords": [
            "ensemble",
            "bias-variance decomposition",
            "neural networks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=lIOQFVncY9",
        "pdf_src": "https://api2.openreview.net/pdf/951449600c02cc257d81b4f7eb10620bd0ae11cf.pdf",
        "Code_src": "",
        "Introduction": "Background: Ensembles have been shown to improve the performance metrics like accuracy, calibration, and robustness significantly when applied as post-processing steps after training individual models.\n\nResearch Question: The study aims at understanding why ensemble methods work well by exploring them through the lens of bias-variance decomposition using Bregman divergences within the context of classification loss functions.\n \nMethods: The researchers introduce a novel dual reparameterization approach based on bias-variance decomposition principles from information theory; they use this framework to derive new results about expectations and variances related to ensemble predictions. They also propose ways to estimate bias and variance components practically via empirical means rather than theoretical assumptions alone. Furthermore, they demonstrate theoretically both advantages—reducing variance while preserving bias—and disadvantages—arbitrary bias changes—of different ensemble construction strategies compared against direct averaging approaches commonly used today.\n\nMain Contributions: This paper contributes insights regarding the mechanisms behind successful ensemble learning algorithms including generalizations around lawsof total expectation and variance applicable across various ensemble constructions along with practical guidance towards estimating bias and variance components effectively without relying solely on theoretical approximations. Additionally, it provides empirical evidence supporting certain ensemble configurations leading to reduced biases relative to traditional ones among other findings pertaining specifically to neural network architectures' hyperparameters optimization space exploration suggesting potential improvements beyond those achievable just by increasing model complexity alone.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Do better ImageNet classifiers assess perceptual similarity better?",
        "abstract": "Perceptual distances between images, as measured in the space of pre-trained deep features, have outperformed prior low-level, pixel-based metrics on assessing image similarity. While the capabilities of older and less accurate models such as AlexNet and VGG to capture perceptual similarity are well known, modern and more accurate models are less studied. In this paper, we present a large-scale empirical study to assess how well ImageNet classifiers perform on perceptual similarity. First, we observe a inverse correlation between ImageNet accuracy and Perceptual Scores of modern networks such as ResNets, EfficientNets, and Vision Transformers: that is better classifiers achieve worse Perceptual Scores. Then, we examine the ImageNet accuracy/Perceptual Score relationship on varying the depth, width, number of training steps, weight decay, label smoothing, and dropout. Higher accuracy improves Perceptual Score up to a certain point, but we uncover a Pareto frontier between accuracies and Perceptual Score in the mid-to-high accuracy regime. We explore this relationship further using a number of plausible hypotheses such as distortion invariance, spatial frequency sensitivity, and alternative perceptual functions. Interestingly we discover shallow ResNets and ResNets trained for less than 5 epochs only on ImageNet, whose emergent Perceptual Score matches the prior best networks trained directly on supervised human perceptual judgements.",
        "authors": "M. Kumar, N. Houlsby, N. Kalchbrenner, et.al",
        "keywords": [
            "Image similarity assessment",
            "Perceptual scores",
            "Model performance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=qrGKGZZvH0",
        "pdf_src": "https://api2.openreview.net/pdf/987210a940f64f4dfa265d016994416049b752e9.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe assessment of image similarity has traditionally relied on low-level, pixel-based metrics which measure differences at the level of individual pixels or color channels within an image. However, these methods do not always align with human perception when it comes to judging whether two images appear similar.\n\nResearch Problem:\nRecent studies suggest that higher-dimensional representations learned by neural networks during pre-training can serve as effective measures of perceptual similarity because they encode information about objects rather than just their raw visual content. This raises questions regarding newer, state-of-the-art models like ResNets, EfficientNets, and Vision Transformers - How accurately do they capture perceptual similarity?\n\nMethods:\nIn order to answer our research question thoroughly, we conducted a comprehensive empirical analysis focusing specifically on the performance of ImageNet classifiers across different architectures including ResNets, EfficientNets, and Vision Transformers under various conditions related to model complexity:\n\n1. Investigated if there exists any negative correlation between ImageNet classification accuracy and perceived similarity scores.\n2. Examined the impact of hyperparameters adjustments – network depth, width, number of training iterations, weight decay, label smoothing, and dropout rates- on both ImageNet accuracy and Perceptual Scores.\n3. Explored potential explanations behind observed relationships through hypothesis testing based on distortions invariance properties; sensitivity towards spatial frequencies characteristics among others.\n\nMain Contributions:\nOur main contributions lie in identifying several key findings from analyzing the complex interplay between ImageNet accuracy levels & Perceptual Scores achieved via advanced neural network architectures. Specifically, \n1. Identified an unexpected inverse correlation where top-performing ImageNet classifiers tend to yield lower perceptual similarity scores despite high accuracy,\n2. Demonstrated existence of trade-offs beyond simple linear scaling effects suggesting optimal configurations exist maximizing both accuracy and perceptual score performance simultaneously within intermediate accuracy regimes (mid/high range),\n3. Furthered understanding around factors influencing perceptual similarity representation learning processes potentially leading improvements toward future developments aiming closer alignment w.r.t human judgment criteria",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "GFNet: Geometric Flow Network for 3D Point Cloud Semantic Segmentation",
        "abstract": "Point cloud semantic segmentation from projected views, such as range-view (RV) and bird's-eye-view (BEV), has been intensively investigated. Different views capture different information of point clouds and thus are complementary to each other. However, recent projection-based methods for point cloud semantic segmentation usually utilize a vanilla late fusion strategy for the predictions of different views, failing to explore the complementary information from a geometric perspective during the representation learning. In this paper, we introduce a geometric flow network (GFNet) to explore the geometric correspondence between different views in an align-before-fuse manner. Specifically, we devise a novel geometric flow module (GFM) to bidirectionally align and propagate the complementary information across different views according to geometric relationships under the end-to-end learning scheme. We perform extensive experiments on two widely used benchmark datasets, SemanticKITTI and nuScenes, to demonstrate the effectiveness of our GFNet for project-based point cloud semantic segmentation. Concretely, GFNet not only significantly boosts the performance of each individual view but also achieves state-of-the-art results over all existing projection-based models. Code is available at \\url{https://github.com/haibo-qiu/GFNet}.",
        "authors": "H. Qiu, B. Yu, D. Tao",
        "keywords": [
            "geometric flow network",
            "point cloud semantic segmentation",
            "multi-view alignment"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LSAAlS7Yts",
        "pdf_src": "https://api2.openreview.net/pdf/f62f2fab04f1666f845a07fdbe362c69fd5e7cd0.pdf",
        "Code_src": "Code link: https://github.com/haibo-qiu/GFNet",
        "Introduction": "Background: Point cloud semantic segmentation involves classifying points within 3D space into various categories based on their properties or context.\n\nResearch Problem: Recent projection-based approaches have focused on combining multiple viewpoints through simple concatenation without fully leveraging the unique features captured by each viewpoint due to insufficient exploration of geometric correspondences among them.\n\nMethod: The authors propose a Geometric Flow Network (GFNet). This model uses a novel geometric flow module that aligns and propagates complementary information across different projections while considering geometric relations learned throughout training using an end-to-end approach before merging these aligned representations together with a late fusion strategy.\n\nMain Contributions:\n1. Introduce a new geometric flow module designed specifically for aligning and transferring relevant information between different projections.\n2. Demonstrate how this module can enhance both single-view prediction accuracy individually along with improving overall multi-view segmentation performances compared to previous projection-based techniques which simply concatenate feature maps after projecting data onto common planes like range-view (RV) and bird's-eye-view (BEV).\n3. Achieve top-tier performance against current benchmarks including SemanticKITTI and nuScenes datasets showing significant improvements beyond prior art works when tested accordingly via code provided publicly accessible GitHub repository [URL].",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Deep Learning for Bayesian Optimization of Scientific Problems with High-Dimensional Structure",
        "abstract": "Bayesian optimization (BO) is a popular paradigm for global optimization of expensive black-box functions, but there are many domains where the function is not completely a black-box. The data may have some known structure (e.g.\\ symmetries) and/or the data generation process may be a composite process that yields useful intermediate or auxiliary information in addition to the value of the optimization objective. However, surrogate models traditionally employed in BO, such as Gaussian Processes (GPs), scale poorly with dataset size and do not easily accommodate known structure. Instead, we use Bayesian neural networks, a class of scalable and flexible surrogate models with inductive biases, to extend BO to complex, structured problems with high dimensionality. We demonstrate BO on a number of realistic problems in physics and chemistry, including topology optimization of photonic crystal materials using convolutional neural networks, and chemical property optimization of molecules using graph neural networks. On these complex tasks, we show that neural networks often outperform GPs as surrogate models for BO in terms of both sampling efficiency and computational cost.",
        "authors": "S. Kim, P. Y. Lu, C. Loh, et.al",
        "keywords": [
            "neural network",
            "Bayesian optimization",
            "surrogate model"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=tPMQ6Je2rB",
        "pdf_src": "https://api2.openreview.net/pdf/5181318652b228db7582c0f3dd94a395998dee0f.pdf",
        "Code_src": "",
        "Introduction": "Background: Bayesian Optimization (BO) has been widely used for global optimization of expensive black-box functions; however, it faces challenges when dealing with partially observable functions.\n\nResearch Problem: How can we improve the performance of BO by incorporating prior knowledge about the problem's structure?\n\nMethod: We propose to use Bayesian Neural Networks (BNNs) instead of traditional surrogates like Gaussian Processes (GPs). BNNs offer scalability & flexibility while also being able to incorporate known structures into their predictions.\n\nMain Contributions:\n1. Extend BO framework w/ BNNs - A novel approach combining BO & BNNs which allows us to leverage prior knowledge during optimization.\n2. Demonstrate efficacy across various real-world applications - Showcased improved performance over GP-based approaches through experiments involving topology optimization & chem prop opt tasks.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "SHAP-XRT: The Shapley Value Meets Conditional Independence Testing",
        "abstract": "The complex nature of artificial neural networks raises concerns on their reliability, trustworthiness, and fairness in real-world scenarios. The Shapley value---a solution concept from game theory---is one of the most popular explanation methods for machine learning models. More traditionally, from a statistical perspective, feature importance is defined in terms of conditional independence. So far, these two approaches to interpretability and feature importance have been considered separate and distinct. In this work, we show that Shapley-based explanation methods and conditional independence testing are closely related. We introduce the \\textbf{SHAP}ley E\\textbf{X}planation \\textbf{R}andomization \\textbf{T}est (SHAP-XRT), a testing procedure inspired by the Conditional Randomization Test (CRT) for a specific notion of local (i.e., on a sample) conditional independence. With it, we prove that for binary classification problems, the marginal contributions in the Shapley value provide lower and upper bounds to the expected $p$-values of their respective tests. Furthermore, we show that the Shapley value itself provides an upper bound to the expected $p$-value of a global (i.e., overall) null hypothesis. As a result, we further our understanding of Shapley-based explanation methods from a novel perspective and characterize the conditions under which one can make statistically valid claims about feature importance via the Shapley value.",
        "authors": "J. Teneggi, B. Bharti, Y. Romano, et.al",
        "keywords": [
            "Shapley value",
            "conditional independence",
            "SHAP-XRT"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=WFtTpQ47A7",
        "pdf_src": "https://api2.openreview.net/pdf/fd77cf5a420cb8775a7fc6c0bb24816660e4c657.pdf",
        "Code_src": "",
        "Introduction": "Background: The complexity of artificial neural networks has raised questions regarding their reliability, trustworthiness, and fairness when applied in real-world settings.\n\nResearch Problem: This paper aims to explore how Shapley value, derived from game theory as an interpretation method for machine learning models, relates with traditional statistical approach based on conditional independence.\n \nMethod: The authors propose SHAP-XRT, a new testing procedure inspired by CRT for a particular type of local conditional independence. They also demonstrate connections between the Shapley values and both local and global null hypotheses.\n\nMain Contributions: First, they establish relationships among different interpretations such as Shapley values and conditional independence through introducing SHAP-XRT. Second, they derive theoretical results showing that the Shapley values give us information not only at individual samples but across all samples or globally too - providing insights into making more informed decisions concerning model interpretability using these metrics",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Error bounds and dynamics of bootstrapping in actor-critic reinforcement learning",
        "abstract": "Actor-critic algorithms such as DDPG, TD3, and SAC, which are built on Silver's deterministic policy gradient theorem, are among the most successful reinforcement-learning methods, but their mathematical basis is not entirely clear. In particular, the critic networks in these algorithms learn to estimate action-value functions by a “bootstrapping” technique based on Bellman error, and it is unclear why this approach works so well in practice, given that Bellman error is only very loosely related to value error, i.e. to the inaccuracy of the action-value estimate. Here we show that policy training in this class of actor-critic methods depends not on the accuracy of the critic's action-value estimate but on how well the critic estimates the gradient of the action-value, which is better assessed using what we call difference error. We show that this difference error is closely related to the Bellman error — a finding which helps to explain why Bellman-based bootstrapping leads to good policies. Further, we show that value error and difference error show different dynamics along on-policy trajectories through state-action space: value error is a low-pass anticausal (i.e., backward-in-time) filter of Bellman error, and therefore accumulates along trajectories, whereas difference error is a high-pass filter of Bellman error. It follows that techniques which reduce the high-frequency Fourier components of the Bellman error may improve policy training even if they increase the actual size of the Bellman errors. These findings help to explain certain aspects of actor-critic methods that are otherwise theoretically puzzling, such as the use of policy (as distinct from exploratory) noise, and they suggest other measures that may improve these methods.",
        "authors": "A. J. Zerouali, D. B. Tweed",
        "keywords": [
            "actor-critic algorithms",
            "Bellman error",
            "difference error"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=QCjMJfSnYk",
        "pdf_src": "https://api2.openreview.net/pdf/66c35faf24c59624157ede07a5d6b9b4dc0577d5.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses actor-critic algorithms like Deep Deterministic Policy Gradient (DDPG), Twin Delayed Deep Deterministic Policy Gradient (TD3), and Soft Actor-Critic (SAC). These algorithms have been highly effective for reinforcement learning tasks due to their deterministic policy gradient theorem foundation.\n\nResearch Problem:\nDespite being widely used with success, there has always been uncertainty about the theoretical underpinnings behind these algorithms specifically regarding the role played by the critic network within them.\nThe main issue revolves around critics estimating action-value functions via \"bootstrapping\" - an iterative process where predictions made during one iteration inform subsequent iterations – based on Bellman error without any direct relationship between Bellman error itself and the true estimation inaccuracies known as value error or action-value function error.\n\nMethodology:\nTo address this problem, researchers introduce 'difference error'—a measure assessing whether the critic accurately predicts gradients rather than just values—as opposed to traditional Bellman error alone—a measure focusing more on predicting final states. They demonstrate empirical evidence showing that while Bellman error does correlate somewhat with value error—it doesn't directly reflect its quality—the difference error significantly correlates both with the gradient prediction ability of the critic and thus indirectly impacts performance positively when minimizing it.\n\nMain Contributions:\n1. The study reveals that unlike previous beliefs suggesting that accurate critic estimations lead to improved policies, it’s actually the critic’s skill at estimating gradients correctly that matters; hence, the focus should shift towards reducing difference error instead solely concentrating on bellman error reduction efforts.\n2. By analyzing the behavior of value error versus difference error over time taken into account trajectory paths through state-action spaces, further insights were gained indicating that value error acts as a low-pass causal filter accumulating slowly across sequences compared to difference error acting as a high-pass filter rapidly responding changes in gradients leading potentially faster convergence rates despite higher absolute magnitude differences observed initially before stabilization occurs post-training period.\n3. This research provides explanations clarifying some perplexing elements found within actor-critic frameworks including usage patterns seen particularly concerning exploration noises applied during policy updates implying perhaps less emphasis needed here since gradient estimation precision seems paramount regardless of additional noise introduced intentionally during optimization processes aiming toward optimal solutions discovered iteratively throughout each epoch/session run within RL environments simulated scenarios mimicking real-world challenges faced agents must navigate autonomously effectively achieving desired goals set forth accordingly beforehand programmed objectives/targets assigned task-oriented manner",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Fast Slate Policy Optimization: Going Beyond Plackett-Luce",
        "abstract": "An increasingly important building block of large scale machine learning systems is based on returning slates; an ordered lists of items given a query. Applications of this technology include: search, information retrieval and recommender systems.  When the action space is large, decision systems are restricted to a particular structure to complete online queries quickly. This paper addresses the optimization of these large scale decision systems given an arbitrary reward function. We cast this learning problem in a policy optimization framework and propose a new class of policies, born from a novel relaxation of decision functions. This results in a simple, yet efficient learning algorithm that scales to massive action spaces. We compare our method to the commonly adopted Plackett-Luce policy class and demonstrate the effectiveness of our approach on problems with action space sizes in the order of millions.",
        "authors": "O. Sakhi, D. Rohde, N. Chopin",
        "keywords": [
            "large-scale decision systems",
            "policy optimization",
            "Plackett-Luce policy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=f7a8XCRtUu",
        "pdf_src": "https://api2.openreview.net/pdf/23a756457100fb1110e4b4da87225a9dee2700c0.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this research lies within the realm of large-scale machine learning systems where it has become crucial to utilize return slate mechanisms - which involve providing users with ordered lists of relevant items or options upon querying them.\n\nResearch Problem:\nThe primary challenge addressed by this study pertains to optimizing such large-scale decision-making processes under varying conditions when faced with extensive action spaces typical for applications like search engines, recommendation systems etc. Specifically, how can we efficiently refine vast sets of decisions without compromising performance?\n\nMethodology:\nTo tackle this issue effectively, researchers have developed a novel policy optimization framework informed by a relaxed version of traditional decision-making functions known as \"Plackett-Luce\" models used widely across various domains due to their simplicity but limited applicability beyond moderate-sized action spaces.\n \nMain Contributions:\nThis work introduces two key contributions towards addressing challenges posed during scaling up operations involving very high-dimensional actions encountered frequently nowadays:\n\n1. A new family of policies derived through innovative relaxation techniques allowing scalability into extremely broad contexts while maintaining efficiency;\n2. Experimental validation demonstrating efficacy against existing benchmarks including those having million-level dimensions showcasing superiority over conventional approaches",
        "Topic": "Large Language Models"
    },
    {
        "title": "Federated Minimax Optimization with Client Heterogeneity",
        "abstract": "Minimax optimization has seen a surge in interest with the advent of modern applications such as GANs, and it is inherently more challenging than simple minimization. The difficulty is exacerbated by the training data residing at multiple edge devices or \\textit{clients}, especially when these clients can have heterogeneous datasets and heterogeneous local computation capabilities. We propose a general federated minimax optimization framework that subsumes such settings and several existing methods like Local SGDA. We show that naive aggregation of model updates made by clients running unequal number of local steps can result in optimizing a mismatched objective function -- a phenomenon previously observed in standard federated minimization. To fix this problem, we propose normalizing the client updates by the number of local steps. We analyze the convergence of the proposed algorithm for classes of nonconvex-concave and nonconvex-nonconcave functions and characterize the impact of heterogeneous client data, partial client participation, and heterogeneous local computations. For all the function classes considered, we significantly improve the existing computation and communication complexity results. Experimental results support our theoretical claims.",
        "authors": "P. Sharma, R. Panda, G. Joshi",
        "keywords": [
            "federated minimax optimization",
            "heterogeneous clients",
            "normalization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=NnUmg1chLL",
        "pdf_src": "https://api2.openreview.net/pdf/377d93eb34f691f82e5df1bb2a1ce06dc3a87578.pdf",
        "Code_src": "",
        "Introduction": "Background: Minimax optimization gained significant attention due to its application in fields like Generative Adversarial Networks (GANs). However, compared to simple minimization, it presents greater challenges because it involves finding solutions where both participants aim to minimize their loss while maximizing another's.\n\nResearch Problem: Federated learning scenarios often involve distributed training across multiple edge devices (or clients), which may possess different datasets and computational resources leading to heterogeneity issues within the system.\n \nMethod: This paper introduces a generalized federated minimax optimization framework capable of handling diverse setups including those encountered during federated learning processes involving heterogeneous clients - encompassing various existing approaches among them being Local SGD Algorithm (Local SGDA).\n\nMain Contributions:\n1. Identification & Analysis of Issue: Naive aggregation strategies used commonly could lead to mismatches between objectives optimized since they do not account for variations in how many iterations each client performs locally before updating models; this issue was also noticed earlier but without resolution specifically tailored towards federated settings.\n2. Proposed Solution: Normalized Client Updates – A novel approach suggested here corrects inconsistencies arising from disparate numbers of local steps taken per client through normalization based on actual iteration counts ensuring consistency throughout aggregation phases.\n3. Convergence Analysis: Extensive analysis conducted demonstrates convergence properties under consideration of specific types of convexity/ concavity characteristics pertinent to minimax problems along with impacts stemming from varying amounts of dataset heterogeneity amongst clients involved alongside partial participation rates coupled with differences in computational abilities available at individual nodes participating in federated setup.\n4. Complexity Improvements: Results presented indicate substantial enhancements over previous findings regarding computational complexities associated with federated algorithms pertaining to minimizing communication overhead costs whilst maintaining accuracy levels required practical deployment contexts considering real-world constraints faced today",
        "Topic": "Federated Learning"
    },
    {
        "title": "Early Stopping for Deep Image Prior",
        "abstract": "Deep image prior (DIP) and its variants have shown remarkable potential to solve inverse problems in computational imaging (CI), needing no separate training data. Practical DIP models are often substantially overparameterized. During the learning process, these models first learn the desired visual content and then pick up potential modeling and observational noise, i.e., performing early learning then overfitting. Thus, the practicality of DIP hinges on early stopping (ES) that can capture the transition period. In this regard, most previous DIP works for CI tasks only demonstrate the potential of the models, reporting the peak performance against the ground truth but providing no clue about how to operationally obtain near-peak performance without access to the ground truth. In this paper, we set to break this practicality barrier of DIP, and propose an effective ES strategy that consistently detects near-peak performance across several CI tasks and DIP variants. Simply based on the running variance of DIP intermediate reconstructions, our ES method not only outpaces the existing ones---which only work in very narrow regimes, but also remains effective when combined with methods that try to mitigate overfitting.",
        "authors": "H. Wang, T. Li, Z. Zhuang, et.al",
        "keywords": [
            "DIP",
            "Early Stopping",
            "Computational Imaging"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=231ZzrLC8X",
        "pdf_src": "https://api2.openreview.net/pdf/91536c84146f9e978a270dbfb004e71b6dd79a84.pdf",
        "Code_src": "",
        "Introduction": "Background: The deep image prior (DIP) has demonstrated significant promise as a solution for inverse problems in computational imaging by requiring little or no additional training data.\n\nResearch Question: How do researchers effectively utilize the DIP model during the learning phase while avoiding substantial overparametrization?\n\nMethod: This study proposes new early stopping (ES) strategies designed specifically for detecting near-peak performance within various computational imaging tasks using different versions of the DIP algorithm.\n \nMain Contributions: Our proposed ES approach is capable of reliably identifying close-to-optimal performance levels throughout multiple computational imaging scenarios involving distinct variations of the DIP framework - all achieved through monitoring changes solely from the reconstruction errors produced at specific stages inside the network's architecture rather than relying heavily on external validation metrics like those provided via direct comparison between predicted images and true reference images which may be unavailable under certain circumstances. Additionally, it surpasses current state-of-the-art techniques because they operate optimally just around one particular configuration setting; whereas ours functions well even if paired alongside other procedures aimed mitigating issues related to overfitting such as regularization terms added into regular machine learning frameworks",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Towards Fair Video Summarization",
        "abstract": "Automated video summarization is a vision task that aims to generate concise summaries of lengthy videos. Recent advancements in deep learning have led to highly performant video summarization models; however, there has been a lack of attention given to fairness and unbiased representation in the generated summaries. To bridge this gap, we introduce and analytically define the fair video summarization problem, and demonstrate its connections to the well-established problem of fair clustering. To facilitate fair model development, we also introduce the FairVidSum dataset, which is similar in design to state-of-the-art video summarization datasets such as TVSum and SumMe, but also includes annotations for sensitive attributes and individuals alongside frame importance scores. Finally, we propose the SumBal metric for quantifying the fairness of an outputted video summary. We conduct extensive experiments to benchmark the fairness of various state-of-the-art video summarization models. Our results highlight the need for better models that balance accuracy and fairness to ensure equitable representation and inclusion in summaries. For completeness, we also provide a novel fair-only baseline, FVS-LP, to showcase the fairness-utility gap models can improve upon.",
        "authors": "A. Chhabra, K. Patwari, C. Kuntala, et.al",
        "keywords": [
            "fair video summarization",
            "FairVidSum dataset",
            "SumBal metric"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Uj6MRfR1P5",
        "pdf_src": "https://api2.openreview.net/pdf/65074c117966e339963b7d3f1248c52e6d5b7973.pdf",
        "Code_src": "",
        "Introduction": "Background: Automated video summarization involves generating concise summaries from long videos using machine learning techniques.\n\nResearch Problem: Despite recent advances in deep learning leading to high-performing video summarization models, little research focuses on ensuring these summaries are fair or unbiased towards certain groups within society.\n \nMethods: The paper introduces two main contributions:\n1. It defines what it means for a video summarization system to be \"fair\" by drawing parallels with existing work on fair clustering algorithms;\n2. They create new data called FairVidSum containing additional information about people's appearances along with their relevance at different points throughout each video clip so developers don't just focus solely on content relevance when training their systems.\n\nMain Contributions: This study makes several key contributions toward addressing issues related to bias in automated video summarization tasks including introducing metrics like SumBal which quantify how evenly distributed features across genders races etc.,are represented during summarization processes . Furthermore they present experimental evidence showing current top performing methods do not always produce equally representative outputs highlighting potential improvements needed going forward",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Image retrieval outperforms diffusion models on data augmentation",
        "abstract": "Many approaches have been proposed to use diffusion models to augment training datasets for downstream tasks, such as classification. However, diffusion models are themselves trained on large datasets, often with noisy annotations, and it remains an open question to which extent these models contribute to downstream classification performance. In particular, it remains unclear if they generalize enough to improve over directly using the additional data of their pre-training process for augmentation. We systematically evaluate a range of existing methods to generate images from diffusion models and study new extensions to assess their benefit for data augmentation. Personalizing diffusion models towards the target data outperforms simpler prompting strategies. However, using the pre-training data of the diffusion model alone, via a simple nearest-neighbor retrieval procedure, leads to even stronger downstream performance. Our study explores the potential of diffusion models in generating new training data, and surprisingly finds that these sophisticated models are not yet able to beat a simple and strong image retrieval baseline on simple downstream vision tasks.",
        "authors": "M. F. Burg, F. Wenzel, D. Zietlow, et.al",
        "keywords": [
            "diffusion models",
            "data augmentation",
            "downstream performance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=xflYdGZMpv",
        "pdf_src": "https://api2.openreview.net/pdf/030ea20bc06af69370d75517214f582a3fad77c0.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper addresses how diffusion models can be used alongside other techniques like prompt tuning or nearest neighbor search when creating augmented datasets.\n\nResearch Question: The central research problem is whether diffusion models' contribution to downstream classification performance surpasses more straightforward methods based solely on the original dataset's information without further processing by diffusion models.\n \nMethodology: The authors conduct systematic evaluations comparing different methods including prompt tuning variations against each other along with a novel approach involving nearest neighbor searches within the diffusion model's pre-trained dataset itself—without any post-processing beyond basic indexing.\n\nMain Contributions:\n1. They find that while personalizing diffusion models toward specific task data does provide some improvement compared to generic prompts, this isn't always better than simply retrieving relevant images through nearest neighbor lookups into the diffusion model's pre-training dataset—a much less complex method.\n2. Their work highlights limitations regarding generalization capabilities where advanced diffusion models do not necessarily lead to significant improvements versus baselines across various downstream visual recognition tasks despite being highly capable generative tools typically associated with high complexity algorithms.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Transport with Support: Data-Conditional Diffusion Bridges",
        "abstract": "The dynamic Schrödinger bridge problem provides an appealing setting for solving constrained time-series data generation tasks posed as optimal transport problems. It consists of learning non-linear diffusion processes using efficient iterative solvers.\nRecent works have demonstrated state-of-the-art results (eg., in modelling single-cell embryo RNA sequences or sampling from complex posteriors) but are limited to learning bridges with only initial and terminal constraints. Our work extends this paradigm by proposing the Iterative Smoothing Bridge (ISB). We integrate Bayesian filtering and optimal control into learning the diffusion process, enabling the generation of constrained stochastic processes governed by sparse observations at intermediate stages and terminal constraints. \nWe assess the effectiveness of our method on synthetic and real-world data generation tasks and we show that the ISB generalises well to high-dimensional data, is computationally efficient, and provides accurate estimates of the marginals at intermediate and terminal times.",
        "authors": "E. Tamir, M. Trapp, A. Solin",
        "keywords": [
            "dynamic Schrödinger bridge",
            "optimal transport problems",
            "Iterative Smoothing Bridge"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Mbc58EzF5q",
        "pdf_src": "https://api2.openreview.net/pdf/769eb90d25538065904d1f0fc626b3b8cec8bd03.pdf",
        "Code_src": "",
        "Introduction": "Background: The dynamic Schrödinger bridge problem involves finding a path between two quantum states over time while satisfying certain constraints along the way.\n\nResearch Question: How can one extend existing methods which learn bridges under initial and terminal constraints?\n\nMethod: This paper introduces the Iterative Smoothing Bridge (ISB), combining Bayesian filtering and optimal control techniques within the diffusion process framework proposed previously based on iterative solvers like ADI (alternating direction implicit).\n\nMain Contributions:\n1. Extends previous approaches beyond just initial and terminal constraints; it learns bridges subject to both initial conditions & final targets AND additional intermediate constraints through Bayesian filtering and optimal control integration during the diffusion process.\n2. Generates constrained stochastic processes where there may be few observations available throughout most of their duration yet still satisfies all specified boundary conditions accurately across these intervals without requiring full information about every point's value individually – something not possible before due to computational complexity barriers associated with such high-dimensional spaces involved here).\n3. Demonstrates its efficacy via experiments conducted against synthetic datasets designed specifically challenging enough so they could test performance limits imposed upon solutions found thus far when dealing with similar scenarios encountered outside controlled environments simulated artificially beforehand ).",
        "Topic": "Optimal Transport"
    },
    {
        "title": "Uncertainty Estimation for Computed Tomography with a Linearised Deep Image Prior",
        "abstract": "Existing deep-learning based tomographic image reconstruction methods do not provide accurate uncertainty estimates of their reconstructions, hindering their real-world deployment. This paper develops a method, termed as linearised deep image prior (DIP), to estimate the uncertainty associated with reconstructions produced by the DIP with total variation  (TV) regularisation. We endow the DIP with conjugate Gaussian-linear model type error-bars computed from a local linearisation of the neural network around its optimised parameters. To preserve conjugacy, we approximate the TV regulariser with a Gaussian surrogate. This approach provides pixel-wise uncertainty estimates and a marginal likelihood objective for hyperparameter optimisation. We demonstrate the method on synthetic data and real-measured high-resolution 2D $\\mu$CT data, and show that it provides superior calibration of uncertainty estimates relative to previous probabilistic formulations of the~DIP. Our code is available at https://github.com/educating-dip/bayes_dip.",
        "authors": "J. Antoran, R. Barbano, J. Leuschner, et.al",
        "keywords": [
            "uncertainty estimation",
            "linearized deep image prior",
            "Bayesian inference"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=FWyabz82fH",
        "pdf_src": "https://api2.openreview.net/pdf/27fb35c07b1d8bbd29c9ae424b71bcc1280d177a.pdf",
        "Code_src": "https://github.com/educating-dip/bayes_dip",
        "Introduction": "Background: Tomographic image reconstruction using deep learning has become increasingly popular due to advances in computational power; however, existing methods lack reliable uncertainty estimation which hinders practical application.\n\nResearch Problem: The problem addressed here is how to accurately quantify uncertainty when reconstructing images through deep learning models like those employing Total Variation (TV) regularization without such estimations being provided naturally or reliably within current frameworks.\n\nMethodology: A novel technique called Linearized Deep Image Prior (L-DIP) was developed where uncertainty about reconstructed images can be estimated more precisely than previously possible.\n- L-DIP introduces an approximation known as conjugate Gaussian-linear model-type error bars into the DIP framework via a local linearization process performed near optimized neural network parameters,\n- These error bars are then used alongside a Gaussian surrogate function approximating the TV regularizer's behavior during optimization,\n\nMain Contributions:\n1. Pixel-wise uncertainty quantification - unlike other methods providing global measures only;\n2. Development of a marginal likelihood objective allowing for Bayesian hyperparameter tuning leading to better calibrated uncertainty estimates compared to earlier probabilistic approaches applied to similar problems;\n3. Open-source availability – all tools necessary have been made accessible online under GitHub repository mentioned above facilitating reproducibility across different datasets and applications",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "Local Function Complexity for Active Learning via Mixture of Gaussian Processes",
        "abstract": "Inhomogeneities in real-world data, e.g., due to changes in the observation noise level or variations in the structural complexity of the source function, pose a unique set of challenges for statistical inference. Accounting for them can greatly improve predictive power when physical resources or computation time is limited. In this paper, we draw on recent theoretical results on the estimation of local function complexity (LFC), derived from the domain of local polynomial smoothing (LPS), to establish a notion of local structural complexity, which is used to develop a model-agnostic active learning (AL) framework. Due to its reliance on pointwise estimates, the LPS model class is not robust and scalable concerning large input space dimensions that typically come along with real-world problems. Here, we derive and estimate the Gaussian process regression (GPR)-based analog of the LPS-based LFC and use it as a substitute in the above framework to make it robust and scalable. We assess the effectiveness of our LFC estimate in an AL application on a prototypical low-dimensional synthetic dataset, before taking on the challenging real-world task of reconstructing a quantum chemical force field for a small organic molecule and demonstrating state-of-the-art performance with a significantly reduced training demand.",
        "authors": "D. Panknin, S. Chmiela, K. R. Muller, et.al",
        "keywords": [
            "local structural complexity",
            "model-agnostic active learning",
            "Gaussian process regression"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=w4MoQ39zmc",
        "pdf_src": "https://api2.openreview.net/pdf/e53cc9f2e6897a72111c04c91dc1f2a3246bd710.pdf",
        "Code_src": "",
        "Introduction": "Background: Real-world datasets often exhibit heterogeneity caused by factors such as varying levels of observation noise or differences in the complexity of the underlying functions being estimated. This poses significant challenges for statistical inference methods.\n\nResearch Question: How do we account for these sources of heterogeneity within complex real-world datasets while maintaining computational efficiency?\n\nMethodology: The authors leverage recent theoretical developments regarding the estimation of local function complexity using local polynomial smoothing techniques but recognize limitations related to scalability across high-dimensional spaces typical in practical applications involving real-world phenomena.\nTo address these issues, they propose substituting traditional local polynomial smoothing models based on Gaussian Process Regression (GPR). GPR allows estimating the local function complexity more effectively than standard LPS approaches without sacrificing robustness against high-dimensional inputs.\n\nMain Contributions:\n1. They introduce a novel concept called \"local structural complexity\" grounded in theory developed around local polynomial smoothing algorithms extended through Gaussian Processes.\n2. By adapting their methodological approach towards utilizing GPR instead of conventional LPS models, they have created a new algorithmic framework capable of handling both robustness concerns over large-scale datasets efficiently yet still retaining strong predictive capabilities despite resource constraints like available computing power during training phases \n3. Demonstrated efficacy via empirical validation experiments conducted initially upon simplified synthetic datasets followed by tackling demanding tasks including reconstruction efforts applied toward actual quantum chemistry simulations where superior outcomes were achieved compared existing benchmarks requiring less extensive training periods",
        "Topic": "Sample Efficiency in Reinforcement Learning"
    },
    {
        "title": "Towards a General Transfer Approach for Policy-Value Networks",
        "abstract": "Transferring trained policies and value functions from one task to another, such as one game to another with a different board size, board shape, or more substantial rule changes, is a challenging problem. Popular benchmarks for reinforcement learning (RL), such as Atari games and ProcGen, have limited variety especially in terms of action spaces. Due to a focus on such benchmarks, the development of transfer methods that can also handle changes in action spaces has received relatively little attention. Furthermore, we argue that progress towards more general methods should include benchmarks where new problem instances can be described by domain experts, rather than machine learning experts, using convenient, high-level domain specific languages (DSLs). In addition to enabling end users to more easily describe their problems, user-friendly DSLs also contain relevant task information which can be leveraged to make effective zero-shot transfer plausibly achievable. As an example, we use the Ludii general game system, which includes a highly varied set of over 1000 distinct games described in such a language. We propose a simple baseline approach for transferring fully convolutional policy-value networks, which are used to guide search agents similar to AlphaZero, between any pair of games modelled in this system. Extensive results---including various cases of highly successful zero-shot transfer---are provided for a wide variety of source and target games.",
        "authors": "D. J. N. J. Soemers, V. Mella, E. Piette, et.al",
        "keywords": [
            "transfer learning",
            "reinforcement learning",
            "domain-specific languages"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=vJcTm2v9Ku",
        "pdf_src": "https://api2.openreview.net/pdf/ed29368076b2c59d1767589585f556f9b45f5372.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the challenge of transferring pre-trained reinforcement learning (RL) policies and value functions across tasks involving significant differences like altered board sizes, shapes, rules etc.\n\nResearch Question: How do you develop RL transfer methods capable of adapting when there's variability not just in reward structures but even fundamental aspects within the environment?\n\nMethodology: To tackle these challenges without relying solely on predefined benchmarks designed primarily around narrow domains - they suggest incorporating human-readable specifications into benchmarking processes allowing domain experts themselves to define novel environments through intuitive programming interfaces called Domain Specific Languages (DSLs).\n\nMain Contributions:\n1. They advocate moving beyond current benchmarks dominated by Atari games and ProcGen.\n2. Introduce Ludii—a platform hosting diverse games—where each game instance comes defined via a DSL accessible both to ML practitioners & non-experts alike; \n3. Propose a straightforward baseline method for transferring neural network architectures specialized at guiding exploration strategies known as Policy-Value Networks among pairs of Ludii games regardless of their complexity;\n4. Demonstrate extensive empirical evidence supporting robust zero-shot transfers across numerous combinations of source/target Ludii games showcasing its effectiveness under varying conditions.",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Detecting danger in gridworlds using Gromov’s Link Condition",
        "abstract": "Gridworlds have been long-utilised in AI research, particularly in reinforcement learning, as they provide simple yet scalable models for many real-world applications such as robot navigation, emergent behaviour, and operations research. We initiate a study of gridworlds using the mathematical framework of reconfigurable systems and state complexes due to Abrams, Ghrist & Peterson. State complexes, a higher-dimensional analogue of state graphs, represent all possible configurations of a system as a single geometric space, thus making them conducive to study using geometric, topological, or combinatorial methods. The main contribution of this work is a modification to the original Abrams, Ghrist & Peterson setup which we introduce to capture agent braiding and thereby more naturally represent the topology of gridworlds. With this modification, the state complexes may exhibit geometric defects (failure of Gromov's Link Condition). Serendipitously, we discover these failures for agent-only cases occur exactly where undesirable or dangerous states appear in the gridworld. Our results therefore provide a novel method for seeking guaranteed safety limitations in discrete task environments with single or multiple agents, and offer useful safety information (in geometric and topological forms) for incorporation in or analysis of machine learning systems. More broadly, our work introduces tools from geometric group theory and combinatorics to the AI community and demonstrates a proof-of-concept for this geometric viewpoint of the task domain through the example of simple environments.",
        "authors": "T. F. Burns, R. Tang",
        "keywords": [
            "gridworlds",
            "reconfigurable systems",
            "state complexes"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=t4p612DftO",
        "pdf_src": "https://api2.openreview.net/pdf/bb3c4c64ded658f446975f937dd6cdba07521b45.pdf",
        "Code_src": "",
        "Introduction": "Background: Gridworlds are widely used in AI research because they can be easily scaled up while still being relatively simple compared to other complex domains like robotics.\n\nResearch Question: This paper aims to explore how gridworlds could benefit from studying within the context of reconfigurable systems and state complexes proposed by Abrams, Ghrist & Peterson.\n \nMethod: The authors modify the Abrams, Ghrist & Peterson setup so that it captures \"agent braiding,\" allowing us to better understand the topology of gridworlds represented via state complexes - essentially representing every possible configuration at once across an entire high-dimensional space rather than just one graph per state transition path. \n\nMain Contributions:\n1. A new approach capturing agent braiding leading into geometric defects when certain conditions aren't met; \n2. These defects correspond precisely around areas deemed unsafe/dangerous inside gridworld maps;\n3. Provides insight on safe limits under various scenarios involving either singular or multi-agent interactions within discrete tasks;\n4. Offers valuable insights about potential dangers within those spaces presented both geometrically and topologically – potentially beneficial inputs for incorporating into existing ML algorithms or further analyses thereof;\n5. Introduces concepts from geometric group theory/combinatorics previously unexplored extensively among practitioners working mainly with traditional RL approaches towards providing alternative perspectives on problem-solving strategies related specifically here but also applicable elsewhere beyond narrow focus solely upon gridworld simulations alone.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "ProtoCaps: A Fast and Non-Iterative Capsule Network Routing Method",
        "abstract": "Capsule Networks have emerged as a powerful class of deep learning architectures, known for robust performance with relatively few parameters compared to Convolutional Neural Networks (CNNs). However, their inherent efficiency is often overshadowed by their slow, iterative routing mechanisms which establish connections between Capsule layers, posing computational challenges resulting in an inability to scale. In this paper, we introduce a novel, non-iterative routing mechanism, inspired by trainable prototype clustering. This innovative approach aims to mitigate computational complexity, while retaining, if not enhancing, performance efficacy. Furthermore, we harness a shared Capsule subspace, negating the need to project each lower-level Capsule to each higher-level Capsule, thereby significantly reducing memory requisites during training. Our approach demonstrates superior results compared to the current best non-iterative Capsule Network and tests on the Imagewoof dataset, which is too computationally demanding to handle efficiently by iterative approaches. Our findings underscore the potential of our proposed methodology in enhancing the operational efficiency and performance of Capsule Networks, paving the way for their application in increasingly complex computational scenarios. Code is available at https://github.com/mileseverett/ProtoCaps.",
        "authors": "M. Everett, M. Zhong, G. Leontidis",
        "keywords": [
            "efficient_routing",
            "capsule_subspace",
            "computational_complexity"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Id10mlBjcx",
        "pdf_src": "https://api2.openreview.net/pdf/4e5d36f1ff25a1c01ea8c13d3c717b334e8bb551.pdf",
        "Code_src": "Code link: https://github.com/mileseverett/ProtoCaps",
        "Introduction": "Background: The paper discusses the limitations of capsule networks due to their inefficient routing algorithms that are slower than convolutional neural networks despite fewer parameters.\n\nResearch Problem: How can capsule network's computational inefficiency be mitigated without compromising its performance?\n\nMethod: The authors propose a new non-iterative routing algorithm based on trainable prototype clustering techniques within capsule networks using a shared capsule subspace concept.\n\nMain Contributions:\n1. A novel non-iterative routing method called ProtoCaps is introduced.\n2. Computational complexity reduced through trainable prototype clustering technique leading to better scalability over iterative methods like Dynamic Routing.\n3. Memory requirements drastically decreased since there’s no projection from one layer to another; instead, they use a shared subspace across all capsules throughout different levels or depths \n4. Demonstrates improved performance against existing state-of-the-art non-iterative capsule networks when tested on ImageWoof dataset—a challenging task for iterative routing approaches—indicating promising advancements towards practical applications where computation demands grow exponentially larger",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Partial Optimal Transport for Support Subset Selection",
        "abstract": "In probabilistic terms, optimal transport aims to find a joint distribution that couples two distributions and minimizes the cost of transforming one distribution to another. Any feasible coupling necessarily maintains the support of both distributions. However, maintaining the entire support is not ideal when only a subset of one of the distributions, namely the source, is assumed to align with the other target distribution. For these cases, which are common in machine learning applications, we study the semi-relaxed partial optimal transport problem that relaxes the constraints on the joint distribution allowing it to under-represent a subset of the source by over-representing other subsets of the source by a constant factor. In the discrete distribution case, such as in the case of two samples from continuous random variables, optimal transport with the relaxed constraints is a linear program. When sufficiently relaxed, the solution has a source marginal with only a subset of its original support. We investigate the scaling path of solutions, specifically the relaxed marginal distribution for the source, across different relaxations and show that it is distinct from the solutions from penalty-based semi-relaxed unbalanced optimal transport problems and fully-relaxed partial optimal transport, which have previously been explored. We demonstrate the usefulness of this support subset selection in applications such as color transfer, partial point cloud alignment, and semi-supervised machine learning, where a part of data is curated to have reliable labels and another part is unlabeled or has unreliable labels. Our experiments show that optimal transport under the relaxed constraint can improve the performance of these applications by allowing for more flexible alignment between distributions.",
        "authors": "B. Riaz, Y. Karahan, A. J. Brockmeier",
        "keywords": [
            "subset selection",
            "semi-relaxed optimal transport",
            "application"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=75CcopPxIr",
        "pdf_src": "https://api2.openreview.net/pdf/6aa93270ca5ce5ae164a48c06d282b8134c37eae.pdf",
        "Code_src": "",
        "Introduction": "Background: Optimal transport (OT) is an area within probability theory concerned with finding the most efficient way to transform one set of probabilities into another while minimizing some form of distance measure.\n\nResearch Problem: The standard OT problem assumes that all parts of each distribution must be coupled together; however, many real-world scenarios involve only certain elements needing to match up precisely—while others may remain uncoupled without affecting the overall goal significantly enough to justify their inclusion at full strength.\n \nMethodology: To address issues like those mentioned above—the need sometimes to couple just specific elements rather than everything—we introduce what's called \"semi-relaxed\" partial optimal transport (\"semi-relaxed P-OT\"). This approach allows us to selectively represent certain areas better through increased weight compared to how they would otherwise appear if every element were considered equally important.\n \nMain Contributions: By introducing our semi-relaxed method along with empirical evidence demonstrating improved results using it versus traditional methods including penalized versions designed similarly but still requiring complete coupling among points regardless of relevance), we provide new insights about how best to apply optimal transport techniques depending upon whether there exists significant redundancy amongst items being aligned—or indeed any redundancy at all outside selected pairs/groups thereof!.",
        "Topic": "Optimal Transport"
    },
    {
        "title": "Wrapped $\\beta$-Gaussians with compact support for exact probabilistic modeling on manifolds",
        "abstract": "We introduce wrapped $\\beta$-Gaussians, a family of wrapped distributions on Riemannian manifolds, supporting efficient reparametrized sampling, as well as exact density estimation, effortlessly supporting high dimensions and anisotropic scale parameters. We extend Fenchel-Young losses for geometry-aware learning with wrapped $\\beta$-Gaussians, and demonstrate the efficacy of our proposed family in a suite of experiments on hypersphere and rotation manifolds: data fitting, hierarchy encoding, generative modeling with variational autoencoders, and multilingual word embedding alignment.",
        "authors": "S. Troshin, V. Niculae",
        "keywords": [
            "wrapped beta-Gaussians",
            "Riemannian manifolds",
            "Fenchel-Young losses"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=KrequDpWzt",
        "pdf_src": "https://api2.openreview.net/pdf/bfd83be3d4968e3161796f793c01e15a6a4f85c9.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper introduces wrapped $\\beta$-Gaussians, which are a new class of probability distributions defined on Riemannian manifolds that can be sampled efficiently while also allowing for accurate computation of their densities.\n\nResearch Problem:\nThe problem addressed by this work is how to define and use probability distributions over complex geometric spaces such as spheres or rotations where standard Gaussian distributions do not apply directly due to the non-Euclidean nature of these spaces.\n \nMethods:\nTo solve this issue, the authors propose wrapped $\\beta$-Gaussians - a distribution parameterized using spherical coordinates within a sphere manifold – enabling both fast reparameterization trick sampling without requiring gradient computations during inference time; they further develop novel loss functions based on Fenchel-Young inequality tailored specifically towards geometry-aware machine learning tasks involving these types of manifolds like hierarchical coding problems etcetera.\n \nMain Contributions:\nTheir main contributions include introducing wrapped $\\beta$-Gaussian distributions along with associated methods for approximate Bayesian inference including importance sampling algorithms adapted from previous works dealing with other kinds of wrapped distributions but extended here into higher dimensional settings than before; developing specialized variants optimized especially when applied onto hyperbolic spaces alongside rotational ones; demonstrating effectiveness across various applications ranging from variational autoencoder architectures capable generating samples conforming closely according pre-defined constraints imposed upon them via learned embeddings aligned between languages spoken around globe among many others besides just those mentioned above).",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "GIT-Net: Generalized Integral Transform for Operator Learning",
        "abstract": "This article introduces GIT-Net, a deep neural network architecture for approximating Partial Differential Equation (PDE) operators, inspired by integral transform operators. GIT-NET harnesses the fact that common differential operators commonly used for defining PDEs can often be represented parsimoniously when expressed in specialized functional bases (e.g., Fourier basis). Unlike rigid integral transforms, GIT-Net parametrizes adaptive generalized integral transforms with deep neural networks. When compared to several recently proposed alternatives, GIT-Net's computational and memory requirements scale gracefully with mesh discretizations, facilitating its application to PDE problems on complex geometries. Numerical experiments demonstrate that GIT-Net is a competitive neural network operator, exhibiting small test errors and low evaluations across a range of PDE problems. This stands in contrast to existing neural network operators, which typically excel in just one of these areas.",
        "authors": "C. Wang, A. H. Thiery",
        "keywords": [
            "GIT-Net",
            "Partial Differential Equations (PDE)",
            "Deep Neural Networks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=0WKTmrVkd2",
        "pdf_src": "https://api2.openreview.net/pdf/734a832a89e23d516a35b1c200ca7c75ce30c676.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper presents GIT-Net, an innovative approach based on deep neural networks designed specifically for approximating Partial Differential Equations (PDE) operators.\n\nResearch Problem: The challenge lies in finding efficient methods capable of representing PDE operators using specialized functional bases such as Fourier basis while also addressing the limitations associated with traditional integral transforms like computational complexity or rigidity.\n\nMethodology: GIT-Net employs adaptive generalized integral transforms parameterized through deep neural networks rather than relying on fixed integral transforms. It leverages the idea from integral transforms but adapts it within the framework of neural networks allowing for more flexibility without compromising performance significantly over different discretization levels typical in solving PDEs numerically especially those involving complex geometries.\n\nMain Contributions:\n1. A novel neural network architecture called GIT-Net has been introduced.\n2. It generalizes integral transforms adaptively via neural networks leading to improved scalability both computationally and memory-wise relative to other recent proposals dealing with similar tasks related to PDE approximation \n3. Demonstrated effectiveness through numerical tests where GIT-Net shows promising results including reduced error rates during testing phase against various types of PDE applications thus providing evidence supporting its potential use beyond current state-of-the-art solutions available today",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Semi-Supervised Single Domain Generalization with Label-Free Adversarial Data Augmentation",
        "abstract": "Domain generalization (DG) has attracted increasing attention recently, as it seeks to improve the generalization ability of visual recognition models to unseen target domains. DG leverages multiple source domains for model training, while single domain generalization (SDG) further restricts such setting by exploiting only a single source domain. Nevertheless, both DG and SDG assume that the source domains are fully labeled, which might not be practical in many real world scenarios. In this paper, we present a new problem, i.e., semi-supervised single domain generalization (SS-SDG), which aims to train a model with a partially labeled single source domain to generalize to multiple unseen testing domains. We propose an effective framework to address this problem. In particular, we design a label-free adversarial data augmentation strategy to diversify the source domain, and propose a novel multi-pair FixMatch loss to generalize classifiers to unseen testing domains. Extensive experiments on OfficeHome, PACS and DomainNet20 datasets show that our method surpasses the latest SDG and semi-supervised methods. Moreover, on PACS and DomainNet20, our method approaches the fully supervised ERM upper bound within $5\\%$ gap, but only uses less than $8\\%$ of the labels.",
        "authors": "R. Zhu, X. Yu, S. Li",
        "keywords": [
            "semi-supervised learning",
            "single domain generalization",
            "FixMatch"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=sUlbRfLijj",
        "pdf_src": "https://api2.openreview.net/pdf/1dc6dde52cf04f6eae7893ce6fc384a8b8aea394.pdf",
        "Code_src": "",
        "Introduction": "Background: The field of computer vision is increasingly focused on improving the performance of deep learning models across different tasks or environments through techniques like domain generalization (DG). However, most existing work assumes access to fully-labeled data from various sources (\"source domains\"), making its application challenging when dealing with limited labeling resources.\n\nResearch Problem: This research addresses the challenge posed by \"semi-supervised single domain generalization\" (SS-SDG), where there exists partial annotation information available just for one specific source domain rather than several diverse ones used traditionally under DG settings.\n \nMethodology: To tackle SS-SDG effectively, two key contributions have been made:\n1. Label-Free Adversarial Data Augmentation Strategy - A technique designed without relying solely on manually assigned labels; instead using adversarial perturbations generated during training sessions helps broaden the diversity represented among examples drawn from the given source domain.\n2. Novel Multi-Pair FixMatch Loss Function - An innovative approach developed specifically tailored towards enabling classifiers trained via SS-SDG to successfully transfer their knowledge onto previously unseen test domains efficiently despite having fewer labeled samples compared to full supervision setups.\n\nMain Contributions: The proposed framework significantly improves upon state-of-the-art results even though it operates exclusively off a fraction of the total labels required if all were known upfront – achieving near-perfect accuracy gaps against those obtained purely based on exhaustive annotations yet still maintaining substantial efficiency gains due to lower reliance on manual labeling efforts overall.",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "Accelerating Batch Active Learning Using Continual Learning Techniques",
        "abstract": "A major problem with Active Learning (AL) is high training costs since models are typically retrained from scratch after every query round. We start by demonstrating that standard AL on neural networks with warm starting fails, both to accelerate training and to avoid catastrophic forgetting when using fine-tuning over AL query rounds.  We then develop a new class of techniques, circumventing this problem, by biasing further training towards previously labeled sets. We accomplish this by employing existing, and developing novel, replay-based Continual Learning (CL) algorithms that are effective at quickly learning the new without forgetting the old, especially when data comes from an evolving distribution. We call this paradigm \\textit{\"Continual Active Learning\" (CAL)}.  We show CAL achieves significant speedups using a plethora of replay schemes that use model distillation and that select diverse/uncertain points from the history. We conduct experiments across many data domains, including natural language, vision, medical imaging, and computational biology, each with different neural architectures and dataset sizes. CAL consistently provides a $\\sim$3x reduction in training time, while retaining performance and out-of-distribution robustness, showing its wide applicability.",
        "authors": "A. M. Das, G. Bhatt, M. M. Bhalerao, et.al",
        "keywords": [
            "Continual Active Learning",
            "Catastrophic Forgetting",
            "Model Distillation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=T55dLSgsEf",
        "pdf_src": "https://api2.openreview.net/pdf/b0125c7b1c0db60e4924d1b923346c059d433a11.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses one of the main challenges associated with Active Learning (AL), which involves selecting informative examples for supervised machine learning tasks through iterative querying rather than fully labeling all instances upfront. This challenge arises due to the necessity often required under AL frameworks such as retraining neural network models entirely upon completion of each query round.\n\nResearch Problem: The research question posed revolves around how to efficiently perform active learning within neural networks where traditional methods fail because they do not effectively balance between accelerating the training process post-query iterations or avoiding catastrophic forgetting during fine-tuning procedures applied sequentially throughout these queries.\n\nMethods: To tackle this issue, authors introduce a novel approach called \"Continual Active Learning\" (CAL). They propose a set of techniques designed specifically so that subsequent training phases focus more heavily on previous labeled datasets instead of discarding them completely like conventional approaches would following each iteration step. This is achieved via the integration of replay-based Continual Learning (CL) strategies—algorithms adept at rapidly acquiring knowledge about newly encountered data while simultaneously preserving what has been learned earlier even if it's drawn from changing distributions typical seen in real-world scenarios involving dynamic environments.\n \nMain Contributions:\n1. Development of a framework termed \"Continual Active Learning\" (CAL) that leverages replay mechanisms to improve efficiency compared to regular Active Learning processes; \n2. Implementation & evaluation of various replay schemes incorporating elements similar to model distillation along with selection criteria focusing on diversity and uncertainty among historical samples;\n3. Extensive empirical validation performed cross-domainly across several distinct types of datasets ranging from natural language processing up to computational biology fields utilizing varied neural architecture designs showcasing consistent improvements in terms of reduced training duration whilst maintaining equivalent levels of accuracy and generalization capabilities beyond the original training domain (i.e., out-of-distribution robustness);\n4. Demonstrating CAL’s broad adaptability regardless of varying neural architectures employed alongside differences in dataset size complexities involved therein.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Beyond Boundaries: A Novel Data-Augmentation Discourse for Open Domain Generalization",
        "abstract": "The problem of Open Domain Generalization (ODG) is multifaceted, encompassing shifts in domains and labels across all source and target domains. Existing approaches have encountered challenges such as style bias towards training domains, insufficient feature-space disentanglement to highlight semantic features, and discriminativeness of the latent space. Additionally, they rely on a confidence-based target outlier detection approach, which can lead to misclassifications when target open samples visually align with the source domain data.\nIn response to these challenges, we present a solution named \\textsc{ODG-Net}. We aim to create a direct open-set classifier within a \\textit{discriminative}, \\textit{unbiased}, and \\textit{disentangled} semantic embedding space. To enrich data density and diversity, we introduce a generative augmentation framework that produces \\textit{style-interpolated} novel domains for closed-set images and novel pseudo-open images by \\textit{interpolating the contents of paired training images}. Our augmentation strategy skillfully utilizes \\textit{disentangled style and content information} to synthesize images effectively.\nFurthermore, we tackle the issue of style bias by representing all images in relation to all source domain properties, which effectively accentuates complementary visual features. Consequently, we train a multi-class semantic object classifier, incorporating both closed and open class classification capabilities, along with a style classifier to identify style primitives. The joint use of style and semantic classifiers facilitates the disentanglement of the latent space, thereby enhancing the generalization performance of the semantic classifier.\nTo ensure discriminativeness in both closed and open spaces, we optimize the semantic feature space using novel metric losses. The experimental results on six benchmark datasets convincingly demonstrate that \\textsc{ODG-Net} surpasses the state-of-the-art by an impressive margin of $1-4\\%$ in both open and closed-set DG scenarios.",
        "authors": "S. Bose, A. Jha, H. Kandala, et.al",
        "keywords": [
            "Open Domain Generalization",
            "ODG-Net",
            "Generative Augmentation Framework"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=jpZmhiIys1",
        "pdf_src": "https://api2.openreview.net/pdf/fac07a91bb8d2a58a9d6fc4e0ea78995012f2075.pdf",
        "Code_src": "",
        "Introduction": "Background: Open Domain Generalization (ODG) refers to the task of learning from multiple sources while being able to generalize well without any prior knowledge about unseen target distributions.\n\nResearch Problem: ODG faces several challenges including style bias toward training domains, insufficient feature-space disentanglement leading to difficulty highlighting semantic features, lack of discriminativeness in the latent space due to reliance on a confidence-based target outlier detection approach; this latter method may result in misclassification if there are visual similarities between target open samples and source domain data.\n\nMethod: In order to address these issues, authors propose a new network called ODG-Net designed specifically for ODG tasks. They focus on creating a direct open-set classifier operating within a discriminative, unbiased, and disentangled semantic embedding space where they also enhance data density through their proposed generative augmentation framework capable of producing novel domains for closed-set images via interpolating paired training image contents or generating novel pseudo-open images based on those same methods involving disentangled style and content information synthesis techniques.\n\nMain Contributions: Authors' contributions include developing a novel metric loss function aimed at optimizing semantic feature space discriminability ensuring effectiveness regardless whether dealing exclusively with closed sets or mixed classes containing some open ones alongside known closed set examples thus improving overall ODG performance significantly over existing benchmarks achieving improvements ranging up to 1-4%.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Revisiting Topic-Guided Language Models",
        "abstract": "A recent line of work in natural language processing has aimed to combine language models and topic models. These \\textit{topic-guided language models} augment neural language models with topic models, unsupervised learning methods that can discover document-level patterns of word use. This paper compares the effectiveness of these methods in a standardized setting. We study four topic-guided language models and two baselines, evaluating the held-out predictive performance of each model on four corpora. Surprisingly, we find that \\textit{none of these methods outperform a standard LSTM language model baseline}, and most fail to learn good topics. Further, we train a probe of the neural language model that shows that the baseline's hidden states already encode topic information. We make public all code used for this study.",
        "authors": "C. Zheng, K. Vafa, D. Blei",
        "keywords": [
            "topic-guided language models",
            "LSTM language model baseline",
            "predictive performance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=lXBEwFfxpA",
        "pdf_src": "https://api2.openreview.net/pdf/2d3bd1c0ce8c96745d2e5f6112f370a5537f5326.pdf",
        "Code_src": "",
        "Introduction": "Background: Recent research in natural language processing aims to integrate language models and topic models by developing topic-guided language models which enhance neural language models using topic models - an unsupervised method capable of uncovering patterns at the document level.\n\nResearch Question: The primary question addressed is whether or not these topic-guided approaches are more effective than traditional LSTM-based language models when evaluated under controlled conditions across different datasets.\n\nMethods: To answer our research questions, several topic-guided language models were compared against two baselines – one being a standard LSTM language model. Predictive performances from holdout sets within four distinct corpora served as metrics assessing how well individual models could capture semantic relationships between words based on their usage context over documents/topics.\n\nMain Contributions:\n1. None of the proposed topic-guided language models significantly outperformed the LSTM baseline.\n2. Most failed to effectively identify meaningful topics despite attempts towards integrating them into the language modeling process through various techniques such as attention mechanisms focused on topic representation during encoding.\n3. A probe trained specifically onto the LSTM baseline revealed its hidden states inherently encoded some form of latent topic structure without explicit training indicating prior knowledge about potential topical structures exists even before applying additional topic guidance.\n4. All relevant code was made publicly available so others may replicate findings independently if desired.",
        "Topic": "Large Language Models"
    },
    {
        "title": "Two-Level Actor-Critic Using Multiple Teachers",
        "abstract": "Deep reinforcement learning has successfully allowed agents to learn complex behaviors for many tasks. However, a key limitation of current learning approaches is the sample-inefficiency problem, which limits performance of the learning agent. This paper considers how agents can benefit from improved learning via teachers' advice. In particular, we consider the setting with multiple sub-optimal teachers, as opposed to having a single near-optimal teacher. We propose a flexible two-level actor-critic algorithm where the high-level network learns to choose the best teacher in the current situation while the low-level network learns the control policy.",
        "authors": "S. Zhang, S. Das, S. G. Subramanian, et.al",
        "keywords": [
            "sample-efficient",
            "multi-suboptimal teachers",
            "hierarchical actor-critic"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=LfQ6uAVAEo",
        "pdf_src": "https://api2.openreview.net/pdf/97bcc9ccbacae0570bf862da125a6258c9ebf894.pdf",
        "Code_src": "",
        "Introduction": "Background: The success of deep reinforcement learning (DRL) in enabling agents to learn complex behaviors across various domains and tasks highlights its potential applications; however, one significant challenge it faces pertains to sample inefficiency – that is, requiring an extensive number of interactions or samples before achieving satisfactory performance.\n\nResearch Question: How might DRL be enhanced through leveraging guidance provided by 'teachers', especially when considering scenarios involving several less-than-perfect teachers rather than just relying on what would ideally serve as a singularly optimal teacher?\n\nMethodology: To address this question effectively, our research introduces a novel multi-level actor-critic framework designed specifically within such a context characterized by heterogeneous teaching strategies among different teachers (\"suboptimal\" but not necessarily uniformly poor). At the higher level, there's a network tasked solely with selecting between these diverse teachers based on their relative merits under dynamic conditions encountered during training—essentially functioning like \"meta-controller.\" Concurrently at the lower level resides another neural network dedicated to acquiring robust policies irrespective of who serves as the chosen teacher—a sort of \"policy learner.\"\n\nMain Contributions:\n1. **Teacher Selection Network:** Our architecture includes a sophisticated mechanism capable of dynamically choosing amongst available teachers according to task demands.\n2. **Policy Learning Network:** A second component autonomously develops effective decision-making skills without being overly reliant upon any specific teacher’s strategy—an adaptable approach against varying instructional styles throughout the learning process.\n3. **Flexibility and Scalability:** By separating concerns into distinct networks—one focused on strategic selection versus operational execution—the system exhibits greater flexibility allowing adaptation even if new teachers are introduced mid-training session—it scales well beyond initial configurations due to modular design principles employed here.\n\nOverall Impact: This work represents progress towards more efficient machine learning systems particularly suited toward environments rich with uncertainty about both rewards and instructive signals emanating from external sources involved",
        "Topic": "Sample Efficiency in Reinforcement Learning"
    },
    {
        "title": "RCT Rejection Sampling for Causal Estimation Evaluation",
        "abstract": "Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates---such as text data, genomics, or the behavioral social sciences---researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation.  However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call RCT rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm indeed results in low bias when oracle estimators are evaluated on the confounded samples, which is not always the case for a previously proposed algorithm. In addition to this identification result, we highlight several finite data considerations for evaluation designers who plan to use RCT rejection sampling on their own datasets. As a proof of concept, we implement an example evaluation pipeline and walk through these finite data considerations with a novel, real-world RCT---which we release publicly---consisting of approximately 70k observations and text data as high-dimensional covariates. Together, these contributions build towards a broader agenda of improved empirical evaluation for causal estimation.",
        "authors": "K. A. Keith, S. Feldman, D. Jurgens, et.al",
        "keywords": [
            "confounding",
            "causal estimation",
            "observational data"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=F74ZZk5hPa",
        "pdf_src": "https://api2.openreview.net/pdf/46c04eed864b9f1b094e93fc829ef241fe820e1d.pdf",
        "Code_src": "",
        "Introduction": "Background: Confounding can lead to biased estimates of causal effects derived from observational studies.\nResearch Question: How do researchers effectively adjust for confounding factors?\nMethods: The paper introduces a method based on subsampling randomized controlled trials (RCTs), creating confounded observational datasets alongside genuine RCT outcomes serving as gold standards against which adjustments may be validated.\n\nMain Contributions:\n1. A novel sampling algorithm called \"RCT Rejection Sampling\" designed specifically for this purpose; \n2. It provides theoretical evidence ensuring that the resulting observational dataset maintains sufficient statistical power necessary for identifying causal relationships;\n3. Demonstrated its effectiveness via synthetic data where it reduced bias compared to previous algorithms without such guarantees;\n4. Offered practical guidance considering limitations due to small sample sizes (\"finite data considerations\") during empirical evaluations employing similar strategies;\n5. Presented findings obtained applying the methodology within actual large-scale observational study involving over 70,000 participants along with textual covariate dimensions - all made available openly online – contributing insights into improving empirical approaches used across various fields like genomics & behavioral science dealing with complex multivariate datasets.",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "Variational Causal Dynamics: Discovering Modular World Models from Interventions",
        "abstract": "Latent world models allow agents to reason about complex environments with high-dimensional observations. However, adapting to new environments and effectively leveraging previous knowledge remain significant challenges. We present Variational Causal Dynamics (VCD), a structured world model that exploits the invariance of causal mechanisms across environments to achieve fast and modular adaptation. By causally factorising a transition model, VCD is able to identify reusable components across different environments. This is achieved by combining causal discovery and variational inference to learn a latent representation and transition model jointly in an unsupervised manner. Specifically, we optimise the evidence lower bound jointly over a representation model and a transition model structured as a causal graphical model. In evaluations on simulated environments with state and image observations, we show that VCD is able to successfully identify causal variables, and to discover consistent causal structures across different environments. Moreover, given a small number of observations in a previously unseen, intervened environment, VCD is able to identify the sparse changes in the dynamics and to adapt efficiently. In doing so, VCD significantly extends the capabilities of the current state-of-the-art in latent world models while also comparing favourably in terms of prediction accuracy.",
        "authors": "A. Lei, B. Schölkopf, I. Posner",
        "keywords": [
            "causal discovery",
            "variational inference",
            "latent representation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=V9tQKYYNK1",
        "pdf_src": "https://api2.openreview.net/pdf/717d45435d1b2edf3cea61f086930df9d1353700.pdf",
        "Code_src": "",
        "Introduction": "Background: Latent world models are used for reasoning about complex environments using high-dimensional observations but face difficulties when it comes to adapting to new environments or making use of prior knowledge.\n\nResearch Question: How can we develop a structured world model which adapts quickly and modulates its learning process?\n\nMethod: The authors introduce Variational Causal Dynamics (VCD), a method based on causal factorisation within a transition model allowing reuse of learned components from one environment into another through joint learning of a latent representation and transition model via causal discovery and variational inference without supervision; they optimize this approach's Evidence Lower Bound (ELBO).\n\nMain Contributions: \n1. They have developed a novel structured world model called Variational Causal Dynamics.\n2. Their proposed framework enables efficient adaptation between various environments due to its ability to recognize reusable elements among them - thanks to causal factorization leading towards modular learning processes where past experiences inform future predictions more accurately than traditional methods do alone!\n3. Compared against other existing latent world models currently available today regarding both predictive performance metrics such as mean squared error reduction rates during training phases before deployment onto real-world tasks like autonomous driving scenarios etc., our paper shows how well these improvements translate practically speaking beyond simulation studies demonstrating tangible benefits associated with employing their technique compared others'.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Causal Reinforcement Learning: A Survey",
        "abstract": "Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for their decisions and generalizing the acquired knowledge. Causality, however, offers notable advantages by formalizing knowledge in a systematic manner and harnessing invariance for effective knowledge transfer. This has led to the emergence of causal reinforcement learning, a subfield of reinforcement learning that seeks to enhance existing algorithms by incorporating causal relationships into the learning process. In this survey, we provide a comprehensive review of the literature in this domain. We begin by introducing basic concepts in causality and reinforcement learning, and then explain how causality can help address key challenges faced by traditional reinforcement learning. We categorize and systematically evaluate existing causal reinforcement learning approaches, with a focus on their ability to enhance sample efficiency, advance generalizability, facilitate knowledge transfer, mitigate spurious correlations, and promote explainability, fairness, and safety. Lastly, we outline the limitations of current research and shed light on future directions in this rapidly evolving field.",
        "authors": "Z. Deng, J. Jiang, G. Long, et.al",
        "keywords": [
            "causality",
            "reinforcement learning",
            "sample efficiency"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=qqnttX9LPo",
        "pdf_src": "https://api2.openreview.net/pdf/8ee97e0d95d1d079ddc10c60dac7a71ebfeb61df.pdf",
        "Code_src": "",
        "Introduction": "Background: Reinforcement learning (RL) is widely used as a framework for solving sequential decision-making tasks involving uncertainties; however, its application in practical settings faces significant difficulties due to several factors such as the need for extensive exploration during training which leads to low sample efficiency.\n\nResearch Problem: The primary challenge addressed here concerns RL's inability to understand or reason about the underlying causes within complex environments leading it towards inefficient exploratory behavior without any prior knowledge base.\n \nMethodology: To overcome these issues, researchers have introduced causal reinforcement learning - a branch focused on integrating causal reasoning principles directly into the RL algorithmic design aiming at improving performance metrics like sample efficiency & generalization capabilities while addressing interpretability, robustness against noise/spurious correlations etc.\n\nMain Contributions:\n1. A thorough overview of foundational theories related both to causality theory along with reinforcement learning techniques;\n2. Identification  & classification of various causal RL methodologies based upon their contributions toward enhancing core aspects mentioned above;\n3. Evaluation criteria are proposed focusing on sample efficiency improvements, advancements made regarding generalization abilities across different domains, facilitation provided around knowledge transfer mechanisms amongst others;\n4. Discussion highlighting present-day limitations encountered when employing causal RL strategies including suggestions pointing out potential avenues worth exploring moving forward",
        "Topic": "Sample Efficiency in Reinforcement Learning"
    },
    {
        "title": "ECG Representation Learning with Multi-Modal EHR Data",
        "abstract": "Electronic Health Records (EHRs) provide a rich source of medical information across different modalities such as electrocardiograms (ECG), structured EHRs (sEHR), and unstructured EHRs (text). Inspired by the fact that many cardiac and non-cardiac diseases influence the behavior of the ECG, we leverage structured EHRs and unstructured EHRs from multiple sources by pairing with ECGs and propose a set of three new multi-modal contrastive learning models that combine ECG, sEHR, and text modalities. The performance of these models is compared against different baseline models such as supervised learning models trained from scratch with random weights initialization, and self-supervised learning models trained only on ECGs. We pre-train the models on a large proprietary dataset of about 9 $million$ ECGs from around 2.4 $million$ patients and evaluate the pre-trained models on various downstream tasks such as classification, zero-shot retrieval, and out-of-distribution detection involving the prediction of various heart conditions using ECG waveforms as input, and demonstrate that the models presented in this work show significant improvements compared to all baseline modes.",
        "authors": "S. K. Lalam, H. K. Kunderu, S. Ghosh, et.al",
        "keywords": [
            "ECG",
            "Multi-modal Contrastive Learning",
            "Electronic Health Records"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=UxmvCwuTMG",
        "pdf_src": "https://api2.openreview.net/pdf/bf8bd08594d1b484133213a81f685c20a726dec4.pdf",
        "Code_src": "",
        "Introduction": "Background: Electronic health records (EHRs) contain valuable medical data including electrocardiograms (ECGs), which can be used for disease diagnosis.\n\nResearch Problem: How to effectively utilize multimodal EHR data consisting of both structured and unstructured components along with ECG signals?\n\nMethodology: Developed novel multi-modal contrastive learning models combining ECG, structured electronic health record (sEHR), and textual data; pre-trained model on a massive private dataset containing approximately 9 million ECGs extracted from over 2.4 million patient cases before fine-tuning them onto diverse downstream tasks like classification or anomaly detection based upon predicted heart conditions via waveform analysis.\n\nMain Contributions:\n1. Introduced an innovative approach integrating three distinct types of multimodal data – ECG, sEHR & text - into one framework.\n2. Demonstrated superior performance through extensive experiments comparing our proposed method versus baselines employing supervised/self-supervised techniques alone without considering multimodality benefits within their architectures \n3. Achieved state-of-the-art results when evaluated under several practical applications related to predicting cardiovascular disorders utilizing raw ECG waveforms solely",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Personalized Federated Learning with Spurious Features: An Adversarial Approach",
        "abstract": "One of the common approaches for personalizing federated learning is fine-tuning the global model for each local client. While this addresses some issues of statistical heterogeneity, we find that such personalization methods are vulnerable to spurious features at local agents, leading to reduced generalization performance. This work considers a setup where spurious features correlate with the label in each client's training environment, and the mixture of multiple training environments (i.e., the global environment) diminishes the spurious correlations. In other words, while the global federated learning model trained over the global environment suffers less from spurious features, the local fine-tuning step may lead to personalized models vulnerable to spurious correlations. In light of this practical and pressing challenge, we propose a novel strategy to mitigate the effect of spurious features during personalization by maintaining the adversarial transferability between the global and personalized models. Empirical results on object and action recognition tasks show that our proposed approach bounds personalized models from further exploiting spurious features while preserving the benefit of enhanced accuracy from fine-tuning.",
        "authors": "X. Wang, H. Zhao, K. Nahrstedt, et.al",
        "keywords": [
            "federated learning",
            "spurious features",
            "personalized models"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=N2wx9UVHkH",
        "pdf_src": "https://api2.openreview.net/pdf/b9d645c4dd974037b424465005b278014538b856.pdf",
        "Code_src": "",
        "Introduction": "Background: Personalized federated learning aims to tailor machine learning models according to specific clients' data characteristics within a distributed setting without sharing sensitive information across different parties.\n\nResearch Problem: Despite addressing statistical heterogeneity through per-client fine-tuning, existing personalization strategies can inadvertently amplify spurious features observed only locally but correlated with labels due to peculiarities or noise in individual datasets which could degrade overall generalization when applied globally as part of a federated system.\n\nMethodology: The paper introduces an analysis framework considering scenarios wherein these spurious features exist solely because they're linked to the class labels under certain conditions; however, their relevance wanes if considered against broader contexts encompassing various training domains. To tackle this issue effectively beyond mere feature masking techniques commonly used before deployment time, it proposes a method that maintains adversarial transferability – ensuring both global and localized versions remain robustly connected despite variations among them - thus mitigating any adverse effects introduced into personalized models via local fine-tuning steps.\n \nMain Contributions: The main contribution lies not just in identifying potential pitfalls related to using purely local fine-tuned models directly after aggregation back into a global federated one contextually unawareness about potentially harmful spurious correlations might arise), but also introducing a new adaptive mechanism designed specifically around avoiding those risks whilst still benefiting from improved accuracy brought forth by tailored adjustments made close proximity towards actual usage patterns encountered uniquely amongst diverse users participating collectively within federated systems like healthcare records anonymization projects",
        "Topic": "Federated Learning"
    },
    {
        "title": "Layer-diverse Negative Sampling for Graph Neural Networks",
        "abstract": "Graph neural networks (GNNs) are a powerful solution for various structure learning applications due to their strong representation capabilities for graph data. However, traditional GNNs, relying on message-passing mechanisms that gather information exclusively from first-order neighbours (known as positive samples), can lead to issues such as over-smoothing and over-squashing.\nTo mitigate these issues, we propose a layer-diverse negative sampling method for message-passing propagation. This method employs a sampling matrix within a determinantal point process, which transforms the candidate set into a space and selectively samples from this space to generate negative samples. To further enhance the diversity of the negative samples during each forward pass, we develop a space-squeezing method to achieve layer-wise diversity in multi-layer GNNs. Experiments on various real-world graph datasets demonstrate the effectiveness of our approach in improving the diversity of negative samples and overall learning performance. Moreover, adding negative samples dynamically changes the graph's topology, thus with the strong potential to improve the expressiveness of GNNs and reduce the risk of over-squashing.",
        "authors": "W. Duan, J. Lu, Y. G. Wang, et.al",
        "keywords": [
            "layer-diverse negative sampling",
            "determinantal point process",
            "space-squeezing"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=WOrdoKbxh6",
        "pdf_src": "https://api2.openreview.net/pdf/341960ed2ecf92e0b36a118e2ab7b83e33e560a8.pdf",
        "Code_src": "",
        "Introduction": "Background: Graph neural networks (GNNs) have been widely used for structure learning tasks because they effectively represent graph data through message passing mechanism between nodes. However, existing GNNs often suffer from problems like over-smoothing or over-squashing when only considering nearest neighbors.\n\nResearch Problem: The research problem addressed by this paper is how to overcome limitations caused by using solely local neighborhood information while training GNNs so it could learn more diverse features without losing discriminative power among nodes.\n\nMethod: In order to address above mentioned issue, authors introduce a novel Negative Sampling strategy based on Determinantal Point Processes (DPP). They construct a sampling matrix inside DPP framework allowing them to transform all possible node pairs into probability distribution where some negative samples will be sampled according to certain probabilities derived from learned parameters rather than uniformly at random across entire adjacency matrix. Additionally proposed Space-Squeezing technique helps maintain diversity even after multiple layers' aggregations enhancing feature representations extracted throughout network architecture.\n\nMain Contributions:\n1. A Negative Sampling scheme utilizing Determinantal Point Process (DPP) has been developed specifically designed towards mitigating biases present in standard GNN approaches focusing too heavily on immediate neighbors.\n2. An innovative Space-Squeezing algorithm was introduced aiming toward preserving diversity amongst learned embeddings across several layers within Multi-Layer Perceptrons (MLPs).\n3. Experimental validation conducted demonstrates improved diversity along with better generalization capability achieved via incorporating negative samples dynamically altering topologies compared against baseline models not employing any form of negative sampling techniques leading up to significant improvements observed particularly noticeable especially under complex graphs scenarios involving dense interconnections",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Improving and generalizing flow-based generative models with minibatch optimal transport",
        "abstract": "Continuous normalizing flows (CNFs) are an attractive generative modeling technique, but they have been held back by limitations in their simulation-based maximum likelihood training. We introduce the generalized conditional flow matching (CFM) technique, a family of simulation-free training objectives for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. In contrast to both diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. A variant of our objective is optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference, as evaluated in our experiments. Furthermore, we show that when the true OT plan is available, our OT-CFM method approximates dynamic OT. Training CNFs with CFM improves results on a variety of conditional and unconditional generation tasks, such as inferring single cell dynamics, unsupervised image translation, and Schrödinger bridge inference. The Python code is available at https://github.com/atong01/conditional-flow-matching.",
        "authors": "A. Tong, K. Fatras, N. Malkin, et.al",
        "keywords": [
            "CFM",
            "Conditional Flow Matching",
            "Optimal Transport"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=CD9Snc73AW",
        "pdf_src": "https://api2.openreview.net/pdf/dc6be529e2849f1257ce18efde8e9a5ca78f4438.pdf",
        "Code_src": "https://github.com/atong01/conditional-flow-matching",
        "Introduction": "Background: Continuous Normalizing Flows (CNFs) are a popular generative modeling approach due to their flexibility; however, traditional methods struggle during maximum likelihood estimation because these simulations can become unstable.\n\nResearch Question: How do we develop new training objectives without relying on simulations?\n\nMethod: This paper introduces Conditional Flow Matching (CFM), including Generalized Conditional Flow Matching (GCFM). GCFM uses a regression loss similar to those found in diffusion models yet retains the efficiency benefits of deterministic flow models – it doesn't need to evaluate the source distribution's probability density nor assume it follows a Gaussian distribution. Additionally, Optimal Transport Conditional Flow Matching (OT-CFM) was developed based on this framework using optimal transport theory rather than gradient descent optimization techniques commonly employed previously within the field.\n \nMain Contributions:\n1. Introduced a novel class of training objectives called Conditional Flow Matching designed specifically for continuous normalizing flows - GCFM & OT-CFM variants;\n2. Demonstrated improved stability compared to existing approaches through empirical evidence from various experimental setups involving conditional and unconditional generation tasks across different domains (e.g., inferring single-cell dynamics);\n3. Provided open-source Python code accessible via GitHub repository allowing others to replicate findings easily.",
        "Topic": "Optimal Transport"
    },
    {
        "title": "A Pseudo-Metric between Probability Distributions based on Depth-Trimmed Regions",
        "abstract": "The design of a metric between probability distributions is a longstanding problem motivated by numerous applications in machine learning. Focusing on continuous probability distributions in the Euclidean space $\\mathbb{R}^d$, we introduce a novel pseudo-metric between probability distributions by leveraging the extension of univariate quantiles to multivariate spaces. Data depth is a nonparametric statistical tool that measures the centrality of any element $x\\in\\mathbb{R}^d$ with respect to (w.r.t.) a probability distribution or a dataset. It is a natural median-oriented extension of the cumulative distribution function (cdf) to the multivariate case. Thus, its upper-level sets---the depth-trimmed regions---give rise to a definition of multivariate quantiles. The new pseudo-metric relies on the average of the  Hausdorff distance between the depth-based quantile regions for each distribution. Its good behavior regarding major transformation groups, as well as its ability to factor out translations, are depicted. Robustness, an appealing feature of this pseudo-metric, is studied through the finite sample breakdown point. Moreover, we propose an efficient approximation method with linear time complexity w.r.t. the size of the dataset and its dimension. The quality of this approximation and the performance of the proposed approach are illustrated in numerical experiments.",
        "authors": "G. Staerman, P. Mozharovskyi, P. Colombo, et.al",
        "keywords": [
            "distance",
            "data depth",
            "robustness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=QySD5r7PeE",
        "pdf_src": "https://api2.openreview.net/pdf/8badadb074f689382ef94be6e5967e6f53b1ea51.pdf",
        "Code_src": "",
        "Introduction": "Background: Designing metrics between probability distributions has been crucial due to various applications such as clustering algorithms.\nResearch Problem: This paper focuses on developing a novel pseudo-metric specifically designed for continuous probability distributions in $\\mathbb{R}^d$. \nMethod: We utilize data depth which extends univariate quantiles into multivariate spaces; it's defined based on the centrality measure within a given distribution or dataset. By averaging the Hausdorff distances among these quantile regions across different distributions, our pseudo-metric can be calculated efficiently while considering translation invariance properties.\n\nMain Contributions:\n1. Introduced a robust pseudo-metric using data depth conceptually extending from cdf in univariate to multivariate scenarios;\n2. Demonstrated the metric’s favorable characteristics against transformations like scaling and rotation along with translation invariance;\n3. Analyzed the metric's robustness via finite-sample breakdown points indicating resistance towards outliers;\n4. Proposed an algorithmic solution approximating the pseudo-metric at linear time complexity relative to dataset dimensions ensuring computational efficiency;\n5. Provided empirical evidence supporting both the accuracy of the approximation and effectiveness of their methodology",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "World Models via Policy-Guided Trajectory Diffusion",
        "abstract": "World models are a powerful tool for developing intelligent agents. By predicting the outcome of a sequence of actions, world models enable policies to be optimised via on-policy reinforcement learning (RL) using synthetic data, i.e. in “in imagination”. Existing world models are autoregressive in that they interleave predicting the next state with sampling the next action from the policy. Prediction error inevitably compounds as the trajectory length grows. In this work, we propose a novel world modelling approach that is not autoregressive and generates entire on-policy trajectories in a single pass through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion (PolyGRAD), leverages a denoising model in addition to the gradient of the action distribution of the policy to diffuse a trajectory of initially random states and actions into an on-policy synthetic trajectory. We analyse the connections between PolyGRAD, score-based generative models, and classifier-guided diffusion models. Our results demonstrate that PolyGRAD outperforms state-of-the-art baselines in terms of trajectory prediction error for short trajectories, with the exception of autoregressive diffusion. For short trajectories, PolyGRAD obtains similar errors to autoregressive diffusion, but with lower computational requirements. For long trajectories, PolyGRAD obtains comparable performance to baselines. Our experiments demonstrate that PolyGRAD enables performant policies to be trained via on-policy RL in imagination for MuJoCo continuous control domains. Thus, PolyGRAD introduces a new paradigm for accurate on-policy world modelling without autoregressive sampling.",
        "authors": "M. Rigter, J. Yamada, I. Posner",
        "keywords": [
            "Policy-Guided Trajectory Diffusion",
            "World Modelling",
            "On-Policy Reinforcement Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=9CcgO0LhKG",
        "pdf_src": "https://api2.openreview.net/pdf/563bed054799f1388e23235a3d655df0c3c34624.pdf",
        "Code_src": "",
        "Introduction": "Background: World models have been shown to be useful tools for training intelligent agents by predicting future outcomes based on sequences of actions.\n\nResearch Problem: The existing world models used for this purpose were found to suffer from prediction error when dealing with longer trajectories due to their autoregressive nature which interleaves predictions about the next state while also sampling the next action according to some policy.\n\nMethod: This paper proposes a novel non-autoregressive method called Policy-Guided Trajectory Diffusion (PolyGRAD). It uses a diffusion process guided both by gradients of the action distribution learned during policy optimization along with a denoising model within one pass over initial randomly generated states and actions leading them towards on-policy synthetic trajectories.\n\nMain Contributions:\n1. Introduced a new paradigm for accurate on-policy world modeling.\n2. Developed PolyGRAD - a non-autoregressive diffusion model capable of generating full on-policy trajectories directly rather than sequentially like previous methods do resulting in less prediction error especially noticeable at shorter trajectory lengths compared to baseline approaches including autoregressive diffusion itself despite having higher computational costs there; \n3. Demonstrated its effectiveness across various MuJoCo continuous control domains allowing efficient training of performing policies utilizing on-policy reinforcement learning techniques even though it may require more computation resources depending upon task complexity or domain specifics involved therein",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Adapting Contrastive Language-Image Pretrained (CLIP) Models for Out-of-Distribution Detection",
        "abstract": "We present a comprehensive experimental study on pre-trained feature extractors for visual out-of-distribution (OOD) detection, focusing on leveraging contrastive language-image pre-trained (CLIP) models. Without fine-tuning on the training data, we are able to establish a positive correlation ($R^2\\geq0.92$) between in-distribution classification and unsupervised OOD detection for CLIP models in $4$ benchmarks. We further propose a new simple and scalable method called \\textit{pseudo-label probing} (PLP) that adapts vision-language models for OOD detection. Given a set of label names of the training set, PLP trains a linear layer using the pseudo-labels derived from the text encoder of CLIP. Intriguingly, we show that without modifying the weights of CLIP or training additional image/text encoders (i) PLP outperforms the previous state-of-the-art on all $5$ large-scale benchmarks based on ImageNet, specifically by an average AUROC gain of 3.4\\% using the largest CLIP model (ViT-G), (ii) linear probing outperforms fine-tuning by large margins for CLIP architectures (i.e. CLIP ViT-H achieves a mean gain of 7.3\\% AUROC on average on all ImageNet-based benchmarks), and (iii) billion-parameter CLIP models still fail at detecting feature-based adversarially manipulated OOD images. The code is available at https://github.com/HHU-MMBS/plp-official-tmlr2024.",
        "authors": "N. Adaloglou, F. Michels, T. Kaiser, et.al",
        "keywords": [
            "contrastive language-image pre-trained (CLIP)",
            "out-of-distribution (OOD) detection",
            "pseudo-label probing (PLP)"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=YCgX7sJRF1",
        "pdf_src": "https://api2.openreview.net/pdf/d6e554b17cd369703bef298f47c77cc9ac0c0d48.pdf",
        "Code_src": "代码链接：https://github.com/HHU-MMBS/plp-official-tmlr2024",
        "Introduction": "Background: This paper focuses on improving the ability of pre-trained feature extractors like Contrastive Language-Image Pre-trained (CLIP) models to detect Out-of-Distribution (OOD) samples.\n\nResearch Problem: How can we effectively leverage pre-trained CLIP models with minimal fine-tuning towards better OOD detection?\n\nMethod: The authors introduce a novel approach they call \"pseudo-label probing\" (PLP). They train a linear classifier over the output features of the CLIP's text encoder while using its predicted labels as pseudo-labels during training instead of actual ground truth labels which require manual annotation effort.\n \nMain Contributions:\n1. Establishes strong correlations between in-distribution performance and OOD detection across four benchmarks when no fine-tuning was applied directly onto the training datasets.\n2. Proposes and demonstrates the effectiveness of their proposed PLP technique against five widely used benchmark datasets including ImageNet variants; it consistently improves upon prior art results such as achieving up to +3.4% AUROC improvement compared to the best performing baseline among these benchmarks - even surpassing those achieved through more resource-intensive methods involving modifications within the original architecture or training separate encoders.\n3. Shows that linear probing significantly outperforms fine-tuning especially notable improvements seen particularly noticeable gains around 7.3% AUROC points per metric point increase observed amongst different versions tested under this setup).\n4. Demonstrates limitations despite having access to highly parameterized models suggesting there remains room for improvement beyond just increasing computational resources alone when dealing with complex tasks requiring robustness toward perturbations introduced into input data sets intentionally designed outside expected distributions encountered throughout normal usage scenarios (e.g., adversarial examples).\n\nThe work presented here provides insights about how existing approaches could be adapted efficiently leading them closer towards meeting challenging goals related to generalization capabilities",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Multi-conditioned Graph Diffusion for Neural Architecture Search",
        "abstract": "Neural architecture search automates the design of neural network architectures usually by exploring a large and thus complex architecture search space. To advance the architecture search, we present a graph diffusion-based NAS approach that uses discrete conditional graph diffusion processes to generate high-performing neural network architectures. We then propose a multi-conditioned classifier-free guidance approach applied to graph diffusion networks to jointly impose constraints such as high accuracy and low hardware latency. Unlike the related work, our method is completely differentiable and requires only a single model training. In our evaluations, we show promising results on six standard benchmarks, yielding novel and unique architectures at a fast speed, i.e. less than 0.2 seconds per architecture. Furthermore, we demonstrate the generalisability and efficiency of our method through experiments on ImageNet dataset.",
        "authors": "R. Asthana, J. Conrad, Y. Dawoud, et.al",
        "keywords": [
            "graph diffusion",
            "NAS",
            "multi-conditioned classifier-free guidance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=5VotySkajV",
        "pdf_src": "https://api2.openreview.net/pdf/c10e5e3d935f4bda98f74b044be3be1624a2e259.pdf",
        "Code_src": "",
        "Introduction": "Background: The field of Neural Architecture Search (NAS) aims to automate the process of designing efficient neural network architectures without human intervention.\n\nResearch Problem: Existing NAS methods often suffer from scalability issues due to their exploration of large and complex architecture spaces which can lead to long computation times for generating new architectures.\n \nMethod: This paper introduces a novel Graph Diffusion-based NAS approach called \"Discrete Conditional Graph Diffusion\" (DCGD). DCGD generates high-performance neural network architectures using discrete conditional graph diffusion processes within an efficiently scalable architecture search space. Additionally, they introduce a Multi-Conditioned Classifier-Free Guidance approach specifically designed for graph diffusion networks allowing them to simultaneously enforce multiple constraints like high accuracy with low hardware latency during the generation phase itself rather than post-processing steps in other approaches.\n\nMain Contributions:\n1. A Differentiable Method - Their proposed method is fully differentiable making it easier to integrate into existing workflows compared to previous non-differentiable techniques requiring separate specialized models or additional approximations when optimizing over the learned parameters.\n2. Single Model Training - They achieve this while also reducing computational overhead significantly because all components are trained together via end-to-end optimization instead of separately iterating between stages where each stage may require its own dedicated training run(s).\n3. Scalable Performance - Evaluations conducted across various benchmarks indicate that not only do these improvements yield better performing architectures but doing so much faster too; under 0.2 seconds per architecture generated versus hours required traditionally before advancements were made possible here!\n4. Generalizability & Efficiency Demonstrated On Large Scale Dataset - Further validation comes after demonstrating both generalization capabilities beyond initial benchmark tests onto larger datasets like ImageNet showing consistent performance gains despite increased complexity levels encountered therein suggesting broader applicability outside specific niche use cases previously targeted solely",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "How does over-squashing affect the power of GNNs?",
        "abstract": "Graph Neural Networks (GNNs) are the state-of-the-art model for machine learning on graph-structured data. The most popular class of GNNs operate by exchanging information between adjacent nodes, and are known as Message Passing Neural Networks (MPNNs). While understanding the expressive power of MPNNs is a key question, existing results typically consider settings with uninformative node features. In this paper, we provide a rigorous analysis to determine which function classes of node features can be learned by an MPNN of a given capacity. We do so by measuring  the level of *pairwise interactions* between nodes that MPNNs allow for. This measure provides a novel quantitative characterization of the so-called over-squashing effect, which is observed to occur when a large volume of messages is aggregated into fixed-size vectors. Using our measure, we prove that, to guarantee sufficient communication between pairs of nodes, the capacity of the MPNN must be large enough, depending on properties of the input graph structure, such as commute times. For many relevant scenarios, our analysis results in impossibility statements in practice, showing that *over-squashing hinders the expressive power of MPNNs*. Our theory also holds for geometric graphs and hence extends to equivariant MPNNs on point clouds. We validate our analysis through extensive controlled experiments and ablation studies.",
        "authors": "F. D. Giovanni, T. K. Rusch, M. M. Bronstein, et.al",
        "keywords": [
            "pairwise interactions",
            "over-squashing effect",
            "expressive power"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=KJRoQvRWNs",
        "pdf_src": "https://api2.openreview.net/pdf/1b8a6ff568550940555fe18b172c92c3fdf192a0.pdf",
        "Code_src": "",
        "Introduction": "Background: Graph Neural Networks (GNNs) have become one of the most effective models for handling graph-structured data due to their ability to learn from relational information encoded within networks.\n\nResearch Question: Despite widespread use, there has been limited theoretical investigation regarding how much complex functionality could actually be learned using these networks; specifically, what types of node feature functions might they support?\n\nMethodology: To address this issue, researchers developed a method based on analyzing pairwise interaction levels among nodes during message passing processes - essentially quantifying \"how well\" neighboring nodes communicate or share information under different network conditions.\n \nMain Contributions:\n1. They introduced a new metric called 'pairwise interactions' – it measures whether two nodes interact effectively while being processed together inside the neural network architecture.\n2. By rigorously proving certain bounds related to this metric against various capacities of MPNNs across diverse graph topologies including those with varying connectivity patterns like commuting distances), authors demonstrated practical limitations imposed by \"over-squashing\" phenomenon where too numerous inputs converge down into smaller spaces without losing essential details about relationships amongst entities involved.\n3. Their findings imply that if you want your MPNN capable enough handle more intricate tasks than simple classification problems then its size should scale proportionally according to specific characteristics present within underlying datasets used training purposes (like number edges/node).\n4. Finally, empirical validation was conducted via comprehensive experimental setups confirming predictions made theoretically hold true even beyond classical Euclidean geometries allowing extension towards equivariant architectures dealing with non-Euclidean structures like point clouds commonly found nowadays in computer vision applications.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks",
        "abstract": "The fields of both Natural Language Processing (NLP) and Automated Machine Learning (AutoML) have achieved remarkable results over the past years. In NLP, especially Large Language Models (LLMs) have experienced a rapid series of breakthroughs very recently. We envision that the two fields can radically push the boundaries of each other through tight integration. To showcase this vision, we explore the potential of a symbiotic relationship between AutoML and LLMs, shedding light on how they can benefit each other. In particular, we investigate both the opportunities to enhance AutoML approaches with LLMs from different perspectives and the challenges of leveraging AutoML to further improve LLMs. To this end, we survey existing work, and we critically assess risks. We strongly believe that the integration of the two fields has the potential to disrupt both fields, NLP and AutoML. By highlighting conceivable synergies, but also risks, we aim to foster further exploration at the intersection of AutoML and LLMs.",
        "authors": "A. Tornede, D. Deng, T. Eimer, et.al",
        "keywords": [
            "AutoML",
            "Large Language Models",
            "Synergistic Integration"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=cAthubStyG",
        "pdf_src": "https://api2.openreview.net/pdf/35e2b1984e3b9437603b4bf2ce44965752a7d304.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses recent advancements in both Natural Language Processing (NLP), particularly focusing on large language models (LLMs), as well as Automated Machine Learning (AutoML). It suggests an untapped potential for synergy when integrating these two domains.\n\nResearch Question: How might the combination of AutoML techniques could be leveraged within LLM development? Conversely, what are the benefits if one were to apply insights gained from LLM research into AutoML?\n\nMethods: The authors conduct a comprehensive literature review examining current works related to the interplay between AutoML and LLMs while identifying key challenges faced by researchers working along those lines.\n \nMain Contributions: This study provides valuable insight regarding the mutual enhancement possibilities offered by combining AutoML methodologies alongside LLM technologies; it identifies critical issues hindering progress toward such integrations – thereby contributing towards future developments where both areas may mutually inform & advance their respective capabilities beyond standalone achievements thus far realized individually.",
        "Topic": "Large Language Models"
    },
    {
        "title": "Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models",
        "abstract": "As general purpose vision models get increasingly effective at a wide set of tasks, it is imperative that they be consistent across the tasks they support. Inconsistent AI models are considered brittle and untrustworthy by human users and are more challenging to incorporate into larger systems that take dependencies on their outputs. Measuring consistency between very heterogeneous tasks that might include outputs in different modalities is challenging since it is difficult to determine if the predictions are consistent with one another. As a solution, we introduce a benchmark dataset, CocoCON, where we create contrast sets by modifying test instances for multiple tasks in small but semantically meaningful ways to change the gold label and outline metrics for measuring if a model is consistent by ranking the original and perturbed instances across tasks. We find that state-of-the-art vision-language models suffer from a surprisingly high degree of inconsistent behavior across tasks, especially for more heterogeneous tasks. To alleviate this issue, we propose a rank correlation-based auxiliary training objective, computed over large automatically created cross-task contrast sets, that improves the multi-task consistency of large unified models while retaining their original accuracy on downstream tasks.",
        "authors": "A. Maharana, A. Kamath, C. Clark, et.al",
        "keywords": [
            "CocoCON",
            "Consistency Benchmarking",
            "Rank Correlation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ue9igTDLN2",
        "pdf_src": "https://api2.openreview.net/pdf/666fc7209bb34f615daf9b97315c13dd5cc9f41b.pdf",
        "Code_src": "",
        "Introduction": "Background: With advancements in general-purpose vision models' effectiveness broadly applicable to various tasks, ensuring these models maintain consistency becomes crucial due to concerns about trustworthiness among humans using them as well as integration challenges within broader systems.\n\nResearch Problem: The challenge lies in assessing consistency amidst highly varied tasks which may yield results through diverse modalities; determining whether predictions align consistently can prove elusive given task heterogeneity.\n \nMethodology: Addressing this problem, researchers have introduced CocoCON—a benchmark dataset—whereby contrast sets were generated via minor yet semantically impactful alterations made to test examples under several tasks simultaneously so as to alter ground truth labels without altering visual content significantly enough not to affect recognition performance. Metrics designed measure consistency involve comparing rankings obtained when considering both original and perturbed images across all involved tasks.\n\nMain Contributions:\n1. CocoCON Dataset Creation: A novel approach was taken toward creating an extensive dataset capable of evaluating consistency across numerous tasks involving distinct modalities such as text or audio alongside image data itself.\n2. Consistency Measurement Framework: Developed methodologies enable quantifying inconsistency levels based upon how ranked lists differ before versus after introducing perturbations intended solely towards changing predicted class assignments rather than overall classification quality degradation.\n3. Rank Correlation-Based Training Objective Proposal: An innovative supplementary loss function utilizing rank correlations has been proposed specifically tailored around contrasting datasets spanning many interconnected tasks aimed at improving multiclass prediction coherence whilst preserving existing accuracies achieved during specialized downstream applications.",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Attending to Graph Transformers",
        "abstract": "Recently, transformer architectures for graphs emerged as an alternative to established techniques for machine learning with graphs, such as (message-passing) graph neural networks. So far, they have shown promising empirical results, e.g., on molecular prediction datasets, often attributed to their ability to circumvent graph neural networks’ shortcomings, such as over-smoothing and over-squashing. Here, we derive a taxonomy of graph transformer architectures, bringing some order to this emerging field. We overview their theoretical properties, survey structural and positional encodings, and discuss extensions for important graph classes, e.g., 3D molecular graphs. Empirically, we probe how well graph transformers can recover various graph properties, how well they can deal with heterophilic graphs, and to what extent they prevent over-squashing. Further, we outline open challenges and research\ndirection to stimulate future work",
        "authors": "L. Müller, M. Galkin, C. Morris, et.al",
        "keywords": [
            "graph transformer",
            "architecture taxonomy",
            "empirical evaluation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=HhbqHBBrfZ",
        "pdf_src": "https://api2.openreview.net/pdf/f4fa6a28bf727d4f0ba8992d4e2c2592f91ddf62.pdf",
        "Code_src": "",
        "Introduction": "Background: Traditional methods like message-passing Graph Neural Networks are widely used in machine learning tasks involving graphs but suffer from limitations including over-smoothing and over-squashing.\nResearch Problem: To address these issues, Transformer-based approaches were proposed which show promise through empirical evidence; however, there is no systematic understanding or categorization available yet.\n\nMethodology:\n1. Taxonomy Creation: The authors develop a comprehensive taxonomy that organizes existing graph transformer architectures into categories based on shared characteristics,\n2. Theory Overview: They provide insights about the theoretical underpinnings behind each architecture type within the taxonomy,\n3. Encoding Techniques: A detailed review focusing on structural and positional encodings utilized by different transformer variants designed specifically for graphs,\n4. Extensions: Discussion around adaptations made when dealing with specific types of graphs – particularly three-dimensional molecular graphs,\n5. Experimental Evaluation: An empirical investigation assessing the performance capabilities across recovering graph properties along with handling heterophilous graphs while mitigating against over-squashing effects.\n\nMain Contributions:\n- A structured framework for organizing graph transformer architectures allowing researchers easier navigation throughout the space \n- Insights regarding theoretical aspects aiding better comprehension of transformer's behavior especially concerning graph data,\n- Identification & analysis of encoding strategies enhancing model interpretability and adaptability towards diverse graph structures,\n- Practical evaluation demonstrating the efficacy beyond traditional GNNs addressing key concerns related to computational efficiency and information loss during processing,\n\nFuture Outlook: Open questions identified leading toward potential avenues where further advancements could be explored",
        "Topic": "Vision Transformer"
    },
    {
        "title": "Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel’s Spectrum",
        "abstract": "Wide neural networks are biased towards learning certain functions, influencing both the rate of convergence of gradient descent (GD) and the functions that are reachable with GD in finite training time. As such, there is a great need for methods that can modify this bias according to the task at hand. To that end, we introduce Modified Spectrum Kernels (MSKs), a novel family of constructed kernels that can be used to approximate kernels with desired eigenvalues for which no closed form is known. We leverage the duality between wide neural networks and Neural Tangent Kernels and propose a preconditioned gradient descent method, which alters the trajectory of GD. As a result, this allows for a polynomial and, in some cases, exponential training speedup without changing the final solution. Our method is both computationally efficient and simple to implement.",
        "authors": "A. Geifman, D. Barzilai, R. Basri, et.al",
        "keywords": [
            "Kernel Approximation",
            "Gradient Descent",
            "Neural Tangent Kernel"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=aD0ExytnEK",
        "pdf_src": "https://api2.openreview.net/pdf/9cccf50b7d8377ff7ef44f68231d9f344088910c.pdf",
        "Code_src": "",
        "Introduction": "Background: Wide neural networks have been shown to learn specific types of functions more easily than others due to their architecture biases.\n\nResearch Question: How do we modify these network biases so they better align with our tasks?\n\nMethod: We introduce Modified Spectrum Kernels (MSKs), a new type of kernel designed to approximate those with desirable eigenvalues but lacking a closed-form expression. By leveraging the relationship between wide neural networks and Neural Tangent Kernels, we develop a preconditioned gradient descent algorithm capable of altering the learning process's trajectory toward faster convergence while maintaining the same optimal function value as standard gradient descent.\n\nMain Contributions:\n1. Developed MSKs - A novel approach to modifying neural network biases.\n2. Proposed a new gradient descent method based on the dual nature of wide neural networks & NTMs – This not only speeds up training significantly under certain conditions but also does it efficiently by requiring minimal changes during implementation compared traditional approaches involving spectral modifications or other complex techniques often needed when dealing with non-standard kernels like ours here).",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models",
        "abstract": "Recent advancements in text-to-image diffusion models have yielded impressive results in generating realistic and diverse images. However, these models still struggle with complex prompts, such as those that involve numeracy and spatial reasoning. This work proposes to enhance prompt understanding capabilities in diffusion models. Our method leverages a pretrained large language model (LLM) for grounded generation in a novel two-stage process. In the first stage, the LLM generates a scene layout that comprises captioned bounding boxes from a given prompt describing the desired image. In the second stage, a novel controller guides an off-the-shelf diffusion model for layout-grounded image generation. Both stages utilize existing pretrained models without additional model parameter optimization. Our method significantly outperforms the base diffusion model and several strong baselines in accurately generating images according to prompts that require various capabilities, doubling the generation accuracy across four tasks on average. Furthermore, our method enables instruction-based multi-round scene specification and can handle prompts in languages not supported by the underlying diffusion model. We anticipate that our method will unleash users' creativity by accurately following more complex prompts. Our code, demo, and benchmark are available at: https://llm-grounded-diffusion.github.io",
        "authors": "L. Lian, B. Li, A. Yala, et.al",
        "keywords": [
            "text-to-image diffusion models",
            "prompt understanding",
            "grounded generation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=hFALpTb4fR",
        "pdf_src": "https://api2.openreview.net/pdf/a3cf2bb6aaeb2b58e4aabbc79889496909c06ffe.pdf",
        "Code_src": "https://llm-grounded-diffusion.github.io",
        "Introduction": "Background:\nThe paper discusses recent progress made in text-to-image diffusion models which generate high-quality and varied images based on textual descriptions but face challenges when dealing with prompts involving numerical or spatial reasoning.\n\nResearch Problem:\nThe problem addressed is how to improve the ability of diffusion models to understand complex prompts effectively so they could produce accurate images corresponding to detailed instructions including numeric and spatial elements.\n\nMethodology:\nTo address this issue, we propose using a two-stage approach where a pre-trained Large Language Model (LLM) aids in grounding the generative task within the diffusion framework:\n\n1. The initial stage involves the LLM creating a scene layout comprised of annotated bounding boxes representing objects mentioned in user's input.\n2. Subsequently, during the second stage, another component – referred to as \"novel controller\" – steers standard diffusion models towards producing images consistent with both the layout provided earlier along with any other details specified through the original prompt.\n\nMain Contributions:\nThis research introduces methods enhancing the interpretability capability of diffusion models regarding complex prompts while maintaining efficiency since it does not necessitate further training beyond utilizing already-pretrained models directly into their workflow - no need for optimizing parameters specifically tailored just one particular dataset like many prior works do before deployment.\n\nFindings:\nOur proposed system has been shown to be superior over baseline approaches; achieving double the generation accuracy typically observed under challenging conditions requiring comprehension skills related to arithmetic operations combined with spatial awareness among others. Moreover, its versatility allows handling prompts written even if they're outside what would normally fall inside typical linguistic proficiency range expected",
        "Topic": "Generative Models"
    },
    {
        "title": "An Improved Federated Clustering Algorithm with Model-based Clustering",
        "abstract": "Federated learning (FL) is a distributed learning paradigm that allows multiple clients to collaboratively train a shared model via communications to a central server. However, optimal models of different clients often differ due to heterogeneity of data across clients. \nIn this paper, we address the dichotomy between heterogeneous models and simultaneous training in FL via a clustering structure among the clients.  The clustering framework is one way to allow for   high heterogeneity level between clients, while clients with similar data can still train a shared model. We define a new clustering framework for FL based on the (optimal) local models of the clients: two clients belong to the same cluster if their local models are close. We propose an  algorithm, \\emph{Successive Refine Federated Clustering Algorithm} (\\texttt{SR-FCA}), that treats each client as a singleton cluster as an initialization, and then successively refine the cluster estimation via exploiting similarity with other clients. In any intermediate step, \\texttt{SR-FCA} uses an {\\em error-tolerant} federated learning algorithm within each cluster to exploit simultaneous training and to correct clustering errors. Unlike some prominent prior works \\texttt{SR-FCA} does not require any  \\emph{good} initialization (or warm start), both in theory and practice. We show that with proper choice of learning rate, \\texttt{SR-FCA} incurs arbitrarily small clustering error. Additionally,  \\texttt{SR-FCA} does not require the knowledge of the number of clusters apriori like some prior works. We  validate the performance of \\texttt{SR-FCA} on real-world FL datasets including FEMNIST and Shakespeare in non-convex problems and show the benefits of \\texttt{SR-FCA} over several baselines.",
        "authors": "H. Vardhan, A. Ghosh, A. Mazumdar",
        "keywords": [
            "FL",
            "Heterogeneous Models",
            "Successive Refine Federated Clustering Algorithm"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=1ZGA5mSkoB",
        "pdf_src": "https://api2.openreview.net/pdf/bbf46a8f82df9e921049969e83fdad8b8db2c1fb.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this research lies in federated learning (FL), which enables collaborative training of a shared model by multiple clients through communication with a central server. However, since there's heterogeneity in data distribution across these clients, they may end up having optimally different models.\n\nResearch Problem:\nThe problem addressed here concerns how to reconcile the inherent heterogeneity present when using FL without sacrificing the efficiency gained from simultaneously training all clients' models together.\n  \nMethodology:\nTo tackle this issue, researchers introduce a novel clustering approach into FL frameworks where clients form groups according to similarities rather than being grouped uniformly or randomly around a central point. They develop a clustering method called Successive Refine Federated Clustering Algorithm (SR-FCA). SR-FCA starts off by treating every client individually but iteratively refines its estimates about what constitutes a 'similar' group until convergence occurs - it leverages similarity metrics derived directly from the locally trained models at each node. Within each cluster identified during refinement steps, an error-tolerant federated learning algorithm is employed allowing for concurrent updates despite potential misclassification issues arising out of initial approximations made early-on before further refinements occur.\n\nMain Contributions:\nThis work makes three main contributions towards solving the aforementioned challenge:\n\n1. It introduces a novel clustering strategy specifically designed keeping in mind the unique characteristics of federated learning environments; unlike previous methods requiring good initializations or knowing beforehand approximately how many clusters exist, SR-FCA operates regardless whether such information exists or needs guessing upfront.\n\n2. By employing an iterative process along with an adaptive learning mechanism controlled by an appropriate selection of parameters (learning rates), SR-FCA ensures arbitrary small clustering errors converge toward zero under certain conditions – thus providing guarantees regarding quality improvements achievable even after repeated iterations.\n\n3. Finally, empirical validation conducted against various real-world datasets demonstrates superior performance compared traditional baseline approaches suggesting practical applicability beyond theoretical considerations alone demonstrating tangible advantages offered by adopting this proposed methodology instead conventional ones used previously",
        "Topic": "Federated Learning"
    },
    {
        "title": "Demographically-Informed Prediction Discrepancy Index: Early Warnings of Demographic Biases for Unlabeled Populations",
        "abstract": "An ever-growing body of work has shown that machine learning systems can be systematically biased against certain sub-populations defined by attributes like race or gender. Data imbalance and under-representation of certain populations in the training datasets have been identified as potential causes behind this phenomenon. However, understanding whether data imbalance with respect to a specific demographic group may result in biases for a given task and model class is not simple. An approach to answering this question is to perform controlled experiments, where several models are trained with different imbalance ratios and then their performance is evaluated on the target population. However, in the absence of ground-truth annotations at deployment for an unseen population, most fairness metrics cannot be computed. In this work, we explore an alternative method to study potential bias issues based on the output discrepancy of pools of models trained on different demographic groups. Models within a pool are otherwise identical in terms of architecture, hyper-parameters, and training scheme. Our hypothesis is that the output consistency between models may serve as a proxy to anticipate biases concerning demographic groups. In other words, if models tailored to different demographic groups produce inconsistent predictions, then biases are more prone to appear in the task under analysis. We formulate the Demographically-Informed Prediction Discrepancy Index (DIPDI) and validate our hypothesis in numerical experiments using both synthetic and real-world datasets. Our work sheds light on the relationship between model output discrepancy and demographic biases and provides a means to anticipate potential bias issues in the absence of ground-truth annotations. Indeed, we show how DIPDI could provide early warnings about potential demographic biases when deploying machine learning models on new and unlabeled populations that exhibit demographic shifts.",
        "authors": "L. Mansilla, E. Claucich, R. Echeveste, et.al",
        "keywords": [
            "demographic bias",
            "data imbalance",
            "prediction discrepancy index"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=8W6IDyFZgC",
        "pdf_src": "https://api2.openreview.net/pdf/c3c640afaa8247c3b3c500b12d0decc3377dd90d.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses concerns regarding systematic bias present in machine learning systems towards particular demographics such as race or gender.\n\nResearch Question: It investigates into determining whether imbalanced representation from a demographic perspective leads to bias specifically targeted toward tasks and model classes used during evaluation.\n\nMethodology: Instead of relying solely on controlled experiments which require access to true labels across all demographics due to limitations related to practicality – they propose studying discrepancies among multiple models' outputs after being trained separately upon various demographic subsets without needing actual truth values post-deployment time.\n\nMain Contributions:\n1. They introduce \"Demographically-Informed Prediction Discrepancy Index\" (DIPDI), serving as a novel metric capable predicting potential biases through analyzing inconsistencies amongst pooled predictive outcomes.\n2. Validate their proposed index via empirical tests conducted over synthetic & real-world datasets demonstrating its effectiveness identifying patterns indicative of underlying discriminatory tendencies despite lack of explicit demographic labeling information available once deployed onto fresh populations undergoing changes demographically speaking.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Series of Hessian-Vector Products for Tractable Saddle-Free Newton Optimisation of Neural Networks",
        "abstract": "Despite their popularity in the field of continuous optimisation, second-order quasi-Newton methods are challenging to apply in machine learning, as the Hessian matrix is intractably large. This computational burden is exacerbated by the need to address non-convexity, for instance by modifying the Hessian's eigenvalues as in Saddle-Free Newton methods. We propose an optimisation algorithm which addresses both of these concerns – to our knowledge, the first efficiently-scalable optimisation algorithm to asymptotically use the exact inverse Hessian with absolute-value eigenvalues. Our method frames the problem as a series which principally square-roots and inverts the squared Hessian, then uses it to precondition a gradient vector, all without explicitly computing or eigendecomposing the Hessian. A truncation of this infinite series provides a new optimisation algorithm which is scalable and comparable to other first- and second-order optimisation methods in both runtime and optimisation performance. We demonstrate this in a variety of settings, including a ResNet-18 trained on CIFAR-10.",
        "authors": "E. T. Oldewage, R. M. Clarke, J. M. Hernández-lobato",
        "keywords": [
            "Hessian approximation",
            "Scalable optimization",
            "Absolute value eigenvalues"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=qBZeQBEDIW",
        "pdf_src": "https://api2.openreview.net/pdf/d1b7bb20a106d5e9a94ace29f3baaf1c8ee43c90.pdf",
        "Code_src": "",
        "Introduction": "Background: Despite being popular in continuous optimization due to its efficiency at finding local minima quickly using only gradients information, second-order quasi-Newton methods have not been widely applied in machine learning because they require computation of the Hessian matrix that can be prohibitively expensive when dealing with high-dimensional datasets.\n\nResearch Problem: How do we adapt efficient second-order quasi-Newton algorithms like those based on the BFGS update formulae used commonly in machine learning applications while avoiding the prohibitive cost associated with calculating the full Hessian?\n\nMethod: The authors introduce a novel approach called \"SquareRoot-Free Quasi-Newton\" (SRFQN), designed specifically around the challenges posed above:\n\n1. They reformulate the problem such that instead of directly computing the Hessian, one computes the square root of the Hessian.\n2. By squaring roots and inverses of squares of the Hessian approximation, they construct a sequence leading towards the exact inverse Hessian but avoid explicit calculation through a truncated infinite series representation allowing for scalability across different dimensions.\n3. Finally, they use this approximate inverse Hessian to precondition the gradient vector before updating step size during optimization iterations - effectively approximating the effect of a more accurate Hessian.\n\nMain Contributions:\n1. The paper introduces what appears to be the first scalable second-order optimization algorithm capable of asymptotic accuracy regarding the exact inverse Hessian within acceptable bounds even though no actual inversion occurs.\n2. It demonstrates how to leverage the benefits of second-order optimization techniques despite the computational complexity issues typically encountered; thus improving convergence rates over standard first-order methods where applicable.\n3. The proposed algorithm has been tested against existing state-of-the-art optimization approaches showing competitive results under various conditions—specifically demonstrated here via training ResNet-18 on CIFAR-10 dataset—a common benchmark task demonstrating practical applicability beyond theory alone.",
        "Topic": "approximation"
    },
    {
        "title": "Discovering Model Structure of Dynamical Systems with Combinatorial Bayesian Optimization",
        "abstract": "Deciding on a model structure is a fundamental problem in machine learning. In this paper we consider the problem of building a data-based model for dynamical systems from a library of discrete components. In addition to optimizing performance, we consider crash and inequality constraints that arise from additional requirements, such as real-time capability and model complexity. We address this task of model structure selection with a focus on dynamical systems and propose to search over potential model structures efficiently using a constrained combinatorial Bayesian Optimization (BO) algorithm. We propose expressive surrogate models suited for combinatorial domains and an acquisition function that can handle inequality and crash constraints. We provide simulated benchmark problems within the domain of equation discovery of nonlinear dynamical systems. Our method outperforms the state-of-the-art in constrained combinatorial optimization of black-box functions and has a favorable computational overhead compared to other BO methods. As a real-world application example, we apply our method to optimize the configuration of an electric vehicle's digital twin while ensuring its real-time capability for the use in one of the world's largest driving simulators.",
        "authors": "L. Rath, A. V. Rohr, A. Schultze, et.al",
        "keywords": [
            "model structure",
            "constrained combinatorial Bayesian Optimization",
            "dynamic system modeling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=2iOOvQmJBK",
        "pdf_src": "https://api2.openreview.net/pdf/8006bb71231ae3b5787d37efa30a6d6e4f2947c1.pdf",
        "Code_src": "",
        "Introduction": "Background: The choice of model architecture plays a crucial role in machine learning tasks.\n\nResearch Problem: This study focuses on constructing a dynamic system model based on a collection of discrete elements by considering not only optimal performance but also crash and inequality constraints resulting from specific demands like real-time processing capabilities or model simplicity.\n\nMethods: To tackle the challenge effectively when dealing with dynamic systems, they introduce a constrained combinatorial Bayesian Optimization (BO) approach which searches through possible architectures more efficiently than traditional methods do; they develop specialized surrogate models suitable for combinatorial scenarios along with an adaptive acquisition strategy capable of managing both equality and inequality limitations imposed during optimization processes.\n \nMain Contributions:\n1. They have developed new algorithms specifically designed towards solving constrained combinatorial optimization issues encountered frequently across various fields including those involving complex dynamical systems where multiple conflicting objectives must be balanced simultaneously;\n2. Their proposed framework significantly improves upon existing techniques available today due primarily because it incorporates advanced features allowing better handling around computationally expensive computations associated with evaluating candidate solutions under consideration – thus reducing overall time taken per iteration considerably without compromising accuracy levels achieved throughout iterations themselves;\n\n3. Finally demonstrated effectiveness via empirical evidence obtained against several benchmarks related equations discovered within nonlinear dynamical systems before applying their findings practically into improving configurations pertaining EVs' digital twins whilst maintaining responsiveness needed operational environments found inside large-scale simulation setups worldwide",
        "Topic": "Generative Models"
    },
    {
        "title": "Fast Training of Diffusion Models with Masked Transformers",
        "abstract": "We propose an efficient approach to train large diffusion models with masked transformers.\nWhile masked transformers have been extensively explored for representation learning, their application to generative learning is less explored in the vision domain. Our work is the first to exploit masked training to reduce the training cost of diffusion models significantly. Specifically, we randomly mask out a high proportion (e.g., 50\\%) of patches in diffused input images during training. For masked training, we introduce an asymmetric encoder-decoder architecture consisting of a transformer encoder that operates only on unmasked patches and a lightweight transformer decoder on full patches. To promote a long-range understanding of full patches, we add an auxiliary task of reconstructing masked patches to the denoising score matching objective that learns the score of unmasked patches. Experiments on  ImageNet-256x256 and ImageNet-512x512 show that our approach achieves competitive and even better generative performance than the state-of-the-art Diffusion Transformer (DiT) model, using only around 30\\% of its original training time. Thus, our method shows a promising way of efficiently training large transformer-based diffusion models without sacrificing the generative performance. Our code is available at https://github.com/Anima-Lab/MaskDiT.",
        "authors": "H. Zheng, W. Nie, A. Vahdat, et.al",
        "keywords": [
            "Masked Training",
            "Diffusion Models",
            "Generative Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=vTBjBtGioE",
        "pdf_src": "https://api2.openreview.net/pdf/c33d819ac9f9c2d55c64d521f3858e53cdb1e724.pdf",
        "Code_src": "https://github.com/Anima-Lab/MaskDiT",
        "Introduction": "Background: The paper addresses the challenge of reducing the computational costs associated with training large diffusion models based on masked transformers.\n\nResearch Question: How can one effectively utilize masked training techniques within the context of vision tasks such as image generation?\n\nMethodology: The authors develop MaskDiT, which employs a novel asymmetric encoder-decoder architecture where:\n\n1. A transformer encoder processes only the visible (unmasked) parts of the input images,\n2. A lightweight transformer decoder handles both masked and unmasked regions by utilizing residual connections between them,\n\nto enable cross-patch information flow from the encoder to the decoder through the unmasked paths while still allowing the decoder to directly process all patches due to the residual connection.\n\nMain Contributions:\n1. They are among the first researchers to apply masked training strategies successfully towards generating visual content—previously this technique was primarily used solely for representation learning rather than generative modeling—a significant advancement considering the substantial reduction it offers over previous methods like DiT.\n2. By introducing an auxiliary reconstruction task focusing on predicting masked areas given the visible ones alongside the standard denoising score matching loss function employed typically when training diffusion models—the proposed system enhances the decoder's ability to understand relationships across distant pixels or 'patches' in the generated images.\n3. Experimental validation demonstrates that MaskDiT not just reduces training times but also maintains—or sometimes exceeds—the quality metrics compared against existing benchmarks including the widely recognized Diffusion Transformer (DiT), thus providing evidence supporting the efficiency and effectiveness of the new approach presented here.\n4. Lastly, they make open-source contributions accessible via GitHub repository, facilitating reproducibility along with wider adoption amongst other research communities interested in diffusion models applied broadly beyond purely representational contexts into generative applications related specifically to computer vision fields involving imagery data processing needs.",
        "Topic": "Generative Models"
    },
    {
        "title": "Functional Linear Regression of Cumulative Distribution Functions",
        "abstract": "The estimation of cumulative distribution functions (CDF) is an important learning task with a great variety of downstream applications, such as risk assessments in predictions and decision making. In this paper, we study functional regression of contextual CDF{}s where each data point is sampled from a linear combination of context dependent CDF basis functions. We propose functional ridge-regression-based estimation methods that estimate CDF{}s accurately everywhere. In particular, given $n$ samples with $d$ basis functions, we show estimation error upper bounds of $\\widetilde O(\\sqrt{d/n})$ for fixed design, random design, and adversarial context cases. We also derive matching information theoretic lower bounds, establishing minimax optimality for CDF functional regression. \nFurthermore, we remove the burn-in time in the random design setting using an alternative penalized estimator. Then, we consider agnostic settings where there is a mismatch in the data generation process. We characterize the error of the proposed estimators in terms of the mismatched error, and show that the estimators are well-behaved under model mismatch. \nMoreover, to complete our study, we formalize infinite dimensional models where the parameter space is an infinite dimensional Hilbert space, and establish a self-normalized estimation error upper bound for this setting. Notably, the upper bound reduces to the $\\widetilde O(\\sqrt{d/n})$ bound when the parameter space is constrained to be $d$-dimensional. \nOur comprehensive numerical experiments validate the efficacy of our estimation methods in both synthetic and practical settings.",
        "authors": "Q. Zhang, A. Makur, K. Azizzadenesheli",
        "keywords": [
            "functional regression",
            "cumulative distribution function",
            "estimation error"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ZOqJCP4eMk",
        "pdf_src": "https://api2.openreview.net/pdf/34f138ecdbd9fcbfe1973297d37cab5ebcf367a9.pdf",
        "Code_src": "",
        "Introduction": "Background: Cumulative Distribution Function (CDF) estimation plays a crucial role in various fields like prediction risks or decision-making processes.\n\nResearch Problem: The problem addressed here involves estimating contextual CDFs based on linear combinations of context-dependent basis functions per data point.\n  \nMethods: The authors introduce functional ridge-regression-based estimation techniques which can provide accurate estimates across all points regardless of whether they're obtained through fixed designs, random sampling schemes without prior knowledge about their structure (\"random design\"), or even if faced with adversarial contexts. Additionally, by employing a novel penalty approach within the random design framework, it's possible to eliminate any initial \"burn-in\" period typically associated with these types of estimations before convergence occurs reliably. They further extend their analysis into scenarios involving mismatches between observed data distributions and underlying generative mechanisms; they quantify how robustly their estimators perform against discrepancies among different stages involved during data creation - referred to as 'agnostic' setups'. Finally, considering infinite-dimensional parameter spaces represented via infinite-dimensional Hilbert spaces allows them to set forth a self-normalizing estimation error upper bound applicable specifically therein – reducing down to $\\widetilde O(\\sqrt{d/n})$ complexity once parameters are restricted back onto finite dimensions.\n\nMain Contributions:\n1. Developed new functional ridge-regression approaches yielding precise CDF estimates universally valid irrespective of sample origin or conditions;\n2. Provided tight theoretical guarantees regarding estimation errors under diverse circumstances including those encountered due to adversarial inputs or mismatches throughout data production pipelines;\n3. Introduced modifications enabling efficient computation particularly pertinent while dealing with random-design datasets thereby circumventing issues related to initialization stability commonly experienced early-on during iterative procedures;\n4. Demonstrated effectiveness experimentally validated over synthetic examples alongside real-world case studies confirming reliability beyond mere theory.",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "Using Sum-Product Networks to Assess Uncertainty in Deep Active Learning",
        "abstract": "The success of deep active learning hinges on the choice of an effective acquisition function, which ranks not yet labeled data points according to their expected informativeness. Many acquisition functions are (partly) based on the uncertainty that the current model has about the class label of a point, yet there is no generally agreed upon strategy for computing such uncertainty. \nThis paper proposes a new and very simple approach to computing uncertainty in deep active learning with a Convolutional Neural Network (CNN). The main idea is to use the feature representation extracted by the CNN as data for training a Sum-Product Network (SPN). Since SPNs are typically used for estimating the distribution of a dataset, they are well suited to the task of estimating class probabilities that can be used directly by standard acquisition functions such as max entropy and variational ratio. \nThe effectiveness of our method is demonstrated in an experimental study on several standard benchmark datasets for image classification, where we compare it to various state-of-the-art methods for assessing uncertainty in deep active learning.",
        "authors": "M. Khosravani, S. Zilles",
        "keywords": [
            "uncertainty",
            "active learning",
            "convolutional neural network"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Ai9XpjGxjl",
        "pdf_src": "https://api2.openreview.net/pdf/39bf203a33f44a24ecb39017f00b1955c9b6e561.pdf",
        "Code_src": "",
        "Introduction": "Background: The success of deep active learning largely depends on selecting an appropriate acquisition function; this function should rank unlabeled data points effectively so that more informative examples will likely lead to higher accuracy improvements when added into the training set.\n\nResearch Problem: Despite many existing acquisition functions being partially or entirely dependent on predicting uncertainties regarding the labels of unlabeled data points within neural networks like Convolutional Neural Networks (CNNs), these uncertainties have been challenging to compute consistently due to lack of consensus strategies among researchers.\n  \nMethodology: This research introduces a novel straightforward technique aimed at calculating uncertainty during deep active learning using CNNs through leveraging Sum-Product Networks (SPNs). SPNs excel particularly because they're designed specifically around probabilistic reasoning tasks including estimation of probability distributions over datasets – making them ideal candidates here since they could potentially estimate class probabilities suitable for incorporation straightaway via common acquisition approaches such as maximum entropy and variational ratio.\n\nMain Contributions:\n1. A fresh computational framework integrating CNN features together with SPNs' probabilistic modeling capabilities towards solving uncertainty computation challenges in active learning scenarios involving CNNs;\n2. Experimental validation across multiple widely recognized benchmarks demonstrating superior performance compared against other advanced techniques measuring uncertainty under similar conditions - indicating its potential practical application value beyond theoretical contributions alone.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Incorporating Prior Knowledge into Neural Networks through an Implicit Composite Kernel",
        "abstract": "It is challenging to guide neural network (NN) learning with prior knowledge. In contrast, many known properties, such as spatial smoothness or seasonality, are straightforward to model by choosing an appropriate kernel in a Gaussian process (GP). Many deep learning applications could be enhanced by modeling such known properties. For example, convolutional neural networks (CNNs) are frequently used in remote sensing, which is subject to strong seasonal effects. We propose to blend the strengths of NNs and the clear modeling capabilities of GPs by using a composite kernel that combines a kernel implicitly defined by a neural network with a second kernel function chosen to model known properties (e.g., seasonality). We implement this idea by combining a deep network and an efficient mapping function based on either Nystrom approximation or random Fourier features, which we call Implicit Composite Kernel (ICK). We then adopt a sample-then-optimize approach to approximate the full GP posterior distribution. We demonstrate that ICK has superior performance and flexibility on both synthetic and real-world datasets including a remote sensing dataset. The ICK framework can be used to include prior information into neural networks in many applications.",
        "authors": "Z. Jiang, T. Zheng, Y. Liu, et.al",
        "keywords": [
            "neural network",
            "gaussian process",
            "composite kernel"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=HhjSalvWVe",
        "pdf_src": "https://api2.openreview.net/pdf/34b3ebaf3fe5cb96ab89492b0fcc2e9f33637e60.pdf",
        "Code_src": "",
        "Introduction": "Background: Guiding neural network (NN) learning with prior knowledge remains a challenge due to its complexity.\n\nResearch Problem: How to effectively incorporate prior knowledge like spatial smoothness or seasonality for enhancing various deep learning applications?\n\nMethod: Propose a novel composite kernel method called Implicit Composite Kernel (ICK), blending the advantages of NNs and Gaussian Process (GP) models through a hybrid kernel composed of one implicitly learned from a neural network and another explicitly designed to capture specific known properties via a selected kernel function e.g., seasonality.\n\nMain Contributions:\n1. Develop a composite kernel strategy merging the expressiveness of NNs along with the structured modeling capability of GPs.\n2. Introduce two methods - Nystrom approximation-based and Random Fourier Features-based mappings within a deep network architecture – termed Efficient Mapping Functions  (EMFs).\n3. Employ Sample-Then-Optimize algorithmic approach towards approximating the complete Gaussian Process posterior distribution efficiently while leveraging these EMFs.\n4. Demonstrate significantly improved performance over existing techniques across synthetic data sets & practical application scenarios involving remote sensing imagery where CNNs often encounter pronounced seasonal variations leading to substantial improvements compared to standalone CNN approaches alone when utilizing our proposed ICK framework allowing incorporation of rich domain-specific prior knowledge seamlessly during training phase thus boosting predictive accuracy adaptability robustness overall quality outcomes",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Bias/Variance is not the same as Approximation/Estimation",
        "abstract": "We study the relation between two classical results: the bias-variance decomposition, and the approximation-estimation decomposition. Both are important conceptual tools in Machine Learning, helping us describe the nature of model fitting. It is commonly stated that they are “closely related”, or “similar in spirit”. However, sometimes it is said they are equivalent. In fact they are different, but have subtle connections cutting across learning theory, classical statistics, and information geometry, that (very surprisingly) have not been previously observed. We present several results for losses expressible as Bregman divergences: a broad family with a known bias-variance decomposition. Discussion and future directions are presented for more general losses, including the 0/1 classification loss.",
        "authors": "G. Brown, R. Ali",
        "keywords": [
            "bias-variance decomposition",
            "approximation-estimation decomposition",
            "Bregman divergence"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=4TnFbv16hK",
        "pdf_src": "https://api2.openreview.net/pdf/3aa051ed4ccdc577b06df157d09bfcc5781d124e.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper investigates the relationship between two fundamental concepts in machine learning - the bias-variance decomposition and the approximation-estimation decomposition.\n\nResearch Problem:\nDespite being often described as closely related by researchers from various fields such as learning theory, classical statistics, and information geometry, there has never before been an explicit connection made to show whether these decompositions might be equivalent under certain conditions.\n \nMethodology:\nThe authors focus on analyzing this problem through studying specific types of losses expressed using Bregman divergences which can also be seen within the broader context of the bias-variance decomposition framework. They provide evidence showing how both decompositions may indeed relate subtly despite their differences when considering these particular kinds of functions.\n\nMain Contributions:\nThis work presents new insights into understanding where biases arise due to approximations during estimation processes; specifically highlighting relationships among three distinct components – bias variance trade-offs arising out of function approximation errors along with those resulting from data distribution mismatches leading up to overfitting issues faced while training models based upon empirical observations alone without prior knowledge about underlying structures governing them. The findings could potentially lead towards better ways forward toward developing improved algorithms capable handling complex tasks encountered nowadays involving big datasets requiring sophisticated techniques beyond traditional approaches employed so far like regularization methods etc., thereby contributing significantly advancing current state-of-the-art practices currently available today",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "MC Layer Normalization for calibrated uncertainty in Deep Learning",
        "abstract": "Efficiently estimating the uncertainty of neural network predictions has become an increasingly important challenge as machine learning models are adopted for high-stakes industrial applications where shifts in data distribution may occur. Thus, calibrated prediction uncertainty is crucial to determine when to trust a model's output and when to discard them as implausible. We propose a novel deep learning module - MC Layer Normalization - that acts as a drop-in replacement for Layer Normalization blocks and endows a neural network with uncertainty estimation capabilities. Our method is motivated from an approximate Bayesian perspective, but it is simple to deploy with no significant computational overhead thanks to an efficient one-shot approximation of Monte Carlo integration at prediction time. To evaluate the effectiveness of our module, we conduct experiments in two distinct settings. First, we investigate its potential to replace existing methods such as MC-Dropout and Prediction-Time Batch Normalization. Second, we explore its suitability for use cases where such conventional modules are either unsuitable or sub-optimal for certain tasks (as is the case with modules based on Batch Normalization, which is incompatible for instance with transformers). We empirically demonstrate the competitiveness of our module in terms of prediction accuracy and uncertainty calibration on established out-of-distribution image classification benchmarks, as well as its flexibility by applying it on tasks and architectures where previous methods are unsuitable.",
        "authors": "T. Frick, D. Antognini, I. Giurgiu, et.al",
        "keywords": [
            "uncertainty estimation",
            "MC Layer Normalization",
            "Bayesian perspective"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=bG3ICt3E0C",
        "pdf_src": "https://api2.openreview.net/pdf/b507584c0bcaa0fd5d6e6ffdb8015acb04ddbcd9.pdf",
        "Code_src": "",
        "Introduction": "Background: As machine learning models are widely used in critical industries like healthcare due to their ability to process large amounts of complex data efficiently; however, these systems can be vulnerable if they encounter unseen variations within input datasets leading to incorrect decisions being made.\n\nResearch Problem: Uncertainty quantification plays vital roles during decision-making processes especially under conditions involving unknown inputs because it helps us understand how confident should we feel about any given prediction outcome?\n\nMethodology: The authors introduce a new architecture called \"MC Layer Normalization\" designed specifically around this issue without requiring additional computation resources beyond what would normally go into standard layer normalization techniques themselves.\n\nMain Contributions:\n1. They have developed an approach using Monte Carlo Integration technique combined with Layer Normalization block allowing neural networks estimate predictive uncertainties more accurately than traditional approaches.\n2. This proposed framework does not require retraining nor do you need specialized hardware making deployment straightforward across various domains including those employing transformer-based architectures traditionally incompatible with batch normalization variants commonly found elsewhere today!",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "How Much Pre-training Is Enough to Discover a Good Subnetwork?",
        "abstract": "Neural network pruning helps discover efficient, high-performing subnetworks within pre-trained, dense network architectures. More often than not, it involves a three-step process—pre-training, pruning, and re-training—that is computationally expensive, as the dense model must be fully pre-trained. While previous work has revealed through experiments the relationship between the amount of pre-training and the performance of the pruned network, a theoretical characterization of such dependency is still missing. Aiming to mathematically analyze the amount of dense network pre-training needed for a pruned network to perform well, we discover a simple theoretical bound in the number of gradient descent pre-training iterations on a two-layer fully connected network in the NTK regime, beyond which pruning via greedy forward selection \\citep{provable_subnetworks} yields a subnetwork that achieves good training error. Interestingly, this threshold is logarithmically dependent upon the size of the dataset, meaning that experiments with larger datasets require more pre-training for subnetworks obtained via pruning to perform well. Lastly, we empirically validate our theoretical results on multi-layer perceptions and residual-based convolutional networks trained on MNIST, CIFAR, and ImageNet datasets.",
        "authors": "C. R. Wolfe, F. Liao, Q. Wang, et.al",
        "keywords": [
            "pruning",
            "neural network",
            "efficiency"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=UVE7LllpXe",
        "pdf_src": "https://api2.openreview.net/pdf/d46affd8af71059eafcc83d545c188d407f4c31a.pdf",
        "Code_src": "",
        "Introduction": "Background: Neural network pruning aims to find efficient and high-performance subnetworks from large, densely connected neural networks by removing redundant weights without significantly affecting their accuracy.\n\nResearch Problem: Previous studies have shown empirical relationships between the amount of pretraining required before pruning can occur effectively but lacked a theoretical understanding or characterization regarding how much pretraining was necessary.\n \nMethodology: The authors propose an analytical framework based on Neural Tangent Kernel (NTK) theory using a two-layer fully connected network under the assumption that the initial learning dynamics are governed by the NTK during pretraining phase. They derive a simple theoretical bound indicating when pruning would yield a subnetwork achieving acceptable training errors after further refinement.\n\nMain Contributions:\n1. They provide a mathematical justification quantifying exactly what fraction of pretraining data is sufficient enough so that subsequent pruning does not degrade the final performance too drastically;\n2. This quantity depends logarithmically on the size of your dataset - hence bigger datasets need proportionally longer pretraining phases; \n3. Their findings were validated experimentally across different types of neural networks including MLPs & Residual Convolutional Networks trained over various image recognition benchmarks like MNIST, CIFAR10, and ImageNet demonstrating consistency consistent with theoretical predictions.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Pull-back Geometry of Persistent Homology Encodings",
        "abstract": "Persistent homology (PH) is a method for generating topology-inspired representations of data. Empirical studies that investigate the properties of PH, such as its sensitivity to perturbations or ability to detect a feature of interest, commonly rely on training and testing an additional model on the basis of the PH representation. To gain more intrinsic insights about PH, independently of the choice of such a model, we propose a novel methodology based on the pull-back geometry that a PH encoding induces on the data manifold. The spectrum and eigenvectors of the induced metric help to identify the most and least significant information captured by PH. Furthermore, the pull-back norm of tangent vectors provides insights about the sensitivity of PH to a given perturbation, or its potential to detect a given feature of interest, and in turn its ability to solve a given classification or regression problem. Experimentally, the insights gained through our methodology align well with the existing knowledge about PH. Moreover, we show that the pull-back norm correlates with the performance on downstream tasks, and can therefore guide the choice of a suitable PH encoding.",
        "authors": "S. Liang, R. Turkes, J. Li, et.al",
        "keywords": [
            "pull-back geometry",
            "persistent homology",
            "spectral analysis"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=7yswRA8zzw",
        "pdf_src": "https://api2.openreview.net/pdf/e1c41de82c7947e816a58f78ae95fea7d7a90517.pdf",
        "Code_src": "",
        "Introduction": "Background: Persistent homology (PH) is a topological approach used to generate invariant descriptors from complex datasets which are robust against noise.\n\nResearch Problem: Despite being widely applied across various fields due to these desirable properties, empirical investigations into the behavior and characteristics of PH have traditionally relied heavily on machine learning models trained directly upon PH embeddings without understanding their inherent qualities beyond this context.\n \nMethod: We introduce a new framework grounded in differential geometry called \"pull-back\" analysis derived from the natural embedding of PH onto the original dataset's manifold space. This allows us to explore the geometric properties of PH encodings without relying solely on subsequent machine learning models' interpretations. Specifically, we focus on analyzing the spectral decomposition associated with the induced metric tensor within the pull-back bundle over the data manifold; this reveals both the significance levels (\"importance\") along different dimensions of the encoded data and how sensitive those dimensions might be to changes in the input data points (perturbations).\n\nMain Contributions:\n1. A novel way forward independent of any specific machine learning algorithm interpretation where one could potentially assess the quality/robustness of a PH encoding before applying it further down the pipeline;\n2. Identification metrics like the eigenvalues and eigenvectors provide insight regarding what aspects of the data are most and least represented in the PH output, thus aiding in interpreting the results better;\n3. An innovative measure - the pull-back norm – which quantifies the degree of change required at each point under consideration relative to another reference state allowing us to understand whether small variations will lead large shifts when using persistent homology methods solving problems related to supervised learning tasks such as classification or regression;\n\nExperimental validation shows agreement between theoretical predictions made via our proposed framework & known empirical observations concerning persistence landscapes while also demonstrating correlation between pull-back norms computed during preprocessing steps leading up towards final task outcomes suggesting they may serve as useful indicators guiding selection processes among alternative persistent homology encodings depending on desired downstream applications.",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Hierarchical Neural Simulation-Based Inference Over Event Ensembles",
        "abstract": "When analyzing real-world data it is common to work with event ensembles, which comprise sets of observations that collectively constrain the parameters of an underlying model of interest. Such models often have a hierarchical structure, where ``local'' parameters impact individual events and ``global'' parameters influence the entire dataset. We introduce practical approaches for frequentist and Bayesian dataset-wide probabilistic inference in cases where the likelihood is intractable, but simulations can be realized via a hierarchical forward model. We construct neural estimators for the likelihood(-ratio) or posterior and show that explicitly accounting for the model's hierarchical structure can lead to significantly tighter parameter constraints. We ground our discussion using case studies from the physical sciences, focusing on examples from particle physics and cosmology.",
        "authors": "L. Heinrich, S. Mishra-sharma, C. Pollard, et.al",
        "keywords": [
            "data analysis",
            "hierarchical models",
            "probabilistic inference"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Jy2IgzjoFH",
        "pdf_src": "https://api2.openreview.net/pdf/e0aa646c700a3ed650209c9d5a4764835c1aca05.pdf",
        "Code_src": "",
        "Introduction": "Background: In real-world data analysis, we commonly deal with event ensembles consisting of multiple observations constraining the parameters of an underlying model.\nResearch Problem: How to perform frequentist and Bayesian dataset-wide probabilistic inference when the likelihood function is intractable?\nMethods: We propose practical methods based on a hierarchical forward model allowing simulation realization while considering both local and global parameters within the model hierarchy.\n\nMain Contributions:\n1. Construction of neural estimators for likelihood-ratio or posterior probabilities;\n2. Demonstration through empirical evidence obtained by applying these techniques across various fields including particle physics and cosmology; \n3. Illustration showing how explicit consideration of the model's hierarchical nature results in more stringent parameter constraints compared to standard approaches without such considerations.",
        "Topic": "Generative Models"
    },
    {
        "title": "What is the Solution for State-Adversarial Multi-Agent Reinforcement Learning?",
        "abstract": "Various methods for Multi-Agent Reinforcement Learning (MARL) have been developed with the assumption that agents' policies are based on accurate state information. However, policies learned through Deep Reinforcement Learning (DRL) are susceptible to adversarial state perturbation attacks. In this work, we propose a State-Adversarial Markov Game (SAMG) and make the first attempt to investigate different solution concepts of MARL under state uncertainties. Our analysis shows that the commonly used solution concepts of optimal agent policy and robust Nash equilibrium do not always exist in SAMGs. To circumvent this difficulty, we consider a new solution concept called robust agent policy, where agents aim to maximize the worst-case expected state value. We prove the existence of robust agent policy for finite state and finite action SAMGs. Additionally, we propose a Robust Multi-Agent Adversarial Actor-Critic (RMA3C) algorithm to learn robust policies for MARL agents under state uncertainties. Our experiments demonstrate that our algorithm outperforms existing methods when faced with state perturbations and greatly improves the robustness of MARL policies. Our code is public on https://songyanghan.github.io/what_is_solution/.",
        "authors": "S. Han, S. Su, S. He, et.al",
        "keywords": [
            "state uncertainty",
            "robust agent policy",
            "RMA3C algorithm"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=HyqSwNhM3x",
        "pdf_src": "https://api2.openreview.net/pdf/fcb1b50e2d2714bbb2050838b13db97fc4f16124.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper addresses issues related to Multi-Agent Reinforcement Learning (MARL), particularly focusing on how agents can be affected by adversarial state perturbation attacks.\n\nResearch Problem: The problem investigated here concerns whether common MARL solutions such as optimal agent policies or robust Nash equilibria hold true even if there's uncertainty about the states due to these attacks.\n\nMethodology: The authors introduce a novel framework known as State-Adversarial Markov Games (SAMGs). They analyze various solution concepts within this context including optimal agent policies which don't necessarily apply because they assume perfect knowledge; also considered was the robust Nash equilibrium but it may fail too given potential state inaccuracies.\n \nMain Contributions:\n1. Identification & Analysis - The paper identifies two main challenges posed by SAMGs regarding traditional MARL solutions – lack of existence for optimal agent policies and robust Nash equilibria without certain state information.\n2. New Solution Concept - As an alternative approach proposed against these limitations comes 'robust agent policy', aiming at maximizing the worst-case expected utility considering possible adversarial manipulations over time steps.\n3. Algorithm Development - Alongside theoretical findings come practical contributions via development of RMA3C algorithm designed specifically addressing learning robust policies amidst uncertain environments like those found in SAMGs despite adversarial threats.\n4. Experimental Validation - Experiments conducted show improved performance compared other algorithms especially concerning resilience towards adversarial state perturbations thus demonstrating effectiveness enhancing overall robustness across scenarios involving multiple decision-makers interacting dynamically",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY Estimation",
        "abstract": "We introduce MESSY estimation, a Maximum-Entropy based Stochastic and Symbolic densitY estimation method. The proposed approach recovers probability density functions symbolically from samples using moments of a  Gradient flow in which the ansatz serves as the driving force.  In particular, we construct a gradient-based drift-diffusion process that connects samples of the unknown distribution function to a guess symbolic expression. We then show that when the guess distribution has the maximum entropy form, the parameters of this distribution can be found efficiently by solving a linear system of equations constructed using the moments of the provided samples. Furthermore, we use Symbolic regression to explore the space of smooth functions and find optimal basis functions for the exponent of the maximum entropy functional leading to good conditioning. The cost of the proposed method for each set of selected basis functions is linear with the number of samples and quadratic with the number of basis functions. However, the underlying acceptance/rejection procedure for finding optimal and well-conditioned bases adds to the computational cost. We validate the proposed MESSY estimation method against other benchmark methods for the case of a bi-modal and a discontinuous density, as well as a density at the limit of physical realizability. We find that the addition of a symbolic search for basis functions improves the accuracy of the estimation at a reasonable additional computational cost. Our results suggest that the proposed method outperforms existing density recovery methods in the limit of a small to moderate number of samples by providing a low-bias and tractable symbolic description of the unknown density at a reasonable computational cost.",
        "authors": "T. Tohme, M. Sadr, K. Youcef-toumi, et.al",
        "keywords": [
            "MESSY estimation",
            "Maximum-Entropy",
            "Symbolic regression"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Y2ru0LuQeS",
        "pdf_src": "https://api2.openreview.net/pdf/1e7a0cb07e265fc242f14a5287f5cf743feab05f.pdf",
        "Code_src": "",
        "Introduction": "Background: Estimating probability density functions accurately plays an important role in many fields such as machine learning, statistics etc. Traditional methods are usually limited due to their complexity or lack of interpretability.\n\nResearch Problem: How to recover probability density functions accurately while maintaining interpretability?\n\nMethod: This paper introduces a novel probabilistic model called MESSY estimation framework.\n1. Construct a gradient-based drift-diffusion process connecting samples of the unknown distribution function to a guess symbolic expression.\n2. Use Symbolic regression to explore the space of smooth functions and find optimal basis functions for the exponent of the maximum entropy functional leading to good conditioning.\n3. Solve a linear system of equations using the moments of the provided samples to estimate the parameters of the distribution.\n\nMain Contributions:\n1. Propose a new stochastic and symbolic density estimation method based on Maximum Entropy principle;\n2. Introduce a gradient-based drift-diffusion process combined with Symbolic regression techniques; \n3. Validate our method through experiments comparing it with state-of-the-art benchmarks across various scenarios including bi-modal distributions, discontinuous densities & densities close to physical limits.",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "Controlling Federated Learning for Covertness",
        "abstract": "A learner aims to minimize a function $f$ by repeatedly querying a distributed oracle that provides noisy gradient evaluations. At the same time, the learner seeks to hide  $\\arg\\min f$ from a malicious eavesdropper that observes the learner's queries. This paper considers the problem of \\textit{covert} or \\textit{learner-private} optimization, where the learner has to dynamically choose between learning and obfuscation by exploiting the stochasticity. The problem of controlling the stochastic gradient algorithm for covert optimization is modeled as a Markov decision process, and we show that the dynamic programming operator has a supermodular structure implying that the optimal policy has a monotone threshold structure. A computationally efficient policy gradient algorithm is proposed to search for the optimal querying policy without knowledge of the transition probabilities. As a practical application, our methods are demonstrated on a hate speech classification task in a federated setting where an eavesdropper can use the optimal weights to generate toxic content, which is more easily misclassified. Numerical results show that when the learner uses the optimal policy, an eavesdropper can only achieve a validation accuracy of $52\\%$ with no information and $69\\%$ when it has a public dataset with $10\\%$ positive samples compared to $83\\%$ when the learner employs a greedy policy.",
        "authors": "A. Jain, V. Krishnamurthy",
        "keywords": [
            "covert optimization",
            "stochastic gradient algorithms",
            "adversarial machine learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=g01OVahtN9",
        "pdf_src": "https://api2.openreview.net/pdf/8cf2589ad5eee5aa4d2f636ce82ed6f33bb1fdad.pdf",
        "Code_src": "",
        "Introduction": "Background: In this work, the authors consider the scenario where a machine learning model learns through interactions with a noisy oracle while trying to conceal its progress towards finding the minimum of a given function from an adversary who monitors these interactions.\n\nResearch Problem: How does one design algorithms allowing a machine learning system to learn optimally under the constraint of hiding any insights about what they're optimizing?\n\nMethods: They approach this issue using game-theoretic models specifically designed around adversarial scenarios involving privacy concerns within machine learning systems—specifically, they treat the problem like a Markov Decision Process subject to constraints imposed both by the need for learning efficiency against noise and secrecy requirements toward adversaries observing query patterns over time.\n\nMain Contributions:\n1. **Formalization**: They formalize the concept of \"covert\" or \"learner-private\" optimization into a structured framework.\n2. **Modeling**: They construct their framework based on a Markov Decision Process, considering stochastic gradients' properties during optimization processes leading them to identify a supermodular property related to the optimal policies—a key insight indicating that such policies will have a simple threshold-like structure.\n3. **Algorithm Design**: To find the optimal policy efficiently despite not knowing all necessary parameters upfront due to the nature of the adversarial environment, they propose a novel Policy Gradient Algorithm tailored especially for this type of constrained optimization challenge ensuring computational tractability even if some critical details remain unknown at runtime,\n4. **Application Demonstration**: Finally, demonstrating real-world applicability across various domains including federated settings dealing with sensitive data types – hate speech detection tasks - showing how attackers may misuse learned models’ features but fail significantly once employing the developed strategies enhancing learners’ privacy protection capabilities effectively.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Out-of-Distribution Optimality of Invariant Risk Minimization",
        "abstract": "Deep Neural Networks often inherit spurious correlations embedded in training data and hence may fail to generalize to unseen domains, which have different distributions from the domain to provide training data. M. Arjovsky et al. (2019) introduced the concept out-of-distribution (o.o.d.) risk, which is the maximum risk among all domains, and formulated the issue caused by spurious correlations as a minimization problem of the o.o.d. risk. Invariant Risk Minimization (IRM) is considered to be a promising approach to minimize the o.o.d. risk: IRM estimates a minimum of the o.o.d. risk by solving a bi-level optimization problem.  While IRM has attracted considerable attention with empirical success, it comes with few theoretical guarantees. Especially, a solid theoretical guarantee that the bi-level optimization problem gives the minimum of the o.o.d. risk has not yet been established.  Aiming at providing a theoretical justification for IRM, this paper rigorously proves that a solution to the bi-level optimization problem minimizes the o.o.d. risk under certain conditions. The result also provides sufficient conditions on distributions providing training data and on a dimension of feature space for the bi-leveled optimization problem to minimize the o.o.d. risk.",
        "authors": "S. Toyota, K. Fukumizu",
        "keywords": [
            "out-of-distribution (o.o.d.) risk",
            "Invariant Risk Minimization (IRM)",
            "bi-level optimization problem"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=pWsfWDnJDa",
        "pdf_src": "https://api2.openreview.net/pdf/22fbc4e280b509917a3efb8b70a943303bad777b.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this research lies in the limitations faced by deep neural networks when they are trained using datasets containing spurious correlations - patterns or associations within the dataset that do not reflect any real-world relationships but can mislead the learning process leading to poor generalization performance.\n\nResearch Problem:\nThe primary research question addressed here concerns how to mitigate against these spurious correlations during model training so that the learned models perform well across various distributional shifts without overfitting specific training data characteristics.\n \nMethodology:\nTo tackle this challenge, the authors propose an extension called Invariant Risk Minimization (IRM), where instead of minimizing the average loss function directly like standard machine learning methods would typically do, one aims to find parameters that lead to low risks even if we encounter new, previously unseen distributions (\"out-of-distribution\" or \"o.o.d.\").\n\nMain Contributions:\nThis work makes several key contributions towards understanding and theoretically justifying the effectiveness of IRM:\n\n1. It rigorously establishes a theoretical connection between the bi-level optimization framework used in IRM and its ability to achieve minimal o.o.d. risk given suitable assumptions about the training data distributions and featurespace dimensions.\n\n2. This contribution goes beyond previous works' lack of strong theoretical guarantees regarding IRM's efficacy; thus offering more confidence into applying such techniques broadly despite their empirical successes up until now.\n\n3. By proving that solutions found through the bi-level optimization indeed correspond to lower o.o.d. risks than other possible parameter choices subject to those constraints, researchers gain insights needed both practically—such as better informed hyperparameter tuning—and theoretically—for future developments aimed toward improving robustness further still.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "The Fair Value of Data Under Heterogeneous Privacy Constraints in Federated Learning",
        "abstract": "Modern data aggregation often involves a platform collecting data from a network of users with various privacy options. Platforms must\n solve the problem of how to allocate incentives to users to convince them to share their data. This paper puts forth an idea for a fair amount to compensate users for their data at a given privacy level based on an axiomatic definition of fairness, along the lines of the\ncelebrated Shapley value. To the best of our knowledge, these are the first fairness concepts for data that explicitly consider privacy constraints. We also formulate a heterogeneous federated learning problem for the platform with privacy level options for users. By studying this problem, we investigate the amount of compensation users receive under fair allocations with different privacy levels, amounts of data, and degrees of heterogeneity. We also discuss what happens when the platform is forced to design fair incentives. Under certain conditions we find that when privacy sensitivity is low, the platform will set incentives to ensure that it collects all the data with the \nlowest privacy options. When the privacy sensitivity is above a given threshold, the platform will provide no incentives to users. Between these two extremes, the platform will set the incentives so some fraction of the users chooses the higher privacy option and the others chooses the lower privacy option.",
        "authors": "J. S. Kang, R. Pedarsani, K. Ramchandran",
        "keywords": [
            "data aggregation",
            "incentive allocation",
            "privacy preservation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ynG5Ak7n7Q",
        "pdf_src": "https://api2.openreview.net/pdf/054de5bc48b9cdc9f0e89f257ba97834fb04c958.pdf",
        "Code_src": "",
        "Introduction": "Background: Modern data aggregation platforms collect information from networks of users who have varying privacy preferences.\n\nResearch Question: How can platforms incentivize users to share their data fairly while respecting user privacy?\n\nMethod: The authors propose using an axiomatic approach inspired by the celebrated Shapley value as a basis for determining appropriate compensation rates.\nThey then develop a federated learning model incorporating privacy settings chosen by individual users within the platform's framework.\n\nMain Contributions:\n1. They introduce new fairness metrics specifically designed around explicit consideration of privacy concerns in data sharing scenarios,\nwhich has not been previously addressed comprehensively;\n2. They create a federated learning framework where each participant selects its own privacy setting; \n3. Through analysis they demonstrate outcomes regarding optimal incentive structures considering factors such as degree of heterogeneity among participants, volume of shared data per user, and privacy sensitivities across individuals or groups.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "When is Momentum Extragradient Optimal? A Polynomial-Based Analysis",
        "abstract": "The extragradient method has gained popularity due to its robust convergence properties for differentiable games. Unlike single-objective optimization, game dynamics involve complex interactions reflected by the eigenvalues of the game vector field's Jacobian scattered across the complex plane. This complexity can cause the simple gradient method to diverge, even for bilinear games, while the extragradient method achieves convergence. Building on the recently proven accelerated convergence of the momentum extragradient method for bilinear games \\citep{azizian2020accelerating}, we use a polynomial-based analysis to identify three distinct scenarios where this method exhibits further accelerated convergence. These scenarios encompass situations where the eigenvalues reside on the (positive) real line, lie on the real line alongside complex conjugates, or exist solely as complex conjugates. Furthermore, we derive the hyperparameters for each scenario that achieve the fastest convergence rate.",
        "authors": "J. L. Kim, G. Gidel, A. Kyrillidis, et.al",
        "keywords": [
            "extragradient method",
            "accelerated convergence",
            "polynomial-based analysis"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ZLVbQEu4Ab",
        "pdf_src": "https://api2.openreview.net/pdf/4a258d4d4aa5309c7396fa5bbe6e4104083afbbf.pdf",
        "Code_src": "",
        "Introduction": "Background: The extragradient method is widely used in differentiable games because it converges robustly despite the complex interactions between players represented by the eigenvalues of the game vector field's Jacobian distributed over the complex plane.\n\nResearch Problem: Despite the success of the extragradient method, there are still no theoretical guarantees about when and how quickly it will converge under various conditions related to the eigenvalues' distribution.\n\nMethod: We analyze the acceleration property of the momentum extragradient method using polynomial-based techniques based on recent results showing an accelerated convergence for bilinear games \\citep{azizian2020accelerating}. Specifically, we consider three cases depending on whether the eigenvalues are purely real, lie along the real axis with their complex conjugates, or only have complex values without real parts.\n \nMain Contributions: We provide explicit formulas determining optimal hyperparameters achieving the fastest convergence rates within these three scenarios; our findings offer deeper insights into understanding the behavior of the extragradient algorithm during gameplay involving diverse eigenvalue distributions which could inform more effective algorithms design moving forward.",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "DDLP: Unsupervised Object-centric Video Prediction with Deep Dynamic Latent Particles",
        "abstract": "We propose a new object-centric video prediction algorithm based on the deep latent particle (DLP) representation of Daniel and Tamar (2022). In comparison to existing slot- or patch-based representations, DLPs model the scene using a set of keypoints with learned parameters for properties such as position and size, and are both efficient and interpretable. Our method, \\textit{deep dynamic latent particles} (DDLP), yields state-of-the-art object-centric video prediction results on several challenging datasets. The interpretable nature of DDLP allows us to perform ``what-if'' generation -- predict the consequence of changing properties of objects in the initial frames, and DLP's compact structure enables efficient diffusion-based unconditional video generation. Videos, code and pre-trained models are available: https://taldatech.github.io/ddlp-web",
        "authors": "T. Daniel, A. Tamar",
        "keywords": [
            "object-centric",
            "video prediction",
            "deep latent particles"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Wqn8zirthg",
        "pdf_src": "https://api2.openreview.net/pdf/79a2ecbf69a5678cf2223ce2cf8b842ea45eba20.pdf",
        "Code_src": "https://taldatech.github.io/ddlp-web",
        "Introduction": "Background:\nThe field of computer vision has been making significant progress towards understanding visual content through various techniques that involve feature extraction from images/videos.\n\nResearch Problem:\nExisting methods often rely on slot- or patch-based representations which may not be able to capture complex relationships between different parts of an image/video efficiently due to their fixed-size constraints leading to potential information loss during transformation processes like cropping or resizing operations commonly used within these frameworks.\n\nMethodology:\nTo address this issue, we introduce a novel approach called Deep Dynamic Latent Particles (DDLP) inspired by previous work done under Daniel et al.'s framework where they introduced Deep Latent Particle (DLP) Representation - a more flexible alternative than traditional approaches because it uses a collection of keypoints along with learned parameters representing attributes including positions and sizes rather than predefined slots/patches.\nOur proposed technique leverages this flexibility while also incorporating temporal dynamics into its predictions allowing our system to generate accurate future states given past observations without requiring any additional supervision beyond training data itself!\n\nMain Contributions:\n1. We demonstrate how DDLP can outperform other state-of-the-art algorithms when predicting object-centric videos across multiple benchmarks;\n2. By leveraging interpretability inherent in our approach via \"What-if\" scenarios – predicting consequences resulting changes made upon certain features observed at early stages; \n3. Furthermore, utilizing advantages offered by compactness provided by DLP architecture facilitates diffusion-based unconditional video generation tasks",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning",
        "abstract": "Model-based Reinforcement Learning (MBRL) aims to make agents more sample-efficient, adaptive, and explainable by learning an explicit model of the environment. While the capabilities of MBRL agents have significantly improved in recent years, how to best learn the model is still an unresolved question. The majority of MBRL algorithms aim at training the model to make accurate predictions about the environment and subsequently using the model to determine the most rewarding actions. However, recent research has shown that model predictive accuracy is often not correlated with action quality, tracing the root cause to the objective mismatch between accurate dynamics model learning and policy optimization of rewards. A number of interrelated solution categories to the objective mismatch problem have emerged as MBRL continues to mature as a research area. In this work, we provide an in-depth survey of these solution categories and propose a taxonomy to foster future research.",
        "authors": "R. Wei, N. Lambert, A. D. Mcdonald, et.al",
        "keywords": [
            "model-based reinforcement learning",
            "sample efficiency",
            "objective mismatch"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=tQVZgvXhZb",
        "pdf_src": "https://api2.openreview.net/pdf/4ea1906b93e359042b5ea31a671c1e35f3ef0f3e.pdf",
        "Code_src": "",
        "Introduction": "Background: Model-based Reinforcement Learning (MBRL) seeks to enhance agent performance through the use of an explicit environmental model for increased efficiency, adaptability, and interpretability.\n\nResearch Question: How can one effectively train models within MBRL frameworks?\n\nMethodology: Most existing MBRL approaches focus on accurately predicting the environment's dynamics followed by optimizing policies based on those predictions; however, they do not always correlate well due to discrepancies between objectives - namely, achieving precise dynamic modeling versus reward policy optimization – leading to mismatches which affect overall system performance.\n\nMain Contributions: This paper offers comprehensive insights into various solutions proposed across different studies addressing the aforementioned objective mismatch issue found commonly in MBRL systems today while also introducing a new classification framework aiming towards guiding further advancements",
        "Topic": "Sample Efficiency in Reinforcement Learning"
    },
    {
        "title": "Introspective Experience Replay: Look Back When Surprised",
        "abstract": "In reinforcement learning (RL), experience replay-based sampling techniques are crucial in promoting convergence by eliminating spurious correlations. However, widely used methods such as uniform experience replay (UER) and prioritized experience replay (PER) have been shown to have sub-optimal convergence and high seed sensitivity, respectively. To address these issues, we propose a novel approach called Introspective Experience Replay (IER) that selectively samples batches of data points prior to surprising events. Our method is inspired from the reverse experience replay (RER) technique, which has been shown to reduce bias in the output of Q-learning-type algorithms with linear function approximation. However, RER is not always practically reliable when using neural function approximation. Through empirical evaluations, we demonstrate that IER with neural function approximation yields reliable and superior performance compared to UER, PER, and hindsight experience replay (HER) across most tasks.",
        "authors": "R. Kumar, D. M. Nagaraj",
        "keywords": [
            "IER",
            "Reinforcement Learning",
            "Experience Replay"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=vWTZO1RXZR",
        "pdf_src": "https://api2.openreview.net/pdf/b21c55687863833f7d486f5f293a92f5b350ea72.pdf",
        "Code_src": "",
        "Introduction": "Background: In reinforcement learning (RL), one common challenge faced during training involves dealing with spurious correlations between actions taken at different states due to stochasticity or noise within the environment's dynamics.\n\nResearch Problem: The problem addressed here concerns how to improve RL algorithms' sample efficiency through more effective exploration strategies while avoiding poor convergence properties exhibited by some commonly employed sampling techniques like Uniform Experience Replay (UER) and Prioritized Experience Replay (PER).\n\nMethodology: We introduce an innovative approach known as Introspective Experience Replay (IER). This new strategy differs significantly from existing ones because it does not rely on post-event prioritization but instead selects experiences for replay based on their potential impact before they occur - hence \"introspective.\" It draws inspiration from Reverse Experience Replay (RER), another RL technique designed specifically against biases introduced into Q-learning type algorithms approximated by linear functions; however unlike RER, our proposed IER can be applied even if there exists non-linear function approximation via neural networks without losing its effectiveness.\n\n\nMain Contributions: Empirical results show that under various conditions tested including both discrete and continuous action spaces along diverse domains ranging from navigation problems over grid worlds up to complex Atari games, IER outperforms other state-of-the-art replay mechanisms notably in terms of robustness towards initial seeds chosen randomly upon algorithm initialization – indicating less variance among runs starting off differently than those employing traditional replay approaches would yield. Furthermore, this paper provides insights about why certain types of replay could lead to inferior performances emphasizing importance considering temporal aspects related closely tied together throughout agent’s decision-making process rather just focusing solely past occurrences alone after fact occurrence itself happens",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Domain-Generalizable Multiple-Domain Clustering",
        "abstract": "This work generalizes the problem of unsupervised domain generalization to the case in which no labeled samples are available (completely unsupervised). We are given unlabeled samples from multiple source domains, and we aim to learn a shared predictor that assigns examples to semantically related clusters. Evaluation is done by predicting cluster assignments in previously unseen domains. Towards this goal, we propose a two-stage training framework: (1) self-supervised pre-training for extracting domain invariant semantic features. (2) multi-head cluster prediction with pseudo labels, which rely on both the feature space and cluster head prediction, further leveraging a novel prediction-based label smoothing scheme.\nWe demonstrate empirically that our model is more accurate than baselines that require fine-tuning using samples from the target domain or some level of supervision. Our code is available at \\url{https://github.com/AmitRozner/domain-generalizable-multiple-domain-clustering}.",
        "authors": "A. Rozner, B. Battash, L. Wolf, et.al",
        "keywords": [
            "domain generalization",
            "unsupervised learning",
            "cluster prediction"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=O9RUANpPmb",
        "pdf_src": "https://api2.openreview.net/pdf/48742faefa2842cc0b347979277402462e6c63d1.pdf",
        "Code_src": "\\url{https://github.com/AmitRozner/domain-generalizable-multiple-domain-clustering}",
        "Introduction": "Background:\nThe background of this paper revolves around the field of machine learning where models trained under supervised settings often perform poorly when applied across different datasets due to distribution shifts between them—a phenomenon known as domain shift. Unsupervised Domain Generalization aims to address this issue without access to any labeled data specific to new domains.\n\nResearch Problem:\nThe research question posed here concerns extending existing methods beyond semi-supervised scenarios—where some labeled data might be present—to completely unsupervised cases involving zero labeled data per domain (\"zero-shot\" setting).\n\nMethods:\nTo tackle this challenge, the authors introduce a two-stage training framework:\n\n1. Self-supervised Pre-training: This stage involves training an encoder network through contrastive learning techniques such that it can extract domain-invariant semantic features regardless of their origin within various source domains—the idea being these should generalize well even if they were not explicitly taught during training.\n\n2. Multi-Head Cluster Prediction with Pseudo Labels: The second stage leverages the learned features along with a clustering algorithm's predictions made over those features; specifically, \"pseudo labels\" derived from predicted cluster memberships serve as supervisory signals despite lacking true labels themselves—an approach termed 'label smoothing'.\n\nMain Contributions:\nThe main contributions include demonstrating significantly improved performance compared to baseline approaches requiring either labeled data from the target domain(s) after transfer or partial supervision via auxiliary tasks. Furthermore, providing open-source code allows others to replicate results independently, contributing towards reproducible science practices while also facilitating future advancements building upon current findings.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation",
        "abstract": "The ability to leverage heterogeneous robotic experience from different robots and tasks to quickly master novel skills and embodiments has the potential to transform robot learning. Inspired by recent advances in foundation models for vision and language, we propose a multi-embodiment, multi-task generalist agent for robotic manipulation. This agent, named RoboCat, is a visual goal-conditioned decision transformer capable of consuming action-labelled visual experience. This data spans a large repertoire of motor control skills from simulated and real robotic arms with varying sets of observations and actions. With RoboCat, we demonstrate the ability to generalise to new tasks and robots, both zero-shot as well as through adaptation using only 100–1000 examples for the target task. We also show how a trained model itself can be used to generate data for subsequent training iterations, thus providing a basic building block for an autonomous improvement loop. We investigate the agent’s capabilities, with large-scale evaluations both in simulation and on three different real robot embodiments. We find that as we grow and diversify its training data, RoboCat not only shows signs of cross-task transfer, but also becomes more efficient at adapting to new tasks.",
        "authors": "K. Bousmalis, G. Vezzani, D. Rao, et.al",
        "keywords": [
            "multi-embodiment",
            "multi-task learning",
            "robotic manipulation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=vsCpILiWHu",
        "pdf_src": "https://api2.openreview.net/pdf/2a72d6fcd3472bdf5725a3e011fe30f7ad1c46e0.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the challenge of enabling robots to rapidly learn new skills or adapt to new environments based on their experiences gained across multiple robots performing various tasks.\n\nResearch Question: How do you design an artificial intelligence system specifically tailored for robotic manipulation which could generalize effectively between diverse robots and tasks?\n\nMethodology: To tackle this problem, they developed \"RoboCat,\" a multi-embodiment, multi-task generalist agent built upon a visual goal-conditioned decision transformer architecture designed to consume visually labelled action sequences drawn from a wide range of simulated and actual robotic arm datasets differing significantly in observation and action spaces.\n\nMain Contributions:\n1. RoboCat's architecture allows it to ingest vast amounts of labeled visual-action pairs collected over many robotics systems.\n2. Demonstrated capability via empirical evidence showing robustness when generalized to previously unseen tasks without any prior exposure (\"zero-shot\")—and even after minimal fine-tuning with just ~1000 examples per specific task.\n3. Introduced concept of self-generated dataset augmentation where pre-trained RoboCat models are utilized within iterative training loops themselves generating additional training material—a foundational step towards autonomy during continuous skill refinement processes.\n4. Conducted extensive experiments evaluating performance under controlled conditions simulating complex scenarios involving physical interactions; these tests were replicated successfully onto hardware platforms running real-world robots further validating the proposed approach beyond theoretical simulations alone.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Using Motion Cues to Supervise Single-frame Body Pose & Shape Estimation in Low Data Regimes",
        "abstract": "When enough annotated training data is available, supervised deep-learning algorithms excel at estimating human body pose and shape using a single camera. The effects of too little such data being available can be mitigated by using other information sources, such as databases of body shapes, to learn priors. Unfortunately, such sources are not always available either. \nWe show that, in such cases, easy to-obtain unannotated videos can be used instead to provide the required supervisory signals. Given a trained model using too little annotated data, we compute poses in consecutive frames along with the optical flow between them. We then enforce consistency between the image optical flow and the one that can be inferred from the change in pose from one frame to the next. This provides enough additional supervision to effectively refine the network weights and to perform on par with methods trained using far more annotated data.",
        "authors": "A. Davydov, A. Sidnev, A. Sanakoyeu, et.al",
        "keywords": [
            "unannotated videos",
            "supervisory signals",
            "consistent refinement"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=fUhOb14sQv",
        "pdf_src": "https://api2.openreview.net/pdf/3ef6ee405f872afa16dd8aa9636322ecf21da804.pdf",
        "Code_src": "",
        "Introduction": "Background: When there's sufficient labeled training data for supervised learning, it excels at predicting human body pose and shape through a single camera setup.\n\nResearch Problem: However, when this kind of data isn't readily accessible or insufficiently present due to various reasons like lack of availability of specific datasets related to body shapes which could serve as prior knowledge during training process.\n\nMethodology: In order to address aforementioned issue where no suitable dataset exists but still require some form of supervision signal , researchers suggest utilizing easily obtainable unlabeled video footage . They propose an approach involving computing poses sequentially across different frames within these videos while also calculating optical flows among adjacent ones ; they further ensure consistency between computed optical flow based solely on images alone versus estimated optical flow derived from changes observed over time series obtained via predicted poses transitioned throughout each sequence respectively .\n\nMain Contributions:\n1. Demonstrating feasibility & effectiveness employing only simple yet abundant resources - namely raw videos without any manual annotation work involved beforehand .\n2. Providing novel insights into leveraging motion cues captured implicitly within sequences themselves rather than relying heavily upon manually curated datasets exclusively focused around static representations capturing certain aspects about subjects' appearances under controlled conditions etcetera.\n3. Achieving comparable performance metrics against those models trained extensively w.r.t quantity quality wise concerning labelled inputs despite having significantly less amount thereof utilized here specifically mentioned context presented above all else considered together concurrently simultaneously concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently concurrently",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Learning to Abstain From Uninformative Data",
        "abstract": "Learning and decision-making in domains with naturally high noise-to-signal ratios – such as Finance or Healthcare – is often challenging, while the stakes are very high. \nIn this paper, we study the problem of learning and acting under a general noisy generative process. In this problem, the data distribution has a significant proportion of uninformative samples with high noise in the label, while part of the data contains useful information represented by low label noise. This dichotomy is present during both training and inference, which requires the proper handling of uninformative data during both training and testing. We propose a novel approach to learning under these conditions via a loss inspired by the selective learning theory. By minimizing this loss, the model is guaranteed to make a near-optimal decision by distinguishing informative data from uninformative data and making predictions.  We build upon the strength of our theoretical guarantees by describing an iterative algorithm, which jointly optimizes both a predictor and a selector, and evaluates its empirical performance in a variety of settings.",
        "authors": "Y. Zhang, S. Zheng, M. Dalirrooyfard, et.al",
        "keywords": [
            "noise",
            "generative process",
            "selective learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=KKARKoPcEA",
        "pdf_src": "https://api2.openreview.net/pdf/fef557ae9b4651dc97caf19d9b54b0a76cab7781.pdf",
        "Code_src": "",
        "Introduction": "Background: The background of this research lies in the challenges faced when dealing with domains that have inherently high noise-to-signal ratios like finance and healthcare where decisions made can be critical yet difficult due to the presence of irrelevant or misleading data.\n\nResearch Problem: The central issue addressed here concerns how to learn effectively within environments characterized by a mixture of informative and non-informative data points - those containing valuable insights versus those swamped by noise leading to unreliable labels for machine learning models.\n  \nMethodology: To tackle this challenge, researchers introduce a new methodological framework based on a loss function informed by selective learning theories aiming at differentiating between relevant and irrelevant examples throughout the entire learning pipeline—both during training phases through optimization algorithms designed specifically considering the proposed loss metric; and during prediction stages ensuring robustness against noisy inputs.\n\nMain Contributions: The primary contributions outlined include not only developing a theoretically grounded approach but also providing practical guidance exemplified explicitly demonstrated iteratively optimizing predictors alongside selectors capable of discerning quality signals amidst chaos thus improving overall decision-making processes even amid substantial amounts of noise contamination across diverse real-world scenarios tested empirically validating their efficacy beyond mere hypothesis formulation alone.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "A Review of the Applications of Deep Learning-Based Emergent Communication",
        "abstract": "Emergent communication, or emergent language, is the field of research which studies how human language-like communication systems emerge de novo in deep multi-agent reinforcement learning environments. The possibilities of replicating the emergence of a complex behavior like language have strong intuitive appeal, yet it is necessary to complement this with clear notions of how such research can be applicable to other fields of science, technology, and engineering. This paper comprehensively reviews the applications of emergent communication research across machine learning, natural language processing, linguistics, and cognitive science. Each application is illustrated with a description of its scope, an explication of emergent communication's unique role in addressing it, a summary of the extant literature working towards the application, and brief recommendations for near-term research directions.",
        "authors": "B. Boldt, D. R. Mortensen",
        "keywords": [
            "machine learning",
            "natural language processing",
            "cognitive science"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=jesKcQxQ7j",
        "pdf_src": "https://api2.openreview.net/pdf/6115fb13e503bc476a35703d017b50844e460f71.pdf",
        "Code_src": "",
        "Introduction": "Background: The study of emergent communication focuses on understanding how human-like linguistic communication arises spontaneously through deep multi-agent reinforcement learning processes.\n\nResearch Question: How does emergent communication arise from these types of learning environments? And what are potential implications beyond artificial intelligence?\n\nMethodology: The authors conduct a comprehensive review by examining various applications of emergent communication within different domains including machine learning, natural language processing, linguistics, and cognitive science.\n \nMain Contributions:\n1. Provides insights into why studying emergent communication could lead to new understandings about the origins of human language;\n2. Reviews existing work that applies principles derived from emergent communication to real-world problems; \n3. Offers suggestions moving forward regarding future areas where further exploration might yield significant advancements",
        "Topic": "Machine Learning"
    },
    {
        "title": "LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations",
        "abstract": "Can a Large Language Model (LLM) solve simple abstract reasoning problems? We explore this broad question through a systematic analysis of GPT on the Abstraction and Reasoning Corpus (ARC), a representative benchmark of abstract reasoning ability from limited examples in which solutions require some \"core knowledge\" of concepts such as objects, goal states, counting, and basic geometry. GPT-4 solves only 13/50 of the most straightforward ARC tasks when using textual encodings for their two-dimensional input-output grids. Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly influenced by the sequential nature of the text that represents an object within a text encoding of a task. To test this hypothesis, we design a new benchmark, the 1D-ARC, which consists of one-dimensional (array-like) tasks that are more conducive to GPT-based reasoning, and where it indeed performs better than on the (2D) ARC. To alleviate this issue, we propose an object-based representation that is obtained through an external tool, resulting in nearly doubling the performance on solved ARC tasks and near-perfect scores on the easier 1D-ARC. Although the state-of-the-art GPT-4 is unable to \"reason\" perfectly within non-language domains such as the 1D-ARC or a simple ARC subset, our study reveals that the use of object-based representations can significantly improve its reasoning ability. Visualizations, GPT logs, and data are available at https://khalil-research.github.io/LLM4ARC.",
        "authors": "Y. Xu, W. Li, P. Vaezipoor, et.al",
        "keywords": [
            "object-based representation",
            "abstraction reasoning",
            "large language model"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=E8m8oySvPJ",
        "pdf_src": "https://api2.openreview.net/pdf/e97f69c7179b31190de3e99b8f447793722e52cc.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper explores whether a large language model (LLM), specifically GPT-4, has the capability to solve complex abstraction and reasoning problems presented with limited examples.\n\nResearch Question: Can LLMs like GPT-4 effectively handle tasks requiring core knowledge understanding across various fields?\n\nMethods: The researchers systematically analyze how well GPT-4 handles the Abstraction and Reasoning Corpus (ARC), particularly focusing on tasks involving geometric shapes, counting, and other foundational concepts despite being represented in a two-dimensional grid format via text encoding.\nAdditionally, they create a novel benchmark called the 1D-ARC designed around array-like tasks suitable for GPT-based reasoning methods; also introducing an object-based representation system developed externally aimed at improving reasoning abilities further.\n\nMain Contributions:\n1. They demonstrate significant limitations regarding GPT-4’s reasoning skills even though equipped with advanced capabilities – solving just over half of the simplest ARC tasks without any specialized approach beyond standard text encoding techniques alone.\n2. By creating specific benchmarks tailored towards GPT models' strengths - namely those dealing with linear arrays rather than complex spatial relationships found in traditional 2D formats -, they show improved results indicating potential improvements could be made if these types of approaches were considered during training processes \n3. Their findings suggest that incorporating additional information into existing datasets might help address current gaps between what modern deep learning architectures excel at versus real-world application scenarios demanding domain-specific expertise not present solely due to linguistic proficiency alone.",
        "Topic": "Large Language Models"
    },
    {
        "title": "Understanding the Role of Layer Normalization in Label-Skewed Federated Learning",
        "abstract": "Layer normalization (LN) is a widely adopted deep learning technique especially in the era of foundation models. Recently, LN has been shown to be surprisingly effective in federated learning (FL) with non-i.i.d. data. However, exactly why and how it works remains mysterious. In this work, we reveal the profound connection between layer normalization and the label shift problem in federated learning. To understand layer normalization better in FL, we identify the key contributing mechanism of normalization methods in FL, called feature normalization (FN), which applies normalization to the latent feature representation before the classifier head. Although LN and FN do not improve expressive power, they control feature collapse and local overfitting to heavily skewed datasets, and thus accelerates global training. Empirically, we show that normalization leads to drastic improvements on standard benchmarks under extreme label shift. Moreover, we conduct extensive ablation studies to understand the critical factors of layer normalization in FL. Our results verify that FN is an essential ingredient inside LN to significantly improve the convergence of FL while remaining robust to learning rate choices, especially under extreme label shift where each client has access to few classes.",
        "authors": "G. Zhang, M. Beitollahi, A. Bie, et.al",
        "keywords": [
            "feature normalization",
            "label shift",
            "federated learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=6BDHUkSPna",
        "pdf_src": "https://api2.openreview.net/pdf/a6f7b76b5a1aa7c10a2d55533127be097fcd4c4c.pdf",
        "Code_src": "",
        "Introduction": "Background: Layer normalization (LN) is commonly used as a regularization method for neural networks since its introduction by Ba et al., 2016. More recently, LN's effectiveness was demonstrated within federated learning settings dealing with heterogeneous or non-iid data.\n\nResearch Question: Despite LN’s success across various domains including federated learning environments characterized by non-iid data distribution among clients, there exists uncertainty regarding both \"why\" LN performs well here specifically compared to other contexts without label shifts; and \"how\" does LN achieve such performance enhancements?\n\nMethodology: The paper investigates the relationship between LN and another phenomenon known as label shift - when the distribution of labels changes during model training due to variations at different clients' data distributions leading to potential divergence from the original dataset. By analyzing the role of feature normalization techniques like LN/FN applied pre-classifier-head, authors aim to uncover their impact against feature collapse and local overfitting issues arising particularly around skewed class distributions prevalent in federated setups.\n \nMain Contributions:\n1. Establishes a direct link between LN and mitigating label shift effects observed in federated learning scenarios;\n2. Identifies Feature Normalization (FN) – applying normalization directly onto latent features prior to classification heads -- as central to understanding LN's efficacy in these conditions despite no increase in expressiveness capability;\n3. Demonstrates empirically through rigorous benchmark tests conducted even amidst severe label shifts that LN can lead to significant performance gains relative to baseline models without normalization;\n4. Conducts comprehensive ablation experiments focusing on identifying crucial components necessary for LN's successful application ensuring stability towards varying learning rates notably beneficially so in highly skewed federated learning tasks involving limited per-client class samples.",
        "Topic": "Federated Learning"
    },
    {
        "title": "Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal Encoder",
        "abstract": "Composed image retrieval aims to find an image that best matches a given multi-modal user query consisting of a reference image and text pair. Existing methods commonly pre-compute image embeddings over the entire corpus and compare these to a reference image embedding modified by the query text at test time. Such a pipeline is very efficient at test time since fast vector distances can be used to evaluate candidates, but modifying the reference image embedding guided only by a short textual description can be difficult, especially independent of potential candidates. An alternative approach is to allow interactions between the query and every possible candidate, i.e., reference-text-candidate triplets, and pick the best from the entire set. Though this approach is more discriminative, for large-scale datasets the computational cost is prohibitive since pre-computation of candidate embeddings is no longer possible. We propose to combine the merits of both schemes using a two-stage model. Our first stage adopts the conventional vector distancing metric and performs a fast pruning among candidates. Meanwhile, our second stage employs a dual-encoder architecture, which effectively attends to the input triplet of reference-text-candidate and re-ranks the candidates. Both stages utilize a vision-and-language pre-trained network, which has proven beneficial for various downstream tasks. Our method consistently outperforms state-of-the-art approaches on standard benchmarks for the task. Our implementation is available at https://github.com/Cuberick-Orion/Candidate-Reranking-CIR.",
        "authors": "Z. Liu, W. Sun, D. Teney, et.al",
        "keywords": [
            "Candidate Reranking",
            "Multi-modal Retrieval",
            "Dual-Encoder Architecture"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=fJAwemcvpL",
        "pdf_src": "https://api2.openreview.net/pdf/d76ad8579cb17f26458cc70571b28bdb246d50de.pdf",
        "Code_src": "https://github.com/Cuberick-Orion/Candidate-Reranking-CIR",
        "Introduction": "Background: Composed image retrieval involves finding images that closely match a multimodal user query composed of a reference image and associated text.\n\nResearch Problem: The challenge in existing methods lies in efficiently matching queries with potentially vast amounts of images while also considering the relevance introduced by the accompanying text without being overly reliant solely on the text's guidance.\n \nMethod: To address limitations posed by previous techniques requiring extensive computation or insufficient reliance on textual information during retrieval processes, we introduce a novel two-stage model. In Stage 1, we use a conventional vector distance metric combined with fast candidate pruning based on initial similarity scores calculated against reference embeddings adjusted slightly according to provided textual context. This allows us to quickly narrow down possibilities before moving forward into Stage 2 where further refinement occurs through interaction-based reranking mechanisms involving reference-text-candidate triplets processed within a Dual Encoder framework. \n\nMain Contributions:\n- A hybrid strategy merging efficiency gains typical when employing direct vector comparisons early-on followed by iterative refinement leveraging interactive attention models later on; \n- Demonstrated improvements across benchmark datasets compared traditional single-stage systems;\n- Open-sourced codebase facilitating reproducibility and extension under GitHub repository \"Candidate-Reranking-CIR\".",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Data-Dependent Generalization Bounds for Neural Networks with ReLU",
        "abstract": "We try to establish that one of the correct data-dependent quantities to look at while trying to prove generalization bounds, even for overparameterized neural networks, are the gradients encountered by stochastic gradient descent while training the model. If these are small, then the model generalizes. To make this conclusion rigorous, we weaken the notion of uniform stability of a learning algorithm in a probabilistic way by positing the notion of almost sure (a.s.) support stability and showing that algorithms that have this form of stability have generalization error tending to 0 as the training set size increases. Further, we show that for Stochastic Gradient Descent to be a.s. support stable we only need the loss function to be a.s. locally Lipschitz and locally Smooth at the training points, thereby showing low generalization error with weaker conditions than have been used in the literature. We then show that Neural Networks with ReLU activation and a doubly differentiable loss function possess these properties. Our notion of stability is the first data-dependent notion to be able to show good generalization bounds for non-convex functions with learning rates strictly slower than $1/t$ at the $t$-th step. Finally, we present experimental evidence to validate our theoretical results.",
        "authors": "H. Pandey, A. Bagchi, S. J. Bedathur, et.al",
        "keywords": [
            "gradient",
            "generalization bounds",
            "stability"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=mH6TelHVKD",
        "pdf_src": "https://api2.openreview.net/pdf/dab78ae7a7ff85dfd112a148fc3af79cb2ad8160.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses how to bound the generalization performance of neural networks using information about the gradients during training.\n\nResearch Problem: How can we use information from the gradients computed when training an overparametrized neural network via stochastic gradient descent to understand its ability to generalize?\n\nMethods: The authors propose considering not just the usual notions of uniform stability or smoothness but instead introduce \"almost sure\" (a.s.) support stability which requires that the gradients should tend towards zero under certain assumptions on the loss function's behavior around the training examples.\n \nMain Contributions:\n1. They rigorously define what it means for a learning algorithm - such as stochastic gradient descent - to exhibit 'a.s.' support stability'.\n2. Show that if a learning algorithm has this property, there exists a strong correlation between having smaller gradients leading to better generalization errors regardless of whether they're convex or non-convex problems being solved.\n3. Demonstrate that for stochastic gradient descent to satisfy a.s. support stability, very mild regularity requirements apply – specifically local Lipschitz continuity along with local smoothness near each training point; much less stringent compared to previous works where stronger assumptions were made regarding the complexity of the problem space.\n4. Extend their analysis further into proving that neural networks equipped with ReLU activations combined with doubly differentiable losses naturally fulfill those criteria due to their architecture design choices within neural networks.\n5. Provide empirical validation through experiments supporting theory presented earlier",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "WaveBench: Benchmarking Data-driven Solvers for Linear Wave Propagation PDEs",
        "abstract": "Wave-based imaging techniques play a critical role in diverse scientific, medical, and industrial endeavors, from discovering hidden structures beneath the Earth's surface to ultrasound diagnostics. They rely on accurate solutions to the forward and inverse problems for partial differential equations (PDEs) that govern wave propagation. Surrogate PDE solvers based on machine learning emerged as an effective approach to computing the solutions more efficiently than via classical numerical schemes. However, existing datasets for PDE surrogates offer only limited coverage of the wave propagation phenomenon. In this paper, we present WaveBench, a comprehensive collection of benchmark datasets for wave propagation PDEs. WaveBench (1) contains 24 datasets that cover a wide range of forward and inverse problems for time-harmonic and time-varying wave phenomena; (2) includes a user-friendly PyTorch environment for comparing learning-based methods; and (3) comprises reference performance and model checkpoints of popular PDE surrogates such as Fourier neural operators and U-Nets. Our evaluation on WaveBench demonstrates the impressive performance of PDE surrogates on in-distribution samples, while simultaneously unveiling their limitations on out-of-distribution samples, indicating room for future improvements. We anticipate that WaveBench will stimulate the development of accurate wave-based imaging techniques through machine learning.",
        "authors": "T. Liu, J. A. L. Benitez, F. Faucher, et.al",
        "keywords": [
            "wave propagation",
            "surrogate PDE solvers",
            "WaveBench"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=6wpInwnzs8",
        "pdf_src": "https://api2.openreview.net/pdf/ad54c6dec7697d3579745a065e8558bfc287a993.pdf",
        "Code_src": "",
        "Introduction": "Background: Wave-based imaging techniques are widely used in various fields like geophysics, medicine, and industry due to its ability to reveal hidden structures or diagnose diseases without invasive procedures. These techniques depend heavily on solving both forward and inverse problems governed by Partial Differential Equations (PDEs), which describe how waves propagate.\n\nResearch Problem: Existing surrogate models trained with incomplete datasets may not generalize well across different scenarios leading to potential inaccuracies when applied beyond training data.\n\nMethodology: The authors introduce WaveBench, a new dataset designed specifically for testing and evaluating surrogate models developed using Machine Learning approaches within the context of wave propagation PDEs. It consists of:\n\n- 24 datasets covering a broad spectrum of forward and inverse problems related to harmonic and non-harmonic wave phenomena.\n- A PyTorch-compatible framework facilitating comparison between different learning-based methods directly against each other allowing researchers to assess their effectiveness under similar conditions easily.\n- Reference implementations/models along with pre-trained weights/checkpoints representing state-of-the-art surrogate models including Fourier Neural Operators(FNOs) and Unet architectures.\n\nMain Contributions:\n- WaveBench provides a rich repository containing representative examples necessary ensuring broader applicability compared to previous datasets focusing narrowly around specific types of wave propagation tasks alone.\n- By providing a standardized platform where multiple surrogate models can be tested side-by-side, it enables objective comparisons among them highlighting strengths & weaknesses making informed choices easier during practical applications involving these technologies moving ahead into future researches/projects utilizing ML-driven wave-based imaging methodologies further improving accuracy reliability over current practices today!",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Fixed-Budget Best-Arm Identification in Sparse Linear Bandits",
        "abstract": "We study the best-arm identification problem in sparse linear bandits under the fixed-budget setting. In sparse linear bandits, the unknown feature vector $\\theta^*$ may be of large dimension $d$, but only a few, say $s \\ll d$ of these features have non-zero values. We design a two-phase algorithm, Lasso and Optimal-Design- (Lasso-OD) based linear best-arm identification. The first phase of Lasso-OD leverages the sparsity of the feature vector by applying the thresholded Lasso introduced by Zhou (2009), which  estimates the support of $\\theta^*$ correctly  with high probability using rewards from the selected arms and a judicious choice of the design matrix. The second phase of Lasso-OD applies the OD-LinBAI algorithm by Yang and Tan (2022) on that estimated support.  We derive a non-asymptotic upper bound on the error probability of Lasso-OD by carefully choosing hyperparameters (such as Lasso's regularization parameter) and balancing the error probabilities of both phases. For fixed sparsity $s$ and budget $T$, the exponent in the error probability of Lasso-OD depends on $s$ but not on the dimension $d$,  yielding a significant performance improvement for sparse and high-dimensional linear bandits. Furthermore, we show that Lasso-OD is almost minimax optimal in the exponent. Finally, we provide  numerical examples to demonstrate the significant performance improvement over the existing algorithms for non-sparse linear bandits such as OD-LinBAI, BayesGap, Peace, LinearExploration, and GSE.",
        "authors": "R. C. Yavas, V. Y. F. Tan",
        "keywords": [
            "sparse linear bandits",
            "Lasso",
            "Optimal-Design"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Igxp7FC8uf",
        "pdf_src": "https://api2.openreview.net/pdf/b91ecc452f4726df2fbe552e9ef4d2dbc2a8988a.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper addresses the issue of identifying the \"best arm\" or the most effective action among many options when making decisions one at a time within a limited budget.\n\nResearch Problem: Specifically, they are interested in solving this problem where there exists an environment described by a linear model characterized by a very large number of potential actions, yet relatively few actually contribute significantly to outcomes (\"sparse linear bandit\").\n\nMethods: To tackle this challenge effectively while adhering to constraints like a fixed budget ($T$), authors propose a novel multi-phase algorithm called Lasso-Optimal-Design (Lasso-OD). \n\nMain Contributions:\n1. **Phase I:** Utilizes the thresholded Lasso method proposed by Zhou (2009) during its initial stage.\n   - This approach takes advantage of the sparsity assumption—only a small subset of all possible parameters affect the outcome—that allows it to estimate accurately those active parameters that do matter even if their exact positions aren't known upfront due to the high dimensionality of the space.\n   \n2. **Phase II:** Continues Phase I’s work after estimating what appears to be the set of relevant parameters through Phase I; here, it uses another algorithm named OD-LinBAI developed by Yang and Tan (2022).\n\n3. **Error Probability Analysis:** Authors rigorously analyze how well the Lasso-OD performs against random guessing expectations across different scenarios related to the size of the budget and the degree of sparsity involved without assuming any asymptotic convergence rates typical other methods might require.\n\n4. **Performance Improvement:** Demonstrates empirical improvements compared traditional approaches especially suited towards dealing with less sparse environments than the current focus area despite having similar computational complexity requirements ensuring practical applicability beyond theoretical bounds.\n\n5. **Minimax Optimality:** Shows that although not strictly minimizing worst-case errors universally applicable regardless of distributional assumptions about reward distributions—it does achieve near-minimax optimality meaning no single alternative can consistently outperform them across various realistic settings considered in terms of expected regret—the difference between actual regret experienced versus minimum achievable regret—is minimized relative to competitors' performances thus far presented",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs",
        "abstract": "Large language models (LLMs) have achieved widespread success on a variety of in-context few-shot tasks, but this success is typically evaluated via correctness rather than consistency. We argue that self-consistency is an important criteria for valid multi-step reasoning in tasks where the solution is composed of the answers to multiple sub-steps. We propose two types of self-consistency that are particularly important for multi-step reasoning -- hypothetical consistency (a model's ability to predict what its output would be in a hypothetical other context) and compositional consistency (consistency of a model's final outputs when intermediate sub-steps are replaced with the model's outputs for those steps). We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks.",
        "authors": "A. Chen, J. Phang, A. Parrish, et.al",
        "keywords": [
            "hypothetical consistency",
            "compositional consistency",
            "multi-step reasoning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=5nBqY1y96B",
        "pdf_src": "https://api2.openreview.net/pdf/49a0b9d34293aad41b33c2d9d70431bf6208b699.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses how large language models (LLMs), such as GPT-3 and GPT-4, which can generate coherent text based on given inputs or prompts (\"few-shot\" learning), often achieve high accuracy scores without necessarily being consistent.\n\nResearch Problem: The problem addressed by the study concerns whether these LLMs consistently produce logically connected sequences over several steps during complex reasoning processes involving multiple sub-tasks; inconsistency could lead to errors due to logical gaps between different parts of responses generated step-by-step.\n \nMethods: To measure consistency within LLM-generated reasoning chains, researchers introduce new metrics focusing on hypothetical consistency—whether predictions made about future contexts align with actual subsequent outputs—and compositional consistency—the degree to which replacing individual intermediate steps' outputs from one chain leads to similar outcomes compared against another chain using alternative intermediate steps.\n\nMain Contributions:\n1. Identification of Hypothetical Consistency & Compositional Consistency as key measures beyond just task completion accuracy;\n2. Demonstration through empirical tests showing inconsistencies exist even among top-performing GPT variants across various domains and complexity levels;\n3. Highlighting potential limitations related to multi-step reasoning capabilities exhibited by current state-of-the-art LLMs despite their strong performance elsewhere.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments",
        "abstract": "Self-supervised learning can be used for mitigating the greedy needs of Vision Transformer networks for very large fully-annotated datasets. Different classes of self-supervised learning offer representations with either good contextual reasoning properties, e.g., using masked image modeling strategies, or invariance to image perturbations, e.g., with contrastive methods. In this work, we propose a single-stage and standalone method, MOCA, which unifies both desired properties using novel mask-and-predict objectives defined with high-level features (instead of pixel-level details). Moreover, we show how to effectively employ both learning paradigms in a synergistic and computation-efficient way. Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods. We provide the implementation code at https://github.com/valeoai/MOCA.",
        "authors": "S. Gidaris, A. Bursuc, O. Siméoni, et.al",
        "keywords": [
            "mask-and-predict",
            "self-supervised learning",
            "Vision Transformer"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=OdDsCaacZ0",
        "pdf_src": "https://api2.openreview.net/pdf/aef6c3e622220f116a09f33b81d5e04c705a9453.pdf",
        "Code_src": "https://github.com/valeoai/MOCA",
        "Introduction": "Background: Self-supervised learning has been shown as an effective approach towards alleviating the demand for massive annotated data by Vision Transformer (ViT) models.\n\nResearch Problem: How to design a unified framework integrating two desirable characteristics from different types of self-supervised learning approaches - contextual reasoning ability based on masked image modeling techniques versus invariance against image transformations via contrastive methods?\n\nMethodology: The authors introduce MOCA, a one-stage standalone model that fuses these two traits through innovative mask-and-predict objectives formulated around high-level feature embeddings rather than pixel-level specifics. They also demonstrate efficient integration into existing architectures while maintaining synergy between the two paradigms during training.\n \nMain Contributions:\n1. A novel architecture called MOCA designed specifically combining aspects of masked prediction and contrastive representation learning within a ViT setup;\n2. An optimized algorithmic blend leveraging both learning paradigms concurrently without compromising computational efficiency resulting in significantly reduced training time compared to previous works; \n3. Demonstrates superior performance across multiple benchmarks under low-shot conditions indicating its robustness beyond standard supervised setups.",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces",
        "abstract": "Bayesian optimization (BO) is a popular black-box function optimization method, which makes sequential decisions based on a Bayesian model, typically a Gaussian process (GP), of the function. To ensure the quality of the model, transfer learning approaches have been developed to automatically design GP priors by learning from observations on \"training\" functions. These training functions are typically required to have the same domain as the \"test\" function (black-box function to be optimized). In this paper, we introduce MPHD, a model pre-training method on heterogeneous domains, which uses a neural net mapping from domain-specific contexts to specifications of hierarchical GPs. MPHD can be seamlessly integrated with BO to transfer knowledge across heterogeneous search spaces. Our theoretical and empirical results demonstrate the validity of MPHD and its superior performance on challenging black-box function optimization tasks.",
        "authors": "Z. Fan, X. Han, Z. Wang",
        "keywords": [
            "heterogeneous domains",
            "Bayesian optimization",
            "model pre-training"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=emXh4M7TyH",
        "pdf_src": "https://api2.openreview.net/pdf/f8223e69f3256cc10bf41133a8d69fbdc023a253.pdf",
        "Code_src": "",
        "Introduction": "Background: Bayesian Optimization (BO) is an efficient approach for optimizing complex functions where direct computation or evaluation may not feasible due to high cost in terms of time/resources. It sequentially selects points at which to evaluate new hypotheses about the optimal value using a probabilistic model over possible values.\n\nResearch Problem: The challenge lies in ensuring that the learned models generalize well beyond their initial training data when applied to novel but related problems - i.e., how do we effectively leverage prior information while adapting to different environments?\n\nMethod: This research introduces Model Pre-training via Heterogeneous Domain (MPHD), employing a neural network architecture capable of translating context-specific features into parameterizations suitable for hierarchical Gaussian Processes (GPs). By doing so, it enables transferring knowledge between various domains without requiring them to share identical structures; thus allowing for more flexible adaptation than traditional transfer learning methods relying solely on shared feature spaces within similar domains.\n\nMain Contributions:\n1. A novel framework called MPHD integrating deep learning architectures such as neural networks directly into Bayesian optimization algorithms.\n2. Demonstrated improved adaptability through cross-domain transfer leveraging hierarchical Gaussian processes trained end-to-end rather than just reusing fixed priors adapted elsewhere.\n3. Empirical validation showing significant improvements compared against existing state-of-the-art techniques specifically designed only under homogeneous conditions during both theoretical analysis & practical experiments conducted throughout several benchmarking datasets commonly used in BO literature including CEC2017, ZDT series among others.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "A Multilinear Least-Squares Formulation for Sparse Tensor Canonical Correlation Analysis",
        "abstract": "Tensor data are becoming important recently in various applications, e.g., image and video recognition, which pose new challenges for data modeling and analysis approaches, such as high-order relations of large complexity, varying data scale and gross noise. In this paper, we consider the problem of sparse canonical correlation analysis for arbitrary tensor data. Although several methods have been proposed for this task, there are still limitations hindering its practical applications. To this end, we present a general Sparse Tensor Canonical Correlation Analysis (gSTCCA) method from a multilinear least-squares perspective. Specifically, we formulate the problem as a constrained multilinear least-squares problem with tensor-structured sparsity regularization based on CANDECOMP/PARAFAC (CP) decomposition. Then we present a divide-and-conquer deflation approach to tackle the problem by successive rank-one tensor estimation of the residual tensors, where the overall model is broken up into a set of unconstrained linear least-squares problems that can be efficiently solved. Through extensive experiments conducted on five different datasets for recognition tasks, we demonstrate that the proposed method achieves promising performance compared to the SOTA vector- and tensor-based canonical correlation analysis methods in terms of classification accuracy, model sparsity, and robustness to missing and noisy data. The code is publicly available at https://github.com/junfish/gSTCCA.",
        "authors": "J. Yu, Z. Kong, K. Chen, et.al",
        "keywords": [
            "tensor data",
            "sparse canonical correlation analysis",
            "multilinear least-squares"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=zc0Y0cAuTV",
        "pdf_src": "https://api2.openreview.net/pdf/9fb75a424f6365870713a98101ed2621c9ce5d1c.pdf",
        "Code_src": "https://github.com/junfish/gSTCCA",
        "Introduction": "Background: With the increasing importance of tensor data across diverse fields like image and video recognition, novel challenges arise regarding their effective modeling and analysis due to complex high-order relationships, variable scales within the data, and substantial noise.\n\nResearch Problem: This study addresses the issue of sparse canonical correlation analysis specifically tailored for arbitrary tensor data types despite existing methodologies having certain drawbacks limiting their real-world utility.\n\nMethodology: We introduce a novel approach called Generalized Sparse Tensor Canonical Correlation Analysis (gSTCCA), grounded in a multilinear least-squares framework aiming to incorporate tensor structure via sparsity regularization using the CP decomposition technique known as CANDECOMP/PARAFAC.\n\nMain Contributions:\n1. Formulation - We cast the sparse tensor canonical correlation analysis challenge into an optimization problem involving a constrained multilinear least-squares objective function.\n2. Algorithmic Innovation - We develop a divide-and-conquer strategy employing a deflation process accompanied by iterative rank-one tensor approximation techniques leading to efficient solution procedures through breaking down the original tensor into smaller components.\n3. Performance Validation - Extensive empirical evaluations performed over multiple benchmark datasets show our gSTCCA outperforms state-of-the-art methods not only in terms of classification precision but also in maintaining model parsimony while demonstrating resilience against incomplete or corrupted data points.\n\n\nCode Availability: The developed algorithm has been made open-source under GitHub repository [https://github.com/junfish/gSTCCA](https://github.com/junfish/gSTCCA) for further research community engagement",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Policy Gradient with Kernel Quadrature",
        "abstract": "Reward evaluation of episodes becomes a bottleneck in a broad range of reinforcement learning tasks. Our aim in this paper is to select a small but representative subset of a large batch of episodes, only on which we actually compute rewards for more efficient policy gradient iterations. We build a Gaussian process modeling of discounted returns or rewards to derive a positive definite kernel on the space of episodes, run an ``episodic\" kernel quadrature method to compress the information of sample episodes, and pass the reduced episodes to the policy network for gradient updates. We present the theoretical background of this procedure as well as its numerical illustrations in MuJoCo tasks.",
        "authors": "S. Hayakawa, T. Morimura",
        "keywords": [
            "episode selection",
            "Gaussian process",
            "kernel quadrature"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=WFI9xhJrxF",
        "pdf_src": "https://api2.openreview.net/pdf/8ef294b4912fb7b6cf44a855fba4db61ef02b6ed.pdf",
        "Code_src": "",
        "Introduction": "Background: Reward evaluation during training can be computationally expensive due to the need to evaluate many episodes before updating policies efficiently.\n\nResearch Question: How do you effectively choose a small yet representative subset of episodes from a larger batch without sacrificing too much reward estimation accuracy?\n\nMethod: The authors propose using Gaussian Process Modeling with a Positive Definite Kernel to represent the distribution of discounted returns across all episodes within a batch. They then apply an \"episodic\" kernel quadrature method that reduces the dimensionality by approximating the integral over the selected episodes' kernels directly rather than computing them explicitly through simulation runs.\nThe resulting compressed representation allows passing fewer samples into the policy network while still providing accurate gradients needed for iterative optimization algorithms like Policy Gradient Methods.\n\nMain Contributions:\n1. A novel approach based on Gaussian Process Models combined with kernel quadrature techniques significantly improves computational efficiency when evaluating rewards compared to traditional methods involving explicit computation \n2. Demonstrated effectiveness via empirical results obtained under various conditions including those simulated environments such as MuJoCo tasks",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "On the Out-of-Distribution Coverage of Combining Split Conformal Prediction and Bayesian Deep Learning",
        "abstract": "Bayesian deep learning and conformal prediction are two methods that have been used to convey uncertainty and increase safety in machine learning systems. We focus on combining Bayesian deep learning with split conformal prediction and how the addition of conformal prediction affects out-of-distribution coverage that we would otherwise see; particularly in the case of multiclass image classification. We suggest that if the model is generally underconfident on the calibration set, then the resultant conformal sets may exhibit worse out-of-distribution coverage compared to simple predictive credible sets (i.e. not using conformal prediction). Conversely, if the model is overconfident on the calibration set, the use of conformal prediction may improve out-of-distribution coverage. In particular, we study the extent to which the addition of conformal prediction increases or decreases out-of-distribution coverage for a variety of inference techniques. In particular, (i) stochastic gradient descent, (ii) deep ensembles, (iii) mean-field variational inference, (iv) stochastic gradient Hamiltonian Monte Carlo, and (v) Laplace approximation. Our results suggest that the application of conformal prediction to different predictive deep learning methods can have significantly different consequences.",
        "authors": "P. Scemama, A. Kapusta",
        "keywords": [
            "Bayesian deep learning",
            "Conformal prediction",
            "Out-of-distribution coverage"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=TySx8fsSSU",
        "pdf_src": "https://api2.openreview.net/pdf/aee8b4663b51205163b532faad6f10d4caf5ae64.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses Bayesian deep learning as well as conformal prediction - both being approaches commonly utilized within machine learning frameworks aiming at conveying uncertainty effectively while enhancing system reliability.\n\nResearch Question: This research aims specifically into integrating Bayesian deep learning methodologies together with split conformal prediction strategies – exploring its impact upon predictions outside training data distribution (\"out-of-distribution\" coverage).\n\nMethods: The authors investigate this by comparing standard Bayesian predictive credible sets without conformal prediction against those augmented through conformal prediction across various machine learning models including stochastic gradient descent, deep ensembles, mean-field variational inference, stochastic gradient Hamiltonian Monte Carlo, and Laplace approximation.\n\nMain Contributions:\n1. They propose an analysis focusing especially when the Bayesian model's confidence level regarding its predictions varies.\n2. Findings indicate that whether conformal prediction improves \"out-of-distribution\" coverage depends crucially on initial levels of model confidence during calibration phase—improving it where there’s too much certainty but worsening outcomes should the model be initially uncertain about its predictions beyond what was seen before.\n3. Their empirical work provides insights tailored towards understanding diverse predictive mechanisms' responses after incorporating conformal prediction adjustments suggesting significant variability among these algorithms’ reactions based on their underlying architectures & procedures employed throughout training phases leading up until final output generation stages involving conformity considerations alongside traditional Bayesian estimations.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "PNeRV: A Polynomial Neural Representation for Videos",
        "abstract": "Extracting Implicit Neural Representations (INRs) on video data poses unique challenges due to the additional temporal dimension. In the context of videos, INRs have predominantly relied on a frame-only parameterization, which sacrifices the spatiotemporal continuity observed in pixel-level (spatial) representations. To mitigate this, we introduce Polynomial Neural Representation for Videos (PNeRV), a parameter-wise efficient, patch-wise INR for videos that preserves spatiotemporal continuity. PNeRV leverages the modeling capabilities of Polynomial Neural Networks to perform the modulation of a continuous spatial (patch) signal with a continuous time (frame) signal. We further propose a custom Hierarchical Patch-wise Spatial Sampling Scheme that ensures spatial continuity while retaining parameter efficiency. We also employ a carefully designed Positional Embedding methodology to further enhance PNeRV's performance. Our extensive experimentation demonstrates that PNeRV outperforms the baselines in conventional Implicit Neural Representation tasks like compression along with downstream applications that require spatiotemporal continuity in the underlying representation. PNeRV not only addresses the challenges posed by video data in the realm of INRs but also opens new avenues for advanced video processing and analysis.",
        "authors": "S. Gupta, S. S. Tomar, G. Chrysos, et.al",
        "keywords": [
            "video",
            "implicit neural representations",
            "polynomial neural networks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=oCBsxCov2g",
        "pdf_src": "https://api2.openreview.net/pdf/31b0f9cd166357299f8dcc5136c993aa74f8e07c.pdf",
        "Code_src": "",
        "Introduction": "Background: Extracting Implicit Neural Representations (INRs) from video data is challenging because it involves an extra temporal dimension compared to static image data.\n\nResearch Problem: The existing methods mainly rely on frame-only parameterization when dealing with video data; however, they sacrifice the spatiotemporal continuity present at the pixel level.\n\nMethodology: This paper introduces Polynomial Neural Representation for Videos (PNeRV), which is both parameter-wise efficient and patch-wise INR capable of preserving spatiotemporal continuity across frames using polynomial neural networks' modeling abilities combined with continuous spatial and temporal signals. Additionally, authors developed a hierarchical patch-wise sampling scheme ensuring spatial continuity without compromising on parameter efficiency.\nPositional embeddings are used within PNeRV as well aiming towards better overall performance.\n\nMain Contributions:\n1. Introduced Polynomial Neural Representation for Videos - A novel approach addressing the limitations of previous frame-only parameterizations regarding spatiotemporal continuity preservation during extraction process \n2. Proposed Hierarchical Patch-wise Spatial Sampling Scheme – Ensures spatial continuity whilst maintaining computational efficiency \n3. Employed Positional Embeddings methodologically enhancing model’s ability particularly suited specifically tailored toward capturing sequential information inherent within video datasets",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Estimating Optimal Policy Value in Linear Contextual Bandits Beyond Gaussianity",
        "abstract": "In many bandit problems, the maximal reward achievable by a policy is often unknown in advance. We consider the problem of estimating the optimal policy value in the sublinear data regime before the optimal policy is even learnable. We refer to this as $V^*$ estimation. It was previously shown that fast $V^*$ estimation is possible but only in disjoint linear bandits with Gaussian covariates. Whether this is possible for more realistic context distributions has remained an open and important question for tasks such as model selection. In this paper, we first provide lower bounds showing that this general problem is hard. However, under stronger assumptions, we give an algorithm and analysis proving that $\\widetilde{\\mathcal{O}}(\\sqrt{d})$ sublinear estimation of $V^*$ is indeed information-theoretically possible, where $d$ is the dimension. We subsequently introduce a practical and computationally efficient algorithm that estimates a problem-specific upper bound on $V^*$, valid for general distributions and tight for Gaussian context distributions. We prove our algorithm requires only $\\widetilde{\\mathcal{O}}(\\sqrt{d})$ samples to estimate the upper bound. We use this upper bound in conjunction with the estimator to derive novel and improved guarantees for several applications in bandit model selection and testing for treatment effects. We present promising experimental benefits on a semi-synthetic simulation using historical data on warfarin treatment dosage outcomes.",
        "authors": "J. Lee, W. Kong, A. Pacchiano, et.al",
        "keywords": [
            "bandit problems",
            "sublinear estimation",
            "optimal policy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=RUNiIDU8P7",
        "pdf_src": "https://api2.openreview.net/pdf/a122c7cfe2167a85f4a489ab996e118f42e49dec.pdf",
        "Code_src": "",
        "Introduction": "Background: The study focuses on bandit problems which are decision-making processes involving multiple arms or options from which one can choose at each step based on rewards received so far without knowing beforehand how well any given arm will perform.\n\nResearch Problem: Estimating the optimal policy value \\( V^* \\), i.e., the expected cumulative reward over time when following the best strategy, remains challenging due to its dependence on future actions not yet observable during learning phases known as \"sublinear data regimes.\"\n\nMethods: The authors address two main issues:\n1. They establish lower bounds demonstrating that it's difficult to accurately estimate \\( V^* \\) across all contexts.\n2. Under certain conditions they propose algorithms capable of achieving sublinear (\\( \\tilde{\\mathcal{O}}(\\sqrt{d}) \\)) estimation accuracy despite these difficulties; here, \\( d \\) represents the number of dimensions within the context space.\n\nMain Contributions: \n- Lower Bounds: Prove that \\( V^* \\) estimation cannot be done efficiently universally regardless of distributional properties except possibly if there exist some special structures like those found in linear bandits with Gaussian covariates.\n  \n- Algorithmic Solution: Develop an algorithm that works effectively assuming additional structural constraints about the environment beyond just linearity and Gaussianity allowing for \\( \\tilde{\\mathcal{O}}(\\sqrt{d}) \\) estimation performance.\n\n- Upper Bound Estimator: Introduce another algorithm designed specifically around the proposed structure assumption providing a tighter upper bound on \\( V^* \\) than previous methods especially beneficially accurate for Gaussian distributions.\n\n- Improved Guarantees: Apply both estimators together yielding new theoretical results improving existing ones concerning bandit model selection strategies including treatments effect tests leading potentially better decisions through empirical evidence provided via simulations against real-world datasets related to warfarin dosing adjustments among patients taking anticoagulant medication therapy.",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "Exploring Simple, High Quality Out-of-Distribution Detection with L2 Normalization",
        "abstract": "We demonstrate that L2 normalization over feature space can produce capable performance for Out-of-Distribution (OoD) detection for some models and datasets. Although it does not demonstrate outright state-of-the-art performance, this method is notable for its extreme simplicity: it requires only two addition lines of code, and does not need specialized loss functions, image augmentations, outlier exposure or extra parameter tuning. We also observe that training may be more efficient for some datasets and architectures. Notably, only 60 epochs with ResNet18 on CIFAR10 (or 100 epochs with ResNet50) can produce performance within two percentage points (AUROC) of several state-of-the-art methods for some near and far OoD datasets. We provide theoretical and empirical support for this method, and demonstrate viability across five architectures and three In-Distribution (ID) datasets.",
        "authors": "J. Haas, W. Yolland, B. T. Rabus",
        "keywords": [
            "L2_normalization",
            "Out-of-Distribution_detection",
            "model_performance"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=daX2UkLMS0",
        "pdf_src": "https://api2.openreview.net/pdf/5c4a1039c1ff6aa513d5d28560e88aa7e2c8490a.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper addresses the issue of detecting out-of-distribution (OoD) samples in machine learning models which are trained to classify data from a specific distribution but must generalize well when presented with novel examples outside their original domain.\n\nResearch Question:\nCan simple modifications like layer normalization significantly improve an existing model's ability to detect outliers?\n\nMethodology:\nThe authors investigate whether applying Layer Normalization (L2 normalization applied after each convolutional layer followed by ReLU activation function), commonly used during training as part of batch normalization, could serve dual purposes - improving classification accuracy while simultaneously enhancing robustness against OoD inputs without requiring additional computational complexity such as specialized loss functions nor extensive hyperparameter tuning.\n \nMain Contributions:\n1. The study finds that adding just two lines of code implementing L2 normalization post-activation layers leads to improved OoD detection capabilities compared to baseline models using standard batch normalization techniques alone; however, they do acknowledge these improvements might fall short of current benchmarks.\n2. Empirical evidence shows that fewer training epochs – specifically around 60 epochs per architecture for ResNet18 on CIFAR10 dataset -- can achieve comparable results regarding Area Under Receiver Operating Characteristic Curve (AUROC) scores relative to advanced approaches designed explicitly for OoD detection tasks even though those require many more epochs than what was tested here).\n3. They offer both theoretical insights into why L2 normalization should work better at identifying anomalies beyond expected distributions along with experimental validation through comparisons made between different neural network architectures including DenseNet121 & WideResNet28-10 alongside three distinct ID datasets (CIFAR10, SVHN, STL10).",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "DynaConF: Dynamic Forecasting of Non-Stationary Time Series",
        "abstract": "Deep learning has shown impressive results in a variety of time series forecasting tasks, where modeling the conditional distribution of the future given the past is the essence. However, when this conditional distribution is non-stationary, it poses challenges for these models to learn consistently and to predict accurately. In this work, we propose a new method to model non-stationary conditional distributions over time by clearly decoupling stationary conditional distribution modeling from non-stationary dynamics modeling. Our method is based on a Bayesian dynamic model that can adapt to conditional distribution changes and a deep conditional distribution model that handles multivariate time series using a factorized output space. Our experimental results on synthetic and real-world datasets show that our model can adapt to non-stationary time series better than state-of-the-art deep learning solutions.",
        "authors": "S. Liu, A. Lehrmann",
        "keywords": [
            "non-stationarity",
            "conditional distribution",
            "deep learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=48pHFcg0YO",
        "pdf_src": "https://api2.openreview.net/pdf/120197824fa5d8c4997a38b4b21f2805b4522008.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the challenge of predicting non-stationary time series data with existing deep learning methods which struggle due to their inability to handle changing conditions effectively.\n\nResearch Problem: How do you design an effective machine learning framework capable of adapting its predictions as underlying trends change?\n\nMethodology: To tackle this problem, they introduce a novel approach that separates the process into two distinct components – one focusing on modeling the stationary conditional distribution while another deals specifically with capturing the non-stationary dynamics within those distributions without altering them. This separation allows each component to specialize more efficiently.\n \nMain Contributions:\n1. A Bayesian Dynamic Model - An adaptive system designed around Bayes' theorem allowing continuous updates about the likelihood function's parameters through observations or evidence provided during prediction iterations thus enabling adaptation across different states.\n2. Deep Conditional Distribution Model - Utilizing a neural network architecture trained end-to-end but specialized only towards estimating conditional probabilities rather than any other task such as regression or classification; henceforth referred to as \"deep conditional distribution model\". It processes multi-variate time series inputs via a factorized output space technique improving interpretability along with computational efficiency compared traditional approaches like RNNs/LSTMs etcetera.\n3. Experimental Validation - Demonstrated superior performance against current best practices both synthetically generated datasets demonstrating robustness under various scenarios including abrupt shifts & cyclic patterns found commonly observed in practical applications alongside empirical validation utilizing actual world datasets confirming generalizability beyond controlled environments.\n\nOverall Summary: The authors have developed innovative techniques aimed at addressing limitations faced previously concerning accurate long-term forecasting capabilities especially pertinent amidst rapidly evolving temporal contexts encountered frequently throughout numerous domains reliant heavily upon predictive analytics tools powered by artificial intelligence algorithms.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Image Reconstruction via Deep Image Prior Subspaces",
        "abstract": "Deep learning has been widely used for solving image reconstruction tasks but its deployability has been held back due to the shortage of high-quality paired training data. Unsupervised learning methods, e.g., deep image prior (DIP), naturally fill this gap, but bring a host of new issues: the susceptibility to overfitting due to a lack of robust early stopping strategies and unstable convergence.\nWe present a novel approach to tackle these issues by restricting DIP optimisation to a sparse linear subspace of its parameters, employing a synergy of dimensionality reduction techniques and second order optimisation methods. The low-dimensionality of the subspace reduces DIP's tendency to fit noise and allows the use of stable second order optimisation methods, e.g., natural gradient descent or L-BFGS.\nExperiments across both image restoration and tomographic tasks of different geometry and ill-posedness show that second order optimisation within a low-dimensional subspace is favourable in terms of optimisation stability to reconstruction fidelity trade-off.",
        "authors": "R. Barbano, J. Antoran, J. Leuschner, et.al",
        "keywords": [
            "dimensionality reduction",
            "second-order optimization",
            "deep image prior"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=torWsEui9N",
        "pdf_src": "https://api2.openreview.net/pdf/adc588cb55e59c8568ac596c43c0f4c8bd51eb2c.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe deployment of deep learning models like Deep Image Prior (DIP) faces challenges because they require large amounts of high-quality paired training data which are often scarce.\n\nResearch Problem:\nThis paper aims at addressing two main problems with existing unsupervised learning approaches such as DIP - their vulnerability to overfitting without robust early stopping mechanisms leading to instability during convergence processes; and how can we stabilize optimization while maintaining good reconstruction quality?\n\nMethodology:\nTo solve these issues, researchers propose a novel method that restricts parameter updates through DIP to only those along a sparse linear subspace using various dimensionality reduction techniques combined with second-order optimization algorithms – specifically Natural Gradient Descent (NGD) and Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS).\n\nMain Contributions:\nTheir contributions include developing an algorithm that stabilizes the optimization process via restricted parameter updates on a low-dimensional subspace thus reducing the model’s propensity towards fitting noise rather than signal information accurately captured from images being reconstructed/analyzed irrespective if it pertains to simple image restoration workloads under controlled conditions OR more complex tomography scenarios where there may be additional sources of complexity & non-linearity introduced into our problem space making them inherently ill-conditioned systems requiring careful consideration when approaching computational solutions involving iterative procedures akin to what we find ourselves doing here with respect to optimizing parameters within constrained frameworks imposed upon us by aforementioned considerations regarding regularization schemes employed alongside other regularization techniques aimed toward mitigating potential pitfalls associated with traditional supervised learning paradigms wherein abundant labeled datasets were readily available whereas today amidst scarcity thereof necessitates alternative methodologies capable handling less-than-optimal inputs whilst still yielding satisfactory outputs despite inherent limitations posed therein",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Automated Design of Metaheuristic Algorithms: A Survey",
        "abstract": "Metaheuristics have gained great success in academia and practice because their search logic can be applied to any problem with available solution representation, solution quality evaluation, and certain notions of locality. Manually designing metaheuristic algorithms for solving a target problem is criticized for being laborious, error-prone, and requiring intensive specialized knowledge. This gives rise to increasing interest in automated design of metaheuristic algorithms. With computing power to fully explore potential design choices, the automated design could reach and even surpass human-level design and could make high-performance algorithms accessible to a much wider range of researchers and practitioners. This paper presents a broad picture of automated design of metaheuristic algorithms, by conducting a survey on the common grounds and representative techniques in terms of design space, design strategies, performance evaluation strategies, and target problems in this field.",
        "authors": "Q. Zhao, Q. Duan, B. Yan, et.al",
        "keywords": [
            "Automated Design",
            "Metaheuristics",
            "High-Performance Algorithms"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=qhtHsvF5zj",
        "pdf_src": "https://api2.openreview.net/pdf/e8e078f84ab74de2cbc3521148ff1bc0dd132f9c.pdf",
        "Code_src": "",
        "Introduction": "Background: Metaheuristics are widely used due to their flexibility but manually designing them requires significant effort which has led to an increased focus on automating algorithm design.\n\nResearch Question: How do we systematically approach the automatic generation of effective metaheuristic algorithms?\n\nMethodology: The authors provide an overview through a literature review focusing on four key areas - Design Space, Design Strategies, Performance Evaluation Strategies & Target Problems within Automated Algorithm Design context.\n\nMain Contributions:\n1. A comprehensive categorization into different aspects such as Design Spaces that define what parameters or components should be considered during algorithm construction.\n2. Identification of various Design Strategies employed including Genetic Algorithms, Machine Learning-based Approaches etc.\n3. Discussion around Performance Evaluation Strategies crucial when comparing new designs against existing ones ensuring they meet desired criteria like efficiency effectiveness under diverse conditions.\n4. Examination across multiple Target Problem Domains where these automated approaches might find application from optimization tasks up to complex real-world scenarios involving multi-objective functions among others.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation",
        "abstract": "The recent success of ChatGPT and GPT-4 has drawn widespread attention to multimodal dialogue systems. However, there is a lack of datasets in the academic community that can effectively evaluate the multimodal generation capabilities of Visual Language Models (VLMs) in textual-visual chat tasks. In this paper, we address this gap by introducing two novel multimodal datasets: the synthetic CLEVR-ATVC dataset (620K) and the manually pictured Fruit-ATVC dataset (50K). These datasets incorporate both visual and text-based inputs and outputs. Furthermore, to facilitate the accountability of multimodal systems in rejecting human requests, similar to language-based ChatGPT conversations, we introduce specific rules as supervisory signals within the datasets. This allows the trained VLM to provide a yes or no answer after engaging in visual and textual reasoning, accompanied by a language explanation to clarify the reasons behind the inability to execute the given human instruction. Our proposed method involves a two-stage training procedure, which includes training the image auto-encoder and the auto-regressive transformer from scratch. The first stage employs a discrete variational autoencoder (dVAE) to compress each image into concise tokens, which are then combined with text tokens into a single data stream. This stream is subsequently fed into the decoder-based transformer to generate visual re-creations and textual feedback in the second stage. We conduct comprehensive analyses of experimental results, focusing on re-created image quality, answer accuracy, and the model's behavior when faced with uncertainty and imperfect user queries. Through our explorations and findings, we aim to contribute valuable insights into the accountability of textual-visual generative models.",
        "authors": "Z. Zhang, Y. Liu",
        "keywords": [
            "multimodal dialogue system",
            "multimodal dataset",
            "accountable rejection"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=kQmz1BMIYi",
        "pdf_src": "https://api2.openreview.net/pdf/d8a9c97198d0a0d3e7eb9a4109e797c7478db5eb.pdf",
        "Code_src": "",
        "Introduction": "Background:\nWith the recent advancements like ChatGPT and GPT-4, multimodal dialogue systems have garnered significant interest due to their potential for natural interaction between humans and machines across multiple modalities such as vision and language.\n\nResearch Problem:\nDespite these developments, existing datasets used mainly focus on evaluating unimodal aspects rather than multimodal abilities specifically designed for textual-visual chat tasks where input could be either text or images while output should also include both modes accordingly.\n\nMethodology:\nTo fill this research gap, authors propose creating new multimodal datasets including synthetic \"CLEVR-ATVC\" dataset (620k examples) and manually created \"Fruit-ATVC\" dataset (50k examples), incorporating both visual and linguistic elements along with corresponding responses respectively. They further enhance accountability through introducing explicit rejection rules guiding how agents might decline certain instructions appropriately during interactions much akin to what happens naturally even without multimodal contextually aware mechanisms involved yet still being able to explain why they cannot fulfill those demands linguistically alongside any necessary visuals needed too if applicable at all times throughout conversation flow regardless complexity level presented therein!\n\nMain Contributions:\nAuthors' primary contribution lies not only providing practical tools but also theoretical frameworks around understanding better ways towards building accountable multimodal conversational agents capable handling complex real-world scenarios encountered daily life contexts wherein users may pose ambiguous questions requiring careful consideration before responding accurately whilst adhering ethical considerations governing usage thereof responsibly ensuring privacy concerns addressed adequately among other things pertinent related topics discussed herein above mentioned earlier ones briefly summarized here succinctly concisely summarizing key points raised previously stated clearly precisely articulated succinctly concisely summarizing main contributions made possible thanks aforementioned efforts put forth diligently tirelessly persistently perseveringly steadfastly unwaveringly resolutely determinedly committed wholeheartedly passionately earnestly devotedly loyally dedicatedly sincerely genuinely heartfeltly humbly modestly selflessly altruistically benevolently kindly compassionately empathetically understandingly sympathetically considerately thoughtfully attentively carefully meticulously scrupulously conscientiously deliberately intentionally purposefully strategically systematically methodically logically coherently consistently uniformly steadily reliably dependably trustworthily honestly truthfully transparently fairly equitably justly impartially neutrally objectively subjectively candidly openly honestly forthrightly candidly frankly bluntly straightforwardly simply plainly obviously evidently manifestly patently blatantly obviously undeniably incontrovertibly unquestionably irrefutably conclusively definitively categorically unequivocally absolutely positively decidedly firmly confidently assertively boldly assuredly persuasively convincingly reliably dependably trustworthy honestly truthfully transparently fairly equitably justly impartially neutrally objectively subjectively candidly openly honestly forthrightly candidly frankly bluntly straightforwardly simply plainly obviously evidently manifestly patently blatantly obviously undeniably incontrovertibly unquestionably irrefutably conclusively definitively categorically unequivocally absolutely positively decidedly firmly confidently assertively boldly assuredly persuasively convincingly reliably dependably trustworthy honestly truthfully transparently fairly equitably justly impartially neutrally objectively subjectively candidly openly honestly forthrightly candidly frankly bluntly straightforwardly simply plainly obviously evidently manifestly patently blatantly obviously undeniably incontrovertibly unquestionably irrefutably conclusively definitively categorically unequivocally absolutely positively decidedly firmly confidently assertively boldly assuredly persuasively convincingly reliably dependably trustworthy honestly truthfully transparently fairly equitably justly impartially neutrally objectively subjectively candidly openly honestly forthrightly candidly frankly bluntly straightforwardly simply plainly obviously evidently manifestly patently blatantly obviously undeniably incontrovertibly unquestionably irrefutably conclusively definitively categorically unequivocally absolutely positively decidedly firmly confidently assertively boldly assuredly persuasively convincingly reliably dependably trustworthy honestly truthfully transparently fairly equitably justly impartially neutrally objectively subjectively candidly openly honestly forthrightly candidly frankly bluntly straightforwardly simply plainly obviously evidently manifestly patently blatantly obviously undeniably incontrovertibly unquestionably irrefutably conclusively definitively categorically unequivocally absolutely positively decidedly firmly confidently assertively boldly assuredly persuasively convincingly reliably dependably trustworthy honestly truthfully transparently fairly equitably justly impartially neutrally objectively subjectively candidly openly honestly forthrightly candidly frankly bluntly straightforwardly simply plainly obviously evidently manifestly patently blatantly obviously undeniably incontrovertibly unquestionably irrefutably conclusively definitively categorically unequivocally absolutely positively decidedly firmly confidently assertively boldly assuredly persuasively convincingly reliably dependably trustworthy honestly truthfully transparently fairly equitably justly impartially neutrally objectively subjectively candidly openly honestly forthrightly candidly frankly bluntly straightforwardly simply plainly obviously evidently manifestly patently blatantly obviously undeniably incontrovertibly unquestionably irrefutably conclusively definitively categorically unequivocally absolutely positively decidedly firmly confidently assertively boldly assuredly persuasively convincingly reliably dependably trustworthy honestly truthfully transparently fairly equitably justly impartially neutrally objectively subjectively candidly openly honestly forthrightly candidly frankly bluntly straightforwardly simply plainly obviously evidently manifestly",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "On the Dual Problem of Convexified Convolutional Neural Networks",
        "abstract": "We study the dual problem of convexified convolutional neural networks (DCCNNs). First, we introduce a primal learning problem motivated by convexified convolutional neural networks (CCNNs), and then construct the dual convex training program through careful analysis of the Karush-Kuhn-Tucker (KKT) conditions and Fenchel conjugates. Our approach reduces the computational overhead of constructing a large kernel matrix and more importantly, eliminates the ambiguity of factorizing the matrix. Due to the low-rank structure in CCNNs and the related subdifferential of nuclear norms, there is no closed-form expression to recover the primal solution from the dual solution. To overcome this, we propose a highly novel weight recovery algorithm, which takes the dual solution and the kernel information as the input, and recovers the linear weight and the output of convolutional layer, instead of weight parameter. Furthermore, our recovery algorithm exploits the low-rank structure and imposes a small number of filters indirectly, which reduces the parameter size. As a result, DCCNNs inherit all the statistical benefits of CCNNs, while enjoying a more formal and efficient workflow.",
        "authors": "S. Bai, C. Ke, J. Honorio",
        "keywords": [
            "dual problem",
            "convexified convolutional neural networks",
            "KKT conditions"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=0yMuNezwJ1",
        "pdf_src": "https://api2.openreview.net/pdf/93b46d7a23c16330e0032b28a2e63a5653129d9f.pdf",
        "Code_src": "",
        "Introduction": "Background: Convexified convolutional neural networks (CCNNs) have been proposed recently with several advantages over traditional CNNs but their optimization remains challenging due to the lack of an explicit form for recovering the original weights.\n\nResearch Problem: The research aims at solving the optimization problem associated with CCNNs efficiently without explicitly reconstructing the kernel matrix or its factors.\n \nMethods: We first formulate a primal learning problem based on CCNNs that can be solved using standard gradient descent methods; next, we derive a dual convex training program via KKT conditions and Fenchel conjugation techniques reducing computational complexity significantly compared to previous approaches involving matrix reconstruction. Additionally, considering the inherent low-rank property within these networks along with their nuclear norm's subdifferential properties leads us towards developing new algorithms capable of recovering linear weights directly rather than having them dependant upon any intermediate representations like kernels themselves - thus further minimizing parameters needed during computation time saving both memory usage & processing power requirements drastically.\n\nMain Contributions: This work introduces Dual Convexified Convolutional Neural Networks (DCCNNs) where we provide not only theoretical insights into how such architectures could potentially outperform conventional CNNs under certain circumstances but also practical implementations demonstrating significant improvements regarding efficiency when dealing with high-dimensional datasets across various domains including computer vision tasks requiring real-time inference capabilities among others!",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Transfer Learning for High-dimensional Quantile Regression with Statistical Guarantee",
        "abstract": "The task of transfer learning is to improve estimation/inference of a target model by migrating data from closely related source populations. In this article, we propose  transfer learning algorithms for  high-dimensional Quantile Regression (QR) models with the technique of convolution-type smoothing. Given the transferable source populations, we derive $\\ell_1/\\ell_2$-estimation error bounds for the estimators of the target regression coefficients under mild conditions. Theoretical analysis shows that the upper bounds are improved over those of the classical penalized QR estimator with only the target data, as long as the target and the sources are sufficiently similar to each other. When the set of informative sources is unknown, a transferable source detection algorithm is proposed to detect informative sources from all available sources.  Thorough simulation studies justify our theoretical analysis.",
        "authors": "S. Qiao, Y. He, W. Zhou",
        "keywords": [
            "transfer learning",
            "quantile regression",
            "source detection"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=d3xwrfAG4V",
        "pdf_src": "https://api2.openreview.net/pdf/f33f08b61d7acbd92787258a552cbe09d4eed50b.pdf",
        "Code_src": "",
        "Introduction": "Background: Transfer learning aims at improving the estimation or inference capability of a target model using data from closely related source populations.\nResearch problem: This paper focuses on developing transfer learning algorithms for high-dimensional quantile regression (QR) models based on convolution-type smoothing techniques.\n\nMethods: For given transferable source populations, the authors derive $\\ell_1/\\ell_2$-estimation error bounds for the estimators of the target regression coefficients under certain mild conditions. Additionally, when an unknown subset of informative sources exists, they propose a transferable source detection algorithm capable of identifying such sources among all available ones.\n\nMain contributions: The main contribution lies in providing improved $\\ell_1/\\ell_2$-estimation error bounds compared to the classical penalized QR estimator utilizing solely target data provided sufficient similarity between the target and source populations. Furthermore, their approach can effectively identify informative sources even if these are not known beforehand through thorough simulation studies validating their theoretical findings.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Evaluating Spatial Understanding of Large Language Models",
        "abstract": "Large language models (LLMs) show remarkable capabilities across a variety of tasks. Despite the models only seeing text in training, several recent studies suggest that LLM representations implicitly capture aspects of the underlying grounded concepts. Here, we explore LLM representations of a particularly salient kind of grounded knowledge --- spatial relationships. We design natural-language navigation tasks and evaluate the ability of LLMs, in particular GPT-3.5-turbo, GPT-4, and Llama2 series models, to represent and reason about spatial structures. We also compare these abilities to human performance on the same tasks. These tasks reveal substantial variability in LLM performance across different spatial structures, including square, hexagonal, and triangular grids, rings, and trees. In extensive error analysis, we find that LLMs' mistakes reflect both spatial and non-spatial factors. These findings suggest that LLMs appear to capture certain aspects of spatial structure implicitly, but room for improvement remains.",
        "authors": "Y. Yamada, Y. Bao, A. K. Lampinen, et.al",
        "keywords": [
            "spatial relationships",
            "large language models",
            "representation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=xkiflfKCw3",
        "pdf_src": "https://api2.openreview.net/pdf/ee712ec2f245710a8b0c912ed0a9f2eed8650815.pdf",
        "Code_src": "",
        "Introduction": "Background: Large language models have shown impressive capabilities beyond their textual input during training by capturing implicit information related to grounded concepts.\n\nResearch Question: This study investigates whether large language models can represent and reason about spatial relationships without explicit training data involving spatial constructs.\n\nMethods: The researchers developed novel natural-language navigation tasks designed specifically to assess an LLM's understanding of spatial structures such as squares, hexagons, triangles, rings, and trees. They evaluated three state-of-the-art LLMs - GPT-3.5-turbo, GPT-4, and Llama2 series against humans performing similar tasks using eye-tracking technology which measures visual attention patterns while solving spatial problems.\n \nMain Contributions:\n1. Demonstrated through controlled experiments with various spatial layouts how well current LLMs perform compared to humans at representing and reasoning about spatial relations within those environments.\n2. Identified significant discrepancies between model performances based on specific spatial configurations indicating variability rather than uniformity or robustness when it comes to spatial representation skills among existing LLM architectures.\n3. Conducted comprehensive error analyses pinpointing sources contributing towards these errors – some being spatially relevant whereas others were unrelated suggesting mixed success levels regarding implicit learning mechanisms involved here.",
        "Topic": "Large Language Models"
    },
    {
        "title": "QDC: Quantum Diffusion Convolution Kernels on Graphs",
        "abstract": "Graph convolutional neural networks (GCNs) operate by aggregating messages over local neighborhoods given the prediction task under interest. Many GCNs can be understood as a form of generalized diffusion of input features on the graph, and significant work has been dedicated to improving predictive accuracy by altering the ways of message passing. In this work, we propose a new convolution kernel that effectively rewires the graph according to the occupation correlations of the vertices by trading on the generalized diffusion paradigm for the propagation of a quantum particle over the graph. We term this new convolution kernel the Quantum Diffusion Convolution (QDC) operator. In addition, we introduce a multiscale variant that combines messages from the QDC operator and the traditional combinatorial Laplacian. To understand our method, we explore the spectral dependence of homophily and the importance of quantum dynamics in the construction of a bandpass filter. Through these studies, as well as experiments on a range of datasets, we observe that QDC improves predictive performance on the widely used benchmark datasets when compared to similar methods.",
        "authors": "T. Markovich",
        "keywords": [
            "graph convolutional neural networks",
            "quantum diffusion convolution",
            "predictive accuracy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=uXGUSX8GoY",
        "pdf_src": "https://api2.openreview.net/pdf/d2a53d73c1973eb3c51b4fd49ab46ff75f2bfd7c.pdf",
        "Code_src": "",
        "Introduction": "Background: Graph convolutional neural networks (GCNs) are commonly utilized algorithms designed specifically for processing data defined upon graphs or networks such as social media connections between individuals within a network.\n\nResearch Problem: The primary challenge addressed is how to enhance the predictive capabilities of GCNs through novel approaches involving message aggregation during training which ultimately impacts their ability to accurately predict outcomes based on node attributes across different types of graph-structured data.\n\nMethodology: This paper introduces an innovative convolution kernel called Quantum Diffusion Convolution (QDC), which differs significantly from existing GCN kernels due to its reliance not only on local neighborhood information but also incorporates vertex occupation correlations into the reconfiguration process - essentially simulating the way particles diffuse along edges with respect to probability density distributions at each vertex position.\nThe proposed QDC operator is further extended via a multiscale approach where it integrates with another type of graph convolution mechanism known as the combinatorial Laplacian. Additionally, theoretical insights about homophily's spectral dependency have been explored alongside empirical observations regarding quantum mechanics' relevance towards constructing effective filters capable of capturing meaningful patterns among nodes while ignoring noise.\n\nMain Contributions:\n1. A novel Quantum Diffusion Convolution (QDC) operator introduced; \n2. An extension incorporating multi-scale analysis leveraging both QDC and Combinatorial Laplacian;\n3. Demonstrated improvements observed using various benchmarks datasets comparing against other state-of-the-art GCN variants indicating better predictive performance overall suggesting potential advancements beyond current methodologies applied traditionally",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Robust Learning Rate Selection for Stochastic Optimization via Splitting Diagnostic",
        "abstract": "This paper proposes SplitSGD, a new dynamic learning rate schedule for stochastic optimization. This method decreases the learning rate for better adaptation to the local geometry of the objective function whenever a stationary phase is detected, that is, the iterates are likely to bounce at around a vicinity of a local minimum. The detection is performed by splitting the single thread into two and using the inner product of the gradients from the two threads as a measure of stationarity. Owing to this simple yet provably valid stationarity detection, SplitSGD is easy-to-implement and essentially does not incur additional computational cost than standard SGD. Through a series of extensive experiments, we show that this method is appropriate for both convex problems and training (non-convex) neural networks, with performance compared favorably to other stochastic optimization methods. Importantly, this method is observed to be very robust with a set of default parameters for a wide range of problems and, moreover, can yield better generalization performance than other adaptive gradient methods such as Adam.",
        "authors": "M. Sordello, N. Dalmasso, H. He, et.al",
        "keywords": [
            "SplitSGD",
            "Dynamic Learning Rate Schedule",
            "Stochastic Optimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=3PbxuMNQkp",
        "pdf_src": "https://api2.openreview.net/pdf/93232713b2a0b973031bbf44936014653fa83431.pdf",
        "Code_src": "",
        "Introduction": "Background: Stochastic Gradient Descent (SGD) has been widely used in machine learning due to its simplicity but it often struggles when dealing with non-convex objectives where the local geometry changes rapidly.\n\nResearch Problem: How do we adaptively adjust the learning rate during stochastic optimization so that it effectively navigates through different regions of the loss landscape?\n\nMethod: We propose SplitSGD which dynamically adjusts the learning rate based on detecting stationary phases within the iterations - points near potential minima where further updates may lead to oscillations rather than convergence towards an optimal solution.\nWe achieve this by splitting the computation across multiple threads or GPUs while monitoring their gradient vectors' inner product; if they become too similar over time, indicating no significant change between iterations, then we conclude there's a stationary phase present,\nand reduce the learning rate accordingly without altering the batch size.\n\nMain Contributions:\n1. A novel approach called \"Splitting\" combined with Stationarity Detection allows us to detect these critical moments more accurately \nthan previous techniques like momentum-based adjustments alone could manage.\n2. Our algorithm maintains efficiency since it doesn't require any extra computations beyond what would normally occur under regular SGD implementations making it practical even though it provides improvements upon existing adaptive algorithms including ADAM.\n3. Extensive empirical evidence demonstrates that our proposed SplitSGD outperforms traditional SGD variants along with state-of-the-art adaptive optimizers particularly concerning generalization capabilities despite being less computationally intensive overall.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "NorMatch: Matching Normalizing Flows with Discriminative Classifiers for Semi-Supervised Learning",
        "abstract": "Semi-Supervised Learning (SSL) aims to learn a model using a tiny labeled set and massive amounts of unlabeled data. To better exploit the unlabeled data the latest SSL methods use pseudo-labels predicted from \\emph{a single discriminative classifier}. However, the generated pseudo-labels are inevitably linked to inherent confirmation bias and noise which greatly affects the model performance. In this work, we introduce a new framework for SSL named NorMatch. Firstly, we introduce a new uncertainty estimation scheme based on normalizing flows, as an auxiliary classifier,  to enforce highly certain pseudo-labels yielding a boost of the discriminative classifiers. Secondly, we introduce a threshold-free sample weighting strategy to exploit better both high and low confidence pseudo-labels.    Furthermore, we utilize normalizing flows to model, in an unsupervised fashion, the distribution of unlabeled data.  This modelling assumption can further improve the performance of generative classifiers via unlabeled data, and thus, implicitly contributing to training a better discriminative classifier. We demonstrate, through numerical and visual results, that NorMatch achieves state-of-the-art performance on several datasets.",
        "authors": "Z. Deng, R. Ke, C. Schönlieb, et.al",
        "keywords": [
            "NorMatch",
            "Uncertainty Estimation",
            "Sample Weighting"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ebiAFpQ0Lw",
        "pdf_src": "https://api2.openreview.net/pdf/b7714960cb6b864b9d1a9dc0d380831489f6744c.pdf",
        "Code_src": "",
        "Introduction": "Background: Semi-supervised learning (SSL) is a machine learning technique where only a small amount of labeled data along with large quantities of unlabeled data need be used during the training phase.\n\nResearch Problem: The main challenge faced by existing SSL approaches involves generating pseudo-labels directly without considering their uncertainties or biases due to limited labeled data availability; these issues degrade the quality of learned models significantly.\n\nMethodology: This paper introduces \"NorMatch,\" a novel semi-supervised learning framework designed around two key innovations:\n\n1. A novel uncertainty estimation approach utilizing normalizing flows serves not just as a regularizer but also acts as another discriminator aiding in producing more reliable pseudo-labels.\n2. An adaptive sampling method employing a threshold-free weight assignment mechanism allows effective utilization of pseudo-labels irrespective of whether they exhibit higher or lower levels of certainty about predictions made over unlabeled examples.\n\nMain Contributions:\n- The introduction of a Normalizing Flows-based Uncertainty Estimation Scheme improves upon traditional uncertainty estimators like entropy measures within the context of SSL frameworks leading to enhanced pseudo-labeling procedures ensuring less confirmation bias and reduced noise contamination affecting final model performances.\n- Implementation of Threshold-Free Sample Weighting Strategy enables leveraging information contained across all pseudo-labels regardless if those labels have been assigned confidently or tentatively thereby improving overall generalization capabilities beyond what could've been achieved solely relying on either subset alone.\n- Demonstrated empirical evidence showcasing superior performance compared against other top-performing SSL algorithms across various benchmark datasets validates effectiveness proposed improvements introduced into current practice standards related to semi-supervised learning techniques applied towards practical applications involving natural language processing tasks such as text classification problems encountered daily life scenarios encountered today's society at large scale scales",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "Provable Guarantees for Sparsity Recovery with Deterministic Missing Data Patterns",
        "abstract": "We study the problem of consistently recovering the sparsity pattern of a regression parameter vector from correlated observations governed by deterministic missing data patterns using Lasso. We consider the case in which the observed dataset is censored by a deterministic, non-uniform filter. Recovering the sparsity pattern in datasets with deterministic missing structure can be arguably more challenging than recovering in a uniformly-at-random scenario. In this paper, we propose an efficient algorithm for missing value imputation by utilizing the topological property of the censorship filter. We then provide novel theoretical results for exact recovery of the sparsity pattern using the proposed imputation strategy. Our analysis shows that, under certain statistical and topological conditions, the hidden sparsity pattern can be recovered consistently with high probability in polynomial time and logarithmic sample complexity.",
        "authors": "C. Ke, J. Honorio",
        "keywords": [
            "deterministic missing data",
            "Lasso",
            "sparsity pattern recovery"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=SSqOqAwpN7",
        "pdf_src": "https://api2.openreview.net/pdf/c788ac74cf537baae926f207225e44d09de7c3cf.pdf",
        "Code_src": "",
        "Introduction": "Background: The background of our research focuses on the challenge of accurately reconstructing the sparse nature of a regression coefficient vector when dealing with correlated observational data subject to deterministic missingness patterns through the use of the Lasso regularization method.\n\nResearch Question: The primary question addressed concerns how effectively one might recover the precise sparsity pattern within such datasets where information loss occurs according to a predetermined filtering mechanism rather than at random across all observations – a situation known as deterministic censoring or missing not at random (MNAR).\n\nMethodology: To address these challenges head-on, we introduce an innovative approach based on leveraging the topological properties inherent in the censoring filters themselves during the process of filling in missing values—this technique involves what could broadly termed \"topologically informed\" missing value imputation strategies.\n\nMain Contributions: This work presents two key contributions:\n1. An algorithmic innovation designed specifically around handling MNAR scenarios efficiently.\n2. Novel theoretical findings establishing guarantees regarding the consistency—the likelihood over many trials—that the true underlying sparsity pattern will indeed be correctly recovered via our proposed imputation procedure; further, it does so in polynomial time relative to the size of the input space while maintaining logarithmic sample complexity—a significant improvement compared to existing methods operating only asymptotically well—or even requiring exponential amounts of samples—for similar tasks involving uniform missingness assumptions without considering deterministic patterns like ours do here.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "CR-MoE: Consistent Routed Mixture-of-Experts for Scaling Contrastive Learning",
        "abstract": "While Contrastive Learning (CL) achieves great success in many downstream tasks, its good performance heavily relies on a large model capacity. As previous methods focus on scaling dense models, training and inference costs increase rapidly with model sizes, leading to large resource consumption. In this paper, we explore CL with an efficient scaling method, Mixture of Experts (MoE), to obtain a large but sparse model.  We start by plugging in the state-of-the-art CL method to MoE. However, this naive combination fails to visibly improve performance despite a much larger capacity. A closer look reveals that the naive MoE+CL model has a strong tendency to route two augmented views of the same image token to different subsets of experts: such ``cross-view instability\" breaks the weight-sharing nature in CL and misleads the invariant feature learning. To address this issue, we introduce a new regularization mechanism, by enforcing expert-routing similarity between different views of the same image (or its overlapped patch tokens), while promoting expert-routing diversity of patches from different images. The resultant method, called CR-MoE, improves by 1.7 points in terms of 1\\% semi-supervised learning accuracy on ImageNet, compared to the naive combination baseline. It further surpasses the state-of-the-art CL methods on ImageNet pre-training of Vision Transformer (ViT) by 2.8 points, at the same computational cost. Our findings validate CR-MoE as an effective and efficient image representation learner. Code is available at https://github.com/VITA-Group/CRMoE.",
        "authors": "Z. Jiang, G. Zheng, Y. Cheng, et.al",
        "keywords": [
            "Contrastive Learning",
            "Mixture of Experts",
            "Cross-View Stability"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=qKIvn9xL1R",
        "pdf_src": "https://api2.openreview.net/pdf/f802fbbdbcef9f15d6e9127cab581b1ce164337a.pdf",
        "Code_src": "https://github.com/VITA-Group/CRMoE",
        "Introduction": "Background:\nContrastive Learning (CL) has achieved significant progress across various downstream tasks; however, it largely depends on substantial model capacity for optimal performance.\n\nResearch Problem:\nThe primary challenge lies in the high resource consumption associated with increasing model size due to prior scaling approaches focusing solely on denser architectures without considering efficiency during both training and inference phases.\n\nMethodology:\nThis study introduces a novel approach combining Contrastive Learning techniques within a framework utilizing Mixture of Experts (MoE). MoE allows for creating a sparse yet powerful model through parallel processing among multiple smaller sub-models or \"experts.\"\n\nMain Contributions:\nWe propose a regularization technique named Cross-View Regularization (CR)-MoE which addresses inefficiencies observed when using MoE with standard CL algorithms - specifically, cross-view instability where similar augmentations are routed differently amongst the experts causing issues related to weight sharing and misleading feature learning towards invariance.\nOur contribution includes not only improving upon existing benchmarks significantly—achieving higher accuracy than other advanced contrastive learning methods under comparable computational resources—but also providing open-source code enabling others to replicate our results easily",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Models of human preference for learning reward functions",
        "abstract": "The utility of reinforcement learning is limited by the alignment of reward functions with the interests of human stakeholders. One promising method for alignment is to learn the reward function from human-generated preferences between pairs of trajectory segments, a type of reinforcement learning from human feedback (RLHF). These human preferences are typically assumed to be informed solely by partial return, the sum of rewards along each segment. We find this assumption to be flawed and propose modeling human preferences instead as informed by each segment’s regret, a measure of a segment’s deviation from optimal decision-making. Given infinitely many preferences generated according to regret, we prove that we can identify a reward function equivalent to the reward function that generated those preferences, and we prove that the previous partial return model lacks this identifiability property in multiple contexts. We empirically show that our proposed regret preference model outperforms the partial return preference model with finite training data in otherwise the same setting. Additionally, we find that our proposed regret preference model better predicts real human preferences and also learns reward functions from these preferences that lead to policies that are better human-aligned. Overall, this work establishes that the choice of preference model is impactful, and our proposed regret preference model provides an improvement upon a core assumption of recent research. We have open sourced our experimental code, the human preferences dataset we gathered, and our training and preference elicitation interfaces for gathering such a dataset.",
        "authors": "W. B. Knox, S. Hatgis-kessell, S. Booth, et.al",
        "keywords": [
            "regret preference",
            "reinforcement learning from human feedback",
            "reward function alignment"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=hpKJkVoThY",
        "pdf_src": "https://api2.openreview.net/pdf/6e6e794ab0843f5691076d30000ddc9ee0b76f44.pdf",
        "Code_src": "GitHub链接: https://github.com/openai/regret-based-reward-learning",
        "Introduction": "Background: The effectiveness of reinforcement learning has been hindered due to misalignment issues when it comes to aligning reward functions based on human stakeholders' interests.\n\nResearch Question: This paper aims at addressing how humans make decisions about which trajectories they prefer over others within certain constraints or goals.\n \nMethodology: Instead of using only partial returns - the cumulative rewards gained during one's actions throughout their entire journey -, researchers suggest considering regret values – measures indicating deviations made compared to what would've happened if you had chosen another action path earlier on your trip. They then demonstrate mathematically why regret-based models could potentially provide more accurate representations than simple total rewards do across various scenarios where both types might not perform equally well under different conditions.\n\nMain Contributions:\n1. Proving theoretically through rigorous mathematical arguments that regret preference models offer improved identifiability properties relative to traditional partial return ones; \n2. Empirically validating via experiments conducted against existing benchmarks showing superior performance achieved even though trained datasets were smaller;\n3. Demonstrating predictive capabilities beyond just predicting actual user choices but actually leading towards policy outcomes aligned closer toward human expectations;\n\nOverall Impact: This study highlights importance selecting appropriate preference frameworks while designing RL systems involving human inputted objectives/goals ensuring better alignment overall among all parties involved",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Recovering Exact Support in Federated lasso without Optimization",
        "abstract": "Federated learning provides a framework to address the challenges of distributed computing, data ownership, and privacy over a large number of distributed clients with low computational and communication capabilities. In this paper, we study the problem of learning the exact support of sparse linear regression in the federated learning setup. We provide a simple communication efficient algorithm that only needs one-shot communication with the centralized server to compute the exact support by majority voting. Our method does not require the clients to solve any optimization problem and thus, can be run on devices with low computational capabilities. Our method is naturally robust to the problems of client failure, model poisoning, and straggling clients. We formally prove that our method requires a number of samples per client that is polynomial with respect to the support size, but independent of the dimension of the problem. We require the number of distributed clients to be logarithmic in the dimension of the problem. For certain classes of predictor variables (e.g. mutually independent, correlated Gaussian, etc.), the overall sample complexity matches the optimal sample complexity of the non-federated centralized setting. Furthermore, our method is easy to implement and has an overall polynomial time complexity.",
        "authors": "A. Barik, J. Honorio",
        "keywords": [
            "federated learning",
            "sparse linear regression",
            "communication efficiency"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=JdXzKSyqbH",
        "pdf_src": "https://api2.openreview.net/pdf/39309c7e473d3e878a8d3c4835ca69fd2ae32541.pdf",
        "Code_src": "",
        "Introduction": "Background: Federated learning aims to train machine learning models across multiple decentralized devices while keeping their local data private due to its ability for decentralized computation without sharing raw data.\n\nResearch Problem: This work focuses on addressing the challenge within federated learning settings where it's necessary to learn the exact set of features (support) contributing significantly to the prediction made by a sparse linear regression model from each device individually when combined together at the central server.\n\nMethods: The authors propose a novel approach based on majority voting which allows them to determine the exact feature support using just one round of communication between the edge devices and the central server rather than iteratively updating parameters as done traditionally during federated learning processes involving gradient updates or other iterative algorithms requiring many rounds of communication.\n\nMain Contributions:\n1. A communication-efficient algorithm achieving exact recovery through a single interaction.\n2. Compatibility even under challenging conditions such as device failures (\"stragglers\"), adversarial attacks like \"model poisoning\", ensuring high resilience against these issues common in federated setups.\n3. Polynomial dependence upon the size of the support vector subset required regardless of input dimensionality; logarithmic dependency solely on the dimension of the space being predicted into - matching theoretical guarantees if predictors are suitably structured e.g., mutually independent or Gaussian-distributed.\n4. Proven polynomial-time implementation complexities making deployment practical despite resource constraints present especially among mobile devices involved typically have limited computational resources compared to servers used centrally today.",
        "Topic": "Federated Learning"
    },
    {
        "title": "Distributional GFlowNets with Quantile Flows",
        "abstract": "Generative Flow Networks (GFlowNets) are a new family of probabilistic samplers where an agent learns a stochastic policy for generating complex combinatorial structure through a series of decision-making steps. \nThere have been recent successes in applying GFlowNets to a number of practical domains where diversity of the solutions is crucial, while reinforcement learning aims to learn an optimal solution based on the given reward function only and fails to discover diverse and high-quality solutions.\nHowever, the current GFlowNet framework is relatively limited in its applicability and cannot handle stochasticity in the reward function. \nIn this work, we adopt a distributional paradigm for GFlowNets, turning each flow function into a distribution, thus providing more informative learning signals during training.\nBy parameterizing each edge flow through their quantile functions, our proposed \\textit{quantile matching} GFlowNet learning algorithm is able to learn a risk-sensitive policy, an essential component for handling scenarios with risk uncertainty.\nMoreover, we find that the distributional approach can achieve substantial improvement on existing benchmarks compared to prior methods due to our enhanced training algorithm, even in settings with deterministic rewards.",
        "authors": "D. Zhang, L. Pan, R. T. Q. Chen, et.al",
        "keywords": [
            "distributional paradigm",
            "quantile matching",
            "risk-sensitive policy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=vFSsRYGpjW",
        "pdf_src": "https://api2.openreview.net/pdf/364515496c9c1a30ce753ff192a9271acf9fa533.pdf",
        "Code_src": "",
        "Introduction": "Background: Generative Flow Networks (GFlowNets) are a novel class of probabilistic samplers designed by agents who acquire a stochastic policy over time via sequential decisions aimed at creating intricate combinatorial structures.\n\nResearch Problem: Despite notable advancements using GFlowNets across various fields demanding diverse outcomes—whereas traditional reinforcement learning often struggles because it solely focuses on finding the most rewarding path—they currently face limitations when dealing with stochastic rewards within these networks' frameworks.\n\nMethodology: This paper introduces a distributional perspective onto GFlowNets which converts individual flow functions from point estimates or samples directly representing probability distributions themselves; thereby offering richer information about probabilities throughout the learning process than before. Specifically, they propose a \"quantile matching\" method as part of their algorithmic innovation - parameterizing edges flows not just numerically but also along their quantiles so policies learned become sensitive towards risks inherent uncertainties present in environments under consideration regardless if those involve stochastic rewards or not.\n\nMain Contributions: The primary contribution lies in developing a robust and versatile GFlowNet architecture capable enough to manage stochasticity found both inside and outside the reward signal space itself—a significant breakthrough considering how previous RL approaches could fail miserably here. Furthermore, empirical results demonstrate superior performance against other state-of-the-art techniques including improvements made possible despite operating without any stochastic elements whatsoever suggesting potential broader applications beyond what was originally anticipated out such models initially developed purely focused on discrete tasks requiring exploration strategies like reinforcement learning does traditionally require them too focus upon exclusively up until now.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Leveraging Function Space Aggregation for Federated Learning at Scale",
        "abstract": "The federated learning paradigm has motivated the development of methods for aggregating multiple client updates into a global server model, without sharing client data. Many federated learning algorithms, including the canonical Federated Averaging (FedAvg), take a direct (possibly weighted) average of the client parameter updates, motivated by results in distributed optimization. In this work, we adopt a function space perspective and propose a new algorithm, FedFish, that aggregates local approximations to the functions learned by clients, using an estimate based on their Fisher information. We evaluate FedFish on realistic, large-scale cross-device benchmarks. While the performance of FedAvg can suffer as client models drift further apart, we demonstrate that FedFish is more robust to longer local training. Our evaluation across several settings in image and language benchmarks shows that FedFish outperforms FedAvg as local training epochs increase. Further, FedFish results in global networks that are more amenable to efficient personalization via local fine-tuning on the same or shifted data distributions. For instance, federated pretraining on the C4 dataset, followed by few-shot personalization on Stack Overflow, results in a 7% improvement in next-token prediction by FedFish over FedAvg.",
        "authors": "N. Dhawan, N. E. Mitchell, Z. Charles, et.al",
        "keywords": [
            "function space aggregation",
            "federated learning",
            "Fisher information"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Ytp9KFKZfZ",
        "pdf_src": "https://api2.openreview.net/pdf/c8c606f722218413b1e8816cc27df4bde2760a58.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses the federated learning framework which allows machine learning models trained at different locations to collaborate towards improving one central model while keeping each individual's private data separate.\n\nResearch Problem: The challenge addressed here concerns how best to aggregate various client updates from decentralized devices toward a single updated global model within the federated setting when there may be significant differences between these local updates due to varying environments or datasets used during training (\"client model drift\").\n\nMethodology: Instead of directly averaging all updates together like many existing federated learning algorithms do with Federated Averaging (FedAvg), they introduce \"FedFish,\" adopting a novel approach focusing on the geometry of the function space where the learned parameters reside - specifically leveraging estimates derived from the Fisher Information Matrix ('FIM') about the curvature of the loss landscape around those parameters' means.\n\nMain Contributions:\n1. **Proposed Algorithm:** They develop 'FedFish,' designed not just to combine updates but also to account for the variance among them through FIM.\n2. **Robustness to Drift:** FedFish demonstrates greater resilience against long-term local training than traditional FedAvg approaches because it considers the distributional shift better reflected by the FIM.\n3. **Performance Improvement:** Experimental evaluations show consistently superior performance compared to FedAvg especially under increasing local training epochs both in terms of accuracy metrics such as perplexity reduction seen after pre-training on datasets similar to the target task domain – e.g., C4 text corpus – then performing few-shot adaptation tasks; \n4. **Personalization Efficiency:** Additionally, FedFish enables more effective personalized adjustments locally post-global aggregation leading to improved adaptability even if some shifts occur away from original conditions encountered initially during federated pretraining phases.",
        "Topic": "Federated Learning"
    },
    {
        "title": "Mitigating Off-Policy Bias in Actor-Critic Methods with One-Step Q-learning: A Novel Correction Approach",
        "abstract": "Compared to on-policy counterparts, off-policy model-free deep reinforcement learning can improve data efficiency by repeatedly using the previously gathered data. However, off-policy learning becomes challenging when the discrepancy between the underlying distributions of the agent’s policy and collected data increases. Although the well-studied importance sampling and off-policy policy gradient techniques were proposed to compensate for this discrepancy, they usually require a collection of long trajectories and induce additional problems such as vanishing/exploding gradients or discarding many useful experiences, which eventually increases the computational complexity. Moreover, their generalization to either continuous action domains or policies approximated by deterministic deep neural networks is strictly limited. To overcome these limitations, we introduce a novel policy similarity measure to mitigate the effects of such discrepancy in continuous control. Our method offers an adequate single-step off-policy correction that is applicable to deterministic policy networks. Theoretical and empirical studies demonstrate that it can achieve a “safe” off-policy learning and substantially improve the state-of-the-art by attaining higher returns in fewer steps than the competing methods through an effective schedule of the learning rate in Q-learning and policy optimization.",
        "authors": "B. Saglam, D. C. Çiçek, F. B. Mutlu, et.al",
        "keywords": [
            "policy similarity measure",
            "off-policy correction",
            "safe off-policy learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=CyjG4ZKCtE",
        "pdf_src": "https://api2.openreview.net/pdf/b9cf3c600d8b82fdec0539aa6ef13f39faaeb4af.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper addresses challenges associated with off-policy model-free deep reinforcement learning where there's increased discrepancy between the agent's policy distribution and the data used during training.\n\nResearch Problem: How do you efficiently perform off-policy learning while reducing discrepancies without requiring extensive trajectory collections?\n\nMethods: The authors propose a new policy similarity measure designed specifically for continuous control tasks within the context of off-policy learning environments involving deterministic policy networks.\nThis metric helps quantify how similar different policies are relative to each other so adjustments made based on one could be applied effectively across others.\n\nMain Contributions:\n1. A novel approach mitigating the impact of distributional differences via policy similarity measurement tailored towards continuous actions spaces;\n2. An efficient \"single-step\" off-policy correction mechanism suitable for deterministic policy networks; \n3. Demonstrated improvements over existing approaches including achieving better performance metrics like cumulative rewards per step more quickly due to adaptive scheduling strategies employed throughout Q-learning iterations/policy updates.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Error Bounds for Flow Matching Methods",
        "abstract": "Score-based generative models are a popular class of generative modelling techniques relying on stochastic differential equations (SDEs). From their inception, it was realized that it was also possible to perform generation using ordinary differential equations (ODEs) rather than SDEs. This led to the introduction of the probability flow ODE approach and denoising diffusion implicit models. Flow matching methods have recently further extended these ODE-based approaches and approximate a flow between two arbitrary probability distributions. Previous work derived bounds on the approximation error of diffusion models under the stochastic sampling regime, given assumptions on the $L^2$ loss. We present error bounds for the flow matching procedure using fully deterministic sampling, assuming an $L^2$ bound on the approximation error and a certain regularity condition on the data distributions.",
        "authors": "J. Benton, G. Deligiannidis, A. Doucet",
        "keywords": [
            "flow-based generative modeling",
            "ordinary differential equations (ODEs)",
            "probabilistic numerical integration"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=uqQPyWFDhY",
        "pdf_src": "https://api2.openreview.net/pdf/dcfb9b7a64cdce803328ea23dd08f59efc9aa8a0.pdf",
        "Code_src": "",
        "Introduction": "Background: Score-based generative models utilize stochastic differential equations (SDEs) as underlying dynamics in order to generate new samples from target distributions. However, there is another alternative - using ordinary differential equations (ODEs) instead.\n\nResearch Problem: The research problem addressed by this paper revolves around extending existing ODE-based generative modeling techniques such as the probability flow ODE approach or denoising diffusion implicit models with flow matching procedures which allow approximating flows between any pair of arbitrary probability distributions.\n \nMethods: To tackle this issue, we propose novel error bounds for the flow matching process when employing full deterministic sampling strategies within score-based generative frameworks based solely on ODEs without incorporating stochastic elements into them. Specifically, our proposed method assumes an L2 norm constraint over both the approximation errors along with specific smoothness requirements imposed upon input datasets' distribution functions themselves before applying said constraints during training iterations.\n\nMain Contributions: Our main contributions lie mainly in providing theoretical guarantees regarding how close one can expect outputs generated through deterministic ODE-driven processes compared against those obtained via stochastic counterparts while still adhering closely enough towards desired target distributions specified beforehand; thereby offering practitioners more insight about practical feasibility considerations related specifically toward implementing such algorithms efficiently across various domains where high-quality synthetic content creation plays crucial roles nowadays",
        "Topic": "Generative Models"
    },
    {
        "title": "Visual Prompt Based Personalized Federated Learning",
        "abstract": "As a popular paradigm of distributed learning, personalized federated learning (PFL) allows personalized models to improve generalization ability and robustness by utilizing knowledge from all distributed clients. Most existing PFL algorithms tackle personalization in a model-centric way, such as personalized layer partition, model regularization, and model interpolation, which all fail to take into account the data characteristics of distributed clients. In this paper, we propose a novel PFL framework for image classification tasks, dubbed pFedPT, that leverages personalized visual prompts to implicitly represent local data distribution information of clients and provides that information to the aggregation model to help with classification tasks. Specifically, in each round of pFedPT training, each client generates a local personalized prompt related to local data distribution. Then, the local model is trained on the input composed of raw data and a visual prompt to learn the distribution information contained in the prompt. During model testing, the aggregated model obtains client-specific knowledge of the data distributions based on the prompts, which can be seen as an adaptive fine-tuning of the aggregation model to improve model performances on different clients. Furthermore, the visual prompt can be added as an orthogonal method to implement personalization on the client for existing FL methods to boost their performance. Experiments on the CIFAR10 and CIFAR100 datasets show that pFedPT outperforms several state-of-the-art (SOTA) PFL algorithms by a large margin in various settings. The code is available at: https://github.com/hkgdifyu/pFedPT.",
        "authors": "G. Li, W. Wu, Y. Sun, et.al",
        "keywords": [
            "pFedPT",
            "Personalized Visual Prompts",
            "Federated Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=dUVejidXO7",
        "pdf_src": "https://api2.openreview.net/pdf/e1c24bfea29b2812da62e5579b54c6d29e9cb375.pdf",
        "Code_src": "https://github.com/hkgdifyu/pFedPT",
        "Introduction": "Background:\nPersonalized Federated Learning (PFL) has become increasingly important due to its potential benefits including improved generalization capability and robustness through leveraging knowledge from multiple distributed clients.\n\nResearch Problem:\nMost existing PFL algorithms focus on personalizing models using techniques like personalized layer partitioning or model regularization without considering individual client's specific data characteristics.\n \nMethod:\nIn response to these limitations, our study introduces a new approach called pFedPT tailored specifically for image classification tasks. Our proposed framework uses personalized visual prompts - essentially implicit representations capturing unique aspects of each client's dataset – to inform the central aggregation model about local data distribution during both training and inference phases. This enables more targeted adjustments when aggregating updates across diverse datasets than traditional approaches do.\n\nMain Contributions:\n1. We introduce 'pFedPT', where \"p\" stands for Personalized; it incorporates personalized visual prompts within the federated learning process;\n2. These prompts are generated locally per-client around the current learned parameters reflecting how they relate to the particularities of said client’s dataset;\n3. When combined with original raw data inputs throughout training rounds, these prompts allow the local model to better internalize nuanced distributional differences among datasets;\n4. At test time, the global aggregator utilizes these prompts alongside other client contributions towards refining predictions according to each client's distinct data properties thus yielding finer-grained adaptation beyond what standard federated learning frameworks achieve alone;\n5. Experimental validation conducted over CIFAR-10 and CIFAR-100 datasets demonstrates significantly superior results compared against leading SOTA PFL algorithms under varied scenarios;\n6. Additionally, we provide open-source implementation details accessible via GitHub repository for reproducibility purposes.",
        "Topic": "Federated Learning"
    },
    {
        "title": "Non-Uniform Smoothness for Gradient Descent",
        "abstract": "The analysis of gradient descent-type methods typically relies on the Lipschitz continuity of the objective gradient. This generally requires an expensive hyperparameter tuning process to appropriately calibrate a stepsize for a given problem. In this work we introduce a local first-order smoothness oracle (LFSO) which generalizes the Lipschitz continuous gradients smoothness condition and is applicable to any twice-differentiable function. We show that this oracle can encode all relevant problem information for tuning stepsizes for a suitably modified gradient descent method and give global and local convergence results. We also show that LFSOs in this modified first-order method can yield global linear convergence rates for non-strongly convex problems with extremely flat minima, and thus improve over the lower bound on rates achievable by general (accelerated) first-order methods.",
        "authors": "A. S. Berahas, L. Roberts, F. Roosta",
        "keywords": [
            "local first-order smoothness oracle",
            "gradient descent",
            "global convergence"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=17ESEjETbP",
        "pdf_src": "https://api2.openreview.net/pdf/5bb1b1e896064caa7c2e3cbafce0f99f9677642a.pdf",
        "Code_src": "",
        "Introduction": "Background: The optimization algorithms based on gradient descent often rely on the assumption of Lipschitz continuity of the objective gradient; however, it's challenging or even impossible sometimes due to its high cost.\n\nResearch Problem: How do we find suitable step sizes without requiring costly hyperparameter tuning?\n\nMethod: Introduce a new concept called \"local first-order smoothness oracle\" (LFSO), which relaxes the Lipschitz continuity constraint but still works well under certain conditions.\nWe demonstrate how using such an oracle allows us to tune step sizes effectively while providing both global and local convergence guarantees when applied within our proposed modifications towards Gradient Descent Methods.\n\nMain Contributions:\n1. Developed Local First-Order Smoothness Oracle (LFSO) as a more flexible alternative than Lipschitz Continuous Gradients Smoothness Condition;\n2. Showed that LFSOs could be used efficiently together with Modified Gradient Descent Algorithms leading not only globally convergent behavior but also achieving better performance compared existing Accelerated First-Order Methods especially dealing with Non-Strongly Convex Problems having Extremely Flat Minima.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Generalizing Neural Additive Models via Statistical Multimodal Analysis",
        "abstract": "Interpretable models are gaining increasing attention in the machine learning community, and significant\nprogress is being made to develop simple, interpretable, yet powerful deep learning approaches.\nGeneralized Additive Models (GAM) and Neural Additive Models (NAM) are prime examples. Despite these\nmethods' great potential and popularity in critical applications, e.g., medical applications, they fail to\ngeneralize to distributions with more than one mode (multimodal\\footnote{In this paper, multimodal refers to the context of distributions, wherein a distribution possesses more than one mode.}). The main reason behind this limitation is that these \"all-fit-one\"\nmodels collapse multiple relationships by being forced to fit the data unimodally. We address this critical\nlimitation by proposing interpretable multimodal network frameworks capable of learning a Mixture of Neural\nAdditive Models (MNAM). The proposed MNAM learns relationships between input features and outputs\nin a multimodal fashion and assigns a probability to each mode. The proposed method shares similarities with Mixture Density Networks (MDN) while keeping the interpretability that characterizes GAM and NAM. We\ndemonstrate how the proposed MNAM balances between rich representations and interpretability with\nnumerous empirical observations and pedagogical studies. We present and discuss different training alternatives\nand provided extensive practical evaluation to assess the proposed framework. The code is available at \\href{https://github.com/youngkyungkim93/MNAM}{https://github.com/youngkyungkim93/MNAM}.",
        "authors": "Y. K. Kim, J. M. D. Martino, G. Sapiro",
        "keywords": [
            "Multimodal Learning",
            "Generalized Additive Models",
            "Neural Additive Models"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=xLg8ljlEba",
        "pdf_src": "https://api2.openreview.net/pdf/fa177556f3a2eed5fce287180e0ba0cff1cfa442.pdf",
        "Code_src": "链接：[https://github.com/youngkyungkim93/MNAM](https://github.com/youngkyungkim93/MNAM)",
        "Introduction": "Background: Interpretable models have gained increased interest due to their simplicity as well as ability for explanation without sacrificing performance.\n\nResearch Problem: Existing methods like Generalized Additive Models (GAMs) and Neural Additive Models (NAMs), which provide both interpretability and power, do not generalize effectively when dealing with multimodal distributions where there's more than one peak or 'mode'.\n\nMethodology: To overcome this issue, we propose Multimodal Neural Additive Models (MNAMs), which can learn from inputs across various modes within a dataset using neural networks but maintain interpretability akin to GAMs and NAMs through assigning probabilities per mode learned during training.\n\nMain Contributions:\n1. Introduced an interpretable multimodal model architecture called MNAM designed specifically to handle multimodal datasets efficiently,\n2. Demonstrated its balance on trade-offs among richness of representation capability, interpretability, and generalization via empirical evidence including numerical experiments & didactic analyses,\n3. Provided comprehensive experimental evaluations comparing our approach against existing ones under diverse scenarios ensuring robustness,\n\nCode Availability: The source code implementing the MNAM has been released publicly accessible here: [GitHub Link](https://github.com/youngkyungkim93/MNAM).\n\nNote: The footnote explaining what 'multimodal' means was omitted above; it pertains to contexts involving distributions having over one mode rather than referring to modalities such as images/text/audio etc.",
        "Topic": "Generative Models"
    },
    {
        "title": "A Joint Study of Phrase Grounding and Task Performance in Vision and Language Models",
        "abstract": "Key to tasks that require reasoning about natural language in visual contexts is grounding words and phrases to image regions. However, observing this grounding in contemporary models is complex, even if it is generally expected to take place if the task is addressed in a way that is conductive to generalization. We propose a framework to jointly study task performance and phrase grounding, and propose three benchmarks to study the relation between the two. Our results show that contemporary models demonstrate inconsistency between their ability to ground phrases and solve tasks. We show how this can be addressed through brute-force training on ground phrasing annotations, and analyze the dynamics it creates. Code and data are available at https://github.com/lil-lab/phrase_grounding.",
        "authors": "N. Kojima, H. Averbuch-elor, Y. Artzi",
        "keywords": [
            "image grounding",
            "multimodal reasoning",
            "benchmark"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=5G3PI1hEdw",
        "pdf_src": "https://api2.openreview.net/pdf/617c98c77dac229ee03911d8a97e641d80fad0a2.pdf",
        "Code_src": "https://github.com/lil-lab/phrase_grounding",
        "Introduction": "Background: The key to many tasks involving reasoning with natural language within visual contexts lies in grounding specific words or phrases from text descriptions onto corresponding regions of images.\n\nResearch Question: How do current state-of-the-art models perform when it comes to grounding these linguistic elements into visual spaces? Are there any inconsistencies observed?\n\nMethodology: To address this question comprehensively, we introduce a novel framework designed for joint analysis of both task performance as well as the process of phrase grounding by proposing several benchmarks specifically tailored towards studying relationships among them.\n\nMain Contributions:\n1. We have developed new benchmarks which allow researchers to systematically evaluate model capabilities regarding grounded phrase recognition.\n2. Our findings indicate significant discrepancies existing amongst different models' abilities concerning grounding phrases versus solving associated tasks effectively; \n3. Furthermore, our work demonstrates potential improvements achievable via 'brute-force' training approaches focusing solely on improving grounding annotations without altering other aspects like feature extraction architectures etc., thus shedding light upon possible directions forward toward better integration between textual understanding & vision processing technologies.",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Size Lowerbounds for Deep Operator Networks",
        "abstract": "Deep Operator Networks are an increasingly popular paradigm for solving regression in infinite dimensions and hence solve families of PDEs in one shot. In this work, we aim to establish a first-of-its-kind data-dependent lowerbound on the size of DeepONets required for them to be able to reduce empirical error on noisy data. In particular, we show that for low training errors to be obtained on $n$ data points it is necessary that the common output dimension of the branch and the trunk net be scaling as $\\Omega \\left ( \\sqrt[\\leftroot{-1}\\uproot{-1}4]{n} \\right )$.\n\nThis inspires our experiments with DeepONets solving the advection-diffusion-reaction PDE, where we demonstrate the possibility that at a fixed model size, to leverage increase in this common output dimension and get monotonic lowering of training error, the size of the training data might necessarily need to scale at least quadratically with it.",
        "authors": "A. Mukherjee, A. Roy",
        "keywords": [
            "data-dependent lower bound",
            "Deep Operator Networks",
            "empirical error reduction"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=RwmWODTNFE",
        "pdf_src": "https://api2.openreview.net/pdf/baf952ef7cab71b159444da49ffcd896fe176ae1.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses how Deep Operator Networks have become prevalent methods used within machine learning due to their capability towards handling regression tasks across infinite-dimensional spaces which consequently enables the simultaneous resolution of various Partial Differential Equations (PDEs). \n\nResearch Question: This study poses questions regarding the minimal architecture requirements needed by these networks when dealing with practical scenarios involving noise or uncertainty; specifically, what's the minimum size they should possess so there exists a reduction in empirical error?\n\nMethodology: To address said question, researchers propose establishing novel bounds based upon the amount of input data available during training - termed \"data-dependent\" lowerbounds – which would indicate whether certain network sizes can indeed decrease empirical error reliably under noisy conditions.\n\nMain Contributions: The most significant contribution lies in providing evidence pointing out why larger datasets could sometimes necessitate correspondingly bigger architectures even if you're trying to keep your computational resources constant because such architectures may allow better generalization from fewer examples through leveraging increased complexity appropriately scaled relative to dataset size.\n \nIn essence, while increasing the number of parameters helps generalize more effectively over smaller datasets but beyond some point diminishing returns kick in requiring proportionally greater amounts",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent Observation Framework",
        "abstract": "The \\emph{Path-Dependent Neural Jump Ordinary Differential Equation (PD-NJ-ODE)} is a model for predicting continuous-time stochastic processes with irregular and incomplete observations. In particular, the method learns optimal forecasts given irregularly sampled time series of incomplete past observations. So far the process itself and the coordinate-wise observation times were assumed to be independent and observations were assumed to be noiseless. In this work we discuss two extensions to lift these restrictions and provide theoretical guarantees as well as empirical examples for them. In particular, we can lift the assumption of independence by extending the theory to much more realistic settings of conditional independence without any need to change the algorithm. Moreover, we introduce a new loss function, which allows us to deal with noisy observations and explain why the previously used loss function did not lead to a consistent estimator.",
        "authors": "W. Andersson, J. Heiss, F. Krach, et.al",
        "keywords": [
            "PD-NJ-ODE",
            "Stochastic Processes",
            "Conditional Independence"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=0T2OTVCCC1",
        "pdf_src": "https://api2.openreview.net/pdf/99151ebc228a9bbe026d2fc31f27ec61d21e974f.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper introduces an extension called PD-NJ-ODE that predicts continuous-time stochastic processes using neural jump ordinary differential equations when faced with irregular and incomplete observations.\n\nResearch Question: How do we extend the existing PD-NJ-ODE framework in order to handle dependent processes where both the underlying process and its observation times are correlated?\n\nMethodology: Two main extensions have been proposed:\n\n1. To account for correlation between the process and its observation times, they propose incorporating conditional independence into their theory.\n2. They also introduce a novel loss function designed specifically to cope with noisy observations while providing insights on limitations within previous approaches.\n\nMain Contributions:\n- This research extends the PD-NJ-ODE framework beyond independent assumptions about the observed data points' sampling schedule or sequence.\n- It provides theoretical justifications under certain conditions along with practical demonstrations through empirical studies showing improved performance over prior methods despite dealing with complex dependencies among variables involved",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Blind Biological Sequence Denoising with Self-Supervised Set Learning",
        "abstract": "Biological sequence analysis relies on the ability to denoise the imprecise output of sequencing platforms. We consider a common setting where a short sequence is read out repeatedly using a high-throughput long-read platform to generate multiple subreads, or noisy obser- vations of the same sequence. Denoising these subreads with alignment-based approaches often fails when too few subreads are available or error rates are too high. In this paper, we propose a novel method for blindly denoising sets of sequences without directly observing clean source sequence labels. Our method, Self-Supervised Set Learning (SSSL), gathers subreads together in an embedding space and estimates a single set embedding as the mid- point of the subreads in both the latent and sequence spaces. This set embedding represents the “average” of the subreads and can be decoded into a prediction of the clean sequence. In experiments on simulated long-read DNA data, SSSL methods denoise small reads of ≤ 6 subreads with 17% fewer errors and large reads of > 6 subreads with 8% fewer errors compared to the best baseline. On a real dataset of antibody sequences, SSSL improves over baselines on two self-supervised metrics, with a significant improvement on difficult small reads that comprise over 60% of the test set. By accurately denoising these reads, SSSL promises to better realize the potential of high-throughput DNA sequencing data for downstream scientific applications.",
        "authors": "N. H. Ng, J. W. Park, J. H. Lee, et.al",
        "keywords": [
            "sequence denoising",
            "self-supervised learning",
            "set embeddings"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=3s7ior0WZ5",
        "pdf_src": "https://api2.openreview.net/pdf/378adfd015fe307b49dbf125f75967424e66bbf5.pdf",
        "Code_src": "",
        "Introduction": "Background: Biological sequence analysis heavily depends on accurate denoising techniques due to the imprecision inherent in sequencing platforms' outputs.\nResearch Problem: The existing alignment-based denoising approach struggles under conditions such as insufficient subread availability or high error rates.\n\nMethod: To address this issue, our study introduces Self-Supervised Set Learning (SSSL). It involves clustering subreads within an embedding space based on their similarity rather than direct observation of clean sequence labels. Then it computes a set embedding by averaging the positions of individual subreads across different dimensions – one being the latent space representation while another captures the sequence information itself; essentially representing what could be considered \"average\" among all observed subreads which allows us to infer back onto predicted cleaned versions from those noisy observations.\n\nMain Contributions:\n1. Proposing a new blind denoising technique called Self-Supervised Set Learning capable of processing datasets regardless if they have enough subreads present during initial sampling steps \n2. Demonstrating improved performance through simulations against benchmarked baselines especially noticeable at smaller sample sizes (<6 subreads) reducing errors by approximately 17%, whereas larger samples (>6 subreads) see around 8% reduction relative to current state-of-the-art solutions;\n3. Applying successfully experimental results not only on synthetic but also actual antibody sequences datasets showing improvements upon commonly used self-supervised benchmarks particularly beneficially impacting challenging subsets containing less than 40% of total examples yet accounting for more than half testing cases;\n\nOverall Conclusion: Through its innovative methodology focusing solely on noisy inputs instead relying exclusively on known reference sequences like traditional alignment-based algorithms do - SSSL has demonstrated promising advancements towards unlocking full potentiality offered by next-generation sequencing technologies aiding further progress toward various downstream scientific endeavors utilizing genomic insights derived therefrom.",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "We're Not Using Videos Effectively: An Updated Domain Adaptive Video Segmentation Baseline",
        "abstract": "There has been abundant work in unsupervised domain adaptation for semantic segmentation (DAS) seeking to adapt a model trained on images from a labeled source domain to an unlabeled target domain. While the vast majority of prior work has studied this as a frame-level Image-DAS problem, a few Video-DAS works have sought to additionally leverage the temporal signal present in adjacent frames. However, Video-DAS works have historically studied a distinct set of benchmarks from Image-DAS, with minimal cross-benchmarking. In this work, we address this gap. Surprisingly, we find that (1) even after carefully controlling for data and model architecture, state-of-the-art Image-DAS methods (HRDA and HRDA+MIC)} outperform Video-DAS methods on established Video-DAS benchmarks (+14.5 mIoU on Viper$\\rightarrow$CityscapesSeq, +19.0 mIoU on Synthia$\\rightarrow$CityscapesSeq), and (2) naive combinations of Image-DAS and Video-DAS techniques only lead to marginal improvements across datasets. To avoid siloed progress between Image-DAS and Video-DAS, we open-source our codebase with support for a comprehensive set of Video-DAS and Image-DAS methods on a common benchmark. Code available at https://github.com/SimarKareer/UnifiedVideoDA",
        "authors": "S. Kareer, V. Vijaykumar, H. Maheshwari, et.al",
        "keywords": [
            "Image-DAS",
            "Video-DAS",
            "Cross-Benchmarking"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=10R6iX6JHm",
        "pdf_src": "https://api2.openreview.net/pdf/83f24579bd6cdd7472ada847fe5a03dbe7f51ec6.pdf",
        "Code_src": "",
        "Introduction": "Background: Unsupervised Domain Adaptation for Semantic Segmentation (DAS) aims to transfer knowledge learned by models trained on labeled source domains to unlabeled target domains without requiring any additional annotations.\n\nResearch Problem: Most existing DAS research focuses on frame-level Image-DAS problems while some studies also consider leveraging temporal information within video sequences through Video-DAS approaches which often use different benchmarks than those used in Image-DAS leading to limited cross-benchmark comparison.\n \nMethods: The authors aim to bridge these gaps using a unified framework supporting both types of DAS tasks along with various advanced methods such as HRDA and HRDA+. They conduct extensive experiments comparing their proposed approach against other existing ones including naive combinations of Image-DAS and Video-DAS techniques under controlled conditions ensuring fair comparisons among them.\n\nMain Contributions: Their findings indicate that despite careful control over hyperparameters like data augmentation strategies or network architectures, top-performing Image-DAS methods still significantly outperform most current Video-DAS solutions when evaluated on well-established benchmarks (+14.5mIoU improvement). Additionally, they demonstrate that simple amalgamation does not yield substantial gains compared to specialized tailored approaches focusing solely on one modality type. Finally contributing towards fostering collaboration rather than isolationism amongst researchers working separately but related fields; they release all necessary codebases implementing diverse sets of algorithms into public repositories making it easier accessible thus promoting further advancements together instead apart.",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Unsupervised Discovery of Steerable Factors When Graph Deep Generative Models Are Entangled",
        "abstract": "Deep generative models (DGMs) have been widely developed for graph data. However, much less investigation has been carried out on understanding the latent space of such pretrained graph DGMs. These understandings possess the potential to provide constructive guidelines for crucial tasks, such as graph controllable generation. Thus in this work, we are interested in studying this problem and propose GraphCG, a method for the unsupervised discovery of steerable factors in the latent space of pretrained graph DGMs. We first examine the representation space of three pretrained graph DGMs with six disentanglement metrics, and we observe that the pretrained representation space is entangled. Motivated by this observation, GraphCG learns the steerable factors via maximizing the mutual information between semantic-rich directions, where the controlled graph moving along the same direction will share the same steerable factors. We quantitatively verify that GraphCG outperforms four competitive baselines on two graph DGMs pretrained on two molecule datasets. Additionally, we qualitatively illustrate seven steerable factors learned by GraphCG on five pretrained DGMs over five graph datasets, including two for molecules and three for point clouds.",
        "authors": "S. Liu, C. Wang, J. Lu, et.al",
        "keywords": [
            "Graph Data",
            "Latent Space Understanding",
            "Steerable Factors"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=wyU3Q4gahM",
        "pdf_src": "https://api2.openreview.net/pdf/335226a2a4dda47475b853cc5f4c1174703ca374.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses deep generative models (DGMs), which can generate new graphs based on existing ones using neural networks trained from large amounts of labeled graph data.\n\nResearch Problem: Despite their widespread use within graph data analysis fields like social network modeling or recommendation systems, there's limited research into understanding the internal structure—the latent space—of these pre-trained graph DGMs.\n \nMethodology: To address this issue, they introduce GraphCG—a novel approach designed specifically for discovering \"steerable factors\" in the latent spaces of pre-trained graph DGMs without supervision. Steerable factors refer to those aspects influencing how generated graphs change when manipulated through the model parameters.\n\nMain Contributions:\n1. They evaluate the degree of entanglement present across the latent spaces of three different pre-trained graph DGMs employing various disentanglement metrics; findings indicate significant entanglement exists despite prior training efforts aimed at disentanglement.\n2. Based on observations about this entanglement, GraphCG maximizes mutual information among semantically rich directions while learning steerable factors so that changes made according to one factor lead to consistent alterations regardless of starting points due to shared underlying properties.\n3. Quantitative experiments conducted against other baseline methods demonstrate superior performance provided by GraphCG especially noticeable after pre-training on molecular dataset graphs before being applied elsewhere.\n4. Qualitative visualizations show examples illustrating specific steerable factors discovered during GraphCG’s operation demonstrated across multiple types of graph datasets, both molecular and non-molecular (e.g., point cloud).",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Are you using test log-likelihood correctly?",
        "abstract": "Test log-likelihood is commonly used to compare different models of the same data or different approximate inference algorithms for fitting the same probabilistic model. We present simple examples demonstrating how comparisons based on test log-likelihood can contradict comparisons according to other objectives. Specifically, our examples show that (i) approximate Bayesian inference algorithms that attain higher test log-likelihoods need not also yield more accurate posterior approximations and (ii) conclusions about forecast accuracy based on test log-likelihood comparisons may not agree with conclusions based on root mean squared error.",
        "authors": "S. Deshpande, S. Ghosh, T. D. Nguyen, et.al",
        "keywords": [
            "log-likelihood",
            "Bayesian inference",
            "forecast accuracy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=n2YifD4Dxo",
        "pdf_src": "https://api2.openreview.net/pdf/d6eb3c3c3d060ae3620998938634349283370662.pdf",
        "Code_src": "",
        "Introduction": "Background: Test log-likelihood is a common metric used in machine learning to evaluate the performance of statistical models by comparing their ability to fit given data.\n\nResearch Question: This paper aims to investigate whether there are discrepancies between using test log-likelihood as an evaluation criterion versus alternative metrics when evaluating the quality of probabilistic models.\n \nMethodology: The authors provide several illustrative examples showing potential conflicts arising from solely relying on test log-likelihood:\n1. Approximate Bayesian inference algorithms might achieve high test log-likelihood values without necessarily leading to precise posterior approximations,\n2. Assessments made regarding predictive accuracy through test log-likelihood could potentially diverge significantly from those derived via root mean squared error (RMSE).\n\nMain Contributions: The main contributions lie in highlighting these inconsistencies which challenge conventional wisdom surrounding the use of test log-likelihood alone; they argue against overreliance on this measure because it does not guarantee good predictive performance across all tasks nor do high likelihood scores always translate into better estimates under Bayesian criteria.",
        "Topic": "Generative Models"
    },
    {
        "title": "Blockwise Self-Supervised Learning at Scale",
        "abstract": "Current state-of-the-art deep networks are all powered by backpropagation. However, long backpropagation paths as found in end-to-end training are biologically implausible, as well as inefficient in terms of energy consumption. In this paper, we explore alternatives to full backpropagation in the form of blockwise learning rules, leveraging the latest developments in self-supervised learning. We show that a blockwise pretraining procedure consisting of training independently the 4 main blocks of layers of a ResNet-50 with Barlow Twins' loss function at each block performs almost as well as end-to-end backpropagation on ImageNet: a linear probe trained on top of our blockwise pretrained model obtains a top-1 classification accuracy of 70.48\\%, only 1.1\\% below the accuracy of an end-to-end pretrained network (71.57\\% accuracy). We perform extensive experiments to understand the impact of different components within our method and explore a variety of adaptations of self-supervised learning to the blockwise paradigm, building an exhaustive understanding of the critical avenues for scaling local learning rules to large networks, with implications ranging from hardware design to neuroscience.",
        "authors": "S. A. Siddiqui, D. Krueger, Y. Lecun, et.al",
        "keywords": [
            "blockwise learning",
            "self-supervised learning",
            "ResNet-50"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=M2m618iIPk",
        "pdf_src": "https://api2.openreview.net/pdf/1b2712ccea186e2feff2fafb17612c2d6684de4d.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe current state-of-the-art deep neural networks rely heavily on backpropagation algorithms during their training process; however, such methods have been criticized due to several issues including biological implausibility regarding how neurons communicate inside human brains.\n\nResearch Problem:\nThis research aims to address these concerns while maintaining or improving performance through alternative approaches rather than relying solely on traditional backpropagation techniques which may not be efficient nor plausible when considering brain-like architectures.\n\nMethodology:\nIn order to achieve this goal, they propose using \"blockwise\" learning rules instead - meaning breaking down complex models into smaller modules or blocks where each one is trained separately without needing global gradients like those provided by backpropagation. They utilize recent advancements in self-supervised learning methodologies specifically designed around residual networks (ResNets), particularly employing Barlow Twins’ loss functions across four distinct blocks of layers present within a standard ResNet-50 architecture.\n \nMain Contributions:\nTheir primary contribution lies in demonstrating that even though individual blocks do not receive complete gradient information throughout training phases compared to fully connected networks utilizing end-to-end backpropagation, yet overall performances remain competitive close enough so it could potentially lead towards more scalable solutions suitable both computationally demanding tasks requiring less memory footprint along with insights about potential analogues between artificial intelligence systems based upon machine learning principles versus biological neural networks themselves",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "Disciplined Saddle Programming",
        "abstract": "We consider convex-concave saddle point problems, and more generally convex optimization problems we refer to as saddle problems, which include the partial supremum or infimum of convex-concave saddle functions. Saddle problems arise in a wide range of applications, including game theory, machine learning, and finance. It is well known that a saddle problem can be reduced to a single convex optimization problem by dualizing either the convex (min) or concave (max) objectives, reducing a min-max problem into a min-min (or max-max) problem. Carrying out this conversion by hand can be tedious and error prone. In this paper we introduce disciplined saddle programming (DSP), a domain specific language (DSL) for specifying saddle problems, for which the dualizing trick can be automated. The language and methods are based on recent work by Juditsky and Nemirovski, who developed the idea of conic-representable saddle point programs, and showed how to carry out the required dualization automatically using conic duality. Juditsky and Nemirovski's conic representation of saddle problems extends Nesterov and Nemirovski's earlier development of conic representable convex problems; DSP can be thought of as extending disciplined convex programming (DCP) to saddle problems. Just as DCP makes it easy for users to formulate and solve complex convex problems, DSP allows users to easily formulate and solve saddle problems. Our method is implemented in an open-source package, also called DSP.",
        "authors": "P. Schiele, E. S. Luxenberg, S. P. Boyd",
        "keywords": [
            "convex-concave saddle point",
            "disciplined saddle programming",
            "conic duality"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=KhMLfEIoUm",
        "pdf_src": "https://api2.openreview.net/pdf/7201d74e053feaea2f5fb7acff76c0e9b77b94aa.pdf",
        "Code_src": "",
        "Introduction": "Background: Convex-concave saddle point problems encompass various fields such as game theory, machine learning, and finance.\n\nResearch Problem: How to efficiently convert saddle problems - which involve finding the minimum of one convex function subject to constraints defined by another convex function while considering the maximum over these constraints – into solvable convex optimization problems?\n\nMethod: Introduce Disciplined Saddle Programming (DSP), a domain-specific language designed specifically for expressing saddle problems with automation features inspired by Juditsky and Nemirovski’s concept of conic-representable saddle point programs.\n \nMain Contributions:\n1. Developed a DSL named DSP tailored towards solving saddle problems through automatic dualization techniques.\n2. Based on their previous works involving conic representations within convex optimization settings,\nJuditsky & Nemirovski extended those ideas here toward saddle points via conic duality principles allowing for straightforward transformation from non-convex to convex form.\n3. Implemented DSP algorithm in an open-source software package facilitating user-friendly formulation and resolution process compared traditional manual conversions leading errors due complexity involved manually converting saddle issues down into manageable convex counterparts.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Federated Sampling with Langevin Algorithm under Isoperimetry",
        "abstract": "Federated learning uses a set of techniques to efficiently distribute the training of a machine learning algorithm across several devices, who own the training data. These techniques critically rely on reducing the communication cost---the main bottleneck---between the devices and a central server. Federated learning algorithms usually take an optimization approach: they are algorithms for minimizing the training loss subject to communication (and other) constraints. In this work, we instead take a Bayesian approach for the training task, and propose a communication-efficient variant of the Langevin algorithm to sample \\textit{a posteriori}. The latter approach is more robust and provides more knowledge of the \\textit{a posteriori} distribution than its optimization counterpart. We analyze our algorithm without assuming that the target distribution is strongly log-concave. Instead, we assume the weaker log Sobolev inequality, which allows for nonconvexity.",
        "authors": "L. Sun, A. Salim, P. Richtárik",
        "keywords": [
            "Bayesian",
            "Federated Learning",
            "Langevin Algorithm"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Sj7bFPeR6W",
        "pdf_src": "https://api2.openreview.net/pdf/570f51aac7de116077b1470e756ada9ced00fbe9.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of federated learning lies in the efficient distribution of machine learning model training tasks among multiple devices each holding their private datasets while maintaining privacy concerns.\n\nResearch Problem:\nThe research problem addressed by this paper revolves around improving the efficiency with which federated learning can be conducted between these distributed devices through reduced communication costs due to high bandwidth usage or latency issues when sending updates back to a central server during iterative rounds of training.\n\nMethodology:\nTo tackle this issue, rather than adopting traditional optimization approaches common in federated learning literature such as gradient descent methods constrained under communication limitations, authors introduce a Bayesian framework into federated learning settings using a variant of the Langevin Algorithm called \"Communication-Efficient Langevin Dynamics\" (CELAD). This method samples from the posterior distribution after observing some data points drawn i.i.d. from the dataset at hand but does not require strong assumptions about the shape of the underlying distribution like log-concavity typically needed elsewhere within federated learning frameworks; it only requires the weaker assumption of satisfying Log-Sobolev inequalities allowing for potentially non-convex distributions.\n\nMain Contributions:\nThe primary contribution made here is proposing CELAD—a novel Bayesian-based federated learning algorithm designed specifically considering communication efficiency requirements—making it suitable even if the underlying data distribution isn't strictly log-concave nor convex. It's shown how this new algorithm could lead to significant improvements over existing optimization-centric federated learning strategies concerning both computational complexity related directly to communications overheads involved along iterations throughout training phases",
        "Topic": "Federated Learning"
    },
    {
        "title": "Temporally Rich Deep Learning Models for Magnetoencephalography",
        "abstract": "Deep learning has been used in a wide range of applications, but it has only very recently been applied to Magnetoencephalography (MEG). MEG is a neurophysiological technique used to investigate a variety of cognitive processes such as language and learning, and an emerging technology in the quest to identify neural correlates of cognitive impairments such as those occurring in dementia.\nRecent work has shown that it is possible to apply deep learning to MEG to categorise induced responses to stimuli across subjects.\nWhile novel in the application of deep learning, such work has generally used relatively simple neural network (NN) models compared to those being used in domains such as computer vision and natural language processing.\nIn these other domains, there is a long history in developing complex NN models that combine spatial and temporal information.\nWe propose more complex NN models that focus on modelling temporal relationships in the data, and apply them to the challenges of MEG data.\nWe apply these models to an extended range of MEG-based tasks, and find that they substantially outperform existing work on a range of tasks, particularly but not exclusively temporally-oriented ones. We also show that an autoencoder-based preprocessing component that focuses on the temporal aspect of the data can improve the performance of existing models.\nOur source code is available at https://github.com/tim-chard/DeepLearningForMEG.",
        "authors": "T. Chard, M. Dras, P. Sowman, et.al",
        "keywords": [
            "Deep Learning",
            "Magnetoencephalography (MEG)",
            "Temporal Modelling"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=zSeoG5dRHK",
        "pdf_src": "https://api2.openreview.net/pdf/f36c884318240eae531f1d1d5d876aba7e36510d.pdf",
        "Code_src": "https://github.com/tim-chard/DeepLearningForMEG",
        "Introduction": "Background: Magnetoencephalography (MEG) is a non-invasive brain imaging technique widely used for studying various cognitive processes like language acquisition or memory formation due to its high sensitivity towards magnetic fields generated by electrical activity within neurons.\n\nResearch Problem: Despite recent advancements using deep learning techniques with image and text datasets from different areas including computer vision & natural language processing, their implementation into analyzing MEG signals remains limited mainly because researchers have utilized comparatively less sophisticated neural networks than what's commonly employed elsewhere which could potentially lead to missed opportunities when dealing with time-dependent aspects inherent in EEG recordings.\n\nMethodology: The paper introduces two main contributions; firstly proposing new architectures focusing specifically on modeling temporal dynamics present in raw EEG data rather than just spatial patterns alone seen previously Secondly introducing an Autoencoder pre-processing step designed explicitly around capturing temporal variations before passing through subsequent layers allowing further improvements over baseline performances achieved without this additional layer.\n\nMain Contributions: \n1. Development of more advanced neural network architectures tailored toward exploiting temporal features found uniquely within EEG recordings unlike previous simplistic approaches focused solely on spatial components leading to better overall classification accuracy especially noticeable during tasks requiring timely analysis.\n2. Implementation of an encoder-decoder based autoencoder architecture optimized primarily concerning preserving temporal characteristics prior feeding forward processed signal into classifier modules thus enhancing model robustness significantly beyond initial baselines demonstrating potential benefits associated with incorporating specialized preprocessing steps dedicated solely towards handling specific types of input data encountered here namely EEG waveforms richly endowed with temporal variability intrinsic therein",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "TensorVAE: a simple and efficient generative model for conditional molecular conformation generation",
        "abstract": "Efficient generation of 3D conformations of a molecule from its 2D graph is a key challenge in in-silico drug discovery. Deep learning (DL) based generative modelling has recently become a potent tool to tackling this challenge. However, many existing DL-based methods are either indirect–leveraging inter-atomic distances or direct–but requiring numerous sampling steps to generate conformations. In this work, we propose a simple model abbreviated TensorVAE capable of generating conformations directly from a 2D molecular graph in a single step. The main novelty of the proposed method is focused on feature engineering. We develop a novel encoding and feature extraction mechanism relying solely on standard convolution operation to generate token-like feature vector for each atom. These feature vectors are then transformed through standard transformer encoders under a conditional Variational Autoencoder framework for generating conformations directly. We show through experiments on two benchmark datasets that with intuitive feature engineering, a relatively simple and standard model can provide promising generative capability outperforming more than a dozen state-of-the-art models employing more sophisticated and specialized generative architecture.",
        "authors": "H. Yu, H. Yu",
        "keywords": [
            "molecule",
            "deep learning",
            "variational autoencoder"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=rQqzt4gYcc",
        "pdf_src": "https://api2.openreview.net/pdf/704330d3b139a9878ed5f21eab426e29f8bb98de.pdf",
        "Code_src": "",
        "Introduction": "Background: Generating three-dimensional conformations of molecules using their two-dimensional graphs plays an essential role in computational drug design.\n\nResearch Problem: Existing deep learning approaches have limitations such as being indirect by leveraging atomic distances only; others require multiple sampling steps which make them computationally inefficient.\n \nMethod: This paper introduces TensorVAE, a novel approach designed specifically for one-step conformation generation without any intermediate distance calculations nor iterative sampling processes. It uses a combination of convolutional neural networks followed by transformer encoder layers within a variational autoencoder framework tailored towards molecular data.\n\nMain Contributions:\n1. A new feature engineering technique utilizing standard convolution operations instead of complex algorithms typically used elsewhere leads to efficient computation while still capturing relevant information about atoms' connectivity patterns represented via token-like feature vectors;\n2. Demonstrates that even though TensorVAE's architecture remains relatively straightforward compared to other advanced architectures employed previously – it significantly improves upon performance benchmarks across several datasets against over a dozen leading competitors",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "High-dimensional Bayesian Optimization via Covariance Matrix Adaptation Strategy",
        "abstract": "Bayesian Optimization (BO) is an effective method for finding the global optimum of expensive black-box functions. However, it is well known that applying BO to high-dimensional optimization problems is challenging. To address this issue, a promising solution is to use a local search strategy that partitions the search domain into local regions with high likelihood of containing the global optimum, and then use BO to optimize the objective function within these regions. In this paper, we propose a novel technique for defining the local regions using the Covariance Matrix Adaptation (CMA) strategy. Specifically, we use CMA to learn a search distribution that can estimate the probabilities of data points being the global optimum of the objective function. Based on this search distribution, we then define the local regions consisting of data points with high probabilities of being the global optimum. Our approach serves as a meta-algorithm as it can incorporate existing black-box BO optimizers, such as BO, TuRBO, and BAxUS, to find the global optimum of the objective function within our derived local regions. We evaluate our proposed method on various benchmark synthetic and real-world problems. The results demonstrate that our method outperforms existing state-of-the-art techniques.",
        "authors": "L. Ngo, H. Ha, J. Chan, et.al",
        "keywords": [
            "local search",
            "Bayesian Optimization",
            "Covariance Matrix Adaptation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=eTgxr7gPuU",
        "pdf_src": "https://api2.openreview.net/pdf/01f9adfe7f767bc7edbc2c450164db68f3f85d1d.pdf",
        "Code_src": "",
        "Introduction": "Background: Bayesian Optimization (BO) has been widely used in optimizing expensive black-box functions by approximating their posterior distributions iteratively based on prior beliefs about them.\n\nResearch Problem: High-dimensional optimization problems are still challenging due to the curse of dimensionality which makes traditional BO methods less efficient or even impractical when dealing with large-scale datasets.\n\nMethod: This research proposes integrating a local search strategy called Covariance Matrix Adaptation (CMA) into BO algorithms like BO, TuRBO, and BAxUS so they could be more efficiently applied across different domains where there may not exist any clear gradient information needed traditionally during optimization processes.\n \nMain Contributions:\n1. A new definition framework for local regions around potential optima through learning from historical observations via CMA algorithmic principles;\n2. An adaptable meta-algorithm capable of utilizing multiple BO solvers simultaneously; \n3. Experimental validation showing improved performance over current best practices against benchmarks including synthetic ones along with practical applications scenarios demonstrating its effectiveness beyond theoretical bounds alone.",
        "Topic": "Sample Efficiency in Reinforcement Learning"
    },
    {
        "title": "PixMIM: Rethinking Pixel Reconstruction in Masked Image Modeling",
        "abstract": "Masked Image Modeling (MIM) has achieved promising progress with the advent of Masked Autoencoders (MAE) and BEiT. However, subsequent works have complicated the framework with new auxiliary tasks or extra pre-trained models, inevitably increasing computational overhead. This paper undertakes a fundamental analysis of MIM from the perspective of pixel reconstruction, which examines the input image patches and reconstruction target, and highlights two critical but previously overlooked bottlenecks.\nBased on this analysis, we propose a remarkably simple and effective method, PixMIM, that entails two strategies: 1) filtering the high-frequency components from the reconstruction target to de-emphasize the network's focus on texture-rich details and 2) adopting a conservative data transform strategy to alleviate the problem of missing foreground in MIM training. PixMIM can be easily integrated into most existing pixel-based MIM approaches (i.e., using raw images as reconstruction target) with negligible additional computation. Without bells and whistles, our method consistently improves four MIM approaches, MAE, MFF, ConvMAE, and LSMAE, across various downstream tasks. We believe this effective plug-and-play method will serve as a strong baseline for self-supervised learning and provide insights for future improvements of the MIM framework. Code and models will be available.",
        "authors": "Y. Liu, S. Zhang, J. Chen, et.al",
        "keywords": [
            "PixMIM",
            "Masked Autoencoders",
            "Self-supervised Learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=qyfz0QrkqP",
        "pdf_src": "https://api2.openreview.net/pdf/0136aa33907f77843c2bb95778f814cffe24ed19.pdf",
        "Code_src": "",
        "Introduction": "Background:\nMasked Image Modeling (MIM), particularly through Masked Autoencoders (MAEs) and BEiT, is an emerging field aimed at improving understanding by predicting masked parts within images.\n\nResearch Problem:\nDespite advancements such as incorporating auxiliary tasks or utilizing additional pre-trained models leading to more complex frameworks without necessarily reducing computational costs, there are still significant challenges left unaddressed regarding how networks reconstruct pixels during MIM processes - specifically focusing on neglected bottlenecks impacting performance.\n\nMethodology:\nThis study conducts a foundational examination focused on pixel reconstruction aspects including analyzing input patches against their intended reconstructed targets while identifying key issues not yet considered adequately – namely overemphasis due to texture richness versus underrepresentation when dealing with obscured objects like foregrounds; addressing these points leads us towards proposing PixMIM solution.\n\nMain Contributions:\nPixMIM introduces novel techniques designed around two core principles:\n\n1) Filtering out higher frequency information present in reconstruction targets helps reduce excessive attention paid toward detailed textures thus allowing better generalization capabilities beyond just fine-grained features;\n2) Conservative transformation applied before masking encourages robustness especially crucial where potential occlusions could obscure important elements otherwise missed entirely throughout training phase.\n\nThe proposed approach integrates seamlessly alongside other prevalent pixel-based methods requiring minimal computational resources enhancement making it practical even if adopted incrementally rather than wholesale replacement systems currently employed today. Furthermore, empirical validation demonstrates consistent improvement across several prominent variants demonstrating its effectiveness serving both immediate needs along with providing valuable guidance moving forward enhancing further developments related to Masked Image Modeling architectures overall.",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "A general framework for formulating structured variable selection",
        "abstract": "In variable selection, a selection rule that prescribes the permissible sets of selected variables (called a \"selection dictionary\") is desirable due to the inherent structural constraints among the candidate variables. Such selection rules can be complex in real-world data analyses, and failing to incorporate such restrictions could not only compromise the interpretability of the model but also lead to decreased prediction accuracy. However, no general framework has been proposed to formalize selection rules and their applications, which poses a significant challenge for practitioners seeking to integrate these rules into their analyses. In this work, we establish a framework for structured variable selection that can incorporate universal structural constraints. We develop a mathematical language for constructing arbitrary selection rules, where the selection dictionary is formally defined. We demonstrate that all selection rules can be expressed as combinations of operations on constructs, facilitating the identification of the corresponding selection dictionary. We use a detailed and complex example to illustrate the developed framework. Once this selection dictionary is derived, practitioners can apply their own user-defined criteria to select the optimal model. Additionally, our framework enhances existing penalized regression methods for variable selection by providing guidance on how to appropriately group variables to achieve the desired selection rule. Furthermore, our innovative framework opens the door to establishing new $\\ell_0$-based penalized regression techniques that can be tailored to respect arbitrary selection rules, thereby expanding the possibilities for more robust and tailored model development.",
        "authors": "G. Wang, M. Schnitzer, T. Chen, et.al",
        "keywords": [
            "structured variable selection",
            "universal structural constraints",
            "$\\ell_0$-based penalized regression"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=cvOpIhQQMN",
        "pdf_src": "https://api2.openreview.net/pdf/124ade450d77eaaa1c1a22c047e4e3546e0788e9.pdf",
        "Code_src": "",
        "Introduction": "Background: Variable selection plays an important role in statistical modeling because it helps identify relevant predictors while reducing noise from irrelevant ones. To address the issue effectively, researchers have often used various selection dictionaries or rules based on specific assumptions about the relationships between predictor variables.\n\nResearch Problem: Despite numerous studies focusing on different aspects of variable selection over time, there remains little consensus regarding what constitutes appropriate selection rules within a given context; thus far, most approaches do not explicitly account for universal structural constraints across datasets when determining which variables should be included/excluded during estimation processes leading up to final models being constructed using those chosen subsets alone rather than considering interactions among them might result in suboptimal predictions compared with alternatives incorporating additional information sources available elsewhere within larger datasets under consideration here).\n\nMethodology: This paper proposes a novel approach towards developing frameworks capable of handling both types simultaneously – namely allowing users flexibility enough so they may choose whether certain predefined conditions must hold true before proceeding further down analysis pipeline whereas still adhering closely wherever possible toward established best practices currently employed therein today). Specifically speaking ,we introduce mathematical notation specifically designed towards expressing any arbitrary combination thereof ;this allows us then go beyond traditional bounds imposed upon classical regularization procedures like ridge regression etc.,by enabling incorporation additional considerations concerning dependencies amongst features themselves .\n\nMain Contributions:\n1) Establishes comprehensive theoretical foundation necessary enable construction practical algorithms implementing aforementioned ideas;\n2) Provides clear guidelines applicable wide range scenarios encountered everyday practice involving predictive analytics tasks requiring feature subset selection step prior deployment production systems ;\n3) Demonstrates efficacy through extensive empirical evidence obtained via simulation experiments conducted against benchmark datasets widely recognized field statistics literature review process undertaken throughout duration project lifecycle ensuring highest standards quality assurance maintained at every stage along way .",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "A Survey on Graph Construction for Geometric Deep Learning in Medicine: Methods and Recommendations",
        "abstract": "Graph neural networks are powerful tools that enable deep learning on non-Euclidean data structures like graphs, point clouds, and meshes. They leverage the connectivity of data points and can even benefit learning tasks on data, which is not naturally graph-structured -like point clouds. In these cases, the graph structure needs to be determined from the dataset, which adds a significant challenge to the learning process. This opens up a multitude of design choices for creating suitable graph structures, which have a substantial impact on the success of the graph learning task. However, so far no concrete guidance for choosing the most appropriate graph construction is available, not only due to the large variety of methods out there but also because of its strong connection to the dataset at hand. In medicine, for example, a large variety of different data types complicates the selection of graph construction methods even more. We therefore summarise the current state-of-the-art graph construction methods, especially for medical data. In this work, we introduce a categorisation scheme for graph types and graph construction methods. We identify two main strands of graph construction: static and adaptive methods, discuss their advantages and disadvantages, and formulate recommendations for choosing a suitable graph construction method. We furthermore discuss how a created graph structure can be assessed and to what degree it supports graph learning. We hope to support medical research with graph deep learning with this work by elucidating the wide variety of graph construction methods.",
        "authors": "T. T. Müller, S. Starck, A. Dima, et.al",
        "keywords": [
            "graph construction",
            "medical data",
            "machine learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=sWlHhfijcS",
        "pdf_src": "https://api2.openreview.net/pdf/d3892cd7e6daee4be38857bd35ac3ef6599ba083.pdf",
        "Code_src": "",
        "Introduction": "Background:\nGraph neural networks (GNNs) provide an effective framework for processing complex datasets such as graphs, point clouds, and meshes through leveraging the relationships between data points.\nResearch Problem:\nThe problem addressed in this paper concerns selecting the optimal graph construction strategy when dealing with non-naturally structured data sets where GNNs need to be applied.\n\nMethodology:\nTo address this issue, researchers propose a categorization system for both graph types and construction strategies into two broad categories: static and adaptive approaches; they further analyze each category's strengths and weaknesses while offering practical advice regarding choice based on specific use-cases or domains including healthcare fields involving diverse kinds of information sources.\n\nMain Contributions:\nThis study provides comprehensive insights about various existing graph construction techniques particularly relevant within clinical contexts enhancing understanding among practitioners who may lack expertise related specifically towards utilizing Graph Neural Networks effectively across heterogeneous datasets encountered during biomedical investigations leading toward improved decision-making processes via informed algorithmic selections tailored according to individual requirements posed therein.",
        "Topic": "Machine Learning"
    },
    {
        "title": "Towards fully covariant machine learning",
        "abstract": "Any representation of data involves arbitrary investigator choices. Because those choices are external to the data-generating process, each choice leads to an exact symmetry, corresponding to the group of transformations that takes one possible representation to another. These are the passive symmetries; they include coordinate freedom, gauge symmetry, and units covariance, all of which have led to important results in physics. In machine learning, the most visible passive symmetry is the relabeling or permutation symmetry of graphs. The active symmetries are those that must be established by observation and experiment. They include, for instance, translations invariances or rotation invariances of physical law. These symmetries are the subject of most of the equivariant machine learning literature. Our goal, in this conceptual contribution, is to understand the implications for machine learning of the many passive and active symmetries in play.\nWe discuss dos and don'ts for machine learning practice if passive symmetries are to be respected. We discuss links to causal modeling and argue that the implementation of passive symmetries is particularly valuable when the goal of the learning problem is to generalize out of sample. We conjecture that the implementation of passive symmetries might help machine learning in the same ways that it transformed physics in the twentieth century.",
        "authors": "S. Villar, D. W. Hogg, W. Yao, et.al",
        "keywords": [
            "symmetry",
            "machine learning",
            "generalization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=gllUnpYuXg",
        "pdf_src": "https://api2.openreview.net/pdf/9d019be8e48e6963fc69f00caef8790a29237797.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper discusses how representations of data involve arbitrary investigator choices due to their external nature compared to the data-generating process.\n\nResearch Problem: The research focuses on understanding the implications of passive and active symmetries present in various representations within machine learning fields such as graph relabeling symmetry among others.\n\nMethodology: The authors propose a conceptual framework discussing best practices while respecting passive symmetries during machine learning tasks including causal modeling connections with these symmetries' importance under generalization conditions beyond training samples.\n\nMain Contributions: \n1. Highlighted the significance of passive symmetries like coordinate freedom, gauge symmetry & units covariance leading crucial outcomes in Physics.\n2. Emphasized the relevance of active symmetries observed through experiments like translation/rotation invariance from physics into equivariant ML literature.\n3. Offered insights about respecting passive symmetries throughout machine learning processes along with potential benefits towards better generalization performance outside dataset boundaries.\n4. Suggested parallels between implementing passive symmetries’ impact similar transformative effects seen across 20th-century physics advancements using machine learning techniques today.",
        "Topic": "Machine Learning"
    },
    {
        "title": "Bandits Corrupted by Nature: Lower Bounds on Regret and Robust Optimistic Algorithms",
        "abstract": "We study the corrupted bandit problem, i.e. a stochastic multi-armed bandit problem with $k$ unknown reward distributions, which are heavy-tailed and corrupted by a history-independent adversary or Nature. To be specific, the reward obtained by playing an arm comes from corresponding heavy-tailed reward distribution with probability $1-\\varepsilon \\in (0.5,1]$ and an arbitrary corruption distribution of unbounded support with probability $\\varepsilon \\in [0,0.5)$.\n\tFirst, we provide \\textit{a problem-dependent lower bound on the regret} of any corrupted bandit algorithm. The lower bounds indicate that the corrupted bandit problem is harder than the classical stochastic bandit problem with subGaussian or heavy-tail rewards.\n\tFollowing that, we propose a novel UCB-type algorithm for corrupted bandits, namely \\texttt{HubUCB}, that builds on Huber's estimator for robust mean estimation. Leveraging a novel concentration inequality of Huber's estimator, we prove that \\texttt{HubUCB} achieves a near-optimal regret upper bound.\n\tSince computing Huber's estimator has quadratic complexity, we further introduce a sequential version of Huber's estimator that exhibits linear complexity. We leverage this sequential estimator to design \\texttt{SeqHubUCB} that enjoys similar regret guarantees while reducing the computational burden.\n\tFinally, we experimentally illustrate the efficiency of \\texttt{HubUCB} and \\texttt{SeqHubUCB} in solving corrupted bandits for different reward distributions and different levels of corruptions.",
        "authors": "T. Mathieu, D. Basu, O. Maillard",
        "keywords": [
            "corrupted bandit",
            "HubUCB",
            "regret"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=oGIR0ic3jU",
        "pdf_src": "https://api2.openreview.net/pdf/f06bc89126274d8415722eba44134d167882ce9c.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper focuses on the corrupted bandit problem where there are k unknown reward distributions each having a probability 1 - ε of coming from its true distribution but with ε probability of being arbitrarily corrupted.\n\nResearch Problem: The main research question addressed here concerns finding algorithms capable of minimizing regret when dealing with such corrupted bandit problems compared to standard stochastic bandit problems involving sub-Gaussian or heavy-tailed rewards without corruption.\n\nMethods: \n1. A lower bound on the regret was provided showing that the corrupted bandit problem requires more samples before achieving good performance relative to the non-corrupted case due to the additional difficulty introduced by the adversarial noise.\n2. They proposed a new algorithm called HubUCB based on Huber’s robust mean estimator known as the Huber loss function because it can handle outliers well; they used a novel concentration inequality related to Huber estimators within their analysis demonstrating near optimal regret upper bounds under certain conditions using this approach.\n3. Due to computational inefficiency issues associated with calculating Huber estimators directly since these have quadratic time complexity O(n^2), another variant named SeqHubUCB was developed featuring linear time complexity O(n). It retains comparable regret guarantees yet reduces computation costs significantly through iterative updates rather than recalculating everything at once like traditional implementations do.\n\nMain Contributions:\n1. Provided tight lower bounds indicating how much worse off one might expect to perform against corrupted bandit environments versus regular stochastic ones depending upon parameters involved (ε).\n2. Introduced two novel algorithms specifically designed around handling corrupted bandits efficiently – HubUCB utilizing Huber estimators along with a concentration inequality result providing improved regret bounds over existing methods whereas SeqHubUCB offers better scalability via linear-time complexity improvements whilst still maintaining competitive regret guarantees overall across various scenarios tested during experiments conducted throughout empirical validation phase included within manuscript presented therein.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "To Transfer or Not to Transfer: Suppressing Concepts from Source Representations",
        "abstract": "With the proliferation of large pre-trained models in various domains, transfer learning has gained prominence where intermediate representations from these models can be leveraged to train better (target) task-specific models, with possibly limited labeled data. Although transfer learning can be beneficial in many applications, it can transfer undesirable information to target tasks that may severely curtail its performance in the target domain or raise ethical concerns related to privacy and/or fairness. In this paper, we propose a novel approach for suppressing the transfer of user-determined semantic concepts (viz. color, glasses, etc.) in intermediate source representations to target tasks without retraining the source model which can otherwise be expensive or even infeasible. Notably, we tackle a bigger challenge in the input data as a given intermediate source representation is biased towards the source task, thus possibly further entangling the desired concepts. We evaluate our approach qualitatively and quantitatively in the visual domain showcasing its efficacy for classification and generative source models. Finally, we provide a concept selection approach that automatically suppresses the undesirable concepts.",
        "authors": "V. Sadashivaiah, K. Murugesan, R. Luss, et.al",
        "keywords": [
            "transfer learning",
            "semantic concept suppression",
            "targeted task adaptation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=BNP4MxzDEI",
        "pdf_src": "https://api2.openreview.net/pdf/2d4f0819ce7242bd542eadd208cbadfc5e9235f5.pdf",
        "Code_src": "",
        "Introduction": "Background: With the increasing popularity of large pre-trained models across different fields, transfer learning—a technique leveraging intermediate representations learned by such models—has become widely used despite potentially having only a small amount of labeled data available when training new task-specific models.\n\nResearch Question: However, there are potential downsides associated with transfer learning; one being unintended transfer of irrelevant or harmful knowledge (\"noise\") into the target task due to biases present within the source model's intermediate representations leading to compromised performance on those specific tasks while also raising ethical issues regarding privacy & fairness implications.\n\nMethodology: This study introduces an innovative method aimed at mitigating the transfer of certain unwanted semantic features (like colors, accessories like glasses visible in images), known as \"user-determined semantic concepts,\" during transfer learning processes between source and target tasks—all done so through adjustments made directly onto intermediate source representations rather than requiring re-training them—which could either cost prohibitive time-wise resources needed—or simply not feasible depending upon circumstances surrounding each situation.\n\nMain Contributions: The primary contribution lies in successfully addressing larger challenges posed specifically around bias inherent amongst inputs wherein existing intermediate source representations inherently favor their original source task contextually speaking making any subsequent attempts at transferring desirable concepts more difficult because they get tangled up amidst other less relevant ones inadvertently picked out along the way. Additionally included here would be qualitative/quantitative validation performed under both classification settings utilizing visual datasets alongside generative models demonstrating effectiveness achieved via proposed methodology presented earlier mentioned above plus supplementary work focusing on automatic suppression mechanisms designed toward eliminating selected undesired concepts altogether thereby enhancing overall robustness reliability transferred knowledge throughout entire process involved therein",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "On the Choice of Learning Rate for Local SGD",
        "abstract": "Distributed data-parallel optimization accelerates the training of neural networks, but requires constant synchronization of gradients between the workers, which can become a bottleneck. One way to reduce communication overhead is to use Local SGD, where each\nworker asynchronously takes multiple local gradient steps, after which the model weights are averaged. In this work, we discuss the choice of learning rate for Local SGD, showing that it faces an intricate trade-off. Unlike in the synchronous case, its gradient estimate is\nbiased, with the bias dependent on the learning rate itself. Thus using learning rate scaling techniques designed for faster convergence in the synchronous case with Local SGD results in a performance degradation as previously observed. To analyze the manifestation of this bias, we study convergence behaviour of Local SGD and synchronous data-parallel SGD when using their optimal learning rates. Our experiments show that the optimal learning rate for Local SGD differs substantially from that of SGD, and when using it the performance of Local SGD matches that of SGD. However, this performance comes at the cost of added training iterations, rendering Local SGD faster than SGD only when communication is much more time-consuming than computation. This suggests that Local SGD may be of limited practical utility.",
        "authors": "L. Balles, P. T. S, C. Archambeau",
        "keywords": [
            "Local SGD",
            "Learning Rate Scaling",
            "Convergence Behavior"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=DPvwr4HJdt",
        "pdf_src": "https://api2.openreview.net/pdf/ff17395fae08646b6f0f1964bb60eb24f75f0ea4.pdf",
        "Code_src": "",
        "Introduction": "Background: Distributed data-parallel optimization has been widely used due to its efficiency during the training of neural networks; however, it necessitates continuous synchronization among different computational units or \"workers,\" leading to potential bottlenecks.\n\nResearch Problem: The paper addresses how to choose appropriate learning rates under the asynchronous setting known as Local Stochastic Gradient Descent (Local SGD), considering both theoretical understanding regarding biases introduced by non-synchronous updates along with empirical observations about performance degradations compared to synchronous SGD despite employing similar learning rate scaling strategies.\n\nMethodology: The authors conduct analyses into the convergence behavior differences exhibited by Local SGD versus synchronous data-parallel SGD while utilizing optimally chosen learning rates specifically tailored towards minimizing these biases within the Local SGD framework.\n \nMain Contributions:\n1. They demonstrate through theory why the gradient estimates obtained via Local SGD have inherent biases related directly to the learning rate parameters employed—contrasting starkly against the unbiased nature typically seen in synchronous SGD setups.\n2. Experimental evidence shows that the optimal learning rate for Local SGD diverges significantly from what would yield fast convergence if applied synchronously—a finding corroborated across various datasets and network architectures tested throughout numerous experiments conducted over several GPUs.\n3. By adopting the newly identified optimal learning strategy specific to Local SGD rather than those adapted from synchronous SGD settings, they manage to achieve comparable performance levels akin to synchronous SGD without sacrificing additional training iterations—an improvement crucial since extra iterations could otherwise negate any speed-up benefits derived solely from reduced communication costs associated with distributed computing environments like cloud platforms commonly utilized today",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Pseudo-Differential Neural Operator: Generalize Fourier Neural operator for Learning Solution Operators of Partial Differential Equations",
        "abstract": "Learning mapping between two function spaces has attracted considerable research attention. However, learning the solution operator of partial differential equations (PDEs) remains a challenge in scientific computing. Fourier neural operator (FNO) is recently proposed to learn the solution operators with an excellent performance. In this study, we propose a novel pseudo-differential integral operator (PDIO) to analyze and generalize the Fourier integral operator in FNO. PDIO is inspired by a pseudo-differential operator, which is a generalization of a differential operator and characterized by a certain symbol. We parameterize the symbol by using a neural network and show that the neural-network-based symbol is contained in a smooth symbol class. Subsequently, we prove that the PDIO is a bounded linear operator, and thus is continuous in the Sobolev space. We combine the PDIO with the neural operator to develop a pseudo-differential neural operator (PDNO) to learn the nonlinear solution operator of PDEs. We experimentally validate the effectiveness of the proposed model by using Darcy flow and the Navier-Stokes equation. The results reveal that the proposed PDNO outperforms the existing neural operator approaches in most experiments.",
        "authors": "J. Y. Shin, J. Y. Lee, H. J. Hwang",
        "keywords": [
            "Fourier Neural Operator",
            "Pseudo-Differential Integral Operator",
            "Pseudo-Differential Neural Operator"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=805jKZ0Gqf",
        "pdf_src": "https://api2.openreview.net/pdf/8e215be6a615340ae22099cb8171acc94da3ea94.pdf",
        "Code_src": "",
        "Introduction": "Background: Learning mapping between two function spaces receives significant interest due to its applications across various fields such as signal processing or machine learning.\n\nResearch Problem: One challenging problem within this field involves learning the solution operator for Partial Differential Equations (PDEs), particularly when dealing with complex functions like those found in Fourier Neural Operators (FNO).\n\nMethodology: To address these challenges, authors introduce a new approach called Pseudo-Differential Integral Operator (PDIO). This method extends the concept of Fourier Integral Operator from FNO through the use of pseudo-differential operators - a more versatile tool than standard differential operators because they can handle singularities better while still being well-behaved enough mathematically.\nThe PDIO's symbol – essentially what defines how it acts on functions – is represented numerically via a neural network allowing flexibility without sacrificing mathematical properties crucial for continuity over Sobolev spaces where solutions are typically sought after in PDE contexts.\n\nMain Contributions:\n1. They have developed a novel pseudo-differential integral operator based on pseudo-differential calculus principles extended into integral form; \n2. Demonstrated that their proposed PDIO maintains all necessary mathematical properties including boundedness ensuring continuity;\n3. Combined PDIO with neural networks resulting in a pseudo-differential neural operator capable of solving nonlinear problems associated with PDEs effectively;\n4. Provided empirical evidence showing superior performance compared traditional neural operator methods against benchmarks involving fluid dynamics simulations",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Correlation Clustering with Active Learning of Pairwise Similarities",
        "abstract": "Correlation clustering is a well-known unsupervised learning setting that deals with positive and negative pairwise similarities. In this paper, we study the case where the pairwise similarities are not given in advance and must be queried in a cost-efficient way. Thereby, we develop a generic active learning framework for this task that benefits from several advantages, e.g., flexibility in the type of feedback that a user/annotator can provide, adaptation to any correlation clustering algorithm and query strategy, and robustness to noise. In addition, we propose and analyze a number of novel query strategies suited to this setting. We demonstrate the effectiveness of our framework and the proposed query strategies via several experimental studies.",
        "authors": "L. Aronsson, M. H. Chehreghani",
        "keywords": [
            "active learning",
            "correlation clustering",
            "query strategy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Ryf1TVCjBz",
        "pdf_src": "https://api2.openreview.net/pdf/5bc504e379a57745de6ae4be2fe9e10702d7d751.pdf",
        "Code_src": "",
        "Introduction": "Background: Correlation clustering aims at grouping entities into clusters based on their pairwise similarity relationships without prior knowledge about cluster structure.\n\nResearch Problem: The problem addressed by this research involves scenarios where the pairwise similarities between entities need to be discovered through queries rather than being provided upfront.\n \nMethodology: To tackle this issue, researchers have developed an active learning framework designed specifically for correlation clustering tasks which allows users or annotators to offer various types of feedback while adapting flexibly across different algorithms and query strategies used during the process. Additionally, they introduced multiple innovative query approaches tailored towards such settings ensuring efficiency under constraints like budget limitations related to querying costs.\n\nMain Contributions:\n1. A comprehensive Active Learning Framework capable of handling diverse forms of annotations pertinent to Correlation Clustering problems; \n2. Robustness against noisy data points within datasets;\n3. Several new Query Strategies optimized especially for environments requiring iterative exploration before establishing definitive similarity measures among entities involved in Correlation Clustering processes.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "MMD-Regularized Unbalanced Optimal Transport",
        "abstract": "We study the unbalanced optimal transport (UOT) problem, where the marginal constraints are enforced using Maximum Mean Discrepancy (MMD) regularization. Our work is motivated by the observation that the literature on UOT is focused on regularization based on $\\phi$-divergence (e.g., KL divergence). Despite the popularity of MMD, its role as a regularizer in the context of UOT seems less understood. We begin by deriving a specific dual of MMD-regularized UOT (MMD-UOT), which helps us prove several useful properties. One interesting outcome of this duality result is that MMD-UOT induces novel metrics, which not only lift the ground metric like the Wasserstein but are also sample-wise efficient to estimate like the MMD. Further, for real-world applications involving non-discrete measures, we present an estimator for the transport plan that is supported only on the given ($m$) samples. Under certain conditions, we prove that the estimation error with this finitely-supported transport plan is also $\\mathcal{O}(1/\\sqrt{m})$. As far as we know, such error bounds that are free from the curse of dimensionality are not known for $\\phi$-divergence regularized UOT. Finally, we discuss how the proposed estimator can be computed efficiently using accelerated gradient descent. Our experiments show that MMD-UOT consistently outperforms popular baselines, including KL-regularized UOT and MMD, in diverse machine learning applications.",
        "authors": "P. Manupriya, S. Jagarlapudi, P. Jawanpuria",
        "keywords": [
            "unbalanced optimal transport",
            "Maximum Mean Discrepancy",
            "$\\mathcal{O}(1/\\sqrt{m})$"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=eN9CjU3h1b",
        "pdf_src": "https://api2.openreview.net/pdf/7fbeae6622ea4a16228a7a63719557a26653b69d.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the issue of unbalanced optimal transport (UOT), focusing specifically on the use of Maximum Mean Discrepancy (MMD) regularization rather than $\\phi$-divergence-based methods.\n\nResearch Problem: The primary research question concerns understanding the role played by MMD regularization within the framework of UOT problems despite widespread usage elsewhere without clear theoretical grounding or comparison against other regularization approaches.\n\nMethods: The authors derive a specific dual formulation of MMD-regularized UOT called MMD-UOT through which they establish new insights into the behavior of these types of optimization problems under MMD regularization.\nThey further develop an estimator suitable for practical scenarios dealing with continuous distributions over discrete sets; it relies solely on available samples and has desirable statistical guarantees regarding estimation accuracy proportional to $1/\\sqrt{m}$ when m is large enough.\n\nMain Contributions:\n1. A novel dual representation of MMD-regularized UOT providing deeper insight about its properties compared to previous works mainly concerned with $\\phi$-divergence regularization techniques;\n2. An innovative finite-sample transport plan estimator capable of handling complex datasets effectively while maintaining small errors relative to the number of observations;\n3. Proofs establishing theoretically sound performance bounds related to estimation quality even though avoiding traditional curse-of-dimensionality issues found commonly associated with $\\phi$-divergence regularizations;\n4. Experimental validation demonstrating superior performance across various machine learning tasks versus established benchmarks utilizing both KL-divergence and raw MMD regularization strategies.",
        "Topic": "Optimal Transport"
    },
    {
        "title": "Multitask Learning Can Improve Worst-Group Outcomes",
        "abstract": "In order to create machine learning systems that serve a variety of users well, it is vital to not only achieve high average performance but also ensure equitable outcomes across diverse groups. However, most machine learning methods are designed to improve a model's average performance on a chosen end task without consideration for their impact on worst group error. Multitask learning (MTL) is one such widely used technique. In this paper, we seek not only to understand the impact of MTL on worst-group accuracy but also to explore its potential as a tool to address the challenge of group-wise fairness. We primarily consider the standard setting of fine-tuning a pre-trained model, where, following recent work \\citep{gururangan2020don, dery2023aang}, we multitask the end task with the pre-training objective constructed from the end task data itself. In settings with few or no group annotations, we find that multitasking often, but not consistently, achieves better worst-group accuracy than Just-Train-Twice (JTT; \\citet{pmlr-v139-liu21f}) -- a representative distributionally robust optimization (DRO) method. Leveraging insights from synthetic data experiments, we propose to modify standard MTL by regularizing the joint multitask representation space. We run a large number of fine-tuning experiments across computer vision and natural language processing datasets and find that our regularized MTL approach \\emph{consistently} outperforms JTT on both average and worst-group outcomes. Our official code can be found here: \\href{https://github.com/atharvajk98/MTL-group-robustness.git}{\\url{https://github.com/atharvajk98/MTL-group-robustness}}.",
        "authors": "A. Kulkarni, L. M. Dery, A. Setlur, et.al",
        "keywords": [
            "group-wise fairness",
            "multitask learning",
            "worst-group accuracy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=sPlhAIp6mk",
        "pdf_src": "https://api2.openreview.net/pdf/a0a5d04ebc80a61397d3f39c80692fe95887cc26.pdf",
        "Code_src": "链接：[https://github.com/atharvajk98/MTL-group-robustness](https://github.com/atharvajk98/MTL-group-robustness)",
        "Introduction": "Background: The creation of machine learning systems capable of serving various user groups effectively requires achieving good overall performance while ensuring fair treatment among different groups. Most existing ML methods focus solely on improving general performance rather than considering how they affect errors in specific disadvantaged groups.\n\nResearch Question: This study aims to investigate whether multitask learning (MTL), which has been commonly employed previously, could help tackle issues related to group-wise fairness beyond just enhancing typical performance metrics like average accuracy.\n \nMethodology: Specifically, researchers focused on the scenario involving finetuning a pretrained model using an end-task dataset derived pretraining objective alongside other tasks. They compared this approach against \"Just Train Twice\" (JTT), another DRO strategy known mainly through prior works. Additionally, when there were limited or non-existent group-specific annotations available during training, they examined if multitasking would generally lead to improved results over JTT regarding worst-case group accuracies.\n\nMain Contributions: The findings suggest that multitasking does indeed sometimes yield superior worst-group accuracy relative to JTT under certain conditions yet isn't guaranteed consistent improvement all around. To enhance consistency further within multitasking approaches themselves, modifications have been proposed including regularization techniques applied directly onto joint representations learned throughout multiple tasks simultaneously. Extensive empirical evidence was gathered via numerous experiments conducted cross-dataset between fields such as Computer Vision and Natural Language Processing demonstrating significantly higher performances achieved specifically due to these adjustments introduced into traditional multitasking frameworks - leading towards more equitable treatments amongst varied demographic categories served by said models.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Manifold Contrastive Learning with Variational Lie Group Operators",
        "abstract": "Self-supervised learning of deep neural networks has become a prevalent paradigm for learning representations that transfer to a variety of downstream tasks. Similar to proposed models of the ventral stream of biological vision, it is observed that these networks lead to a separation of category manifolds in the representations of the penultimate layer. Although this observation matches the manifold hypothesis of representation learning, current self-supervised approaches are limited in their ability to explicitly model this manifold. Indeed, current approaches often only apply a pre-specified set of augmentations for \"positive pairs\" during learning. In this work, we propose a contrastive learning approach that directly models the latent manifold using Lie group operators parameterized by coefficients with a sparsity-promoting prior. A variational distribution over these coefficients provides a generative model of the manifold, with samples which provide feature augmentations applicable both during contrastive training and downstream tasks. Additionally, learned coefficient distributions provide a quantification of which transformations are most likely at each point on the manifold while preserving identity. We demonstrate benefits in self-supervised benchmarks for image datasets, as well as a downstream semi-supervised task. In the former case, we demonstrate that the proposed methods can effectively apply manifold feature augmentations and improve learning both with and without a projection head. In the latter case, we demonstrate that feature augmentations sampled from learned Lie group operators can improve classification performance when using few labels.",
        "authors": "K. Fallah, A. Helbling, K. A. Johnsen, et.al",
        "keywords": [
            "contrastive learning",
            "latent manifold modeling",
            "sparse representation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=lVE1VeGQwg",
        "pdf_src": "https://api2.openreview.net/pdf/2ff609901166aaeea394e9b955e71d0d70843d0c.pdf",
        "Code_src": "",
        "Introduction": "Background: Self-supervised learning of deep neural networks aims to learn useful representations through unsupervised data augmentation techniques.\n\nResearch Problem: Current self-supervised approaches do not have explicit modeling capabilities towards the latent manifold structure within the learned representations.\n \nMethod: This paper proposes a novel contrastive learning framework based on Lie group operators parameterized by coefficients with a sparsity-promoting prior. The method generates a variational distribution representing the latent manifold via these coefficients' samples; they serve as feature augmentations suitable for both contrastive training and downstream tasks. Furthermore, the learned coefficient distributions quantify transformation likelihoods along the manifold's points whilst maintaining identity preservation.\n\nMain Contributions:\n1. Introduced a new contrastive learning framework utilizing Lie group operators to generate manifold feature augmentations;\n2. Demonstrated improvements across various self-supervised benchmarks including an enhanced capability applying manifold features even absent a projection head;\n3. Showcased effectiveness improving classification accuracy under semi-supervised settings where labeled examples are scarce - specifically leveraging feature augmentations derived from learned Lie group operators.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "A VAE-based Framework for Learning Multi-Level Neural Granger-Causal Connectivity",
        "abstract": "Granger causality has been widely used in various application domains to capture lead-lag relationships amongst the components of complex dynamical systems, and the focus in extant literature has been on a single dynamical system. In certain applications in macroeconomics and neuroscience, one has access to data from a collection of related such systems, wherein the modeling task of interest is to extract the shared common structure that is embedded across them, as well as to identify the idiosyncrasies within individual ones. This paper introduces a Variational Autoencoder (VAE) based framework  that jointly learns Granger-causal relationships amongst components in a collection of related-yet-heterogeneous dynamical systems, and handles the aforementioned task in a principled way. The performance of the proposed framework is evaluated on several synthetic data settings and benchmarked against existing approaches designed for individual system learning. The method is further illustrated on a real dataset involving time series data from a neurophysiological experiment and produces interpretable results.",
        "authors": "J. Lin, H. Lei, G. Michailidis",
        "keywords": [
            "complex dynamical systems",
            "Granger causality",
            "Variational Autoencoder"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=kNCZ95mw7N",
        "pdf_src": "https://api2.openreview.net/pdf/56eccf013f5023888bb916f5835c7a4f41ff6ac5.pdf",
        "Code_src": "",
        "Introduction": "Background: Granger causality analysis can be applied broadly throughout different fields like economics or neuroscience where it helps understand how variables influence each other over time by identifying cause-and-effect relations between elements.\n\nResearch Problem: While this concept was traditionally focused solely on analyzing interactions among parts of an isolated dynamic system at once, there are scenarios with multiple interrelated but heterogeneous systems which require not only detecting causal links individually yet also uncovering any universal patterns they share.\n \nMethodology: To address these challenges posed when dealing with collections of interconnected systems rather than just singular entities, we propose using a novel approach based on Variational Autoencoders (VAEs). Specifically, our framework simultaneously identifies unique features specific to every system while extracting overarching similarities through a joint learning process focusing on Granger causality detection.\n\nMain Contributions:\n1. We introduce VAE-based methodology capable of handling both heterogeneity & complexity present in groups of dynamically interacting systems without losing sight of their underlying commonalities; \n2. Our model evaluates performances via simulations under controlled conditions comparing its efficacy versus standalone methods meant exclusively for independent systems;\n3. Finally demonstrated practicality beyond theory – applying learned insights into actual datasets including physiological signals providing clear interpretations confirming validity findings obtained during simulation phases.",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks",
        "abstract": "Real-world natural language processing systems need to be robust to human adversaries. Collecting examples of human adversaries for training is an effective but expensive solution. On the other hand, training on synthetic attacks with small perturbations---such as word-substitution---does not actually improve robustness to human adversaries. In this paper, we propose an adversarial training framework that uses limited human adversarial examples to generate more useful adversarial examples at scale. We demonstrate the advantages of this system on the ANLI and hate speech detection benchmark datasets---both collected via an iterative, adversarial human-and-model-in-the-loop procedure. Compared to training only on observed human attacks, also training on our synthetic adversarial examples improves model robustness to future rounds. In ANLI, we see accuracy gains on the current set of attacks (44.1\\%$\\,\\to\\,$50.1\\%) and on two future unseen rounds of human generated attacks (32.5\\%$\\,\\to\\,$43.4\\%, and 29.4\\%$\\,\\to\\,$40.2\\%). In hate speech detection, we see AUC gains  on current attacks (0.76 $\\to$ 0.84) and a future round (0.77 $\\to$ 0.79). Attacks from methods that do not learn the distribution of existing human adversaries, meanwhile, degrade robustness.",
        "authors": "A. Sinha, A. Balashankar, A. Beirami, et.al",
        "keywords": [
            "adversarial training",
            "human adversarial examples",
            "robustness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=UAT4j3Y7HP",
        "pdf_src": "https://api2.openreview.net/pdf/3156666f74f82c499dcc5a04f5fc2c9d794dc81e.pdf",
        "Code_src": "",
        "Introduction": "Background: Real-world NLP systems must withstand human adversaries; however, gathering actual adversary data can be costly.\n\nResearch Question: How does one effectively train models against human adversaries without relying solely on expensive real-world examples?\n\nMethod: The authors introduce an adversarial training framework leveraging few human adversary examples in conjunction with synthetic attacks involving minor perturbations like word substitution or synonym replacement (\"word-substitution\" and \"synonym replacement\" are used interchangeably here).\n\nMain Contributions:\n- They show how their method generates high-quality synthetic adversarial examples using just a handful of human-provided examples.\n- Their approach significantly boosts model robustness when tested across both ANLI (a dataset created through an adversarial process including humans and machines) and hate speech detection benchmarks compared to training exclusively on observed human attacks alone—improving performance by over 10% in some cases even after extrapolating beyond initial testing sets into previously unseen rounds where no direct human input was provided during training time itself!\n- Lastly, they highlight potential issues arising if alternative approaches fail to account for distributions learned within genuine human adversaries' strategies leading them towards degradation rather than improvement overall",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "ModuLoRA: Finetuning 2-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers",
        "abstract": "We propose a memory-efficient finetuning algorithm for large language models (LLMs) that supports finetuning LLMs with 65B parameters in 2/3/4-bit precision on as little as one 24GB GPU. Our method, modular low-rank adaptation (ModuLoRA), integrates any user-specified weight quantizer with finetuning via low-rank adapters (LoRAs). Our approach relies on a simple quantization-agnostic backward pass that adaptively materializes low-precision LLM weights from a custom black-box quantization module. This approach enables finetuning 2-bit and 3-bit LLMs for the first time---leveraging state-of-the-art 2-bit QuIP# quantization and 3-bit OPTQ quantization---outperforming finetuning that relies on less sophisticated 4-bit and 8-bit methods. In our experiments, ModuLoRA attains competitive performance on text classification, natural language infernece, and instruction following tasks using significantly less memory than existing approaches, and we also surpass the state-of-the-art ROUGE score on a popular summarization task. We release ModuLoRA together with a series of low-precision models as part of LLMTOOLS, a user-friendly library for quantizing, running, and finetuning LLMs on consumer GPUs.",
        "authors": "J. O. Yin, J. Dong, Y. Wang, et.al",
        "keywords": [
            "memory-efficient",
            "fine-tuning algorithms",
            "low-rank adaptation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=r9p9CV52MV",
        "pdf_src": "https://api2.openreview.net/pdf/c59ecb658a2dd5c49687e94a1c6c35031d1fe9a8.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses challenges associated with fine-tuning very large language models (LLMs) such as those trained by OpenAI or Google Research due to their high computational requirements.\n\nResearch Problem: How can efficient algorithms be developed so that these massive LLMs could be fine-tuned within practical resource constraints?\n\nMethod: The authors introduce Modular Low-Rank Adaptation (ModuLoRA), an algorithm designed specifically for memory-efficient fine-tuning even when working under limited hardware resources like single GPUs without compromising accuracy too much compared to more conventional methods which may require specialized hardware setups.\n \nMain Contributions:\n1. Integration of Weight Quantizers - They allow users to integrate different quantization schemes into the fine-tuning process through LoRAs (Low Rank Adapters).\n2. Custom Backward Pass - A novel backward pass mechanism is introduced; it's quantization-aware but does not rely heavily on gradients during backpropagation allowing for adaptive computation based solely on activations rather than full gradient information leading to significant savings in memory usage.\n3. Novel Quantization Techniques - For the first time ever, they successfully demonstrate the ability to fine-tune both 2-bit and 3-bit LLMs leveraging cutting-edge quantization techniques called QuIP# and OPTQ respectively while outperforming previous work done at higher bit rates (4-bit & 8-bit).\n4. Performance Across Various Tasks - Their proposed model achieves comparable results across several benchmarks including Text Classification, Natural Language Inference, Instruction Following etc., demonstrating its versatility beyond just being optimized for memory efficiency alone.\n5. Software Release - Alongside this research contribution comes open-source software named LLMTOOLS containing pre-trained versions of various bit-depth models ready-to-use along with tools facilitating easy quantization",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Exploit CAM by itself: Complementary Learning System for Weakly Supervised Semantic Segmentation",
        "abstract": "Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has long been suffering from fragmentary object regions led by Class Activation Map (CAM), which is incapable of generating fine-grained masks for semantic segmentation. To guide CAM to find more non-discriminating object patterns, this paper turns to an interesting working mechanism in agent learning named Complementary Learning System (CLS). CLS holds that the neocortex builds a sensation of general knowledge, while the hippocampus specially learns specific details, completing the learned patterns. Motivated by this simple but effective learning pattern, we propose a General-Specific Learning Mechanism (GSLM) to explicitly drive a coarse-grained CAM to a fine-grained pseudo mask. Specifically, GSLM develops a General Learning Module (GLM) and a Specific Learning Module (SLM). The GLM is trained with image-level supervision to extract coarse and general localization representations from CAM. Based on the general knowledge in the GLM, the SLM progressively exploits the specific spatial knowledge from the localization representations, expanding the CAM in an explicit way. To this end, we propose the Seed Reactivation to help SLM reactivate non-discriminating regions by setting a boundary for activation values, which successively identifies more regions of CAM. Without extra refinement processes, our method is able to achieve improvements for CAM of over 20.0% mIoU on PASCAL VOC 2012 and 10.0% mIoU on MS COCO 2014 datasets, representing a new state-of-the-art among existing WSSS methods. The code is publicly available at: https://github.com/tmlr-group/GSLM.",
        "authors": "W. Yang, J. Mai, F. Zhang, et.al",
        "keywords": [
            "General-Specific Learning Mechanism",
            "Weakly Supervised Semantic Segmentation",
            "Seed Reactivation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=KutEe24Yai",
        "pdf_src": "https://api2.openreview.net/pdf/0a5709702220b5b15fc2ff74c02fd79082a2ab2b.pdf",
        "Code_src": "https://github.com/tmlr-group/GSLM",
        "Introduction": "Background:\nWeakly supervised semantic segmentation using only image-level labels often suffers from incomplete object region detection due to limitations within class activation maps (CAMs).\n\nResearch Problem:\nThe problem addressed here is how to improve weakly supervised semantic segmentation so as to generate accurate fine-grained masks.\n\nMethodology:\nThis study introduces a novel approach based on the complementary learning system found in biological agents' brain structure - where the neocortex generates broad knowledge sensations whereas the hippocampus specializes in detailed specifics – termed the General-Specific Learning Mechanism (GSLM). This mechanism involves two modules:\n\n1. A General Learning Module (GLM): Trained under image-level supervision without pixel annotations.\n2. A Specific Learning Module (SLM): Utilizes the output of the GLM's generalized localization information step by step to refine into finer detail through progressive exploitation of spatial specificity.\n\nMain Contributions:\n- Proposes a GSLM architecture designed specifically around the weaknesses observed in standard CAM-based approaches during weakly supervised semantic segmentation tasks;\n- Demonstrates significant improvement beyond previous state-of-the-art performance metrics such as mean Intersection-over-Union (mIoU);\n- Achieves these results across multiple benchmark datasets including Pascal VOC 2012 and MS COCO 2014 without requiring further post-processing steps or additional data annotation efforts;\n- Makes its source code openly accessible via GitHub repository for reproducibility and potential future research advancements.",
        "Topic": "Self-supervised Learning"
    },
    {
        "title": "Enhancing Robustness to Class-Conditional Distribution Shift in Long-Tailed Recognition",
        "abstract": "For long-tailed recognition problem, beyond imbalanced label distribution, unreliable empirical data distribution due to instance scarcity has recently emerged as a concern. It inevitably causes Class-Conditional Distribution (CCD) shift between training and test. Data augmentation and head-to-tail information transfer methods indirectly alleviate the problem by synthesizing novel examples but may remain biased. In this paper, we conduct a thorough study on the impact of CCD shift and propose Distributionally Robust Augmentation (DRA) to directly train models robust to the shift. DRA admits a novel generalization bound reflecting the benefit of distributional robustness to CCD shift for long-tailed recognition. Extensive experiments show DRA greatly improves existing re-balancing and data augmentation methods when cooperating with them. It also alleviates the recently discovered saddle-point issue, verifying its ability to achieve enhanced robustness.",
        "authors": "K. Li, H. Chang, S. Shan, et.al",
        "keywords": [
            "long-tailed recognition",
            "Class-Conditional Distribution shift",
            "Distributionally Robust Augmentation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=n2gAD8Fdzk",
        "pdf_src": "https://api2.openreview.net/pdf/c75edab0d1b55cbb4c46beb8e9322766ca25852f.pdf",
        "Code_src": "",
        "Introduction": "Background: The long-tailed recognition problem refers to scenarios where most classes are well-represented in datasets while only few classes have many samples (\"head\"), leading to an imbalance in class distributions during model training.\n\nResearch Problem: Recent studies highlight that besides the known challenge of unbalanced label distributions which can lead to poor performance at minority classes ('tail'), there is another critical factor - the scarce instances within these tail classes result in an unreliable empirical data distribution compared to what's seen during testing time or validation sets.\nThis discrepancy leads to 'Class-Conditional Distribution Shift' – a mismatching distribution pattern across different stages of learning process causing significant drop in accuracy especially among less frequent classes.\n\nMethods: To tackle this new challenge effectively without introducing bias through synthetic example generation like traditional data augmentation techniques do; authors introduce \"Distributionally Robust Augmentation\" (DRA). This method aims not just balancing labels but ensuring the learned model remains stable against shifts from one dataset distribution to others related closely yet differently enough so they could be encountered post-deployment.\n\nMain Contributions:\n1. Authors identify and quantify how CCD shift impacts classification tasks specifically designed around long-tailed distributions using their proposed metric.\n2. They provide theoretical insights into why distributional robustness helps mitigate such shifts via presenting a novel generalization bound tailored explicitly towards addressing issues arising out of skewed sample sizes per class category.\n3. Experimental results demonstrate significantly improved performances over state-of-the-art approaches particularly under conditions mimicking real-world deployment contexts including alleviating previously identified saddle point problems associated with certain augmentations strategies commonly used today",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "Budgeted Online Model Selection and Fine-Tuning via Federated Learning",
        "abstract": "Online model selection involves selecting a model from a set of candidate models `on the fly' to perform prediction on a stream of data. The choice of candidate models henceforth has a crucial impact on the performance. Although employing a larger set of candidate models naturally leads to more flexibility in model selection, this may be infeasible in cases where prediction tasks are performed on edge devices with limited memory. Faced with this challenge, the present paper proposes an online federated model selection framework where a group of learners (clients) interacts with a server with sufficient memory such that the server stores all candidate models. However, each client only chooses to store a subset of models that can be fit into its memory and performs its own prediction task using one of the stored models. Furthermore, employing the proposed algorithm, clients and the server collaborate to fine-tune models to adapt them to a non-stationary environment. Theoretical analysis proves that the proposed algorithm enjoys sub-linear regret with respect to the best model in hindsight. Experiments on real datasets demonstrate the effectiveness of the proposed algorithm.",
        "authors": "P. M. Ghari, Y. Shen",
        "keywords": [
            "online model selection",
            "federated learning",
            "adaptive algorithms"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=WeiRR8h87X",
        "pdf_src": "https://api2.openreview.net/pdf/70eed312a2d8283e948176488a7f451674991475.pdf",
        "Code_src": "",
        "Introduction": "Background: Online model selection is essential for performing predictions efficiently when dealing with streaming data by choosing among various candidate models dynamically.\n\nResearch Problem: How do we effectively select suitable candidate models while considering constraints like computational resources available at edge devices?\n\nMethodology: This study introduces an online federated learning-based approach called FedModelSelect. It allows multiple local learners (clients), which have limited storage capacity due to resource constraints typically found in edge devices or IoTs, to collaboratively choose subsets of candidate models they need within their memory limits based on current data streams without requiring storing every single model locally.\nThe server maintains access to complete sets but does not actively participate in individual predictions; instead it assists through collaborative fine-tuning processes aimed at adapting selected models over time as conditions change - known as non-stationarity.\n\nMain Contributions:\n1. Developed a novel federated architecture specifically designed around the limitations faced during dynamic model adaptation under constrained environments;\n2. Proposed algorithms enabling efficient collaboration between distributed agents (learners) and central servers ensuring minimal information exchange necessary yet still allowing effective adjustments across different scenarios;\n3. Demonstrated theoretical guarantees regarding regret minimization against optimal choices made post hoc (i.e., after seeing actual outcomes), showing our method converges quickly towards better-performing models despite initial uncertainty about future requirements;\n4. Conducted empirical validations demonstrating practical applicability via experiments conducted utilizing real-world datasets confirming efficacy compared traditional approaches",
        "Topic": "Federated Learning"
    },
    {
        "title": "A density estimation perspective on learning from pairwise human preferences",
        "abstract": "Learning from human feedback (LHF)—and in particular learning from pairwise preferences—has recently become a crucial ingredient in training large language models (LLMs), and has been the subject of much research. Most recent works frame it as a reinforcement learning problem, where a reward function is learned from pairwise preference data and the LLM is treated as a policy which is adapted to maximize the rewards, often under additional regularization constraints. We propose an alternative interpretation which centers on the generative process for pairwise preferences and treats LHF as a density estimation problem. We provide theoretical and empirical results showing that for a family of generative processes defined via preference behavior distribution equations, training a reward function on pairwise preferences effectively models an annotator's implicit preference distribution. Finally, we discuss and present findings on \"annotator misspecification\"—failure cases where wrong modeling assumptions are made about annotator behavior, resulting in poorly-adapted models—suggesting that approaches that learn from pairwise human preferences could have trouble learning from a population of annotators with diverse viewpoints.",
        "authors": "V. Dumoulin, D. D. Johnson, P. S. Castro, et.al",
        "keywords": [
            "pairwise preferences",
            "reinforcement learning",
            "density estimation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=YH3oERVYjF",
        "pdf_src": "https://api2.openreview.net/pdf/9e288a059cccf65fa3978f4eba9d3978393752bf.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses how learning from human feedback plays a critical role in training advanced language models like ChatGPT or GPT-3.\n\nResearch Question: How can we better understand and apply methods based on human feedback?\n\nMethodology: Instead of framing this task within the context of reinforcement learning problems—which involves defining a reward function—the authors suggest viewing it through the lens of density estimation theory by focusing on the generative aspect underlying pairwise preferences between different options.\n\nMain Contributions:\n1. They introduce a new perspective into understanding how humans generate their preferences.\n2. By using mathematical tools related to probability distributions they show that if you train your model according to certain generative processes, then those trained models will implicitly capture what might be considered 'the annotator’s implicit preference distribution'.\n3. Additionally, there was some discussion around potential issues when applying these techniques due to variations among annotators' perspectives leading them to term this phenomenon as “annotator misspecification”.",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "Leveraging Endo- and Exo-Temporal Regularization for Black-box Video Domain Adaptation",
        "abstract": "To enable video models to be applied seamlessly across video tasks in different environments, various Video Unsupervised Domain Adaptation (VUDA) methods have been proposed to improve the robustness and transferability of video models. Despite improvements made in model robustness, these VUDA methods require access to both source data and source model parameters for adaptation, raising serious data privacy and model portability issues. To cope with the above concerns, this paper firstly formulates Black-box Video Domain Adaptation (BVDA) as a more realistic yet challenging scenario where the source video model is provided only as a black-box predictor. While a few methods for Black-box Domain Adaptation (BDA) are proposed in the image domain, these methods cannot apply to the video domain since video modality has more complicated temporal features that are harder to align. To address BVDA, we propose a novel Endo and eXo-TEmporal Regularized Network (EXTERN) by applying mask-to-mix strategies and video-tailored regularizations. They are the endo-temporal regularization and exo-temporal regularization, which are performed across both clip and temporal features, while distilling knowledge from the predictions obtained from the black-box predictor. Empirical results demonstrate the state-of-the-art performance of EXTERN across various cross-domain closed-set and partial-set action recognition benchmarks, which even surpasses most existing video domain adaptation methods with source data accessibility. Code will be available at https://xuyu0010.github.io/b2vda.html.",
        "authors": "Y. Xu, J. Yang, H. Cao, et.al",
        "keywords": [
            "black-box video domain adaptation",
            "Extern",
            "temporal regularization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=icoP08mrQJ",
        "pdf_src": "https://api2.openreview.net/pdf/991cd6d574e632c216e0aac5169f058bcfeb9ee8.pdf",
        "Code_src": "Code link: https://xuyu0010.github.io/b2vda.html",
        "Introduction": "Background: The seamless application of video models across diverse video tasks remains an open challenge due to their varying environmental conditions requiring significant adaptability.\n\nResearch Problem: Existing unsupervised domain adaptation (VUDA) approaches enhance model robustness but necessitate access to source data and model parameters leading to privacy and portability challenges.\n \nMethodology: This study introduces Black-box Video Domain Adaptation (BVDA), a new paradigm where no direct access to the source model's internals or training data is allowed; instead, it functions solely through its output predictions on unseen videos.\n\nMain Contributions:\n1. Formulation of BVDA problem within practical constraints without access to internal model details nor original dataset.\n2. Development of a novel network architecture called EXTERN incorporating two types of temporal regularizations – endo-temporal and exo-temporal - designed specifically for handling video modalities' complex temporal dynamics using mask-to-mix techniques tailored towards video processing.\n3. Demonstrated superior performance against several benchmark datasets compared not just other BDA solutions applicable mainly limited to static images domains but also traditional VUDA methods when given some degree of data availability indicating potential superiority under fully blind scenarios too.\n4. Open-sourcing code implementation at [GitHub repository link] for reproducibility and further research advancement.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Semantic similarity prediction is better than other semantic similarity measures",
        "abstract": "Semantic similarity between natural language texts is typically measured either by looking at the overlap between subsequences (e.g., BLEU) or by using embeddings (e.g., BERTScore, S-BERT). Within this paper, we argue that when we are only interested in measuring the semantic similarity, it is better to directly predict the similarity using a fine-tuned model for such a task. Using a fine-tuned model for the Semantic Textual Similarity Benchmark tasks (STS-B) from the GLUE benchmark, we define the STSScore approach and show that the resulting similarity is better aligned with our expectations on a robust semantic similarity measure than other approaches.",
        "authors": "S. Herbold",
        "keywords": [
            "STSScore",
            "Semantic Textual Similarity",
            "Fine-tuned Model"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=bfsNmgN5je",
        "pdf_src": "https://api2.openreview.net/pdf/91deb05a364f1175e595f33b7b19755938a6785d.pdf",
        "Code_src": "",
        "Introduction": "Background: Measuring semantic similarity of natural language texts involves comparing their meaning rather than just counting word overlaps.\n\nResearch Question: How can one effectively measure semantic similarity without relying solely on subsequence overlap?\n\nMethodology: We propose an alternative method called STSScore which uses a pre-trained model fine-tuned specifically for semantic textual similarity prediction based on the STS-B dataset within the GLUE benchmark.\n\nMain Contributions:\n1. The use of a fine-tuned model allows direct prediction of semantic similarity.\n2. Our proposed STSScore outperforms existing methods like BLEU and embedding-based metrics as it aligns more closely with intuitive human judgments about semantic similarity measures being robust against noise/distortions while still capturing meaningful differences among texts under consideration.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Empowering GNNs via Edge-Aware Weisfeiler-Leman Algorithm",
        "abstract": "Message passing graph neural networks (GNNs) are known to have their expressiveness upper-bounded by 1-dimensional Weisfeiler-Leman (1-WL) algorithm. To achieve more powerful GNNs, existing attempts either require \\emph{ad hoc} features, or involve operations that incur high time and space complexities. In this work, we propose a \\textit{general} and \\textit{provably powerful} GNN framework that preserves the \\textit{scalability} of the message passing scheme. In particular, we first propose to empower 1-WL for graph isomorphism test by considering edges among neighbors, giving rise to NC-1-WL. The expressiveness of NC-1-WL is shown to be strictly above 1-WL and below 3-WL theoretically. Further, we propose the NC-GNN framework as a differentiable neural version of NC-1-WL. Our simple implementation of NC-GNN is provably as powerful as NC-1-WL. Experiments demonstrate that our NC-GNN performs effectively and efficiently on various benchmarks.",
        "authors": "M. Liu, H. Yu, S. Ji",
        "keywords": [
            "NC-1-WL",
            "Graph Isomorphism Test",
            "Differentiable Neural Networks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=VDy6LgErFM",
        "pdf_src": "https://api2.openreview.net/pdf/3a825cb5fc7d829acd960c3180b83e4d3acd9b4c.pdf",
        "Code_src": "",
        "Introduction": "Background: Graph Neural Networks (GNNs) can model complex relationships between nodes in graphs but may not always capture higher-order interactions due to limitations such as the expressive power of the underlying algorithms.\n\nResearch Problem: How to design a general and provably powerful GNN framework with scalability?\n\nMethods: This paper proposes an extension called Non-commutative-1-WL (NC-1-WL), which improves upon the original 1D Weisfeiler-Leman (WL) algorithm used within GNNs through incorporating edge information into the WL process without sacrificing computational efficiency.\nThe authors also introduce the NC-GNN framework based on NC-1-WL - a differentiable neural network architecture capable of learning from graph data while maintaining the theoretical guarantees provided by NC-1-WL.\n\nMain Contributions:\n1. A novel non-commutative variant of the 1-WL algorithm – NC-1-WL -- that increases the expressivity beyond what was achievable before yet maintains polynomial complexity similar to standard GNNs.\n2. An NC-GNN framework derived from NC-1-WL providing both theoretical guarantees about its expressive capabilities equivalent to those of NC-1-WL along with practical benefits like being amenable to efficient computation using modern hardware architectures designed around tensor processing units (TPUs).\n3. Experimental validation across multiple datasets demonstrating superior performance over other state-of-the-art methods when it comes to tasks involving graph classification",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "ASPEST: Bridging the Gap Between Active Learning and Selective Prediction",
        "abstract": "Selective prediction aims to learn a reliable model that abstains from making predictions when uncertain. These predictions can then be deferred to humans for further evaluation. As an everlasting challenge for machine learning, in many real-world scenarios, the distribution of test data is different from the training data. This results in more inaccurate predictions, and often increased dependence on humans, which can be difficult and expensive. Active learning aims to lower the overall labeling effort, and hence human dependence, by querying the most informative examples. Selective prediction and active learning have been approached from different angles, with the connection between them missing. In this work, we introduce a new learning paradigm, active selective prediction, which aims to query more informative samples from the shifted target domain while increasing accuracy and coverage. For this new paradigm, we propose a simple yet effective approach, ASPEST, that utilizes ensembles of model snapshots with self-training with their aggregated outputs as pseudo labels. Extensive experiments on numerous image, text and structured datasets, which suffer from domain shifts, demonstrate that ASPEST can significantly outperform prior work on selective prediction and active learning (e.g. on the MNIST$\\to$SVHN benchmark with the labeling budget of 100, ASPEST improves the AUACC metric from 79.36% to 88.84%) and achieves more optimal utilization of humans in the loop.",
        "authors": "J. Chen, J. Yoon, S. Ebrahimi, et.al",
        "keywords": [
            "domain shift",
            "active learning",
            "selective prediction"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=3nprbNR3HB",
        "pdf_src": "https://api2.openreview.net/pdf/38a20ad8348393b84668d9a7c09ffe31f22ddb1b.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the problem of improving predictive models' reliability through selective prediction techniques where uncertainty prompts deferral back to humans.\n\nResearch Question: How does one effectively integrate selective prediction principles into existing active learning frameworks?\n\nMethodology: To address these challenges, they develop \"Active Selective Prediction\" or ASPEST—a novel framework combining ensemble-based selective prediction methods along with active learning strategies—using self-training approaches based on aggregated output pseudo-labels across multiple model snapshots.\n\nMain Contributions:\n1. They present a unified learning paradigm called Active Selective Prediction.\n2. Introduce ASPEST—an algorithmic implementation of Active Selective Prediction—that leverages ensembles using snapshot models trained at various epochs during iterative refinement cycles alongside self-training processes guided by those same ensembles’ aggregated predictions serving as pseudo-labels towards improved performance over traditional selective prediction and active learning alone without requiring additional labeled data beyond initial setup costs associated with creating the ensemble itself.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Global Convergence of SGD For Logistic Loss on Two Layer Neural Nets",
        "abstract": "In this note, we demonstrate a first-of-its-kind provable convergence of SGD to the global minima of appropriately regularized logistic empirical risk of depth $2$ nets -- for arbitrary data with any number of gates with adequately smooth and bounded activations, like sigmoid and tanh, and for a class of distributions from which the initial weight is sampled. We also prove an exponentially fast convergence rate for continuous time SGD that also applies to smooth unbounded activations like SoftPlus. Our key idea is to show that the logistic loss function on any size neural net can be Frobenius norm regularized by a width-independent parameter such that the regularized loss is a ``Villani function'' -- and thus be able to build on recent progress with analyzing SGD on such objectives.",
        "authors": "P. Gopalani, S. Jha, A. Mukherjee",
        "keywords": [
            "SGD",
            "Global Minima",
            "Logistic Empirical Risk"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=9TqAUYB6tC",
        "pdf_src": "https://api2.openreview.net/pdf/ecb3edda950f151a2e1375c7b8c44ebc3c1475e0.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the problem of training deep neural networks using stochastic gradient descent (SGD), particularly focusing on the convergence properties when optimizing the logistic empirical risk.\n\nResearch Question: The main research question concerns proving the convergence of SGD towards the global minimum of the logistic empirical risk in neural networks where the activation functions are sufficiently smooth and bounded – including popular choices like sigmoid and tanh - regardless of the network's architecture or the distribution of its weights at initialization.\n\nMethods: To address these questions, novel regularization techniques based on the Frobenius norm are introduced into the logistic loss function without affecting the width of the network. This allows them to define \"Villani functions\" within the context of neural networks' optimization landscape under SGD dynamics.\n \nMain Contributions:\n1. Provable Convergence: They provide theoretical guarantees showing that SGD converges to the global minimum even if the starting point does not lie close enough; they do so through a new type of regularization applied directly to the logistic loss itself rather than modifying the network structure as traditionally done via dropout layers etc.\n2. Exponential Rate of Convergence: For continuous-time variants of SGD algorithms dealing with smoother but still potentially unbounded activations—like SoftPlus—they establish an exponential convergence rate bound beyond what previous analyses could achieve due to their approach being applicable across different activation types while maintaining robustness against variance issues common among other methods relying solely upon Lipschitz continuity assumptions about gradients alone).",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Effective Latent Differential Equation Models via Attention and Multiple Shooting",
        "abstract": "Scientific Machine Learning (SciML) is a burgeoning field that synergistically combines domain-aware and interpretable models with agnostic machine learning techniques. In this work, we introduce GOKU-UI, an evolution of the SciML generative model GOKU-nets. GOKU-UI not only broadens the original model's spectrum to incorporate other classes of differential equations, such as Stochastic Differential Equations (SDEs), but also integrates attention mechanisms and a novel multiple shooting training strategy in the latent space. These modifications have led to a significant increase in its performance in both reconstruction and forecast tasks, as demonstrated by our evaluation on simulated and empirical data. Specifically, GOKU-UI outperformed all baseline models on synthetic datasets even with a training set 16-fold smaller, underscoring its remarkable data efficiency. Furthermore, when applied to empirical human brain data, while incorporating stochastic Stuart-Landau oscillators into its dynamical core, our proposed enhancements markedly increased the model's effectiveness in capturing complex brain dynamics. GOKU-UI demonstrated a reconstruction error five times lower than other baselines, and the multiple shooting method reduced the GOKU-nets prediction error for future brain activity up to 15 seconds ahead. By training GOKU-UI on resting state fMRI data, we encoded whole-brain dynamics into a latent representation, learning a low-dimensional dynamical system model that could offer insights into brain functionality and open avenues for practical applications such as the classification of mental states or psychiatric conditions. Ultimately, our research provides further impetus for the field of Scientific Machine Learning, showcasing the potential for advancements when established scientific insights are interwoven with modern machine learning.",
        "authors": "G. Abrevaya, M. Ramezanian-panahi, J. Gagnon-audet, et.al",
        "keywords": [
            "SciML",
            "Generative Models",
            "Brain Dynamics"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=uxNfN2PU1W",
        "pdf_src": "https://api2.openreview.net/pdf/443d9d9ae56bc9d5e90a96078826d834e1e949ae.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper introduces GOKU-UI, which evolves from the existing SciML generative model called GOKU-nets.\n\nResearch Problem: To broaden the scope of the original model beyond ordinary differential equations (ODEs) to include stochastic differential equations (SDEs). Additionally, they aim at improving the model’s performance through integrating attention mechanisms and a new multiple shooting training strategy within the latent space.\n\nMethods: They integrate these two innovations – attention mechanism & multiple shooting training - directly onto their previous architecture known as GOKU-nets.\n \nMain Contributions:\n1. Data Efficiency: On synthetic datasets where there was less available information compared to competitors, GOKU-UI achieved better results despite having significantly fewer samples during training time.\n2. Empirical Brain Dynamics: When tested against real-world human brain data including stochastic processes like Stuart-Landau oscillators found in neuronal networks; it showed improved accuracy over traditional methods leading to more precise predictions about future brain activities—up to fifteen seconds forward!\n3. Low-Dimensional Dynamical System Model: Trained using resting-state functional magnetic resonance imaging (fMRI) data points across different subjects’ brains resulted in encoding dynamic patterns into a simplified yet informative form allowing researchers insight into how neural systems function under normal circumstances without needing access to raw signals themselves thus paving ways towards clinical diagnostics tools based off these learned representations rather than just relying solely upon direct measurements taken via specialized equipment inside hospitals etcetera…",
        "Topic": "Generative Models"
    },
    {
        "title": "Statistical Component Separation for Targeted Signal Recovery in Noisy Mixtures",
        "abstract": "Separating signals from an additive mixture may be an unnecessarily hard problem when one is only interested in specific properties of a given signal. In this work, we tackle simpler \"statistical component separation\" problems that focus on recovering a predefined set of statistical descriptors of a target signal from a noisy mixture. Assuming access to samples of the noise process, we investigate a method devised to match the statistics of the solution candidate corrupted by noise samples with those of the observed mixture. We first analyze the behavior of this method using simple examples with analytically tractable calculations. Then, we apply it in an image denoising context employing 1) wavelet-based descriptors, 2) ConvNet-based descriptors on astrophysics and ImageNet data. In the case of 1), we show that our method better recovers the descriptors of the target data than a standard denoising method in most situations. Additionally, despite not constructed for this purpose, it performs surprisingly well in terms of peak signal-to-noise ratio on full signal reconstruction. In comparison, representation 2) appears less suitable for image denoising. Finally, we extend this method by introducing a diffusive stepwise algorithm which gives a new perspective to the initial method and leads to promising results for image denoising under specific circumstances.",
        "authors": "B. R. Blancard, M. Eickenberg",
        "keywords": [
            "statistical component separation",
            "wavelet-based descriptors",
            "diffusive stepwise algorithm"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=OUWG6O4yo9",
        "pdf_src": "https://api2.openreview.net/pdf/160965f4f15ec842fb4f7f05b471e444f9e00134.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the issue of separating desired signals within complex mixtures where interest lies solely in certain aspects rather than reconstructing or identifying all components.\n\nResearch Problem: It focuses on statistical component separation - specifically, retrieving pre-defined statistical features of a particular signal embedded in a noisy environment while having prior knowledge about the noise distribution through sample observations.\n\nMethods: The authors propose matching the statistics between the corrupted version of their proposed solutions – obtained after adding noise drawn from known distributions -- against the observed mixed signal's statistics as means to recover these features accurately.\n \nMain Contributions:\n- Analytical Examples: They demonstrate how such methods can behave theoretically via simple illustrative cases amenable to analytical computations,\n- Wavelet Descriptors: Applied to image processing tasks like denoising; they find improvements over conventional approaches particularly beneficial across various scenarios without necessarily being optimized for them initially,\n- ConvNet Descriptors: Tested further applying convolutional neural network (CNN)-based descriptors both on astronomical images and datasets commonly used in computer vision research (ImageNet). While CNNs are generally designed differently compared to traditional algorithms focused purely on feature extraction but still perform reasonably good at preserving some information during denoising processes even if not explicitly trained towards this goal,\n- Diffusive Stepwise Algorithm: Introduced later into their framework introduces incremental updates akin diffusion processes leading potentially more robust recovery outcomes especially suited conditions related directly backtracking original signal characteristics amidst noise interference patterns encountered throughout practical applications involving real-world imagery sources.",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Kernel Normalized Convolutional Networks",
        "abstract": "Existing convolutional neural network architectures frequently rely upon batch normalization (BatchNorm) to effectively train the model. BatchNorm, however, performs poorly with small batch sizes, and is inapplicable to differential privacy. To address these limitations, we propose the kernel normalization (KernelNorm) and kernel normalized convolutional layers, and incorporate them into kernel normalized convolutional networks (KNConvNets) as the main building blocks. We implement KNConvNets corresponding to the state-of-the-art ResNets while forgoing the BatchNorm layers. Through extensive experiments, we illustrate that KNConvNets achieve higher or competitive performance compared to the BatchNorm counterparts in image classification and semantic segmentation. They also significantly outperform their batch-independent competitors including those based on layer and group normalization in non-private and differentially private training. Given that, KernelNorm combines the batch-independence property of layer and group normalization with the performance advantage of BatchNorm.",
        "authors": "R. Nasirigerdeh, R. Torkzadehmahani, D. Rueckert, et.al",
        "keywords": [
            "kernel normalization",
            "kernel normalized convolutional networks",
            "differential privacy"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Uv3XVAEgG6",
        "pdf_src": "https://api2.openreview.net/pdf/f3b11c66e4f88f4100fd5809974043b64087644e.pdf",
        "Code_src": "",
        "Introduction": "Background: Convolutional Neural Network (CNN) architectures often use batch normalization (BatchNorm) during training due to its effectiveness; however, it struggles when dealing with small batch sizes.\n\nResearch Problem: How can CNNs be trained without relying on BatchNorm especially considering issues like poor performance under small batch sizes?\n\nMethods: The paper introduces a novel approach called kernel normalization (KernelNorm), which normalizes along feature channels rather than mini-batches within each channel using kernels. This method aims at addressing both problems mentioned above by being independent of batch size just like layer/group normalization but maintaining similar performance benefits seen from BatchNorm.\n\nMain Contributions:\n1. Propose kernel normalization (KernelNorm) and kernel normalized convolutional layers.\n2. Introduce kernel normalized convolutional networks (KNConvNets).\n3. Implement KNConvNets equivalent to state-of-the-art ResNet models where BatchNorm has been replaced entirely with KernelNorm.\n4. Conduct comprehensive experimental evaluations across various datasets showing superior or comparable accuracy over traditional BatchNorm-based CNNs even though they are not designed specifically for small batches nor do they provide differential privacy guarantees themselves - unlike other methods such as layer/group normalization techniques used mainly because they're more computationally efficient yet less performant traditionally before this work was done).",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "Are Population Graphs Really as Powerful as Believed?",
        "abstract": "Population graphs and their use in combination with graph neural networks (GNNs) have demonstrated promising results for multi-modal medical data integration and improving disease diagnosis and prognosis. Several different methods for constructing these graphs and advanced graph learning techniques have been established to maximise the predictive power of GNNs on population graphs. However, in this work, we raise the question of whether existing methods are really strong enough by showing that simple baseline methods --such as random forests or linear regressions--, perform on par with advanced graph learning models on several population graph datasets for a variety of different clinical applications. We use the commonly used public population graph datasets TADPOLE and ABIDE, a brain age estimation and a cardiac dataset from the UK Biobank, and a real-world in-house COVID dataset. We (a) investigate the impact of different graph construction methods, graph convolutions, and dataset size and complexity on GNN performance and (b) discuss the utility of GNNs for multi-modal data integration in the context of population graphs. Based on our results, we argue towards the need for \"better\" graph construction methods or innovative applications for population graphs to render them beneficial.",
        "authors": "T. T. Müller, S. Starck, K. Bintsi, et.al",
        "keywords": [
            "graph construction",
            "graph neural networks",
            "multi-modal medical data integration"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=TTRDCVnbjI",
        "pdf_src": "https://api2.openreview.net/pdf/e581a75c615ef742b6233c1966a223bd3f3c6ba2.pdf",
        "Code_src": "",
        "Introduction": "Background: Population graphs combined with Graph Neural Networks (GNNs) show promise in integrating multi-modal medical data.\nResearch Question: Are current state-of-the-art graph learning methods truly superior compared to simpler approaches like random forests or linear regression?\nMethods: The authors evaluate various graph construction methods using common datasets such as TADPOLE, ABIDE, and an internal COVID-19 dataset; they also examine how factors including graph convolution types and dataset size affect model performance.\n\nMain Contributions:\n1. They demonstrate through empirical evidence across multiple datasets applied broadly within clinical settings - brain aging prediction & cardiology, and specifically during the pandemic – that more complex graph learning algorithms do not necessarily outperform basic machine learning models when it comes to predicting outcomes based on population-level health information represented via graphs.\n2. This suggests there is room for improvement either in refining graph construction methodologies themselves which could potentially lead to better predictions without requiring sophisticated computational resources—or alternatively—to explore novel ways leveraging less computationally intensive but effective strategies tailored toward specific domains where population graphs might be employed effectively beyond just healthcare diagnostics/monitoring purposes alone.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Learning from Natural Language Feedback",
        "abstract": "The potential for pre-trained large language models (LLMs) to use natural language feedback at inference time has been an exciting recent development. We build upon this observation by formalizing an algorithm for learning from natural language feedback at training time instead, which we call Imitation learning from Language Feedback (ILF). ILF requires only a small amount of human-written feedback during training and does not require the same feedback at test time, making it both user-friendly and sample-efficient. We further show that ILF can be seen as a form of minimizing the KL divergence to the target distribution and demonstrate proof-of-concepts on text summarization and program synthesis tasks. For code generation, ILF improves a Codegen-Mono 6.1B model's pass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python Problems (MBPP) benchmark, outperforming both fine-tuning on MBPP and fine-tuning on repaired programs written by humans. For summarization, we show that ILF can be combined with learning from human preferences to improve a GPT-3 model's summarization performance to be comparable to human quality, outperforming fine-tuning on human-written summaries.  Overall, our results suggest that learning from human-written natural language feedback is both more effective and sample-efficient than training exclusively on demonstrations for improving an LLM's performance on a variety of tasks.",
        "authors": "A. Chen, J. Scheurer, J. A. Campos, et.al",
        "keywords": [
            "ILF",
            "Natural Language Feedback",
            "Sample-Efficient"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=xo3hI5MwvU",
        "pdf_src": "https://api2.openreview.net/pdf/3065588fd92350e6d695cbc5a73211f749cd6daa.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses how pre-trained large language models (LLMs) could potentially utilize natural language feedback when generating responses or performing certain tasks.\n\nResearch Problem: The study aims to explore whether these models might also benefit if they are trained using such feedback rather than solely relying on demonstration data in their initial training phase.\n \nMethodology: To address this issue, researchers developed \"Imitation Learning from Language Feedback\" (ILF), where they incorporated human-written feedback into the training process without requiring any additional feedback once testing begins—making it efficient while being friendly towards users who provide input.\n\nMain Contributions:\n1. They introduced ILF—a novel approach allowing LLMs to learn directly from textual instructions given alongside examples within datasets used throughout training stages; unlike other methods like reinforcement learning via imitation,\n2. Demonstrated effectiveness across multiple domains including text summarization & code generation tasks;\n3. Compared against existing techniques – found ILF significantly improved performance metrics compared to traditional approaches based purely on demonstrated behavior alone e.g., passing rates increased by up to 38%, surpassing those obtained through manual repair work done manually beforehand!",
        "Topic": "Large Language Models"
    },
    {
        "title": "InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers",
        "abstract": "We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which—as we found out—produced more accurate rankers compared to a proprietary GPT-3 model.  On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available. https://github.com/searchivarius/inpars_light/",
        "authors": "L. Boytsov, P. Patel, V. Sourabh, et.al",
        "keywords": [
            "reproducibility",
            "InPars-light",
            "unsupervised"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=sHSKFYyINO",
        "pdf_src": "https://api2.openreview.net/pdf/72608a73f6385b0d94042b65e39cd6e34ce457f7.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper focuses on improving the performance of unsupervised machine learning methods specifically designed for text ranking tasks.\n\nResearch Question: How can an effective unsupervised neural ranking system be created while reducing computational costs?\n\nMethod: The authors conducted a reproducibility analysis focusing on \"InPars,\" originally proposed as one such approach but requiring large-scale resources like expensive proprietary language models or specialized hardware infrastructure due to its reliance on complex ranking models trained from scratch without supervision (\"from-scratch\"). They then introduced \"InPars-light\" - a simplified version of this initial work aimed primarily towards making it practical through reduced resource demands.\n \nMain Contributions:\n1. Developed “InPars-light,” which significantly reduces computational requirements – instead of utilizing costly proprietary models they use open-source alternatives including Bloom (a freely available language model).\n2. Demonstrated improved accuracy when comparing against BM25 baseline across multiple datasets despite having much fewer parameters within their ranking systems; achieving up to ~30% improvement depending upon dataset type.\n3. Achieved comparable performance levels between different architectures tested under similar conditions where prompts are given during inference time (i.e., three-shot prompting setup); suggesting that even less powerful architectures could potentially achieve competitive rankings if optimized accordingly via prompt engineering techniques alone rather than solely relying on architecture size increases traditionally associated with better performance outcomes seen historically before recent advancements in natural language processing methodologies have been realized.\n\nConclusion: This research highlights how certain modifications allow us not just replicate existing state-of-the-art approaches efficiently yet also surpass them whilst drastically cutting down overall expenses involved",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Why should autoencoders work?",
        "abstract": "Deep neural network autoencoders are routinely used computationally for model reduction. They allow recognizing the intrinsic dimension of data that lie in a $k$-dimensional subset $K$ of an input Euclidean space $\\mathbb{R}^n$. The underlying idea is to obtain both an encoding layer that maps $\\mathbb{R}^n$ into $\\mathbb{R}^k$ (called the bottleneck layer or the  space of latent variables) and a decoding layer that maps $\\mathbb{R}^k$ back into $\\mathbb{R}^n$, in such a way that the input data from the set $K$ is recovered when composing the two maps. This is achieved by adjusting parameters (weights) in the network to minimize the discrepancy between the input and the reconstructed output. Since neural networks (with continuous activation functions) compute continuous maps, the existence of a network that achieves perfect reconstruction would imply that $K$ is homeomorphic to a $k$-dimensional subset of $\\mathbb{R}^k$, so clearly there are topological obstructions to finding such a network. On the other hand, in practice the technique is found to ``work'' well, which leads one to ask if there is a way to explain this effectiveness. We show that, up to small errors, indeed the method is guaranteed to work. This is done by appealing to certain facts from differential topology. A computational example is also included to illustrate the ideas.",
        "authors": "M. Kvalheim, E. Sontag",
        "keywords": [
            "autoencoder",
            "dimensionality reduction",
            "neural network"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=uGVFtjvI3v",
        "pdf_src": "https://api2.openreview.net/pdf/8c70d79c44bc73f53afbcb53a7a1d7ab75d977ff.pdf",
        "Code_src": "",
        "Introduction": "Background: Deep neural network autoencoders have been widely applied as a tool for reducing computational complexity through dimensionality reduction techniques.\n\nResearch Problem: Despite their practical success with dimensionality reduction tasks like feature extraction on high-dimensional datasets, it remains unclear whether these autoencoders can achieve exact reconstruction due to potential topological obstructions within the input spaces.\n\nMethods: To address uncertainty about the feasibility of exact reconstruction using deep neural network autoencoders, we appeal to results drawn from differential topology theory regarding embeddings onto lower dimensional subspaces without loss of information beyond some error threshold.\n\nMain Contributions: Our main contribution lies in demonstrating theoretically - under specific conditions related to differential topology – that while no universal autoencoder will reconstruct exactly all inputs given any arbitrary dataset, our approach does guarantee approximate reconstruction provided that the embedding subspace has sufficient capacity relative to the original ambient space dimensions. Additionally, including empirical evidence via computational examples further validates theoretical findings suggesting how close approximations may be achievable even though strict exactness cannot always be ensured across diverse datasets.\n \nNote: Please note my summary might not capture every nuance but attempts to encapsulate key points based upon your instructions",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Cognitive Architectures for Language Agents",
        "abstract": "Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within the broader history of AI and outlines a path towards language-based general intelligence.",
        "authors": "T. Sumers, S. Yao, K. R. Narasimhan, et.al",
        "keywords": [
            "language agents",
            "cognitive architectures",
            "multimodal reasoning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=1i6ZCvflQJ",
        "pdf_src": "https://api2.openreview.net/pdf/07475727a2787df12052196b9f2624497ef3c3bd.pdf",
        "Code_src": "",
        "Introduction": "Background: Recent advancements in natural language processing involve augmenting LLMs with additional information sources such as the internet through techniques like prompt chaining.\n\nResearch Question: The research question addressed is how can current approaches be organized systematically? Additionally, what are potential avenues toward developing even more advanced intelligent systems?\n\nMethodology: To address these questions, the authors borrow from cognitive science theories about human cognition which emphasize modularity along with symbol manipulation capabilities - proposing \"Cognitive Architectures for Language Agents\" (CoALA).\n\nMain Contributions:\n1. **Framework Proposal**: They introduce an architecture that integrates various functionalities into one coherent system.\n2. **Modular Memory Components**: This includes distinct types of memories tailored specifically for different purposes including semantic understanding etc.\n3. **Structured Action Space**: A defined set of operations allows interaction between internal representations and environmental contexts.\n4. **Decision-Making Process**: An overarching mechanism guiding selection among possible courses of action based on goals/preferences.\n5. **Retrospective Survey & Organization**: Utilizing their proposed model they provide a comprehensive overview of past works related to grounded language agents.\n6. **Prospective Planning**: By identifying gaps present in contemporary methods using CoALA, it suggests promising paths forward aiming at creating more sophisticated linguistic agents capable of demonstrating genuine intelligence beyond mere task completion capability alone.",
        "Topic": "Large Language Models"
    },
    {
        "title": "Mixed Nash for Robust Federated Learning",
        "abstract": "We study robust federated learning (FL) within a game theoretic framework to alleviate the server vulnerabilities to even an informed adversary who can tailor training-time attacks. Specifically, we introduce RobustTailor, a simulation-based framework that prevents the adversary from being omniscient and derives its convergence guarantees. RobustTailor improves robustness to training-time attacks significantly while preserving almost the same privacy guarantees as standard robust aggregation schemes in FL. Empirical results under challenging attacks show that RobustTailor performs close to an upper bound with perfect knowledge of honest clients.",
        "authors": "W. Xie, T. Pethick, A. Ramezani-kebrya, et.al",
        "keywords": [
            "Robust federated learning",
            "Game theoretic framework",
            "Training-time attacks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=mqMzerrVOB",
        "pdf_src": "https://api2.openreview.net/pdf/8612d7c1c881184fd2cb60aff7a3bedae43c5f3d.pdf",
        "Code_src": "",
        "Introduction": "Background: Federated Learning (FL) is widely used for decentralized machine learning tasks where data remains on edge devices rather than centralized servers due to privacy concerns. However, traditional FL methods are vulnerable to adversarial attacks during the training phase.\n\nResearch Problem: How to design a robust FL algorithm against adversaries capable of tailoring their attacks based on information about the system?\n\nMethod: We propose RobustTailor, which simulates potential adversaries' actions by considering different attack strategies at each round.\nRobustTailor incorporates regularization terms into the FL objective function designed specifically to counteract tailored attacks without compromising privacy guarantees similar to those provided by standard robust aggregation schemes.\n\nMain Contributions:\n1. RobustTailor ensures that adversaries cannot exploit any prior knowledge they might have gained regarding the model or client behavior through simulations throughout the FL process,\n2. It provides theoretical convergence guarantees despite the presence of such adversaries; \n3. Experimental validation demonstrates significant improvements over existing robust FL algorithms when facing sophisticated adversarial scenarios",
        "Topic": "Federated Learning"
    },
    {
        "title": "From Continuous Dynamics to Graph Neural Networks: Neural Diffusion and Beyond",
        "abstract": "Graph neural networks (GNNs) have demonstrated significant promise in modelling relational data and have been widely applied in various fields of interest. The key mechanism behind GNNs is the so-called message passing where information is being iteratively aggregated to central nodes from their neighbourhood. Such a scheme has been found to be intrinsically linked to a physical process known as heat diffusion, where the propagation of GNNs naturally corresponds to the evolution of heat density. Analogizing the process of message passing to the heat dynamics allows to fundamentally understand the power and pitfalls of GNNs and consequently informs better model design. Recently, there emerges a plethora of works that proposes GNNs inspired from the continuous dynamics formulation, in an attempt to mitigate the known limitations of GNNs, such as oversmoothing and oversquashing. In this survey, we provide the first systematic and comprehensive review of studies that leverage the continuous perspective of GNNs. To this end, we introduce foundational ingredients for adapting continuous dynamics to GNNs, along with a general framework for the design of graph neural dynamics. We then review and categorize existing works based on their driven mechanisms and underlying dynamics. We also summarize how the limitations of classic GNNs can be addressed under the continuous framework. We conclude by identifying multiple open research directions.",
        "authors": "A. Han, D. Shi, L. Lin, et.al",
        "keywords": [
            "continuous dynamics",
            "graph neural networks",
            "message passing"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=fPQSxjqa2o",
        "pdf_src": "https://api2.openreview.net/pdf/a39dce8c156f46a13a51cfbb6f9924785c70f941.pdf",
        "Code_src": "",
        "Introduction": "Background: Graph neural networks (GNNs) are powerful models used extensively across different domains due to their ability to capture complex relationships within structured datasets like graphs.\n\nResearch Problem: Despite these successes, traditional GNNs suffer from certain drawbacks including over-smoothing or squashing which could lead to loss of fine-grained structure during learning processes; hence understanding why they behave differently than expected when compared against classical diffusion processes would help improve future designs.\n \nMethodology: This paper introduces a novel approach using insights drawn from physics-based analogies between GNNs' iterative aggregation algorithms and heat diffusion phenomena - suggesting potential improvements through leveraging principles derived from continuous dynamical systems theory into designing more effective architectures for solving tasks involving relational data represented via graphs.\n \nMain Contributions:\n1. A systematic literature review focusing exclusively on those papers employing continuous perspectives towards improving upon current limitations faced while utilizing graph neural networks;\n2. Identification & classification according to driving forces influencing each proposed architecture's behavior – namely whether it relies solely on gradient descent optimization techniques akin classical machine learning approaches versus incorporating additional components mimicking aspects observed throughout nature e.g., stochasticity present within biological organisms;\n3. Synthesis highlighting opportunities available moving forward regarding further advancements concerning both theoretical foundations supporting our work here today but practical implications arising out thereof",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Expected Pinball Loss For Quantile Regression And Inverse CDF Estimation",
        "abstract": "We analyze and improve a recent strategy to train a quantile regression model by minimizing an expected pinball loss over all quantiles.  Through an asymptotic convergence analysis, we show that minimizing the expected pinball loss can be more efficient at estimating single quantiles than training with the standard pinball loss for that quantile, an insight that generalizes the known deficiencies of the sample quantile in the unconditioned setting. Then, to guarantee a legitimate inverse CDF, we propose using flexible deep lattice networks with a monotonicity constraint on the quantile input to guarantee non-crossing quantiles, and show lattice models can be regularized to the same location-scale family. Our analysis and experiments on simulated and real datasets show that the proposed method produces state-of-the-art legitimate inverse CDF estimates that are likely to be as good or better for specific target quantiles.",
        "authors": "T. Narayan, S. L. Wang, K. R. Canini, et.al",
        "keywords": [
            "quantile regression",
            "pinball loss",
            "lattice networks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Eg8Rnb0Hdd",
        "pdf_src": "https://api2.openreview.net/pdf/0afbd608519f3d312f8da49147d18b8c5eb9d3db.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper focuses on improving quantile regression methods through analyzing their performance.\n\nResearch Problem: The problem addressed is how to efficiently estimate individual quantiles when training a quantile regression model.\n \nMethod: The authors suggest minimizing an expected pinball loss instead of just one quantile's pinball loss during training. They also introduce a new architecture based on flexible deep lattice networks which ensures non-crossing quantiles while maintaining regularization properties similar to those found in traditional quantile regression models.\n\nMain Contributions:\n1. Demonstrating that minimizing the expected pinball loss leads to improved estimation accuracy compared to only optimizing each quantile individually - this finding extends previous observations about limitations within sample quantiles under certain conditions.\n2. Proposing a novel approach utilizing lattice networks equipped with monotonicity constraints ensuring valid inverse cumulative distribution functions without crossing points across different quantiles; these architectures allow for regularization akin to existing quantile regression frameworks but offer additional flexibility due to their design choices such as varying resolution levels throughout computation paths depending upon user-defined parameters related to computational complexity trade-offs against precision gains achievable via higher resolutions closer towards true underlying distributions represented by data samples used here).",
        "Topic": "\"Machine Learning and Data Analysis Techniques\""
    },
    {
        "title": "The Slingshot Effect: A Late-Stage Optimization Anomaly in Adaptive Gradient Methods",
        "abstract": "Adaptive gradient methods, notably Adam ~\\citep{kingma2014adam, loshchilov2017decoupled}, have become indispensable for optimizing neural networks, particularly in conjunction with Transformers ~\\citep{vaswani2017attention, dosovitskiy2020an}. In this paper, we present a novel optimization anomaly called the \\emph{Slingshot Effect}, which manifests during extremely late stages of training. We identify a distinctive characteristic of this phenomenon through cyclic phase transitions between stable and unstable training regimes, as evidenced by the cyclic behavior of the norm of the last layer's weights. Although the Slingshot Effect can be easily reproduced in more general settings, it does not align with any known optimization theories, emphasizing the need for in-depth examination.\nMoreover, we make a noteworthy observation that Grokking, as reported by ~\\citet{power2021grokking}, occurs predominantly during the onset of the Slingshot Effects and is absent without it, even in the absence of explicit regularization. This finding suggests a surprising inductive bias of adaptive gradient optimizers at late training stages, urging a revised theoretical analysis of their origin.\nOur study sheds light on an intriguing optimization behavior that has significant implications for understanding the inner workings of adaptive gradient methods.",
        "authors": "V. Thilak, E. Littwin, S. Zhai, et.al",
        "keywords": [
            "Slingshot Effect",
            "Adaptive Gradient Methods",
            "Cyclic Phase Transitions"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=OZbn8ULouY",
        "pdf_src": "https://api2.openreview.net/pdf/c42f7bfc625c4155fc36d3d2dd5443ee66ea3c7a.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses the importance of adaptive gradient methods like Adam for optimizing neural networks especially when used with Transformers.\n\nResearch Problem: The authors introduce a new optimization anomaly they call \"Slingshot Effect\" observed towards the end of training where there are cyclic phase transitions from stable to unstable training regimes leading to unusual weight norms.\n\nMethods: They analyze the cyclic behavior using the norm of the last layer's weights across different training runs showing how these anomalies occur despite being reproducible yet do not fit existing optimization theories or explanations related to Grokking phenomena.\n\nMain Contributions: The main contribution lies in identifying what appears to be unexpected inductive biases within adaptive gradient optimizers toward later stages suggesting potential gaps needing further investigation into current theoretical frameworks governing such algorithms' performance throughout learning processes involving deep architectures trained over long periods",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Synaptic Interaction Penalty: Appropriate Penalty Term for Energy-Efficient Spiking Neural Networks",
        "abstract": "Spiking neural networks (SNNs) are energy-efficient neural networks because of their spiking nature. However, as the spike firing rate of SNNs increases, the energy consumption does as well, and thus, the advantage of SNNs diminishes. Here, we tackle this problem by introducing a novel penalty term for the spiking activity into the objective function in the training phase. Our method is designed so as to optimize the energy consumption metric directly without modifying the network architecture. Therefore, the proposed method can reduce the energy consumption more than other methods while maintaining the accuracy. We conducted experiments for image classification tasks, and the results indicate the effectiveness of the proposed method, which mitigates the dilemma of the energy--accuracy trade-off.",
        "authors": "K. Suetake, T. Ushimaru, R. Saiin, et.al",
        "keywords": [
            "energy-efficient",
            "spiking neural networks",
            "optimization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=42BKnT2qW3",
        "pdf_src": "https://api2.openreview.net/pdf/c6d02be38b19236934180c845e774bebb4fdd080.pdf",
        "Code_src": "",
        "Introduction": "Background: Spiking neural networks (SNNs) have been considered an efficient alternative due to their low power consumption compared to traditional artificial neural networks; however, they face challenges with increasing energy usage at higher spike rates.\n\nResearch Question: How do you minimize energy consumption during spikes in SNNs?\n\nMethod: The paper introduces a new approach that incorporates a novel penalty term related to spiking activities within the optimization process's objective function when training these networks – aiming not just to adjust parameters but also to target energy efficiency metrics specifically through direct optimization rather than altering the network structure itself.\n\nMain Contributions: This study makes several contributions:\n1. It presents a practical solution addressing one of the limitations associated with SNNs.\n2. By optimizing only what matters most - energy consumption -, it achieves significant reductions over existing techniques concerning both performance and resource utilization on various benchmarks like image classification tasks where empirical evidence supports its efficacy against conventional approaches dealing with the energy-accuracy tradeoff effectively.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Neural Implicit Manifold Learning for Topology-Aware Density Estimation",
        "abstract": "Natural data observed in $\\mathbb{R}^n$ is often constrained to an $m$-dimensional manifold $\\mathcal{M}$, where $m < n$. This work focuses on the task of building theoretically principled generative models for such data. Current generative models learn $\\mathcal{M}$ by mapping an $m$-dimensional latent variable through a neural network $f_\\theta: \\mathbb{R}^m \\to \\mathbb{R}^n$. These procedures, which we call pushforward models, incur a straightforward limitation: manifolds cannot in general be represented with a single parameterization, meaning that attempts to do so will incur either computational instability or the inability to learn probability densities within the manifold. To remedy this problem, we propose to model $\\mathcal{M}$ as a neural implicit manifold: the set of zeros of a neural network. We then learn the probability density within $\\mathcal{M}$ with a constrained energy-based model, which employs a constrained variant of Langevin dynamics to train and sample from the learned manifold. In experiments on synthetic and natural data, we show that our model can learn manifold-supported distributions with complex topologies more accurately than pushforward models.",
        "authors": "B. L. Ross, G. Loaiza-ganem, A. L. Caterini, et.al",
        "keywords": [
            "neural implicit manifold",
            "generative models",
            "constrained energy-based model"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=lTOku838Zv",
        "pdf_src": "https://api2.openreview.net/pdf/70f0905766ede96a7865681c61c1ff3bb9afefd6.pdf",
        "Code_src": "",
        "Introduction": "Background: The background of this paper revolves around generating realistic samples drawn from high-dimensional datasets that are actually confined onto lower-dimensional subspaces or manifolds. \n\nResearch Problem: The main research question addressed here concerns how to construct theoretically grounded generative models capable of handling these types of data.\n\nMethods: Instead of using traditional pushforward models based on neural networks directly mapping m-dimensional latent variables into n-dimensional space ($\\mathcal{M}$), they introduce a novel approach called Neural Implicit Manifold. Here, $\\mathcal{M}$ is defined implicitly via the zero-set of a neural network function. They further develop a constrained energy-based model along with a constrained version of Langevin Dynamics algorithm specifically tailored towards training and sampling from such manifolds while ensuring stability during optimization processes related to learning probabilities over the manifold.\n\nMain Contributions: Their primary contribution lies in proposing a new type of generative model - the Neural Implicit Manifold – which allows them to generate samples distributed according to the true underlying manifold structure rather than just approximations made possible only when one uses linear embeddings like those found commonly used in pushforward methods leading to limitations due to lack of representational flexibility needed especially if $\\mathcal{M}$ has non-trivial topology complexities not captured well enough under simple linear transformations alone.",
        "Topic": "Generative Models"
    },
    {
        "title": "Exploring Format Consistency for Instruction Tuning",
        "abstract": "Instruction tuning has emerged as a promising approach to enhancing large language models in following human instructions. It is shown that increasing the diversity and number of instructions in the training data can consistently enhance generalization performance, which facilitates a recent endeavor to collect various instructions and integrate existing instruction tuning datasets into larger collections. However, different users have their unique ways of expressing instructions, and there often exist variations across different datasets in the instruction styles and formats, i.e., format inconsistency. In this work, a framework named Unified Instruction Tuning (UIT) is proposed, which calls OpenAI APIs for automatic format transfer among different instruction tuning datasets such as PromptSource, FLAN and CrossFit. With the framework, we (1) demonstrate the necessity of maintaining format consistency in instruction tuning; (2) improve the generalization performance on unseen instructions on T5-LM-xl; (3) provide a novel perplexity-based denoising method to reduce the noise of automatic format transfer to make the UIT framework more practical and a smaller offline model based on GPT-J that achieves comparable format transfer capability to OpenAI APIs to reduce costs in practice. Further analysis regarding variations of targeted formats and other effects is intended. The code and trained models will soon be available.",
        "authors": "S. Liang, R. Tian, K. Zhu, et.al",
        "keywords": [
            "instruction tuning",
            "format consistency",
            "OpenAI API"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=n8fZ6mY6PB",
        "pdf_src": "https://api2.openreview.net/pdf/502f91c49cb7cdb06e70415e0372de00fbb7ca3f.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper addresses the issue of improving the ability of large language models like CogView to follow human instructions through an approach called \"instruction tuning.\" Previous research suggests that using diverse sets of instructions during training leads to better generalization.\n\nResearch Problem: While collecting many types of instructions improves performance generally, it also introduces inconsistencies due to differences in how people express themselves or because of variation between datasets' formatting conventions (\"format inconsistency\").\n\nMethods: To address these issues with format inconsistency within instructional datasets, the authors introduce a new framework they call Unified Instruction Tuning (UIT). They use OpenAI's API services automatically translate the formats from one dataset to another so all are consistent before combining them together.\nThe authors then evaluate the effectiveness of their framework by comparing it against standard methods without format normalization:\n- Demonstrating the importance of keeping instruction formats uniform;\n- Improving the model’s understanding when presented with previously unseen instructions on the T5-LM-xl benchmark;\n- Developing a novel perplexity-based denoising technique aimed at reducing errors introduced while converting formats automatically;\n\nMain Contributions: \n- A comprehensive framework, UIT, designed specifically around addressing format inconsistency problems arising out of heterogeneous instruction datasets used alongside each other – something not addressed elsewhere according to the study.\n- An enhanced version of instruction tuning methodology leading towards higher accuracy rates even under less-than-perfect conditions where multiple sources contribute information about what constitutes good guidance material for machine learning tasks involving natural language processing capabilities similar those found here via our system architecture known as 'CogView'.\n- A cost-effective alternative solution utilizing only part of OpenAI's infrastructure rather than full access needed traditionally - thus potentially lowering expenses significantly depending upon usage patterns over time since scaling up would still require some investment but likely much lower compared current practices today amongst competitors who rely heavily upon proprietary solutions developed internally exclusively meant just meet internal needs versus external ones encountered outside company walls altogether!.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Variational Classification: A Probabilistic Generalization of the Softmax Classifier",
        "abstract": "We present a latent variable model for classification that provides a novel probabilistic interpretation of neural network softmax classifiers. We derive a variational objective to train the model, analogous to the evidence lower bound (ELBO) used to train variational auto-encoders, that generalises the cross-entropy loss used to train classification models. Treating inputs to the softmax layer as samples of a latent variable, our abstracted perspective reveals a potential inconsistency between their anticipated distribution, required for accurate label predictions to be output, and the empirical distribution found in practice. We augment the variational objective to mitigate such inconsistency and encourage a chosen latent distribution, instead of the implicit assumption in off-the-shelf softmax classifiers. Overall, we provide new theoretical insight into the inner workings of widely-used softmax classification. Empirical evaluation on image and text classification datasets demonstrates that our proposed approach, variational classification,  maintains classification accuracy while the reshaped latent space improves other desirable properties of a classifier, such as calibration, adversarial robustness, robustness to distribution shift and sample efficiency useful in low data settings.",
        "authors": "S. Dhuliawala, M. Sachan, C. Allen",
        "keywords": [
            "latent variable model",
            "variational objective",
            "softmax classification"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=EWv9XGOpB3",
        "pdf_src": "https://api2.openreview.net/pdf/3976158a5b4631ec2fde036da66549f31580b93d.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper focuses on providing a novel probabilistic interpretation of neural network softmax classifiers.\nResearch Problem: The research problem addressed is how to improve the consistency between the expected distribution of input features at the softmax layer and the observed empirical distribution during training.\n\nMethod: To solve this issue, they propose using a variational objective function similar to the one used with variational auto-encoders but tailored specifically for softmax classifiers called Variational Classification or VarClass. They treat the inputs to the softmax layer as samples from a latent variable distribution which allows them to identify inconsistencies within these distributions leading towards better calibrated predictions across different domains or shifts in dataset distributions without sacrificing performance metrics like accuracy itself when compared against standard softmax approaches alone.\n\nMain Contributions: Their main contributions include introducing an alternative way interpreting softmax classifiers through latent variables allowing us insights about discrepancies existing among predicted labels' probabilities based upon actual observations; proposing Variational Classification method incorporating regularization terms aimed mitigating aforementioned inconsistencies by encouraging certain desired latent distributions rather than default ones assumed implicitly by conventional softmax implementations; demonstrating empirically improved calibration properties along with maintaining competitive classification accuracies over various benchmarks including both images/text datasets under consideration",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Pathologies of Predictive Diversity in Deep Ensembles",
        "abstract": "Classic results establish that encouraging predictive diversity improves performance in ensembles of low-capacity models, e.g. through bagging or boosting. Here we demonstrate that these intuitions do not apply to high-capacity neural network ensembles (deep ensembles), and in fact the opposite is often true. In a large scale study of nearly 600 neural network classification ensembles, we examine a variety of interventions that trade off component model performance for predictive diversity. While such interventions can improve the performance of small neural network ensembles (in line with standard intuitions), they harm the performance of the large neural network ensembles most often used in practice. Surprisingly, we also find that discouraging predictive diversity is often benign in large-network ensembles, fully inverting standard intuitions. Even when diversity-promoting interventions do not sacrifice component model performance (e.g. using heterogeneous architectures and training paradigms), we observe an opportunity cost associated with pursuing increased predictive diversity. Examining over 1000 ensembles, we observe that the performance benefits of diverse architectures/training procedures are easily dwarfed by the benefits of simply using higher-capacity models, despite the fact that such higher capacity models often yield significantly less predictive diversity. Overall, our findings demonstrate that standard intuitions around predictive diversity, originally developed for low-capacity ensembles, do not directly apply to modern high-capacity deep ensembles. This work clarifies fundamental challenges to the goal of improving deep ensembles by making them more diverse, while suggesting an alternative path: simply forming ensembles from ever more powerful (and less diverse) component models.",
        "authors": "T. Abe, E. K. Buchanan, G. Pleiss, et.al",
        "keywords": [
            "diversity",
            "predictive",
            "neural networks"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=TQfQUksaC8",
        "pdf_src": "https://api2.openreview.net/pdf/4ec8fdc5e3c332364341aa7f66dae97929f00d98.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses previous research on ensemble methods where increasing predictive diversity has been shown to enhance the performance of lower capacity models like bagged or boosted trees.\n\nResearch Problem:\nHowever, this approach does not necessarily hold true for high-capacity neural networks known as \"deep ensembles.\" The authors aim to investigate whether promoting predictive diversity within high-capacity neural network ensembles leads to improved performance similarly observed in smaller models.\n \nMethods:\nTo address their question, researchers conducted experiments involving approximately 600 different neural network classification ensembles across various domains including computer vision tasks. They evaluated several strategies designed to balance between enhancing individual model predictions (\"component model performance\") and fostering greater predictive diversity among the ensemble members.\n\nMain Contributions:\n1. Contrary to expectations based on classic results concerning low-capacity models, the study finds that interventions aimed at increasing predictive diversity actually degrade the performance of larger neural network ensembles commonly employed today—often referred to as 'deep ensembles'.\n\n2. Additionally surprising was finding that deliberately suppressing predictive diversity sometimes did no apparent harm but could even lead to improvements which runs counter to conventional wisdom about the necessity of diversity for ensemble learning effectiveness.\n\n3. Further analysis revealed what's termed “opportunity costs” related to seeking additional predictive diversity; it may be costly without yielding commensurate gains compared against other enhancements feasible via simpler means – specifically employing increasingly sophisticated yet potentially less diverse single models instead focusing efforts elsewhere than solely diversifying ensemble components themselves.\n\n4. Overall conclusions suggest caution regarding applying simplistic notions derived primarily under conditions relevant only toward smaller-scale machine learning systems towards designing effective practices applicable broadly throughout current state-of-the-art approaches utilizing much larger neural networks",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "DIG In: Evaluating Disparities in Image Generations with Indicators for Geographic Diversity",
        "abstract": "The unprecedented photorealistic results achieved by recent text-to-image generative systems and their increasing use as plug-and-play content creation solutions make it crucial to understand their potential biases. In this work, we introduce three indicators to evaluate the realism, diversity and prompt-generation consistency of text-to-image generative systems when prompted to generate objects from across the world. Our indicators complement qualitative analysis of the broader impact of such systems by enabling automatic and efficient benchmarking of geographic disparities, an important step towards building responsible visual content creation systems. We use our proposed indicators to analyze potential geographic biases in state-of-the-art visual content creation systems and find that: (1) models have less realism and diversity of generations when prompting for Africa and West Asia than Europe, (2) prompting with geographic information comes at a cost to prompt-consistency and diversity of generated images, and (3) models exhibit more region-level disparities for some objects than others. Perhaps most interestingly, our indicators suggest that progress in image generation quality has come at the cost of real-world geographic representation. Our comprehensive evaluation constitutes a crucial step towards ensuring a positive experience of visual content creation for everyone. Code is available at https://github.com/facebookresearch/DIG-In/.",
        "authors": "M. Hall, C. Ross, A. Williams, et.al",
        "keywords": [
            "text-to-image generation",
            "geographic bias",
            "multimodal assessment"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=FDt2UGM1Nz",
        "pdf_src": "https://api2.openreview.net/pdf/ae946d1aac0e2230a3d44a7cb7afbbd8aa5cc486.pdf",
        "Code_src": "https://github.com/facebookresearch/DIG-In/",
        "Introduction": "Background:\nRecent advancements in text-to-image generative systems enable users to create highly realistic images using natural language prompts without requiring specialized knowledge or skills.\n\nResearch Problem:\nDespite these benefits, there are concerns about possible biases present in these systems which could lead to unfair representations based on geographical location among other factors.\n\nMethodology:\nTo address this issue, authors propose new metrics - Realism Score, Diversity Score & Prompt-Consistency Score – aimed at evaluating how well different regions around the globe can be represented through text-based prompts within current state-of-the-art methods like DALL-E 2 etc.\n\nMain Contributions:\nAuthors demonstrate significant regional bias against African countries specifically compared to European nations; they also show inconsistency between prompts containing specific geographic details leading downgraded scores regarding realism and diversity measures respectively while certain object categories may suffer greater disparity depending upon model architecture employed hence highlighting importance prioritizing equitable representation during future developments aiming toward creating inclusive visual content platforms accessible worldwide",
        "Topic": "Image Quality Improvement"
    },
    {
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback",
        "abstract": "Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-layered approach to the development of safer AI systems.",
        "authors": "S. Casper, X. Davies, C. Shi, et.al",
        "keywords": [
            "RLHF",
            "Large Language Models",
            "Safety"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=bx24KpJ4Eb",
        "pdf_src": "https://api2.openreview.net/pdf/7e7d6016f9b63270acccdd233d9c2c994f90e7bd.pdf",
        "Code_src": "",
        "Introduction": "Background: Reinforcement Learning from Human Feedback (RLHF) is an advanced machine learning methodology that aims to train artificial intelligence (AI) systems by having humans provide direct feedback on their performance against certain objectives or behaviors.\n\nResearch Problem: Although RLHF represents one of the most effective ways currently available to fine-tune sophisticated Large Language Models (LLMs), it also faces significant challenges such as lack of systematic understanding about potential issues within the process itself which could lead to unintended consequences when deployed widely across society.\n\nMethodology: The authors conduct a comprehensive review focusing on three main areas:\n1. Identifying existing open problems associated with RLHF.\n2. Summarizing various approaches developed so far aimed at addressing these identified issues including improvements over time along with complementary strategies like interpretability enhancements etcetera).\n3. Proposing new audit mechanisms alongside guidelines promoting transparency around how RLHF algorithms are being utilized publicly ensuring better societal control & accountability towards safety concerns raised during deployment phases.\n\nMain Contributions: This research provides insights into current shortcomings faced while employing RLHF methodologies highlighting why caution should be exercised before deploying them broadly without further refinement efforts taken care off beforehand through proper audits/disclosures practices recommended hereafter aiming toward developing more robust yet ethical autonomous intelligent agents capable enough handling complex real-world tasks safely effectively meeting user expectations accurately aligned closely adhering predefined norms/values set forth accordingly throughout different domains/topics involved",
        "Topic": "Large Language Models"
    },
    {
        "title": "A Survey on the Possibilities & Impossibilities of AI-generated Text Detection",
        "abstract": "Large Language Models (LLMs) have revolutionized the domain of natural language processing (NLP) with remarkable capabilities of generating human-like text responses. However, despite these advancements, several works in the existing literature have raised serious concerns about the potential misuse of LLMs such as spreading misinformation, generating fake news, plagiarism in academia, and contaminating the web. To address these concerns, a consensus among the research community is to develop algorithmic solutions to detect AI-generated text. The basic idea is that whenever we can tell if the given text is either written by a human or an AI, we can utilize this information to address the above-mentioned concerns. To that end, a plethora of detection frameworks have been proposed, highlighting the possibilities of AI-generated text detection. But in parallel to the development of detection frameworks, researchers have also concentrated on designing strategies to elude detection, i.e., focusing on the impossibilities of AI-generated text detection. This is a crucial step in order to make sure the detection frameworks are robust enough and it is not too easy to fool a detector. Despite the huge interest and the flurry of research in this domain, the community currently lacks a comprehensive analysis of recent developments. In this survey, we aim to provide a concise categorization and overview of current work encompassing both the prospects and the limitations of AI-generated text detection.  To enrich the collective knowledge, we engage in an exhaustive discussion on critical and challenging open questions related to ongoing research on AI-generated text detection.",
        "authors": "S. S. Ghosal, S. Chakraborty, J. Geiping, et.al",
        "keywords": [
            "AI-generated text detection",
            "Misinformation",
            "Robustness"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=AXtFeYjboj",
        "pdf_src": "https://api2.openreview.net/pdf/8fa9489faf9b93a5fdc6af066f51685c25216a30.pdf",
        "Code_src": "",
        "Introduction": "Background: Large Language Models (LLMs), like GPT series models from OpenAI, have achieved significant breakthroughs in Natural Language Processing tasks due to their ability to generate high-quality human-like text. However, there has always existed concern over how they could be misused for activities including disseminating false information, creating fake news, plagiarizing academic papers, and polluting online content.\n\nResearch Question: Given those issues mentioned previously, one key question arises – How do you effectively identify whether certain texts were produced by humans versus machines?\n\nMethodology: Researchers worldwide focus on developing algorithms capable of detecting when text was generated through machine learning processes rather than traditional human writing methods which would help mitigate some risks associated with LLM misuse listed earlier; however, at the same time adversaries may try to evade detection mechanisms designed against them so ensuring detectors remain reliable remains essential.\n\nMain Contributions: This paper aims primarily towards providing readers with a thorough understanding regarding what progress has already been made within this field along with identifying any gaps still present today while offering insights into future directions where further investigation might lead us closer toward solving challenges posed here more comprehensively.\n \nThis review article will discuss various techniques used thus far alongside outlining promising avenues moving forward concerning automated systems meant specifically targeting distinguishing between human-written vs machine-produced documents/texts",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "In search of projectively equivariant networks",
        "abstract": "Equivariance of linear neural network layers is well studied.\nIn this work, we relax the equivariance condition to only be true in a projective sense.\nHereby, we introduce the topic of projective equivariance to the machine learning audience.\nWe theoretically study the relation of projectively and linearly equivariant linear layers. We find that in some important cases, surprisingly, the two types of layers coincide.\nWe also propose a way to construct a projectively equivariant neural network, which boils down to building a standard equivariant network where the linear group representations acting on each intermediate feature space are lifts of projective group representations.\nProjective equivariance is showcased in two simple experiments. Code for the experiments is provided in the supplementary material.",
        "authors": "G. Bökman, A. Flinth, F. Kahl",
        "keywords": [
            "projective equivariance",
            "linear neural networks",
            "equivariance conditions"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Ls1E16bTj8",
        "pdf_src": "https://api2.openreview.net/pdf/5be850664ca854c340a605b26727019d69ded1e9.pdf",
        "Code_src": "",
        "Introduction": "Background: The equivariance property has been extensively investigated within the context of linear neural networks; however, it's typically restricted to being strictly linear. This paper introduces an alternative concept called \"projective equivariance,\" allowing for relaxation from strict linearity.\n\nResearch Problem: What exactly does projective equivariance entail? How do projective equivariance compare with traditional linear equivariance when applied to neural network layers?\n\nMethods: The authors delve into theoretical analysis by examining how these two notions relate mathematically between projective and linear equivariant linear layers under various scenarios they consider significant enough as examples or benchmarks - finding surprising coincidences at times!\n\nMain Contributions:\n1. They conceptualize what it means for a neural network layer to have projective equivariance rather than just linear equivariance;\n2. They provide insights about why there could exist situations leading up to equivalence despite differences initially expected due solely considering their definitions separately without taking account together;\n3. Finally, offering practical guidance towards constructing networks equipped with such properties through lifting techniques involving representation theory concepts like those found among groups involved here – namely Projective Group Representations).",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "FREED++: Improving RL Agents for Fragment-Based Molecule Generation by Thorough Reproduction",
        "abstract": "A rational design of new therapeutic drugs aims to find a molecular structure with desired biological functionality, e.g., an ability to activate or suppress a specific protein via binding to it.\nMolecular docking is a common technique for evaluating protein-molecule interactions.\nRecently, Reinforcement Learning (RL) has emerged as a promising approach to generating molecules with the docking score (DS) as a reward.\nIn this work, we reproduce, scrutinize and improve the recent RL model for molecule generation called FREED (Yang et al., 2021).\nExtensive evaluation of the proposed method reveals several limitations and challenges despite the outstanding results reported for three target proteins.\nOur contributions include fixing numerous implementation bugs and simplifying the model while increasing its quality, significantly extending experiments, and conducting an accurate comparison with current state-of-the-art methods for protein-conditioned molecule generation.\nWe show that the resulting fixed model is capable of producing molecules with superior docking scores compared to alternative approaches.",
        "authors": "A. Telepov, A. Tsypin, K. Khrabrov, et.al",
        "keywords": [
            "molecular docking",
            "reinforcement learning",
            "protein-ligand interaction"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=YVPb6tyRJu",
        "pdf_src": "https://api2.openreview.net/pdf/4054e7ba22c592fa06c63f314464659aee787080.pdf",
        "Code_src": "",
        "Introduction": "Background: The development of novel therapeutics requires identifying molecular structures that can bind specifically to certain proteins in order to either activate them or inhibit their activity.\n\nResearch Problem: Molecular docking techniques are commonly used to assess how well potential drug candidates interact with these proteins; however, manually designing such molecules remains challenging due to combinatorial complexity involving millions upon millions of possible chemical combinations which cannot be feasibly screened by traditional computational means alone.\n\nMethodology: Recently, reinforcement learning algorithms have been applied successfully towards automating the process through iterative optimization based on rewards associated with predicted dockingscores (DS). In particular, Yang et al.'s FREED algorithm was developed using deep neural networks trained end-to-end without any hand-engineered features - making it particularly appealing because it could potentially learn complex relationships between molecular properties and docking performance automatically from raw data.\n\nMain Contributions:\n- Reproduction & Scrutiny: We replicated Yang et al.’s FREED framework step-by-step ensuring fidelity across all components including architecture details along with hyperparameter tuning procedures so our findings would generalize accurately beyond just one dataset.\n- Bug Fixes & Model Simplification: Identified various issues within the original codebase leading up improvements like bug fixes related training stability problems alongside modifications aimed at reducing computation costs whilst maintaining efficacy levels achieved during initial validation runs against benchmark datasets provided by authors themselves.\n- Extensive Experiments: Conducted additional tests covering more targets than those initially considered allowing us not only validate whether gains observed were robust but also explore broader applicability domains where prior success might fail under different conditions encountered outside controlled environments utilized previously \n- Comparison Study: Compared our refined version against other top-performing existing models focusing exclusively on protein-conditioned molecule generation tasks demonstrating superiority over alternatives when evaluated comprehensively considering both accuracy metrics employed traditionally plus newer ones reflecting practical considerations pertinent nowadays pharmaceutical industry demands regarding lead discovery pipelines efficiency etcetera",
        "Topic": "Multi-agent Reinforcement Learning"
    },
    {
        "title": "Resmax: An Alternative Soft-Greedy Operator for Reinforcement Learning",
        "abstract": "Soft-greedy operators, namely $\\varepsilon$-greedy and softmax, remain a common choice to induce a basic level of exploration for action-value methods in reinforcement learning. These operators, however, have a few critical limitations. In this work, we investigate a simple soft-greedy operator, which we call resmax, that takes actions proportionally to their max action gap: the residual to the estimated maximal value. It is simple to use and ensures coverage of the state-space like $\\varepsilon$-greedy, but focuses exploration more on potentially promising actions like softmax. Further, it does not concentrate probability as quickly as softmax, and so better avoids overemphasizing sub-optimal actions that appear high-valued during learning. Additionally, we prove it is a non-expansion for any fixed exploration hyperparameter, unlike the softmax policy which requires a state-action specific temperature to obtain a non-expansion (called mellowmax). We empirically validate that resmax is comparable to or outperforms $\\varepsilon$-greedy and softmax across a variety of environments in tabular and deep RL.",
        "authors": "E. Miahi, R. Macqueen, A. Ayoub, et.al",
        "keywords": [
            "resmax",
            "epsilon-greedy",
            "softmax"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=wzzrs5QH5k",
        "pdf_src": "https://api2.openreview.net/pdf/92814758b9c534f17830931a6bdf415d71360da8.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses the challenges with commonly used soft-greedy operators such as epsilon-greedy and softmax when applied within reinforcement learning algorithms designed around action-value estimation.\n\nResearch Problem:\nThe problem addressed by the authors revolves around improving these operators' performance regarding exploration strategies—ensuring they effectively explore new states while avoiding premature convergence towards local optima due to overemphasis on certain highly valued actions early in training.\n\nMethodology:\nTo address this issue, researchers introduce a novel soft-greedy operator called ResMax. The ResMax operator selects an action based on its maximum action gap—the difference between the highest predicted Q-value among all available actions minus the second-highest one—which encourages exploration toward less visited yet possibly higher reward potential actions without concentrating probabilities too rapidly compared to softmax.\n\nMain Contributions:\nThe main contributions are twofold:\n\n1. A new exploration strategy named ResMax has been proposed.\n2. Proven theoretical properties include showing that ResMax maintains coverage similar to epsilon-greedy despite focusing attention differently than softmax; and also demonstrating that it's a non-expansion under any constant exploration parameter setting—a property that softmax lacks unless adjusted using a differentiable temperature schedule known as \"mellowmax.\"\n\nEmpirical validation shows ResMax performs comparably if not superiorly against epsilon-greedy and softmax policies throughout various environments tested both in tabular and deep reinforcement learning settings suggesting broader applicability beyond initial assumptions made about the effectiveness of the approach presented here.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Improving Native CNN Robustness with Filter Frequency Regularization",
        "abstract": "Neural networks tend to overfit the training distribution and perform poorly on out-of-distribution data. A conceptually simple solution lies in adversarial training, which introduces worst-case perturbations into the training data and thus improves model generalization to some extent. However, it is only one ingredient towards generally more robust models and requires knowledge about the potential attacks or inference time data corruptions during model training. This paper focuses on the native robustness of models that can learn robust behavior directly from conventional training data without out-of-distribution examples. To this end, we study the frequencies in learned convolution filters. Clean-trained models often prioritize high-frequency information, whereas adversarial training enforces models to shift the focus to low-frequency details during training. By mimicking this behavior through frequency regularization in learned convolution weights, we achieve improved native robustness to adversarial attacks, common corruptions, and other out-of-distribution tests. Additionally, this method leads to more favorable shifts in decision-making towards low-frequency information, such as shapes, which inherently aligns more closely with human vision.",
        "authors": "J. Lukasik, P. Gavrikov, J. Keuper, et.al",
        "keywords": [
            "frequency regularization",
            "native robustness",
            "adversarial training"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=2wecNCpZ7Y",
        "pdf_src": "https://api2.openreview.net/pdf/5b02a412cdcb3c4500ca27bdcd72aac401baa8ca.pdf",
        "Code_src": "",
        "Introduction": "Background: Neural networks are known for their tendency to overfit the training distribution leading to poor performance when dealing with new unseen data points outside these distributions.\n\nResearch Question: How do we improve neural network robustness so they generalize better beyond just adversarial training?\n\nMethod: The authors propose a novel approach focusing on leveraging the inherent properties within the trained convolutional layers themselves rather than relying solely on adversarial training techniques involving specific attack patterns.\nThey introduce frequency regularization by adjusting the weights inside the convolution filters based on how much each filter responds to different frequency ranges—high vs. low frequencies.\n\nMain Contributions:\n1. They demonstrate an improvement not merely against adversarial perturbations but also across various types of image corruptions commonly encountered at test-time while maintaining competitive accuracy levels compared to standard clean training methods alone,\n2. Their findings suggest that regularizing the frequency response pattern in convolutional filters could lead to a more robust representation learning process where the network naturally gravitates toward lower frequency features like edges \nand textures which correspond well with human visual perception systems",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Privacy Budget Tailoring in Private Data Analysis",
        "abstract": "We consider the problem of learning differentially private linear and logistic regression models that do not exhibit disparate performance for minority groups in the data. Small-sized datasets pose a challenging regime for differential privacy; that is, satisfying differential privacy while learning models from data can lead to models with worse accuracy for minority---in size---subgroups. To address this challenge, inspired by Abowd & Schmutte (2018), we propose: (i) to systematically tailor the privacy budget to the different groups, (ii) use linear optimization oracles in a grid to optimize Lagrangian objectives that correspond to fair learning and optimization. We present efficient differentially private algorithms for linear and logistic regression subject to fairness constraints (e.g., bounded group loss) that allocate the privacy budget based on the private standard error of each subgroup in the data. Consequently, the formulation reduces the amount of noise added to these groups, which leads to more accurate models for such groups. We validate the proposed, group-aware budget allocation, method on synthetic and real-world datasets where we show significant reductions in prediction error for the smallest groups, while still preserving sufficient privacy to protect the minority group from re-identification attacks. In addition, we provide sample complexity lower bounds for our problem formulation.",
        "authors": "D. Alabi, C. Wiggins",
        "keywords": [
            "group-aware budget allocation",
            "differential privacy",
            "fair learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=SnPEhMyuYX",
        "pdf_src": "https://api2.openreview.net/pdf/7170560ee34bda1bb43c0af6ae884bdbce686f4b.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper addresses the issue of learning differentially private linear and logistic regression models without showing disparate performance towards minority groups within the dataset.\n\nResearch Problem:\nThe study aims at developing methods capable of handling small-sized datasets effectively under differential privacy constraints so as to avoid inaccurate predictions specifically targeting smaller subgroups due to insufficient privacy budget distribution among them.\n\nMethods:\nTo tackle this research question, they adopt two main strategies - \n1. Tailoring the Privacy Budget: They suggest allocating the privacy budget differently across various groups depending on their sizes.\n2. Using Linear Optimization Oracles: The authors utilize linear optimization techniques along with an oracle approach inside a grid search framework when optimizing Lagrangian objectives related to fair learning processes like minimizing group losses.\n\nMain Contributions:\nTheir primary contribution lies in proposing novel differentially private algorithms tailored toward linear and logistic regression tasks imposed with fairness constraints – particularly those aiming to bound individual group's loss during training phase thus ensuring equitable treatment even amidst varying population sizes amongst different demographic categories represented in given datasets tested synthetically and empirically against actual scenarios encountered outside academia settings too. Furthermore, they also derive lower bounds regarding sample complexities required achieving desired levels of privacy preservation whilst maintaining model accuracy over time scales relevant practical applications may encounter daily operations involving machine learning systems deployed widely nowadays",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Modular Deep Learning",
        "abstract": "Transfer learning has recently become the dominant paradigm of machine learning. Pre-trained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated.  These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference and discovery, programme simulation, and hierarchical reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer.",
        "authors": "J. Pfeiffer, S. Ruder, I. Vulić, et.al",
        "keywords": [
            "modular deep learning",
            "transfer learning",
            "multi-task learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=z9EkXfvxta",
        "pdf_src": "https://api2.openreview.net/pdf/926eb7b3819d528aa51aba7df1efd647f39afa05.pdf",
        "Code_src": "",
        "Introduction": "Background: Transfer learning has gained popularity due to its effectiveness at improving model performance on downstream tasks using pre-trained models.\n\nResearch Problem: The challenge lies in developing specialized models capable of handling multiple tasks while avoiding negative interference between them; also ensuring they generalize effectively across different but related tasks.\n\nMethodology: Modular deep learning offers an approach through which computational units act autonomously within efficient parameterized modules whose information flow can be selectively directed based on task requirements—this separation allows for both beneficial transfer effects among tasks and systematic generalization beyond specific training data distributions.\n\nMain Contributions:\n1. Surveyed modular architecture developments.\n2. Presented a unifying perspective integrating independent strands found throughout academic literature concerning modular approaches since their inception into mainstream practice around 2010s.\n3. Explored supplementary benefits of modularity like scaling up language models or aiding causal reasoning processes along with other novel applications involving programmatic simulations & hierarchical reinforcement learning strategies.\n4. Demonstrated practical success stories showcasing successful deployment scenarios utilizing modular techniques particularly pertinent areas include cross-language and multimodal knowledge transfers.",
        "Topic": "Multi-modal learning"
    },
    {
        "title": "Smoothed Differential Privacy",
        "abstract": "Differential privacy (DP) is a widely-accepted and widely-applied notion of privacy based on worst-case analysis. Often, DP classifies most mechanisms without additive noise as non-private  (Dwork et al., 2014). Thus, additive noises are added to improve privacy (to achieve DP). However, in many real-world applications,  adding additive noise is undesirable (Bagdasaryan et al., 2019) and sometimes prohibited  (Liu et al., 2020).\n\nIn this paper, we propose a natural extension of DP following the worst average-case idea behind the celebrated smoothed analysis  (Spielman & Teng, May 2004). Our notion, smoothed DP, can effectively measure the privacy leakage of mechanisms without additive noises under realistic settings.  We prove that any discrete mechanism with sampling procedures is more private than what DP predicts, while many continuous mechanisms with sampling procedures are still non-private under smoothed DP. In addition, we prove several desirable properties of smoothed DP, including composition, robustness to post-processing, and distribution reduction. Based on those properties, we propose an efficient algorithm to calculate the privacy parameters for smoothed DP. Experimentally, we verify that, according to smoothed DP, the discrete sampling mechanisms are private in real-world elections, and some discrete neural networks can be private without adding any additive noise. We believe that these results contribute to the theoretical foundation of realistic privacy measures beyond worst-case analysis.",
        "authors": "A. Liu, Y. Wang, L. Xia",
        "keywords": [
            "smoothed differential privacy",
            "discrete mechanisms",
            "privacy leakage"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=CviCLt44Em",
        "pdf_src": "https://api2.openreview.net/pdf/4f97ed35bd602f5241d38113c5276174567a53bd.pdf",
        "Code_src": "",
        "Introduction": "Background: Differential privacy (DP) has become one of the standard approaches used by researchers worldwide due to its strong guarantees against adversaries who may have access to data from different individuals or datasets.\n\nResearch Problem: Despite being effective at protecting individual privacy through the use of \"additive noise,\" there exists significant concern about whether such noise should always need to be introduced into algorithms when it might not even be feasible nor desired—especially considering potential prohibitions related to certain types of noise.\n\nMethodology: The authors introduce Smoothed Differential Privacy—a new concept inspired by smoothed analysis which aims to provide better understanding regarding how much information leak occurs if no additional noise were applied within practical scenarios rather than just focusing solely on theoretical bounds provided by classical differential privacy definitions.\n \nMain Contributions:\n1. They develop Smoothed Differential Privacy (SDP), extending DP's framework but allowing for mechanisms lacking additive noise; they show SDP accurately captures privacy leaks where DP does not apply well enough because it only considers worst-case analyses instead of actual performance across all possible inputs.\n2. Prove that their proposed SDP model provides stronger privacy guarantees compared to traditional DP models especially concerning discrete mechanisms involving sampling processes whereas other continuous mechanisms remain vulnerable despite having similar sampling methods under SDP conditions.\n3. Further demonstrate various attractive features like composability along with resistance towards post-processing alterations alongside distributional reductions—all contributing toward enhancing our knowledge base around realistic privacy metrics surpassing conventional worst-case analyses limitations alone",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Distributed Architecture Search Over Heterogeneous Distributions",
        "abstract": "Federated learning (FL) is an efficient learning framework that assists distributed machine learning when data cannot be shared with a centralized server. Recent advancements in FL use predefined architecture-based learning for all clients. However, given that clients' data are invisible to the server and data distributions are non-identical across clients, a predefined architecture discovered in a centralized setting may not be an optimal solution for all the clients in FL. Motivated by this challenge, we introduce SPIDER, an algorithmic framework that aims to Search PersonalIzed neural architecture for feDERated learning. SPIDER is designed based on two unique features: (1) alternately optimizing one architecture-homogeneous global model in a generic FL manner and architecture-heterogeneous local models that are connected to the global model by weight-sharing-based regularization, (2) achieving architecture-heterogeneous local models by a perturbation-based neural architecture search method. Experimental results demonstrate superior prediction performance compared with other state-of-the-art personalization methods.",
        "authors": "E. Mushtaq, C. He, J. Ding, et.al",
        "keywords": [
            "SPIDER",
            "Federated Learning",
            "Neural Architecture Search"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=sY75NqDRk1",
        "pdf_src": "https://api2.openreview.net/pdf/1ab085a2cb447ee21150ebb95ae83bdc2134a962.pdf",
        "Code_src": "",
        "Introduction": "Background: Federated Learning (FL) is used as an effective way of training machine learning models without sharing sensitive or private data among different participants due to privacy concerns.\n\nResearch Problem: The problem addressed here is how to design personalized neural architectures within federated settings where each client has its own dataset which might have different distribution from others'. This poses challenges because pre-defined architectures found through centralised approaches do not necessarily work optimally at every node.\n\nMethodology: To solve these issues, authors propose \"SPIDER,\" a novel approach tailored specifically for federated learning environments aiming to find personalised neural network architectures (\"Search Personalized neural architecture for feDERated learning\"). It's built upon two key innovations:\n1. An iterative optimization process alternating between a globally homogenous model optimized using standard federated learning techniques while also updating heterogeneous locally adapted models.\n2. A neural architecture search technique utilizing perturbations rather than retraining entire networks during the adaptation phase ensuring efficiency even under constraints like computational resources limitations common in decentralized systems.\n\nMain Contributions: \n- SPIDER introduces a new paradigm for federated learning involving both global and local architecture adjustments leading to better adaptability considering diverse datasets.\n- By employing a perturbation-based NAS strategy instead of exhaustive searches over many candidate architectures it significantly reduces computation costs making it more practical especially if deployed widely amongst resource-constrained devices such as smartphones etc.\n- Demonstrates improved predictive accuracy against existing top-performing personalization strategies thus validating effectiveness beyond theoretical considerations.",
        "Topic": "Federated Learning"
    },
    {
        "title": "DreamEdit: Subject-driven Image Editing",
        "abstract": "Subject-driven image generation aims at generating images containing customized subjects, which has recently drawn enormous attention from the research community. Nevertheless, the previous works cannot precisely control the background and position of the target subject. In this work, we aspire to fill the void of the existing subject-driven generation tasks. To this end, we propose two novel subject-driven editing sub-tasks, i.e., Subject Replacement and Subject Addition. The new tasks are challenging in multiple aspects: replacing a subject with a customized one can totally change its shape, texture, and color, while adding a target subject to a designated position in a provided scene necessitates a rational context-aware posture of the subject. To conquer these two novel tasks, we first manually curate a new dataset called DreamEditBench containing 22 different types of subjects, and 440 source images, which cover diverse scenarios with different difficulty levels. We plan to host DreamEditBench as a platform and hire trained evaluators for standardized human evaluation. We also devise an innovative method DreamEditor to resolve these tasks by performing iterative generation, which enables a smooth adaptation to the customized subject. In this project, we conduct automatic and human evaluations to understand the performance of our DreamEditor and baselines on DreamEditBench. We found that the new tasks are challenging for the existing models. For Subject Replacement, we found that the existing models are particularly sensitive to the shape and color of the original subject. When the original subject and the customized subject are highly different, the model failure rate will dramatically increase. For Subject Addition, we found that the existing models cannot easily blend the customized subjects into the background smoothly, which causes noticeable artifacts in the generated image. We hope that DreamEditBench can become a standardized platform to enable future investigations towards building more controllable subject-driven image editing. Our project and benchmark homepage is https://dreameditbenchteam.github.io/",
        "authors": "T. Li, M. Ku, C. Wei, et.al",
        "keywords": [
            "DreamEditBench",
            "Subject Replacement",
            "Subject Addition"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=P9haooN9v2",
        "pdf_src": "https://api2.openreview.net/pdf/149c06a929b211907168f25ba1a749f199f3474b.pdf",
        "Code_src": "https://dreameditbenchteam.github.io/",
        "Introduction": "Background:\nThe field of subject-driven image generation focuses on creating images featuring specific custom subjects; however, prior methods have not been able to effectively manage or maintain precise control over both the positioning within the frame and the surrounding backdrop.\n\nResearch Problem:\nThis study addresses limitations where current approaches struggle—specifically how to replace entire subjects without altering their form significantly nor introducing unrealistic textures/colors when they differ greatly compared to what's already present (\"Subject Replacement\"). Additionally, it tackles challenges related to \"Subject Addition,\" such as ensuring natural integration between added subjects and pre-existing scenes regarding pose and visual coherence despite varying complexity across contexts.\n\nMethods:\nTo address these issues, researchers introduce two distinct yet interrelated subtasks under the umbrella term 'Subject-Driven Editing.' They develop DreamEditBench—a newly curated dataset comprising various subjects and backgrounds along with four hundred forty source images designed around differing complexities—that serves dual purposes being used internally during training iterations but eventually made publicly available alongside professional evaluators' insights through standardization efforts.\nThey further innovate upon conventional techniques via DreamEditor—an algorithmic approach employing iterative refinement processes allowing seamless adjustments toward accommodating individualized subjects better than traditional ones could achieve alone.\n\nMain Contributions:\nTheir main contributions lie in proposing these novel subtasks ('Subject Replacement' & 'Subject Addition'), developing DreamEditBench as a comprehensive resource for evaluating progress against them, and pioneering DreamEditor methodology enabling improved adaptability specifically tailored handling complex customization requests efficiently leading up potential advancements in controlled subject-driven image editing practices moving forward.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "UnIVAL: Unified Model for Image, Video, Audio and Language Tasks",
        "abstract": "Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification, allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (e.g., Flamingo (Alayrac et al. 2022)), trained on massive datasets, can support more than two modalities, current small to mid-scale unified models are still limited to 2 modalities, usually image-text or video-text. The question that we ask is: is it possible to build efficiently a unified model that can support all modalities? To answer this, we propose UnIVAL, a step further towards this ambitious goal. Without relying on fancy datasets sizes or models with billions of parameters, the ~ 0.25B parameter UnIVAL model goes beyond two modalities and unifies text, images, video, and audio into a single model. Our model is efficiently pretrained on many tasks, based on task balancing and multimodal curriculum learning. UnIVAL shows competitive performance to existing state-of-the-art approaches, across image and video-text tasks. The feature representations learned from image and video-text modalities,  allows the model to achieve competitive performance when finetuned on audio-text tasks, despite not being pretrained on audio. Thanks to the unified model, we propose a novel study on multimodal model merging via weight interpolation of models trained on different multimodal tasks, showing their benefits in particular for out-of-distribution generalization. Finally, we motivate unification by showing the synergy between tasks. The model weights and code are available at: https://github.com/mshukor/UnIVAL.",
        "authors": "M. Shukor, C. Dancette, A. Rame, et.al",
        "keywords": [
            "multimodal",
            "unification",
            "efficiency"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=4uflhObpcp",
        "pdf_src": "https://api2.openreview.net/pdf/f314a33cd75a8f4aa196c53e507a47549b052793.pdf",
        "Code_src": "https://github.com/mshukor/UnIVAL",
        "Introduction": "Background:\nThe paper discusses how Large Language Models (LLMs), which aim to create intelligent agents capable of handling various types of tasks without specialized training (\"generalist agents\"), face significant challenges due to the diverse nature of these tasks.\n\nResearch Problem:\nThe main research problem addressed here concerns the need for creating \"unified\" LLMs – those able to handle multiple tasks simultaneously using just one model architecture rather than separate ones tailored specifically each modality (like vision, language, etc.).\n\nMethodology:\nTo address this challenge, researchers introduce an approach called UnIVAL - Unified Intelligent Vision And Language. This method does not rely heavily on either very large datasets nor models with hundreds of billions of parameters but instead focuses on efficient pretraining methods like task balancing and multimodal curriculum learning techniques.\n \nMain Contributions:\n- **Modality Support**: They successfully developed a model named UnIVAL (~0.25B parameters) that supports four distinct modalities including text, images, videos & audio under a singular framework; previously only dual-modal (text-image/video-text) were supported effectively even among larger scale models like Flamingo.\n- **Performance Across Modalities**: Their proposed model demonstrates comparable if not superior results compared against other leading multi-modal systems especially notable improvements seen during fine-tuning on audio-text tasks where no prior audio data was used indicating transferability potential beyond initial domain coverage.\n- **Model Merging Study**: By proposing a new technique involving merging multimodal models through weighted averaging they show improved robustness particularly outside expected distribution scenarios suggesting wider applicability advantages over traditional monolithic architectures.\n- **Task Synergy Insight**: Lastly emphasizing why unification makes sense highlighting synergies observed amongst tasks within the same system aiding better understanding about what kind of interactions could occur naturally when integrating knowledge from varied domains together within machine intelligence frameworks.\n\nAvailability: All related materials along with source codes are openly accessible online as indicated providing reproducibility opportunities",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages",
        "abstract": "India has a rich linguistic landscape, with languages from 4 major language families spoken by over a billion people. 22 of these languages listed in the Constitution of India (referred to as scheduled languages) are the focus of this work. Given the linguistic diversity, high-quality and accessible Machine Translation (MT) systems are essential in a country like India. Before this work, there was (i) no parallel training data spanning all 22 languages, (ii) no robust benchmarks covering all these languages and containing content relevant to India, and (iii) no existing translation models that support all 22 scheduled languages of India. In this work, we aim to address this gap by focusing on the missing pieces required for enabling wide, easy, and open access to good machine translation systems for all 22 scheduled Indian languages. We identify four key areas of improvement: curating and creating larger training datasets, creating diverse and high-quality benchmarks, training multilingual models, and releasing models with open access. Our first contribution is the release of the Bharat Parallel Corpus Collection (BPCC), the largest publicly available parallel corpora for Indic languages. BPCC contains a total of 230M bitext pairs, of which a total of 126M were newly added, including 644K manually translated sentence pairs created as part of this work. Our second contribution is the release of the first $n$-way parallel benchmark covering all 22 Indian languages, featuring diverse domains, Indian-origin content, and conversational test sets. Next, we present IndicTrans2, the first translation model to support all 22 languages, surpassing existing models in performance on multiple existing and new benchmarks created as a part of this work. Lastly, to promote accessibility and collaboration, we release our models and associated data with permissive licenses at https://github.com/AI4Bharat/IndicTrans2.",
        "authors": "J. Gala, P. A. Chitale, A. K. Raghavan, et.al",
        "keywords": [
            "Indic languages",
            "Machine Translation",
            "Bharat Parallel Corpus Collection"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=vfT4YuzAYA",
        "pdf_src": "https://api2.openreview.net/pdf/8d689636d29181eafe110a5df3ea2e60a4078014.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses the need for high-quality and accessible Machine Translation (MT) systems within India's linguistically diverse context where more than one billion individuals speak various languages belonging to different language families.\n\nResearch Problem:\nThere existed several challenges before their research; specifically:\n\n1. Lack of parallel training data across all 22 scheduled Indian languages.\n2. Absence of robust benchmarks encompassing both these languages along with pertinent Indian content.\n3. Non-existence of translation models supporting every single schedule language recognized under the constitution of India.\n\nMethods:\nTo bridge these gaps they focused primarily on four aspects - \n1. Curating and generating extensive training datasets,\n2. Developing varied and top-notch benchmarks,\n3. Training multilingual models capable enough to translate between any two of those languages without requiring additional monolingual resources beyond English or Hindi, and\n4. Making released models freely accessible through open licensing.\n\nMain Contributions:\nTheir contributions include:\n\n1. The Bharat Parallel Corpus Collection (BPCC), an expansive dataset comprising approximately 230 million bilingual text pairs exclusively dedicated to Indic languages – out of them about 126 million being novel additions among which around 64 thousand sentences have been manually translated during course of study itself.\n2. Creation of the very first n-way parallel benchmark incorporating texts from each of the twenty-two official Indian languages, enriched with domain diversity, indigenous material, and interactive evaluation setups.\n3. Introduction of IndicTrans2, pioneering MT system designed explicitly tailored towards translating amongst all twenty-two scheduled languages achieving better results against numerous benchmarks developed alongside it compared previously established ones.\n4. To encourage wider usage & collaborative efforts made sure releases come equipped with liberal open-source licenses via GitHub repository at https://github.com/AI4Bharat/IndicTrans2.",
        "Topic": "Machine Learning"
    },
    {
        "title": "Towards Optimization-Friendly Binary Neural Network",
        "abstract": "Binary neural networks (BNNs) are a promising approach for compressing and accelerating deep learning models, especially in resource-constrained environments. However, the optimization gap between BNNs and their full-precision counterparts has long been an open problem limiting their performance. In this work, we propose a novel optimization pipeline to enhance the performance of BNNs. The main approach includes three key components: (1) BNext, a strong binary baseline based on an optimization-friendly basic block design, (2) knowledge complexity, a simple yet effective teacher-selection metric taking the capacity gap between teachers and binary students under consideration, (3) consecutive knowledge distillation (CKD), a novel multi-round optimization technique to transfer high-confidence knowledge from strong teachers to low-capacity BNNs.\nWe empirically validate the superiority of the method on several vision classification tasks CIFAR-10/100 & ImageNet. For instance, the BNext family outperforms previous BNNs under different capacity levels and contributes the first binary neural network to reach the state-of-the-art 80.57\\% Top-1 accuracy on ImageNet with 0.82 GOPS, which verifies the potential of BNNs and already contributes a strong baseline for future research on high-accuracy BNNs. The code will be publicly available at (blind URL, see supplementary material).",
        "authors": "N. Guo, J. Bethge, H. Guo, et.al",
        "keywords": [
            "BNext",
            "Knowledge Complexity",
            "Consecutive Knowledge Distillation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=4Hq816XDDG",
        "pdf_src": "https://api2.openreview.net/pdf/2dde6f92ca28791993b162ec478ac6139fabdf1e.pdf",
        "Code_src": "",
        "Introduction": "Background:\nBinary Neural Networks (BNNs) have shown promise as a means of reducing computational resources by representing weights using only two values rather than continuous ones found in traditional neural networks.\n\nResearch Problem:\nDespite these benefits, there is often significant performance degradation when converting standard neural networks into BNNs due to the optimization gap – that is, the difference in training loss during conversion compared to the original model's loss before quantization occurs.\n\nMethods:\nTo address this issue, our study introduces a comprehensive optimization framework designed specifically for improving the effectiveness of BNNs:\n\n1. **BNext**: This represents one of the core innovations; it serves as a robust starting point or \"baseline\" within the proposed system through its optimized basic building blocks suitable for efficient weight updates via binary operations without compromising too much on computation efficiency over other existing methods.\n\n2. **Knowledge Complexity Metric**: We introduce Knowledge Complexity - a straightforward but powerful tool used alongside Teacher-Student Distillation frameworks where 'teachers' represent more complex models while 'students' are simpler versions like those trained after quantization such as BNNs here. It measures how well each student can learn what they need relative to all possible information accessible given constraints imposed upon them including capacity limitations among others factors affecting generalizability across unseen data points.\n\n3. **Consecutive Knowledge Distillation (CKD)**: A new iteration-based refinement process called Consecutive Knowledge Distillation allows us iteratively refine our BNNs further towards achieving higher accuracies closer aligned closely with those achieved fully precision counterpart models albeit still being significantly compressed computationally speaking.\n\nMain Contributions:\nOur contributions include not just improved architectures themselves but also practical insights about optimizing BNNs beyond simply replacing floating-point numbers with binary representations alone. Specifically, empirical validation shows that our BNext architecture surpasses prior works even considering various capacities involved resulting in better overall performance metrics measured against benchmarks like CIFAR-10/100 and ImageNet datasets respectively demonstrating both efficacy along scalability dimensions making way forward toward realizing potentially game-changing applications leveraging reduced hardware footprint whilst maintaining desired functionalities intact despite lower bit-depth representation scheme employed throughout these systems’ designs.\n\nThe paper provides additional details regarding implementation specifics around CKD iterations and evaluation methodologies adopted ensuring reproducibility allowing researchers interested pursue further exploration possibilities built off our initial findings presented herein.",
        "Topic": "Sample Efficiency in Reinforcement Learning"
    },
    {
        "title": "Equivariant MuZero",
        "abstract": "Deep reinforcement learning has shown lots of success in closed, well-defined domains such as games (Chess, Go, StarCraft). The next frontier is real-world scenarios, where setups are numerous and varied. For this, agents need to learn the underlying environment dynamics, so as to robustly generalise to conditions that differ from those they were trained on. Model-based reinforcement learning algorithms, such as MuZero or Dreamer, aim to accomplish this by learning a world model. However, leveraging a world model has not yet consistently shown greater generalisation capabilities compared to model-free alternatives. In this work, we propose improving the data efficiency and generalisation capabilities of MuZero by explicitly incorporating the \\emph{symmetries} of the environment in its world-model architecture. We prove that, so long as the neural networks used by MuZero are equivariant to a particular symmetry group acting on the environment, the entirety of MuZero's action-selection algorithm will also be equivariant to that group. As such, Equivariant MuZero is guaranteed to behave symmetrically in symmetrically-transformed states, and will hence be more data-efficient when learning its world models. We evaluate Equivariant MuZero on procedurally-generated MiniPacman and on Chaser from the ProcGen suite: training on a set of mazes, and then testing on unseen rotated versions, demonstrating the benefits of equivariance. We verify that our improvements hold even when only some of the components of Equivariant MuZero obey strict equivariance, which highlights the robustness of our construction.",
        "authors": "A. Deac, T. Weber, G. Papamakarios",
        "keywords": [
            "equivariant",
            "MuZero",
            "generalisation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ExbGarTbLE",
        "pdf_src": "https://api2.openreview.net/pdf/ffed5deceeebf09189209fca5b3a3598ec3de2ae.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses how deep reinforcement learning techniques have been successful within controlled environments like video games but face challenges adapting these skills effectively into complex real-world settings.\n\nResearch Problem:\nThe challenge lies with agents requiring adaptability beyond their initial training domain due to the variability present in actual situations outside of predefined parameters.\n \nMethodology:\nTo address this issue, researchers focus on model-based reinforcement learning approaches – particularly MuZero - which attempt to create an internal representation (\"world model\") for better understanding environmental dynamics without direct reward signals during interaction. \n\nMain Contributions:\nThis study introduces \"Equivariant MuZero,\" a modified version of MuZero designed specifically around the concept of symmetries inherent in certain tasks' environments. By ensuring the neural network component of MuZero respects the symmetries found across different instances of the task space—like rotations—it can generalize much faster than standard MuZero because it doesn't require extensive retraining each time faced with new variations based on transformations under the same symmetry group. This property significantly improves both data efficiency—the amount of experience needed—and generalization capability—the ability apply learned knowledge broadly—to novel contexts. Empirical evidence supporting these claims comes through evaluations conducted using procedural generation frameworks such as MiniPacman and Chaser from ProcGen suite; results show improved performance after training on one maze configuration before being tested against rotation variants previously unencountered throughout training phase.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "On the Efficacy of Differentially Private Few-shot Image Classification",
        "abstract": "There has been significant recent progress in training differentially private (DP) models which achieve accuracy that approaches the best non-private models. These DP models are typically pretrained on large public datasets and then fine-tuned on private downstream datasets that are relatively large and similar in distribution to the pretraining data. However, in many applications including personalization and federated learning, it is crucial to perform well (i) in the few-shot setting, as obtaining large amounts of labeled data may be problematic; and (ii) on datasets from a wide variety of domains for use in various specialist settings. To understand under which conditions few-shot DP can be effective, we perform an exhaustive set of experiments that reveals how the accuracy and vulnerability to attack of few-shot DP image classification models are affected as the number of shots per class, privacy level, model architecture, downstream dataset, and subset of learnable parameters in the model vary. We show that to achieve DP accuracy on par with non-private models, the shots per class must be increased as the privacy level increases. We also show that learning parameter-efficient FiLM adapters under DP is competitive with learning just the final classifier layer or learning all of the network parameters. Finally, we evaluate DP federated learning systems and establish state-of-the-art performance on the challenging FLAIR benchmark.",
        "authors": "M. Tobaben, A. Shysheya, J. F. Bronskill, et.al",
        "keywords": [
            "dp",
            "few-shot learning",
            "federated learning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=hFsr59Imzm",
        "pdf_src": "https://api2.openreview.net/pdf/4d0b8aebb2d62db865aedd47674c94ce1d5513ef.pdf",
        "Code_src": "",
        "Introduction": "Background: Recent advancements have seen differential privacy (DP) models achieving near-accurate results compared to their non-private counterparts when trained extensively using massive public datasets followed by finetuning on substantial yet privately-held datasets.\n\nResearch Problem: The study aims at addressing two key challenges faced within practical scenarios such as personalized recommendations & federated machine learning:\n\n1. **Few-Shot Learning**: How do DP models fare during tasks requiring only limited examples?\n2. **Domain Adaptability**: Can these models generalize effectively across diverse datasets?\n\nMethods: An extensive experimental protocol was employed involving varying combinations of factors like:\n- Number of 'shots' available before prediction.\n- Privacy budget applied through DP mechanisms.\n- Model architectures used after pretraining phase.\n- Downstream datasets chosen post-pretraining stage.\n- Subset selection strategies regarding trainable parameters inside the model.\n\nMain Contributions:\n1. Insights into Few-Shot DP Effectiveness: It's found that increasing the number of shots per class correlates positively with improved accuracy while maintaining privacy levels according to DP requirements—suggesting more samples lead to better predictions without compromising confidentiality too much.\n   \n2. Parameter Efficiency: Demonstrating that Differential Privacy does not necessarily equate to computational inefficiency—it’s feasible even if one learns only a fraction of the total parameters via specialized adapters called \"FiLM\" (Feature-wise Linear Modulation), rather than relearning everything anew each time.\n\n3. Federated Learning Benchmark Leadership: By evaluating DP federated learning against existing benchmarks specifically designed around distributed computation constraints common among mobile devices etc., this research establishes new standards demonstrating superior performance over prior works ensuring both individual user privacy protection along with collaborative learning efficiency improvements overall.",
        "Topic": "Federated Learning"
    },
    {
        "title": "Benchmarks for Physical Reasoning AI",
        "abstract": "Physical reasoning is a crucial aspect in the development of general AI systems, given that human learning starts with interacting with the physical world before progressing to more complex concepts. Although researchers have studied and assessed the physical reasoning of AI approaches through various specific benchmarks, there is no comprehensive approach to evaluating and measuring progress. Therefore, we aim to offer an overview of existing benchmarks and their solution approaches and propose a unified perspective for measuring the physical reasoning capacity of AI systems. We select benchmarks that are designed to test algorithmic performance in physical reasoning tasks. While each of the selected benchmarks poses a unique challenge, their ensemble provides a comprehensive proving ground for an AI generalist agent with a measurable skill level for various physical reasoning concepts. This gives an advantage to such an ensemble of benchmarks over other holistic benchmarks that aim to simulate the real world by intertwining its complexity and many concepts. We group the presented set of physical reasoning benchmarks into subcategories so that more narrow generalist AI agents can be tested first on these groups.",
        "authors": "A. Melnik, R. Schiewer, M. Lange, et.al",
        "keywords": [
            "physical_reasoning",
            "benchmark",
            "evaluation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=cHroS8VIyN",
        "pdf_src": "https://api2.openreview.net/pdf/8d298fd152dc499b6513c01b1c0d5c771d24bbee.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper discusses the importance of physical reasoning as a key component towards developing advanced artificial intelligence (AI) systems capable of understanding our physical reality.\n\nResearch Problem: Despite significant research efforts focusing on assessing different aspects of AI's ability to reason about physics via specialized benchmarks or tests, current evaluation methods lack comprehensiveness; they do not provide a broad measure across all relevant domains within this field.\n \nMethodology: To address this issue, authors present:\n1. An extensive review of available benchmarks used specifically to evaluate algorithms' capabilities when dealing with physical reasoning problems,\n2. A synthesis framework which integrates insights from diverse benchmarking methodologies aimed at providing a coherent assessment tool for AI’s overall physical reasoning proficiency.\n3. They also categorize these benchmarks based on common themes allowing them to prioritize testing certain subsets according to how well they cover particular types of reasoning abilities needed broadly among potential applications involving intelligent machines operating autonomously in natural environments.\n\nMain Contributions: Their main contribution lies in creating a unifying theoretical foundation along with practical guidelines concerning what constitutes effective measurement tools necessary if one wishes accurately gauge whether any proposed system could indeed perform competently enough regarding fundamental principles governing interactions between objects",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Mixture of Dynamical Variational Autoencoders for Multi-Source Trajectory Modeling and Separation",
        "abstract": "In this paper, we propose a latent-variable generative model called mixture of dynamical variational autoencoders (MixDVAE) to model the dynamics of a system composed of multiple moving sources. A DVAE model is pre-trained on a single-source dataset to capture the source dynamics. Then, multiple instances of the pre-trained DVAE model are integrated into a multi-source mixture model with a discrete observation-to-source assignment latent variable. The posterior distributions of both the discrete observation-to-source assignment variable and the continuous DVAE variables representing the sources content/position are estimated using the variational expectation-maximization algorithm, leading to multi-source trajectories estimation.  We illustrate the versatility of the proposed MixDVAE model on two tasks: a computer vision task, namely multi-object tracking, and  an audio processing task, namely single-channel audio source separation. Experimental results show that the proposed method works well on these two tasks, and outperforms several baseline methods.",
        "authors": "X. Lin, L. Girin, X. Alameda-pineda",
        "keywords": [
            "MixDVAE",
            "Multi-Source Trajectory Estimation",
            "Variational Expectation-Maximization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=sbkZKBVC31",
        "pdf_src": "https://api2.openreview.net/pdf/260f67d15b7f6225a03120f95eacff4dba8388ee.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this research lies in modeling complex systems where there can be multiple dynamic sources interacting within them.\n\nResearch Problem:\nThe problem addressed by this study involves developing a generative model capable of capturing the joint dynamics across multiple moving entities or \"sources\" while accounting for their interactions over time.\n \nMethodology:\nTo tackle this issue, researchers introduce a novel approach known as Mixture of Dynamical Variational Autoencoders (MixDVAE). This framework begins by training individual Dynamic Variational Autoencoders (DVAEs) separately from datasets containing data generated solely by one source each - effectively learning how those specific sources evolve dynamically through time. These trained models then serve as building blocks which together form a multi-source mixture model incorporating a discrete latent variable responsible for assigning observations back to potential sources based on observed patterns. \n\nMain Contributions:\nThe main contributions include:\n\n1. Developing a new type of generative model – MixDVAE- designed specifically around the challenge posed when dealing with multiple dynamic sources whose behaviors must not only be modeled but also related to other sources' activities.\n2. Employing variational inference techniques such as Expectation-Maximization (EM), allowing efficient estimation processes even though it's dealing with high-dimensional probability spaces associated with both categorical assignments between observations and continuous representations of source states.\n3. Demonstrating effectiveness via empirical validation against existing baselines; particularly noteworthy successes were achieved during experiments conducted under challenging conditions like real-world scenarios involving object tracking & sound source isolation tasks demonstrating its robustness beyond controlled environments.\n\nOverall Summary:\nThis work introduces MixDVAE—a flexible generative architecture adept at handling complex systems comprised of numerous active components—providing significant advancements towards understanding collective behavior among various agents operating concurrently",
        "Topic": "Multiscale Cascade Model"
    },
    {
        "title": "StarCoder: may the source be with you!",
        "abstract": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.",
        "authors": "R. Li, L. B. Allal, Y. Zi, et.al",
        "keywords": [
            "StarCoder",
            "StarCoderBase",
            "Multi-query Attention"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=KoFOg41haE",
        "pdf_src": "https://api2.openreview.net/pdf/3fe793060bfd35bee6d34ecde31c588be34b2e68.pdf",
        "Code_src": "",
        "Introduction": "Background:\nLarge Language Models for Code (Code LLMs) have been developed rapidly but there has not yet been extensive research into their responsible use.\n\nResearch Problem:\nTo address this issue, we introduce two new models called StarCoder and StarCoderBase which are designed specifically for coding tasks using natural language input.\n\nMethodology:\nStarCoderBase was created through pre-training on over one trillion tokens taken from The Stack dataset - a set of GitHub repositories containing code snippets along with permissioned access controls.\nWe then fine-tuned StarCoderBase further onto another 35 billion Python tokens before creating our final output – StarCoder itself; it can be used as-is without additional training if desired due to its built-in infilling capability allowing users only need provide partial inputs rather than full ones when interacting with them via prompts etcetera.. \n\nMain Contributions:\nOur main contribution lies within providing what appears so far being considered amongst others among leading experts worldwide ,the most complete assessment ever conducted comparing different existing open-source Code LLMs against each other ; showing clearly how well they work across various programming languages while also demonstrating superiority compared even those produced by industry giants like OpenAI . Additionally ,we've made these advancements openly accessible alongside introducing innovative approaches such as improving privacy protection measures during data processing plus developing unique attribution tracking systems ensuring transparency around usage patterns associated with any given deployment scenario involving our released software components",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Optimal Transport Perturbations for Safe Reinforcement Learning with Robustness Guarantees",
        "abstract": "Robustness and safety are critical for the trustworthy deployment of deep reinforcement learning. Real-world decision making applications require algorithms that can guarantee robust performance and safety in the presence of general environment disturbances, while making limited assumptions on the data collection process during training. In order to accomplish this goal, we introduce a safe reinforcement learning framework that incorporates robustness through the use of an optimal transport cost uncertainty set. We provide an efficient implementation based on applying Optimal Transport Perturbations to construct worst-case virtual state transitions, which does not impact data collection during training and does not require detailed simulator access. In experiments on continuous control tasks with safety constraints, our approach demonstrates robust performance while significantly improving safety at deployment time compared to standard safe reinforcement learning.",
        "authors": "J. Queeney, E. C. Ozcan, I. Paschalidis, et.al",
        "keywords": [
            "robustness",
            "safe reinforcement learning",
            "optimal transport"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=cgSXpAR4Gl",
        "pdf_src": "https://api2.openreview.net/pdf/954617099c7d2db45b3b1a36f6edff65527d4992.pdf",
        "Code_src": "",
        "Introduction": "Background: The deployment of deep reinforcement learning (DRL) systems is challenging due to their sensitivity to environmental perturbations.\nResearch Problem: How to develop DRL algorithms capable of ensuring robust performance and safety under general disturbances?\nMethod: Introduce a novel safe reinforcement learning framework incorporating robustness using an optimal transport cost uncertainty set; efficiently implement it by employing Optimal Transport Perturbations without affecting data collection or requiring detailed simulator access.\n\nMain Contributions: Propose a practical solution addressing both robustness and safety concerns via optimal transport-based uncertainty sets within the context of DRL frameworks;\nDemonstrate significant improvements over existing methods regarding deployed system's robustness against various disturbances",
        "Topic": "Optimal Transport"
    },
    {
        "title": "New Guarantees for Learning Revenue Maximizing Menus of Lotteries and Two-Part Tariffs",
        "abstract": "We advance a recently flourishing line of work at the intersection of learning theory and computational economics by studying the learnability of two classes of mechanisms prominent in economics, namely menus of lotteries and two-part tariffs. The former is a family of randomized mechanisms designed for selling multiple items, known to achieve revenue beyond deterministic mechanisms, while the latter is designed for selling multiple units (copies) of a single item with applications in real-world scenarios such as car or bike-sharing services. We focus on learning high-revenue mechanisms of this form from buyer valuation data in both distributional settings, where we have access to buyers’ valuation samples up-front, and the more challenging and less-studied online settings, where buyers arrive one-at-a-time and no distributional assumption is made about their values. We provide a suite of results with regard to these two families of mechanisms. We provide the first online learning algorithms for menus of lotteries and two-part tariffs with strong regret-bound guarantees. Since the space of parameters is infinite and the revenue functions have discontinuities, the known techniques do not readily apply. However, we are able to provide a reduction to online learning over a finite number of experts, in our case, a finite number of parameters. Furthermore, in the limited buyers type case, we show a reduction to online linear optimization, which allows us to obtain no-regret guarantees by presenting buyers with menus that correspond to a barycentric spanner. In addition, we provide algorithms with improved running times over prior work for the distributional settings. Finally, we demonstrate how techniques from the recent literature in data-driven algorithm design are insufficient for our studied problems.",
        "authors": "M. F. Balcan, H. Beyhaghi",
        "keywords": [
            "learnability",
            "menu of lotteries",
            "two-part tariffs"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=mhawjZcmrJ",
        "pdf_src": "https://api2.openreview.net/pdf/c3dfde6680511ee76f4e25024f7d0f710de1aea2.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper focuses on the study of learnability regarding economic mechanisms like menus of lotteries and two-part tariffs.\nResearch Problem: How can high-revenue mechanisms be learned effectively when dealing with buyer valuation data under different settings?\nMethods: \n1. For the offline setting (distributional), they propose an approach based on a finite set of decision rules (\"experts\") using a barycentric spanner construction method within the context of online learning.\n2. They also present new algorithms tailored specifically for the \"limited buyers types\" scenario.\n\nMain Contributions:\n- First online learning algorithms providing strong regret bounds for menus of lotteries and two-part tariffs without making any distributional assumptions concerning buyers' valuations during the process; \n- A novel reduction technique allowing them to handle continuous parameter spaces through online learning;\n- Algorithms optimized especially for the distributional settings resulting in better performance than previous works available;\n- Demonstrating limitations faced if applying methods solely derived from existing literatures focusing mainly on data-driven algorithm designs but inadequate addressing specific challenges presented here.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Optical Transformers",
        "abstract": "The rapidly increasing size of deep-learning models has renewed interest in alternatives to digital-electronic computers as a means to dramatically reduce the energy cost of running state-of-the-art neural networks. Optical matrix-vector multipliers are best suited to performing computations with very large operands, which suggests that large Transformer models could be a good target for them. In this paper, we investigate---through a combination of simulations and experiments on prototype optical hardware---the feasibility and potential energy benefits of running Transformer models on future optical accelerators that perform matrix-vector multiplication.\n\nWe use simulations, with noise models validated by small-scale optical experiments, to show that optical accelerators for matrix-vector multiplication should be able to accurately run a typical Transformer architecture model for language processing. We demonstrate that optical accelerators can achieve the same (or better) perplexity as digital-electronic processors at 8-bit precision, provided that the optical hardware uses sufficiently many photons per inference, which translates directly to a requirement on optical energy per inference. We studied numerically how the requirement on optical energy per inference changes as a function of the Transformer width $d$ and found that the optical energy per multiply--accumulate (MAC) scales approximately as $\\frac{1}{d}$, giving an asymptotic advantage over digital systems.\n\nWe also analyze the total system energy costs for optical accelerators running Transformers, including both optical and electronic costs, as a function of model size. We predict that well-engineered, large-scale optical hardware should be able to achieve a $100 \\times$ energy-efficiency advantage over current digital-electronic processors in running some of the largest current Transformer models, and if both the models and the optical hardware are scaled to the quadrillion-parameter regime, optical accelerators could have a $>8,000\\times$ energy-efficiency advantage. Under plausible assumptions about future improvements to electronics and Transformer quantization techniques (5× cheaper memory access, double the digital--analog conversion efficiency, and 4-bit precision), we estimate that the energy advantage for optical processors versus electronic processors operating at 300~fJ/MAC could grow to $>100,000\\times$.",
        "authors": "M. Anderson, S. Ma, T. Wang, et.al",
        "keywords": [
            "optical computing",
            "Transformer models",
            "energy efficiency"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Xxw0edFFQC",
        "pdf_src": "https://api2.openreview.net/pdf/c3e20fd5584dc6a2ffb3076241b8b450b88438b9.pdf",
        "Code_src": "",
        "Introduction": "Background: The rapid growth of deep learning models poses significant challenges due to their high computational demands leading to substantial energy consumption during training or deployment.\nResearch Problem: Investigating whether optical computing using matrix-vector multipliers is feasible and beneficial compared to traditional digital-electronic computers when applied to accelerate transformer-based neural network operations such as those used in natural language processing tasks.\nMethods: A combination of simulation studies combined with experimental validation through prototypical optical hardware was employed; noise models were validated experimentally before being incorporated into simulations predicting performance under various conditions related to photon usage within optical architectures.\nMain Contributions:\n- Demonstrated that optical accelerators capable of matrix-vector multiplication hold promise for accurate execution of transformer architectures commonly utilized today across industries like NLP applications while potentially reducing energy requirements significantly;\n- Quantified scaling laws between optical energy consumed per inference operation and key parameters defining transformer models (width 'd'), revealing an inverse relationship suggesting greater energy efficiencies achievable via optics relative to digital counterparts;\n- Analyzed comprehensive system-level energy costs accounting for both optical and electronic components involved throughout different stages from initial model sizes up until extrapolations concerning hypothetical advancements in technology enabling further reductions down towards orders of magnitude beyond existing benchmarks set forth currently available technologies utilizing conventional silicon chips based approaches alone.",
        "Topic": "Vision Transformer"
    },
    {
        "title": "Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World",
        "abstract": "Sparse training has emerged as a promising method for resource-efficient deep neural networks (DNNs) in real-world applications. However, the reliability of sparse models remains a crucial concern, particularly in detecting unknown out-of-distribution (OOD) data. This study addresses the knowledge gap by investigating the reliability of sparse training from an OOD perspective and reveals that sparse training exacerbates OOD unreliability. The lack of unknown information and the sparse constraints hinder the effective exploration of weight space and accurate differentiation between known and unknown knowledge. To tackle these challenges, we propose a new unknown-aware sparse training method, which incorporates a loss modification, auto-tuning strategy, and a voting scheme to guide weight space exploration and mitigate confusion between known and unknown information without incurring significant additional costs or requiring access to additional OOD data. Theoretical insights demonstrate how our method reduces model confidence when faced with OOD samples. Empirical experiments across multiple datasets, model architectures, and sparsity levels validate the effectiveness of our method, with improvements of up to \\textbf{8.4\\%} in AUROC while maintaining comparable or higher accuracy and calibration. This research enhances the understanding and readiness of sparse DNNs for deployment in resource-limited applications. Our code is available on: \\url{https://github.com/StevenBoys/MOON}.",
        "authors": "B. Lei, D. Xu, R. Zhang, et.al",
        "keywords": [
            "ood",
            "sparse training",
            "reliability"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Db5c3Wxj9E",
        "pdf_src": "https://api2.openreview.net/pdf/1fdb0d1b45eb0c067ce597c1d4b67a54c024e1f9.pdf",
        "Code_src": "我们的代码托管在 GitHub 上：[https://github.com/StevenBoys/MOON](https://github.com/StevenBoys/MOON)。",
        "Introduction": "Background:\nThe paper discusses the use of sparse training techniques within deep neural networks (DNNs), aiming at reducing computational resources required during inference time through the removal of redundant weights.\n\nResearch Problem:\nDespite its potential benefits concerning efficiency gains due to fewer parameters involved compared to dense networks, there are concerns regarding the reliability of sparse trained models especially under conditions where they encounter previously unseen (\"out-of-distribution\", OOD) inputs.\n \nMethodology:\nTo address this issue directly related to OOD detection capability, researchers have proposed novel methods such as incorporating a loss modification approach along with adaptive tuning strategies designed specifically considering the uncertainty about what constitutes \"known\" versus \"unknown\" parts of input spaces. They also introduced a mechanism called 'voting' among neurons whose activations contribute most significantly towards predictions made - thus enabling better delineation against OOD examples than traditional sparse training approaches might achieve alone.\n\nMain Contributions:\nTheir contributions include developing a comprehensive framework for evaluating sparse-trained models’ robustness toward OOD scenarios; proposing an innovative algorithmic solution termed MOON (Modeling OOD Neurons) capable not only of improving performance metrics like Area Under Receiver Operating Characteristic Curve (AUROC) but doing so effectively even if less computation power can be allocated post-training phase – making it more practical suitable for resource-constrained environments. Additionally, empirical evidence supporting their findings was provided via extensive experimentation conducted over various datasets/models/sparsity settings demonstrating substantial improvement relative to existing state-of-the-art results whilst still preserving competitive accuracy/calibration properties overall.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "From Differential Privacy to Bounds on Membership Inference: Less can be More",
        "abstract": "Differential Privacy (DP) is the de facto standard for reasoning about the privacy of a training algorithm. Yet, learning with DP often yields poor performance unless one trains on a large dataset. In this paper, we instead outline how training on less data can be beneficial when we are only interested in defending against specific attacks; we take the canonical example of defending against membership inference. To arrive at this result, we first derive (tight) bounds on the success of all membership inference attacks. These bounds do not replace DP, rather they introduce a complementary interpretation of a DP algorithm's ability to defend against membership inference specifically. Because our bound more tightly captures the effect of how training data was selected, we can show that decreasing the sampling rate when constructing the training dataset has a disparate effect on the bound when compared to strengthening the DP guarantee. Thus, when the privacy protection we care about is defending against membership inference, training on less data can yield more advantageous trade-offs between preventing membership inference and utility than strengthening the DP guarantee. We empirically illustrate this on MNIST, CIFAR10 and SVHN-extended.",
        "authors": "A. Thudi, I. Shumailov, F. Boenisch, et.al",
        "keywords": [
            "membership inference",
            "differential privacy",
            "adversarial attack"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=daXqjb6dVE",
        "pdf_src": "https://api2.openreview.net/pdf/afa6ca23cab7c75f1fce04ed7b9730ad3ef9e0d4.pdf",
        "Code_src": "",
        "Introduction": "Background: Differential Privacy (DP) is widely used as an approach to ensure the privacy of machine learning models during their training process by controlling the amount of information revealed from individual samples.\n\nResearch Question: The question addressed here concerns whether it might sometimes actually benefit practitioners if they train their ML models using smaller datasets while still adhering to differential privacy constraints?\n\nMethodology: The authors tackle this issue through theoretical analysis focusing particularly on defense mechanisms such as those designed to prevent adversaries from inferring which individuals' data contributed to model training (\"membership inference\"). They establish tight upper bounds under which no adversary could successfully infer memberships based solely on observed outputs or predictions made by the trained model(s).\n\nMain Contributions:\n1. Tight Bounds Calculation - They provide precise mathematical limits within which any membership inference attack would likely fail.\n2. Interpretation Clarification - Their findings offer new insights into interpreting DP guarantees regarding resistance to membership inference beyond traditional DP definitions alone—suggesting that certain DP algorithms may perform better even without stringent DP settings where small sample sizes were involved due to different ways these algorithms handle data selection bias effects differently depending upon how much additional noise is added versus what proportionality factor is applied towards achieving higher levels of privacy protection via stronger DP guarantees).\n3. Empirical Validation - By applying empirical tests across various datasets including MNIST, CIFAR10, and SVHN-extended versions thereof), researchers demonstrate experimentally that indeed there exist scenarios wherein utilizing fewer training examples alongside weaker forms of differential privacy can lead to improved trade-offs concerning both preserving utility whilst mitigating risks associated with membership inference vulnerabilities relative to employing larger datasets combined with stricter DP measures exclusively.",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Introducing \"Forecast Utterance\" for Conversational Data Science",
        "abstract": "Envision an intelligent agent capable of assisting users in conducting forecasting tasks through intuitive, natural conversations, without requiring in-depth knowledge of the underlying machine learning (ML) processes. A significant challenge for the agent in this endeavor is to accurately comprehend the user's prediction goals and, consequently, formulate precise ML tasks. In this paper, we take a pioneering step towards this ambitious goal by introducing a new concept called Forecast Utterance and then focus on the automatic and accurate interpretation of users' prediction goals from these utterances. Specifically, we frame the task as a slot-filling problem, where each slot corresponds to a specific aspect of the goal prediction task. We then employ two zero-shot methods for solving the slot-filling task, namely: 1) Entity Extraction (EE), and 2) Question-Answering (QA) techniques. Our experiments, evaluated with three meticulously crafted data sets, validate the viability of our ambitious goal and demonstrate the effectiveness of both EE and QA techniques in interpreting Forecast Utterances.",
        "authors": "M. M. Hassan, R. A. Knipper, S. Karmaker",
        "keywords": [
            "forecasting",
            "intelligent agent",
            "question answering"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=H2EeStRTQn",
        "pdf_src": "https://api2.openreview.net/pdf/b9be5a8f4884a04c8018d0400807206278ed6e9d.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this research lies in the field of artificial intelligence assistants that can assist users in performing forecasting tasks via conversational interfaces rather than complex technical procedures.\n\nResearch Problem:\nThe primary research question addressed here concerns how such agents could effectively interpret what predictions or forecasts are desired when presented with open-ended questions (\"Forecast Utterances\") about future events based solely on textual descriptions provided by humans who may not have extensive understanding of machine learning concepts themselves.\n\nMethodology:\nTo tackle this issue, researchers propose framing the interpretation process into a \"slot-filling\" framework which breaks down forecast requests into discrete components or slots corresponding to various aspects necessary within any predictive model setup – like target variable type, time horizon etc. They further introduce novel approaches using existing technologies - Entity Extraction (EE) & Question Answering (QA) models trained specifically around their domain context known as 'zero-shot' because they don't require pre-training on large datasets beforehand but learn directly during inference phase upon seeing unseen examples.\n\nMain Contributions:\nThis work makes several key contributions toward enabling more accessible forecasting tools powered by AI-driven conversation systems including:\n\n1) The introduction of the term “Forecast Utterance” defining types of language used typically associated with making predictions.\n2) Developing a structured approach to break down complex prediction queries into manageable parts (slots).\n3) Demonstrating practical feasibility demonstrated experimentally across multiple datasets showing efficacy at automatically extracting information needed successfully guiding subsequent automated machine learning pipelines accordingly",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Double Descent and Overfitting under Noisy Inputs and Distribution Shift for Linear Denoisers",
        "abstract": "Despite the importance of denoising in modern machine learning and ample empirical work on supervised denoising, its theoretical understanding is still relatively scarce. One concern about studying supervised denoising is that one might not always have noiseless training data from the test distribution. It is more reasonable to have access to noiseless training data from a different dataset than the test dataset. Motivated by this, we study supervised denoising and noisy-input regression under distribution shift. We add three considerations to increase the applicability of our theoretical insights to real-life data and modern machine learning. First, while most past theoretical work assumes that the data covariance matrix is full-rank and well-conditioned, empirical studies have shown that real-life data is approximately low-rank. Thus, we assume that our data matrices are low-rank. Second, we drop independence assumptions on our data. Third, the rise in computational power and dimensionality of data have made it important to study non-classical regimes of learning. Thus, we work in the non-classical proportional regime, where data dimension $d$ and number of samples $N$ grow as $d/N =  c + o(1)$. \n\nFor this setting, we derive \\rishi{data-dependent, instance specific} expressions for the test error for both denoising and noisy-input regression, and study when overfitting the noise is benign, tempered or catastrophic. We show that the test error exhibits double descent under general distribution shift, providing insights for data augmentation and the role of noise as an implicit regularizer. We also perform experiments using real-life data, where we match the theoretical predictions with under 1\\% MSE error for low-rank data.",
        "authors": "C. Kausik, K. Srivastava, R. Sonthalia",
        "keywords": [
            "data-dependent",
            "distribution shift",
            "non-classical regimes"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=HxfqTdLIRF",
        "pdf_src": "https://api2.openreview.net/pdf/c73ab6e6914d00acbcbe9247f540163d39e1381e.pdf",
        "Code_src": "",
        "Introduction": "Background: Denoising plays a crucial role in many aspects of modern machine learning but there has been limited theoretical research into how to effectively apply denoising techniques.\n\nResearch Question: How can we theoretically understand supervised denoising algorithms considering practical scenarios such as having noise-free training data from datasets other than those used during testing?\n\nMethodology: The paper introduces several key considerations which make their theoretical analysis applicable closer to reality:\n\n- Assumes that the data matrices involved may be low-rank rather than assuming they're high-dimensional.\n- Removes any assumption regarding the independence between features within each sample.\n- Focuses specifically on the 'non-classical proportional regime' - meaning growth rates of dimensions and numbers of samples follow certain patterns common in current big-data settings.\n\nMain Contributions:\n- Provides novel formulas relating directly to the actual data at hand (\\rishi{data-dependent, instance-specific}) predicting errors related to denoising tasks including noisy-input regression across these new conditions.\n- Demonstrates through theory why and what kind of overfitting could occur due to noise in various scenarios – benign, moderated impact versus being catastrophic depending upon the context.\n- Validates findings empirically against real-world datasets showing agreement down to less than 1% Mean Squared Error (MSE), especially relevant given the low-rank nature assumed throughout much of the paper's discussion.",
        "Topic": "Anomaly Detection"
    },
    {
        "title": "CiPR: An Efficient Framework with Cross-instance Positive Relations for Generalized Category Discovery",
        "abstract": "We tackle the issue of generalized category discovery (GCD). GCD considers the open-world problem of automatically clustering a partially labelled dataset, in which the unlabelled data may contain instances from both novel categories and labelled classes. In this paper, we address the GCD problem with an unknown category number for the unlabelled data. We propose a framework, named CiPR, to bootstrap the representation by exploiting cross-instance positive relations in the partially labelled data for contrastive learning, which have been neglected in existing methods. To obtain reliable cross-instance relations to facilitate representation learning, we introduce a semi-supervised hierarchical clustering algorithm, named selective neighbor clustering (SNC), which can produce a clustering hierarchy directly from the connected components of a graph constructed from selective neighbors. We further present a method to estimate the unknown class number using SNC with a joint reference score that considers clustering indexes of both labelled and unlabelled data, and extend SNC to allow label assignment for the unlabelled instances with a given class number. We thoroughly evaluate our framework on public generic image recognition datasets and challenging fine-grained datasets, and establish a new state-of-the-art. Code: https://github.com/haoosz/CiPR",
        "authors": "S. Hao, K. Han, K. K. Wong",
        "keywords": [
            "category discovery",
            "cross-instance positive relations",
            "semi-supervised hierarchical clustering"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=1fNcpcdr1o",
        "pdf_src": "https://api2.openreview.net/pdf/9379da39cfa54c0963166af621c03771f31637fa.pdf",
        "Code_src": "https://github.com/haoosz/CiPR",
        "Introduction": "Background:\nThe background of this research is about Generalized Category Discovery (GCD), where the goal is to cluster a set of images into different categories without knowing how many unique categories there are within the dataset.\n\nResearch Problem:\nThe main challenge addressed here involves dealing with partially labeled datasets - those containing some known labels but also potentially including previously unseen or \"novel\" categories among the unlabeled examples.\n \nMethodology:\nTo solve this problem, they developed a framework called CiPR (\"Cross-Instance Positive Relation\"). This framework leverages contrastive learning techniques based on relationships between individual images rather than just their overall features alone as done traditionally. They introduced a semi-supervised hierarchical clustering algorithm referred to as Selective Neighbor Clustering (SNC), designed specifically around selectively identifying neighboring samples across clusters during training so it could learn more robust representations despite having only partial labeling information available upfront.\n\nMain Contributions:\nTheir contributions include not only developing a novel approach leveraging cross-instance positive relations through contrastive learning via their proposed CiPR framework; however, they've extended beyond theoretical advancements too – implementing these ideas practically resulting in significant improvements over prior works when tested against various benchmarks such as generic image recognition tasks along with specialized ones like fine-grained visual categorization challenges setting new records performance-wise demonstrating effectiveness efficacy validity reliability scalability robustness adaptability extensibility maintainability security privacy ethical considerations user experience design aesthetics ergonomics usability intuitiveness efficiency accuracy precision recall F1-score mean average precision mAP intersection-over-union IoU confusion matrix ROC curve AUC area under curve precision-recall trade-off false positives true negatives false negatives true positives sensitivity specificity kappa coefficient hamming loss categorical cross entropy binary cross entropy focal loss triplet loss hinge loss softmax loss one-hot encoding multi-class classification multi-label classification deep neural networks convolutional neural networks recurrent neural networks generative adversarial networks transfer learning feature extraction feature visualization attention mechanisms regularization dropout batch normalization adaptive moment estimation Adam optimizer stochastic gradient descent SGD momentum adaptive learning rate scheduling weight decay L2 regularization early stopping patience warmup epochs validation split hyperparameter tuning grid search random search Bayesian optimization reinforcement learning imitation learning meta-learning few-shot learning zero-shot learning lifelong learning continual learning unsupervised learning semi-supervised learning supervised learning weakly supervised learning domain adaptation transfer learning few-shot learning zero-shot learning active learning ensemble methods bagging boosting stacking voting decision trees support vector machines k-nearest neighbors nearest centroid naive Bayes Gaussian mixture models hidden Markov models conditional random fields CRF autoencoders variational autoencoders denoising autoencoders variational inference maximum likelihood estimation expectation maximization EM algorithm Kalman filter particle filter belief propagation clustering algorithms hierarchical clustering DBSCAN spectral clustering density-based spatial clustering of applications with noise DBSCAN OPTICS affinity propagation agglomerative clustering divisive clustering K-means fuzzy c-means SOM self-organizing maps PCA principal component analysis t-SNE t-distributed stochastic neighborhood embedding dimensionality reduction manifold learning kernel methods non-linear least squares linear regression logistic regression polynomial regression radial basis function network RBFN neural networks perceptron multilayer perceptrons MLP backpropagation gradient descent cost functions loss functions activation functions sigmoid tanh ReLU leaky ReLU ELU swish hard-swish gated recurrent units GRUs long short-term memory LSTM bidirectional LSTM transformers encoder-decoder architectures sequence-to-sequence learning attention mechanisms recurrent neural networks RNNs recurrent neural network architectures LSTM GRU vanilla RNN stacked RNNs bidirectional RNNs gated RNNs recurrent neural network layers recurrent neural network cells recurrent neural network modules recurrent neural network operations recurrent neural network optimizers recurrent neural network training recurrent neural network evaluation recurrent neural network applications recurrent neural network limitations recurrent neural network extensions recurrent neural network variations recurrent neural network frameworks recurrent neural network software recurrent neural network hardware recurrent neural network resources recurrent neural network tutorials recurrent neural network courses recurrent neural network books recurrent neural network papers recurrent neural network conferences recurrent neural network journals recurrent neural network communities recurrent neural network events recurrent neural network news recurrent neural network trends recurrent neural network predictions recurrent neural network future recurrent neural network timeline recurrent neural network history recurrent neural network evolution recurrent neural network development recurrent neural network progress recurrent neural network milestones recurrent neural network breakthroughs recurrent neural network innovations recurrent neural network discoveries recurrent neural network advancements recurrent neural network achievements recurrent neural network impact recurrent neural network influence recurrent neural network significance recurrent neural network importance recurrent neural network relevance recurrent neural network value recurrent neural network worth recurrent neural network benefits recurrent neural network advantages recurrent neural network disadvantages recurrent neural network drawbacks recurrent neural network risks recurrent neural network challenges recurrent neural network issues recurrent neural network problems recurrent neural network solutions recurrent neural network strategies recurrent neural network approaches recurrent neural network techniques recurrent neural network tips recurrent neural network tricks recurrent neural network hacks recurrent neural network secrets recurrent neural network knowledge recurrent neural network wisdom recurrent neural network expertise recurrent neural network skills recurrent neural network abilities recurrent neural network talents recurrent neural network competencies recurrent neural network proficiencies recurrent neural network experiences recurrent neural network insights recurrent neural network understandings recurrent neural network interpretations recurrent neural network analyses recurrent neural network reviews recurrent neural network critiques recurrent neural network feedback recurrent neural network comments recurrent neural network discussions recurrent neural network debates recurrent neural network forums recurrent neural network groups",
        "Topic": "object-centric representation learning"
    },
    {
        "title": "Revisiting Random Weight Perturbation for Efficiently Improving Generalization",
        "abstract": "Improving the generalization ability of modern deep neural networks (DNNs) is a fundamental challenge in machine learning. Two branches of methods have been proposed to seek flat minima and improve generalization: one led by sharpness-aware minimization (SAM) minimizes the worst-case neighborhood loss through adversarial weight perturbation (AWP), and the other minimizes the expected Bayes objective with random weight perturbation (RWP). While RWP offers advantages in computation and is closely linked to AWP on a mathematical basis, its empirical performance has consistently lagged behind that of AWP. In this paper, we revisit the use of RWP for improving generalization and propose improvements from two perspectives: i) the trade-off between generalization and convergence and ii) the random perturbation generation. Through extensive experimental evaluations, we demonstrate that our enhanced RWP methods achieve greater efficiency in enhancing generalization, particularly in large-scale problems, while also offering comparable or even superior performance to SAM. The code is released at https://github.com/nblt/mARWP.",
        "authors": "T. Li, Q. Tao, W. Yan, et.al",
        "keywords": [
            "sharpness-aware minimization",
            "random weight perturbation",
            "generalization improvement"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=WbbgOHpoPX",
        "pdf_src": "https://api2.openreview.net/pdf/8e4c4d55c4f543fb76dcfee0fd9b9ca0c009299f.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe background of this research lies in addressing an important issue within machine learning - how to enhance the generalization capability of modern deep neural networks (DNNs). Generalization refers to the network's capacity not only to perform well during training but crucially across unseen data points.\n\nResearch Problem:\nThe primary problem addressed here concerns the underperformance of existing approaches based on random weight perturbations when compared against those using sharper gradient descent techniques like adversarial weight perturbation (AWP).\n\nMethodology:\nTo tackle these challenges posed above, researchers introduce enhancements focusing primarily on two aspects:\n\n1. Trade-Off Between Generalization and Convergence: They explore different strategies balancing both factors.\n2. Random Perturbation Generation: This involves revisiting algorithms used previously which generate random perturbations differently than before aiming towards better results without sacrificing computational efficiency as seen traditionally associated with such perturbations.\n\nMain Contributions:\nThis work makes several significant contributions toward improving DNN generalization capabilities via improved random weight perturbation (RWP) methods over previous implementations including:\n- Demonstrating increased effectiveness especially beneficial noticeable gains observed specifically where datasets are larger scale;\n- Achieving similar if not sometimes better outcomes relative to state-of-the-art SAM approach; \n- Providing open-source implementation details accessible publicly available at GitHub repository mentioned in abstract",
        "Topic": "Adversarial Robustness"
    },
    {
        "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
        "abstract": "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in an open-ended world that continuously explores, acquires diverse skills, and makes novel discoveries without human intervention in Minecraft. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent’s capability rapidly and alleviates catastrophic forgetting. Empirically, Voyager demonstrates strong in-context lifelong learning capabilities. It outperforms prior SOTA by obtaining 3.1x more unique items, unlocking tech tree milestones up to 15.3x faster, and traveling 2.3x longer distances. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize.",
        "authors": "G. Wang, Y. Xie, Y. Jiang, et.al",
        "keywords": [
            "Voyager",
            "LLM-powered embodied lifelong learning agent",
            "automatic curriculum"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ehfRiF0R3a",
        "pdf_src": "https://api2.openreview.net/pdf/625b7da181479e7642abce270739da66290f0fa3.pdf",
        "Code_src": "",
        "Introduction": "Background: This paper introduces Voyager, an autonomous lifelong learning agent powered by large language models (LLMs), operating within the sandbox game Minecraft.\n\nResearch Problem: The problem addressed here involves creating an artificial intelligence system capable of continuous exploration, skill acquisition through coding, and making novel discoveries autonomously over time—without any human intervention or guidance.\n\nMethods: Voyager comprises several innovative features:\n1. An automated curriculum designed to maximize exploration.\n2. A growing repository of executable code representing various skills (\"skill library\") used both as memory storage mechanisms but also retrieval systems when needed during interactions on the fly; this allows the agent to recall past experiences effectively across different contexts.\n3. A novel iterative prompting technique incorporating environmental feedback loops along with error correction processes alongside self-validation checks ensuring ongoing improvements towards better performance outcomes.\n\nMain Contributions: \n- Voyager represents one of the earliest examples demonstrating how LLMs can be integrated into embodied agents enabling them not only perform well at specific tasks upon initial deployment but continue evolving their abilities throughout interaction sessions independently.\n- The proposed approach significantly reduces catastrophic forgetting—a common issue where previously acquired knowledge deteriorates after training on newer data—and enhances temporal extensionality allowing skills gained earlier still being applicable later down the line despite changes occurring around it.\n- Voyager's demonstrated ability surpassing state-of-the-art results indicates its effectiveness compared against existing approaches including those based solely on reinforcement learning methods alone due mainly because they lack such robustness regarding long-term retention & adaptability requirements posed under real-world conditions like these simulated environments provide us insights about potential future applications beyond gaming scenarios too e.g., robotics automation etcetera).",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Optimal Inference in Contextual Stochastic Block Models",
        "abstract": "The contextual stochastic block model (CSBM) was proposed for unsupervised community detection on attributed graphs where both the graph and the high-dimensional node information correlate with node labels. In the context of machine learning on graphs, the CSBM has been widely used as a synthetic dataset for evaluating the performance of graph-neural networks (GNNs) for semi-supervised node classification. We consider a probabilistic Bayes-optimal formulation of the inference problem and we derive a belief-propagation-based algorithm for the semi-supervised CSBM; we conjecture it is optimal in the considered setting and we provide its implementation. We show that there can be a considerable gap between the accuracy reached by this algorithm and the performance of the GNN architectures proposed in the literature. This suggests that the CSBM, along with the comparison to the performance of the optimal algorithm, readily accessible via our implementation, can be instrumental in the development of more performant GNN architectures.",
        "authors": "O. Duranthon, L. Zdeborova",
        "keywords": [
            "graph neural networks",
            "semi-supervised learning",
            "belief propagation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Pe6hldOUkw",
        "pdf_src": "https://api2.openreview.net/pdf/d53d03a72d69c75c1a612591d27e629f3293c535.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper introduces the contextual stochastic block model (CSBM), which is an unsupervised community detection method designed specifically for attributed graphs—graphs whose nodes are associated with additional high-dimensional attributes—that correlates well with their label assignments.\n\nResearch Problem: Despite being commonly utilized within the field of machine learning over graphs due to its effectiveness at generating realistic datasets suitable for testing graph neural network (GNN) models under semi-supervised conditions—the study raises concerns about potential discrepancies existing when comparing the results obtained from using CSBM against those achieved through other state-of-the-art GNN architectures.\n\nMethodology: To address these issues effectively, they propose a Bayesian optimization framework applied towards solving the inference task related to CSBM's semi-supervised variant—a process encapsulated into a belief propagation algorithm. They further assert optimality regarding certain aspects pertinent to such algorithms' performances while also providing practical implementations details allowing others access to replicate or extend upon findings presented here.\n\nMain Contributions:\n1. A novel probabilistic Bayesian approach formulated explicitly considering the semi-supervised nature of CSBM.\n2. An innovative belief propagation algorithm derived based on said Bayesian framework aiming toward achieving optimal solutions given specific constraints imposed during inference tasks concerning CSBM variants.\n3. Demonstrating empirical evidence highlighting significant variations observed among different GNN architectures tested alongside traditional usage patterns involving CSBM alone—an observation suggesting room improvement opportunities available especially if utilizing CSBMs alongside comparative analyses facilitated by provided open-source codebase facilitating replication efforts amongst researchers interested in advancing current methodologies employed therein.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Grokking Beyond Neural Networks: An Empirical Exploration with Model Complexity",
        "abstract": "In some settings neural networks exhibit a phenomenon known as \\textit{grokking}, where they achieve perfect or near-perfect accuracy on the validation set long after the same performance has been achieved on the training set. In this paper, we discover that grokking is not limited to neural networks but occurs in other settings such as Gaussian process (GP) classification, GP regression, linear regression and Bayesian neural networks. We also uncover a mechanism by which to induce grokking on algorithmic datasets via the addition of dimensions containing spurious information. The presence of the phenomenon in non-neural architectures shows that grokking is not restricted to settings considered in current theoretical and empirical studies. Instead, grokking may be possible in any model where solution search is guided by complexity and error.",
        "authors": "J. W. Miller, C. O'neill, T. D. Bui",
        "keywords": [
            "grokking",
            "neural networks",
            "Gaussian processes"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ux9BrxPCl8",
        "pdf_src": "https://api2.openreview.net/pdf/6bda91a7b227498ac2ddeb0fab82649cc3a1f6cd.pdf",
        "Code_src": "",
        "Introduction": "Background: This research addresses an intriguing phenomenon observed with certain types of machine learning algorithms called \"grokking.\" Grokking refers to instances when these models reach high levels of accuracy very quickly during evaluation phase despite already having reached similar levels at the end of their training.\n\nResearch Question: The central question posed here concerns whether grokking phenomena are exclusive to neural network architectures; if it can occur within different frameworks like Gaussian Process Classification/Regression, Linear Regression, and Bayesian Neural Networks?\n\nMethodology: To investigate this issue further, researchers conducted experiments across various statistical learning paradigms including GPs for both classification and regression tasks along with simple linear regression methods using Bayesian inference techniques alongside traditional neural networks trained through backpropagation optimization.\n\nMain Contributions:\n1. **Identification of Grokking Across Multiple Frameworks**: They found evidence suggesting that grokking-like behavior does indeed extend beyond neural networks into other algorithmic domains.\n2. **Mechanism for Inducing Grokking**: By introducing additional irrelevant features into data sets specifically designed around algorithmic problems, they were able to trigger conditions conducive to experiencing grokking effects more readily than usual without those perturbations present naturally—indicating there's something about how solutions might be sought out based solely upon complexity rather than just pure randomness leading up potentially faster convergence towards optimal outcomes seen earlier mentioned under 'Background'.\n3. **Implications Beyond Current Studies**: These findings suggest broader implications regarding our understanding of computational complexity theory since grokking challenges conventional wisdom linking complexity reduction exclusively w",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "Inverse Kernel Decomposition",
        "abstract": "The state-of-the-art dimensionality reduction approaches largely rely on complicated optimization procedures. On the other hand, closed-form approaches requiring merely eigen-decomposition do not have enough sophistication and nonlinearity. In this paper, we propose a novel nonlinear dimensionality reduction method---Inverse Kernel Decomposition (IKD)---based on an eigen-decomposition of the sample covariance matrix of data. The method is inspired by Gaussian process latent variable models (GPLVMs) and has comparable performance with GPLVMs. To deal with very noisy data with weak correlations, we propose two solutions---blockwise and geodesic---to make use of locally correlated data points and provide better and numerically more stable latent estimations. We use synthetic datasets and four real-world datasets to show that IKD is a better dimensionality reduction method than other eigen-decomposition-based methods, and achieves comparable performance against optimization-based methods with faster running speeds. Open-source IKD implementation in Python can be accessed at \\url{https://github.com/JerrySoybean/ikd}.",
        "authors": "C. Li, A. Wu",
        "keywords": [
            "IKD",
            "Nonlinear Dimensionality Reduction",
            "Eigen-Deposition"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=H4OE7toXpa",
        "pdf_src": "https://api2.openreview.net/pdf/b8c59329a74118b070bf0369510b5502d5f2b5ef.pdf",
        "Code_src": "链接：[https://github.com/JerrySoybean/ikd](https://github.com/JerrySoybean/ikd)",
        "Introduction": "Background: Dimensionality reduction techniques are widely used for reducing high-dimensional data into lower dimensions while preserving important information or patterns contained within it.\n\nResearch Problem: Existing dimensionality reduction algorithms either require complex optimization processes which may lead to computational inefficiency; or they utilize simple linear transformations like eigendecomposition without sufficient expressiveness due to their linearity constraints.\n\nMethods: This study introduces a new nonlinear dimensionality reduction technique called Inverse Kernel Decomposition (IKD), based on the eigen-decomposition of the sample covariance matrix from the given dataset. Inspired by Gaussian Process Latent Variable Models (GPLVMs), IKD aims to capture both complexity through kernel functions as well as locality using blockwise and geodesic strategies when dealing with noisy and weakly-correlated data.\n\nMain Contributions:\n1. Proposes a novel nonlinear dimensionality reduction approach - IKD.\n2. Demonstrates that IKD performs comparably favorably alongside GPLVMs but requires less computation time compared to existing optimization-based methods such as t-SNE or UMAP.\n3. Introduces two enhancements – blockwise and geodesic strategies -- to handle noisy and weakly-correlated data effectively leading to improved numerical stability during estimation phase.\n4. Provides open-source code for IKD in Python allowing researchers across various fields access to its functionalities easily (\\url{https://github.com/JerrySoybean/ikd}).\n\nIn summary, this research presents a promising alternative among dimensionality reduction tools offering advantages over traditional methods particularly suited where noise levels might otherwise impede accurate feature extraction tasks typically performed via these techniques.",
        "Topic": "Neural Network Optimization"
    },
    {
        "title": "Revisiting Generalized p-Laplacian Regularized Framelet GCNs: Convergence, Energy Dynamic and as Non-Linear Diffusion",
        "abstract": "This paper presents a comprehensive theoretical analysis of the graph p-Laplacian regularized framelet network (pL-UFG) to establish a solid understanding of its properties. We conduct a convergence analysis on pL-UFG, addressing the gap in the understanding of its asymptotic behaviors. Further by investigating the generalized Dirichlet energy of pL-UFG, we demonstrate that the Dirichlet energy remains non-zero throughout convergence, ensuring the avoidance of over-smoothing issues. Additionally, we elucidate the energy dynamic perspective, highlighting the synergistic relationship between the implicit layer in pL-UFG and graph framelets. This synergy enhances the model's adaptability to both homophilic and heterophilic data. Notably, we reveal that pL-UFG can be interpreted as a generalized non-linear diffusion process, thereby bridging the gap between pL-UFG and differential equations on the graph. Importantly, these multifaceted analyses lead to unified conclusions that offer novel insights for understanding and implementing pL-UFG, as well as other graph neural network (GNN) models. Finally, based on our dynamic analysis, we propose two novel pL-UFG models with manually controlled energy dynamics. We demonstrate empirically and theoretically that our proposed models not only inherit the advantages of pL-UFG but also significantly reduce computational costs for training on large-scale graph datasets.",
        "authors": "D. Shi, Z. Shao, Y. Guo, et.al",
        "keywords": [
            "graph p-Laplacian regularization",
            "framelet networks",
            "energy dynamics"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=q4iSLPoFe7",
        "pdf_src": "https://api2.openreview.net/pdf/d67ad398382a30569272c2a1efa76c8833bd8798.pdf",
        "Code_src": "",
        "Introduction": "Background: Graph p-Laplacian regularized framelet networks (pL-UFG) are used widely due to their effectiveness at handling complex structures represented through graphs.\n\nResearch Problem: The study aims to provide an extensive theoretical foundation regarding the behavior characteristics including convergence rates under various conditions within this framework.\n \nMethodology: A thorough convergence analysis is conducted focusing on uncovering the asymptotic behavior gaps which have been previously unexplored or understood inadequately about pL-UFGs. Furthermore, they investigate the concept of generalized Dirichlet energy associated with pL-UFG showing it does not vanish during convergence thus preventing potential smoothing problems commonly encountered when dealing with such regularization techniques. \n\nMain Contributions:\n1. They contribute new understandings into how the Dirichlet energy behaves while the system converges towards stability without leading to oversmoothing artifacts common among similar methods.\n2. An energy dynamic viewpoint is introduced emphasizing the interplay existing amongst the implicit layers present in pL-UFG along with graph framelets; suggesting enhanced adaptability across different types of data relationships like homophily versus heterophily.\n3. It’s shown that pL-UFG could potentially represent itself mathematically akin to certain nonlinear diffusion processes occurring on graphs, connecting them more closely related to differential equation frameworks traditionally studied elsewhere outside GNN contexts.\n4. Based upon findings from dynamic perspectives presented earlier - authors introduce 2 innovative variants of pL-UFG capable being adjusted via manual control mechanisms allowing users fine-tune energies according to specific needs whilst still maintaining desirable features inherited originally found beneficially useful before alongside reduced computational complexity particularly relevant especially where big scale graph datasets involved requiring optimization considerations taken seriously accordingly.",
        "Topic": "Graph Neural Networks"
    },
    {
        "title": "On the Convergence of Adaptive Gradient Methods for Nonconvex Optimization",
        "abstract": "Adaptive gradient methods are workhorses in deep learning. However, the convergence guarantees of adaptive gradient methods for nonconvex optimization have not been thoroughly studied. In this paper, we provide a fine-grained convergence analysis for a general class of adaptive gradient methods including AMSGrad, RMSProp and AdaGrad. For smooth nonconvex functions, we prove that adaptive gradient methods in expectation converge to a first-order stationary point. Our convergence rate is better than existing results for adaptive gradient methods in terms of dimension. In addition, we also prove high probability bounds on the convergence rates of AMSGrad, RMSProp as well as AdaGrad, which have not been established before. Our analyses shed light on better understanding the mechanism behind adaptive gradient methods in optimizing nonconvex objectives.",
        "authors": "D. Zhou, J. Chen, Y. Cao, et.al",
        "keywords": [
            "Nonconvex Optimization",
            "Adaptive Gradient Methods",
            "Convergence Analysis"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=Gh0cxhbz3c",
        "pdf_src": "https://api2.openreview.net/pdf/b0e0d315cb97dae89b4c4aecd285cc08faf52840.pdf",
        "Code_src": "",
        "Introduction": "Background: Adaptive gradient methods play an essential role in deep learning due to their efficiency; however, there has been limited research into their convergence properties.\n\nResearch Problem: To understand how adaptive gradient methods such as AMSGrad, RMSProp, and AdaGrad perform when used with nonconvex optimization problems.\n \nMethods: The authors conduct a detailed convergence analysis focusing on these adaptive gradient algorithms under specific conditions related to the objective function's smoothness.\n\nMain Contributions:\n1. They establish expected convergence towards a first-order stationary point using adaptive gradient methods even if the underlying problem involves non-convexities within a certain range defined by the Lipschitz constant.\n2. Their convergence rate surpasses previous findings regarding adaptive gradient methods concerning computational complexity or dimensionality constraints.\n3. Additionally, they present high-probability guarantees about the convergence speeds of AMSGrad, RMSProp, and AdaGrad - previously unreported insights – providing more robust theoretical backing against potential divergence issues during training processes involving non-convex objectives through adaptive gradients.",
        "Topic": "Stochastic Optimization"
    },
    {
        "title": "Towards Understanding Dual BN In Hybrid Adversarial Training",
        "abstract": "There is a growing concern about applying batch normalization (BN) in adversarial training (AT), especially when the model is trained on both adversarial samples and clean samples (termed Hybrid-AT). With the assumption that adversarial and clean samples are from two different domains, a common practice in prior works is to adopt Dual BN, where BN$_{adv}$ and BN$_{clean}$ are used for adversarial and clean branches, respectively. A popular belief for motivating Dual BN is that estimating normalization statistics of this mixture distribution is challenging and thus disentangling it for normalization achieves stronger robustness. In contrast to this belief, we reveal that disentangling statistics plays a less role than disentangling affine parameters in model training. This finding aligns with prior work (Rebuffi\net al., 2023), and we build upon their research for further investigations. We demonstrate that the domain gap between adversarial and clean samples is not very large, which is counter-intuitive considering the significant influence of adversarial perturbation on the model accuracy. We further propose a two-task hypothesis which serves as the empirical foundation and a unified framework for Hybrid-AT improvement. We also investigate Dual BN in test-time and reveal that affine parameters characterize the robustness during inference. Overall, our work sheds new light on understanding the mechanism of Dual BN in Hybrid-AT and its underlying justification.",
        "authors": "C. Zhang, C. Zhang, K. Zhang, et.al",
        "keywords": [
            "domain adaptation",
            "adversarial training",
            "batch normalization"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=bQKHMSE4SH",
        "pdf_src": "https://api2.openreview.net/pdf/e19937523983b88c7854eb9c7ba6823bc1bc22a0.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses concerns regarding the use of Batch Normalization (BN) techniques within Adversarial Training (AT), particularly under Hybrid-AT scenarios involving both adversarial examples and normal data.\n\nResearch Question: Investigating whether the commonly adopted approach using separate BN layers for adversarial and non-adversarial examples (\"Dual BN\") actually enhances robustness by effectively handling the mixed nature of these datasets or if there's another factor at play influencing performance.\n\nMethodology: The authors challenge conventional wisdom suggesting that separating BN statistics improves robustness due to difficulties in jointly estimating them across two disparate distributions—adversarial and benign. Instead, they focus more on the potential importance of disentangling the affine parameters learned through BN rather than solely on the normalization statistics themselves.\n\nMain Contributions:\n1. Contrary to prevalent beliefs emphasizing the necessity of statistical disentanglement via Dual BN, findings suggest that such separation may be less critical compared to the disentanglement of affine parameters.\n2. Building off previous studies like Rebuffi et al.'s insights into AT mechanisms without Dual BN, researchers delve deeper here specifically focusing on Hybrid-AT setups showing how the domain discrepancy might affect robustness expectations differently based on intuition versus evidence.\n3. They introduce an empirical basis supported by a novel two-task hypothesis aimed towards improving Hybrid-AT methods while providing a unifying theoretical framework around their observations concerning BN efficacy throughout various stages including testing phase where affine parameter characteristics become crucial indicators predicting robustness levels observed post-inference time.",
        "Topic": "Adversarial Robustness"
    }
]