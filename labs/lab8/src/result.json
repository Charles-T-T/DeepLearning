[
    {
        "title": "Proving Test Set Contamination in Black-Box Language Models",
        "abstract": "Large language models are trained on vast amounts of internet data, prompting concerns that they have memorized public benchmarks. Detecting this type of contamination is challenging because the pretraining data used by proprietary models are often not publicly accessible.\n\nWe propose a procedure for detecting test set contamination of language models with exact false positive guarantees and without access to pretraining data or model weights. Our approach leverages the fact that when there is no data contamination, all orderings of an exchangeable benchmark should be equally likely. In contrast, the tendency for language models to memorize example order means that a contaminated language model will find certain canonical orderings to be much more likely than others. Our test flags potential contamination whenever the likelihood of a canonically ordered benchmark dataset is significantly higher than the likelihood after shuffling the examples.\n\nWe demonstrate that our procedure is sensitive enough to reliably detect contamination in challenging situations, including models as small as 1.4 billion parameters, on small test sets only 1000 examples, and datasets that appear only a few times in the pretraining corpus. Finally, we evaluate LLaMA-2 to apply our test in a realistic setting and find our results to be consistent with existing contamination evaluations.",
        "authors": "Y. Oren, N. Meister, N. S. Chatterji, et.al",
        "keywords": [
            "language modeling",
            "memorization",
            "dataset contamination"
        ],
        "venue": "ICLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=KS8mIvetg2",
        "pdf_src": "https://api2.openreview.net/pdf/cfd79aaab7bdcd4f7c032c57fe7e607058042c80.pdf",
        "Code_src": "",
        "Introduction": "Background: Large language models like GPT3 are trained using massive amounts of text scraped from the web which may lead them to \"memorize\" these benchmarks rather than learn generalizable patterns.\nResearch Problem: How can one detect if a large language model has been contaminated during training?\nMethod: The authors develop a method called Test Set Contamination Detection (TSD), based on the observation that uncontaminated models would produce equivalent probabilities across different permutations of a benchmark dataset while contaminated ones might favor specific orders due to memorization bias towards exemplars seen earlier within the dataset.\nMain Contributions:\n1. Exact False Positive Guarantees - TSD provides certainty about whether contamination exists; it does so even though neither pretraining nor model weights need to be accessed directly,\n2. No Need for Pretraining Data Access - Unlike other methods requiring access to pretraining corpora such as those proposed previously at NeurIPS '21, their technique operates independently once the model's predictions become available post-training,\n\nThe paper also shows empirical evidence demonstrating its effectiveness against various types of contamination scenarios where traditional approaches fail – including smaller-scale models down to just over 1.4 billion parameters size range commonly found nowadays along with very limited exposure"
    },
    {
        "title": "Growing Tiny Networks: Spotting Expressivity Bottlenecks and Fixing Them Optimally",
        "abstract": "Machine learning tasks are generally formulated as optimization problems, where one searches for an optimal function within a certain functional space. In practice, parameterized functional spaces are considered, in order to be able to perform gradient descent. Typically, a neural network architecture is chosen and fixed, and its parameters (connection weights) are optimized, yielding an architecture-dependent result. This way of proceeding however forces the evolution of the function during training to lie within the realm of what is expressible with the chosen architecture, and prevents any optimization across architectures. Costly architectural hyper-parameter optimization is often performed to compensate for this. Instead, we propose to adapt the architecture on the fly during training. We show that the information about desirable architectural changes, due to expressivity bottlenecks when attempting to follow the functional gradient, can be extracted from backpropagation. To do this, we propose a mathematical definition of expressivity bottlenecks, which enables us to detect, quantify and solve them while training, by adding suitable neurons. Thus, while the standard approach requires large networks, in terms of number of neurons per layer, for expressivity and optimization reasons, we provid tools and properties to develop an architecture starting with a very small number of neurons. As a proof of concept, we show results~on the CIFAR dataset, matching large neural network accuracy, with competitive training time, while removing the need for standard architectural hyper-parameter search.",
        "authors": "M. Verbockhaven, T. Rudkiewicz, G. Charpiat, et.al",
        "keywords": [
            "architecture adaptation",
            "expressivity bottleneck",
            "neural network"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=hbtG6s6e7r",
        "pdf_src": "https://api2.openreview.net/pdf/afffc3036e8cf229dab86eb2bfbe45d3e1ee3862.pdf",
        "Code_src": "",
        "Introduction": "Background: Machine learning tasks traditionally involve optimizing functions defined over parameterized functional spaces using gradient descent methods.\n\nResearch Problem: The problem addressed here concerns how to optimize machine learning models without being constrained solely by the capabilities of their chosen neural network architectures.\n \nMethod: The authors suggest adapting the architecture dynamically throughout the training process based on insights gained into expressivity bottlenecks revealed through backpropagation. They introduce a novel mathematical framework defining expressivity bottlenecks so they may identify these issues effectively; then add new neurons selectively at critical points along the gradients to resolve such bottlenecks.\n\nMain Contributions:\n1. A theoretical understanding underpinning dynamic architecture adaptation driven by expressivity bottlenecks detection via backpropagation.\n2. An innovative method for detecting quantifying expressivity bottlenecks allowing for their resolution mid-training phase rather than requiring costly pre-defined hyperparameter tuning or larger initial model sizes just to ensure expressiveness.\n3. Demonstrated empirical evidence showing that smaller neural networks could achieve similar performance metrics typically associated with much more complex architectures – specifically referencing CIFAR datasets - suggesting significant potential benefits regarding computational efficiency alongside comparable predictive power after addressing identified bottlenecks proactively instead of passively accommodating them upfront design choices."
    },
    {
        "title": "Data-Centric Defense: Shaping Loss Landscape with Augmentations to Counter Model Inversion",
        "abstract": "Machine Learning models have shown susceptibility to various privacy attacks, with model inversion (MI) attacks posing a significant threat. Current defense techniques are mostly \\emph{model-centric}, involving modifying model training or inference. However, these approaches require model trainers' cooperation, are computationally expensive, and often result in a significant privacy-utility tradeoff. To address these limitations, we propose a novel \\emph{data-centric} approach to mitigate MI attacks. Compared to traditional model-centric techniques, our approach offers the unique advantage of enabling each individual user to control their data's privacy risk, aligning with findings from a Cisco survey that only a minority actively seek privacy protection. Specifically, we introduce several privacy-focused data augmentations that modify the private data uploaded to the model trainer. These augmentations shape the resulting model's loss landscape, making it challenging for attackers to generate private target samples. Additionally, we provide theoretical analysis to explain why such augmentations can reduce the risk of model inversion. We evaluate our approach against state-of-the-art MI attacks and demonstrate its effectiveness and robustness across various model architectures and datasets. Specifically, in standard face recognition benchmarks, we reduce face reconstruction success rates to $\\leq5\\%$, while maintaining high utility with only a 2\\% classification accuracy drop, significantly surpassing state-of-the-art model-centric defenses. This is the first study to propose a data-centric approach for mitigating model inversion attacks, showing promising potential for decentralized privacy protection.",
        "authors": "S. Chen, F. Kang, N. Abhyankar, et.al",
        "keywords": [
            "data-centric",
            "model inversion attack",
            "privacy preservation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=r8wXaLJBIS",
        "pdf_src": "https://api2.openreview.net/pdf/052dbf3d080eb89c54ca8ca8edee3d9c0800f506.pdf",
        "Code_src": "",
        "Introduction": "Background: Machine learning models may be vulnerable to privacy threats like model inversion (MI), where an attacker reconstructs sensitive information about individuals based on their interactions with the model.\n\nResearch Problem: The current defense strategies mainly focus on altering the ML model itself during training or inference phase, which requires collaboration between model developers but also comes at the cost of computational resources as well as potentially compromising performance due to increased privacy settings.\n\nMethodology: In contrast, this paper introduces a new \"data-centric\" strategy aimed at defending against MI by focusing directly on the input data rather than changing the model architecture themselves – thus allowing users greater autonomy over how much they compromise when sharing personal data without sacrificing too much functionality.\n\nMain Contributions:\n1. Propose innovative methods called 'privacy-focused data augmentations' designed specifically around protecting privacy through modifications made before any machine learning process begins.\n2. Analyze theoretically what makes certain types of transformations applied within these augmentations effective in reducing risks associated with model inversion attacks; \n3. Conduct experiments comparing different kinds of MI attack scenarios using both real-world datasets along with synthetic ones demonstrating efficacy under varying conditions;\n4. Achieve substantial improvements compared existing model-centric solutions particularly noticeable reductions in facial feature reconstruction successes down below five percent whilst still retaining nearly full classification capability despite minor losses in terms of accuracy levels (only two per cent)."
    },
    {
        "title": "Pretraining a Neural Operator in Lower Dimensions",
        "abstract": "There has recently been increasing attention towards developing foundational neural Partial Differential Equation (PDE) solvers and neural operators through large-scale pertaining. However, unlike vision and language models that make use of abundant and inexpensive (unlabeled) data for pretraining, these neural solvers usually rely on simulated PDE data, which can be costly to obtain, especially for high dimensional PDEs. In this work, we aim to Pretrain neural PDE solvers on Lower Dimensional PDEs (PreLowD) where data collection is the least expensive. We evaluated the effectiveness of this pretraining strategy in similar PDEs in higher dimensions. We use the Factorized Fourier Neural Operator (FFNO) due to having the necessary flexibility to be applied to PDE data of arbitrary spatial dimensions and reuse trained parameters in lower dimensions. In addition, our work sheds light on the effect of the fine-tuning configuration to make the most of this pretraining strategy. Code is available at https://github.com/BaratiLab/PreLowD.",
        "authors": "A. Hemmasian, A. B. Farimani",
        "keywords": [
            "PreLowD",
            "Factorized Fourier Neural Operator",
            "Fine-tuning"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=ZewaRoZehI",
        "pdf_src": "https://api2.openreview.net/pdf/048de99f0e38401009f7327ccc26dd21739ce86a.pdf",
        "Code_src": "https://github.com/BaratiLab/PreLowD",
        "Introduction": "Background: There have been recent advancements in using deep learning techniques like neural networks as partial differential equation (PDE) solvers or neural operators by leveraging massive datasets without requiring manual labeling.\n\nResearch Problem: The existing neural PDE solvers primarily depend on computationally expensive simulations rather than readily available unlabeled data because simulating complex high-dimensional PDEs requires significant computational resources.\n \nMethod: This paper introduces a novel approach called \"Pretrain neural PDE solvers on Lower Dimensional PDEs\" (PreLowD). Instead of training directly with high-dimensional PDEs whose simulation costs are prohibitive, they train their model first only within low-dimensional spaces before transferring those learned features into solving problems across various dimensions including high ones. They utilize the Factorized Fourier Neural Operator (FFNO), an architecture designed flexibly enough to adapt to different spatial dimensions while reusing trainable weights from previous layers when dealing with smaller dimensions. Furthermore, it explores how adjusting hyperparameters during fine-tuning phase impacts performance after initial pretraining step.\n\nMain Contributions:\n1. Demonstrates efficacy & efficiency gains via transfer learning between low-dimensional & high-dimensional PDEs;\n2. Proposes FFNO as suitable candidate for such cross-dimensional adaptation owing to its inherent properties allowing parameter sharing across scales;\n3. Provides insights about optimal configurations needed throughout entire pipeline"
    },
    {
        "title": "Confidence Intervals and Simultaneous Confidence Bands Based on Deep Learning",
        "abstract": "Deep learning models have significantly improved prediction accuracy in various fields, gaining recognition across numerous disciplines. Yet, an aspect of deep learning that remains insufficiently addressed is the assessment of prediction uncertainty. Producing reliable uncertainty estimators could be crucial in practical terms. For instance, predictions associated with a high degree of uncertainty could be sent for further evaluation. Recent works in uncertainty quantification of deep learning predictions, including Bayesian posterior credible intervals and a frequentist confidence-interval estimation,  have proven to yield either invalid  or overly conservative intervals. Furthermore, there is currently no method for quantifying uncertainty that can accommodate deep neural networks for survival (time-to-event) data that involves right-censored outcomes.  In this work, we provide a non-parametric bootstrap method that disentangles data uncertainty from the noise inherent in the adopted optimization algorithm. %, ensuring that based on deep learning estimators with small bias, the resulting point-wise confidence intervals or the  simultaneous confidence bands are accurate (i.e., valid and not overly conservative).  The validity of the proposed approach is demonstrated through an extensive simulation study, which shows that the method is accurate (i.e., valid and not overly conservative) as long as the network is sufficiently deep to ensure that the estimators provided by the deep neural network exhibit minimal bias. Otherwise, undercoverage of up to 8\\% is observed.  The proposed ad-hoc method can be easily integrated into any deep neural network without interfering with the training process. The utility of the proposed approach is demonstrated through two applications: constructing simultaneous confidence bands for survival curves generated by deep neural networks dealing with right-censored survival data, and constructing a confidence interval for classification probabilities in the context of binary classification regression. Code for the data analysis and reported simulation is available at Githubsite: \\url{https://github.com/Asafba123/Survival_bootstrap}.",
        "authors": "A. B. Arie, M. Gorfine",
        "keywords": [
            "bootstrap",
            "uncertainty quantification",
            "survival analysis"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=PdbaruPVUY",
        "pdf_src": "https://api2.openreview.net/pdf/b2c017ea60a9a27f2b6038484a56b693d589d206.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper discusses how despite significant advancements made by deep learning models in predicting accurately within different domains, their ability to estimate prediction uncertainty has been inadequately explored.\n\nResearch Problem:\nThe central issue tackled here concerns developing methods capable of producing trustworthy uncertainty estimations using deep learning models—specifically those addressing over-estimation issues commonly found in existing approaches like Bayesian credible intervals and frequentist confidence intervals when applied to such models.\n \nMethodology:\nTo address these challenges posed above, authors propose a novel non-parametric bootstrap technique designed specifically tailored towards accommodating deep neural networks processing survival (time-to-event) data where outcomes may involve censoring. This method aims to separate genuine data variability from the noise introduced during optimization algorithms' iterative processes while utilizing biased estimators derived from deep learning frameworks.\n \nMain Contributions:\nThis research introduces an innovative solution incorporating a non-parametric bootstrap strategy that effectively distinguishes between true uncertainties present in datasets versus random fluctuations due to optimization procedures used inside machine learning architectures particularly beneficial if they're complex enough so biases remain negligible; otherwise potential coverage deficiencies reach approximately 8%. Moreover, it's worth noting its seamless integration capability allowing incorporation directly alongside current workflows without hindering ongoing training activities making implementation straightforward even after initial setup phase completion. Finally, empirical validation via simulations confirms reliability along with real-world application examples demonstrating efficacy both calculating simultaneous confidence bands around predicted survival distributions handling censored cases appropriately plus estimating classification probability bounds within binary classification tasks respectively showcasing versatility usefulness beyond mere theoretical interest alone."
    },
    {
        "title": "Modeling Causal Mechanisms with Diffusion Models for Interventional and Counterfactual Queries",
        "abstract": "We consider the problem of answering observational, interventional, and counterfactual queries in a causally sufficient setting where only observational data and the causal graph are available. Utilizing the recent developments in diffusion models, we introduce diffusion-based causal models (DCM) to learn causal mechanisms, that generate unique latent encodings.  These encodings enable us to directly sample under interventions and perform abduction for counterfactuals. Diffusion models are a natural fit here, since they can encode each node to a latent representation that acts as a proxy for exogenous noise. Our empirical evaluations demonstrate significant improvements over existing state-of-the-art methods for answering causal queries. Furthermore, we provide theoretical results that offer a methodology for analyzing counterfactual estimation in general encoder-decoder models, which could be useful in settings beyond our proposed approach.",
        "authors": "P. Chao, P. Blöbaum, S. K. Patel, et.al",
        "keywords": [
            "causal modeling",
            "diffusion models",
            "counterfactual inference"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=EDHQDsqiSe",
        "pdf_src": "https://api2.openreview.net/pdf/1d225039f2f9222c305252d97f030bfa008920c3.pdf",
        "Code_src": "",
        "Introduction": "Background: The paper addresses the challenge of addressing observational, interventional, and counterfactual queries within a causally adequate scenario when just observational information along with the causal diagram is accessible.\n\nResearch Question: How would you design an algorithm capable of learning causal relationships from observational data alone?\n\nMethodology: To tackle this issue, researchers propose diffusion-based causal models (DCMs). DCMs utilize diffusion processes to create novel latent representations by encoding every variable into these latent spaces representing external disturbances or \"noise.\" This enables direct sampling during interventions without needing additional intervention-specific training datasets while also facilitating counterfactual reasoning through abductive inference techniques based on learned encodings derived via diffusion models' properties.\n\nMain Contributions:\n1. They've developed diffusion-based causal models tailored specifically towards inferring causal relations solely using observational evidence.\n2. Their model significantly outperforms current top-performing approaches at answering various types of causal questions across different domains tested empirically—indicating improved accuracy & robustness compared traditional methods like structural equation modeling etcetera).\n3. Additionally, theoretically grounded findings have been presented regarding how one might analyze counterfactual estimations generally applicable even outside their specific framework potentially aiding future research endeavors related similar problems involving complex systems governed by underlying causes rather than observed correlations alone."
    },
    {
        "title": "DTRNet: Precisely Correcting Selection Bias in Individual-Level Continuous Treatment Effect Estimation by Reweighted Disentangled Representation",
        "abstract": "Estimating the individual-level continuous treatment effect holds significant practical importance in various decision-making domains, such as personalized healthcare and customized marketing. However, most current methods for individual treatment effect estimation are limited to discrete treatments and struggle to precisely adjust for selection bias under continuous settings, leading to inaccurate estimation. To address these challenges, we propose a novel Disentangled Representation Network (DTRNet) to estimate the individualized dose-response function (IDRF), which learns disentangled representations and precisely adjusts for selection bias. To the best of our knowledge, our work is the first attempt to precisely adjust for selection bias in continuous settings. Extensive results on synthetic and semi-synthetic datasets demonstrate that our DTRNet outperforms most state-of-the-art methods. Our code is available at \\href{https://github.com/xuanxuan03021/DTRNet_final_2}{DTRNet}.",
        "authors": "M. Hu, Z. Chu, S. Li",
        "keywords": [
            "Disentangled Representation Network",
            "Individualized Dose-Response Function",
            "Selection Bias Adjustment"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=1ZTfzA9bXw",
        "pdf_src": "https://api2.openreview.net/pdf/3e01b4024e13cdd02b99c4ea06252cbedbbdc1e8.pdf",
        "Code_src": "",
        "Introduction": "Background: Estimating the individual-level continuous treatment effect plays crucial roles in many real-world applications like personalized healthcare and customized marketing.\n\nResearch Problem: Most existing approaches focus on estimating the effects of discrete treatments; however, they fail to accurately account for confounding factors when dealing with continuous treatments due to their inability to effectively correct for selection bias.\n\nMethodology: We introduce an innovative approach called Disentangled Representation Network (DTRNet). This network aims not only to learn disentangled representations but also to make precise adjustments towards mitigating the impact of selection bias during the estimation process by leveraging continuous treatment data.\n\nMain Contributions: \n1. Our study represents one of the pioneering efforts addressing the issue of correcting for selection bias within the context of continuous treatment estimations.\n2. The proposed DTRNet significantly improves upon previous methodologies through its ability to handle complex relationships between different variables involved while accounting for potential biases present throughout the dataset being analyzed.\n\n\nThe paper's findings have been validated using both synthetic and semi-synthetic datasets where it has demonstrated superior performance compared against other advanced techniques currently employed across this field - further details can be found along with open-source implementation via GitHub repository provided in the abstract."
    },
    {
        "title": "Feature Distillation Improves Zero-Shot Transfer from Synthetic Images",
        "abstract": "Vision-language foundation models such as CLIP have showcased impressive zero-shot capabilities. However, their applicability in resource-constrained environments is limited due to their size and the resulting latency. Knowledge distillation allows to mitigate these challenges by distilling small image encoders that can replace the large CLIP image encoder. In a zero-shot setting, where only the class names are known, no real domain images can be used for this process. Instead, we investigate the use of synthetic images for this purpose. Unlike existing works that focus on improving the quality of synthetic images to bridge the performance gap compared to training on natural images, we find the choice of loss to be a crucial factor. Specifically, minimizing only the distance between the student and teacher image features, without incorporating image captions in the loss function, increases the robustness to spurious features and data corruptions. As a result, this feature distillation approach greatly improves the transfer performance from synthetic to real images. Leveraging these insights, we are able to train domain-specific students that achieve zero-shot performance comparable to a ViT-B/32 teacher on six fine-grained classification datasets while using up to 92% fewer parameters.",
        "authors": "N. Popp, J. H. Metzen, M. Hein",
        "keywords": [
            "synthetic images",
            "knowledge distillation",
            "feature distillation"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=SP8DLl6jgb",
        "pdf_src": "https://api2.openreview.net/pdf/eb6c4c883a7dc8f4616fe39a994103f7b2069fde.pdf",
        "Code_src": "",
        "Introduction": "Background: Vision-language foundation models like CLIP demonstrate strong abilities with little or no prior knowledge about tasks (\"zero-shot\"). However, they suffer limitations when deployed under resource constraints because of high computational costs.\n\nResearch Question: How do we effectively apply vision-language models trained extensively but computationally expensive within constrained settings?\n\nMethod: We explore knowledge distillation techniques which involve transferring knowledge learned during extensive training into smaller networks called \"students.\" This helps reduce model size leading to less computation time at the cost of some accuracy trade-off typically mitigated through careful selection of the distillation loss functions.\nSpecifically:\n- Synthetic Images: Since actual domain images cannot be utilized directly here since just class labels exist instead,\nwe consider employing synthetic ones - an alternative not commonly explored before our work despite its potential benefits over purely focusing on enhancing synthetic image quality relative to those obtained naturally.\n- Loss Function Choice: Our key insight lies in choosing appropriate losses; unlike others who optimize solely based on similarity metrics among encoded representations across both teachers and students, \nour study incorporates caption information alongside encoding distances thus enabling more resilient learning against noisy inputs and corruption issues common especially amongst synthetic datasets.\n\nMain Contributions: The paper introduces novel approaches towards optimizing feature distillation processes specifically tailored toward leveraging synthetic imagery sources efficiently whilst maintaining competitive zero-shot generalization performances akin to larger pre-trained counterparts even though significantly reduced in terms of parameter count by utilizing around 92%."
    },
    {
        "title": "Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity",
        "abstract": "On tabular data, a significant body of literature has shown that current deep learning (DL) models perform at best similarly to Gradient Boosted Decision Trees (GBDTs), while significantly underperforming them on outlier data. However, these works often study problem settings which may not fully capture the complexities of real-world scenarios. We identify a natural tabular data setting where DL models can outperform GBDTs: tabular Learning-to-Rank (LTR) under label scarcity. Tabular LTR applications, including search and recommendation, often have an abundance of unlabeled data, and scarce labeled data. We show that DL rankers can utilize unsupervised pretraining to exploit this unlabeled data. In extensive experiments over both public and proprietary datasets, we show that pretrained DL rankers consistently outperform GBDT rankers on ranking metrics, sometimes by as much as 38%, both overall and on outliers.",
        "authors": "C. Hou, K. K. Thekumparampil, M. Shavlovsky, et.al",
        "keywords": [
            "DL rankers",
            "Learning-to-Rank",
            "tabular data"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=093Q9VxaWt",
        "pdf_src": "https://api2.openreview.net/pdf/65ea072ec4157a2664c254d20d86ba59708ac806.pdf",
        "Code_src": "",
        "Introduction": "Background:\nThe paper addresses performance disparities between Deep Learning (DL) models and Gradient Boosted Decision Trees (GBDTs) in handling tabular data with outliers.\n\nResearch Problem:\nDespite showing similar accuracy levels compared to GBDTs for clean tabular data, existing DL models are found to be less effective when dealing with outlier-rich environments typical in many practical use cases such as search engines or recommender systems due to their limited ability to handle noisy labels effectively from large amounts of unlabeled data.\n\nMethodology:\nTo tackle this issue specifically within the context of tabular Learning-to-Rank (LTR) tasks characterized by abundant unlabeled but sparse labeled examples, they propose using unsupervised pretraining techniques before training the DL ranker model directly on the task-specific dataset without requiring any additional annotations beyond what is already present—labels only available during testing time rather than throughout training phase.\n\nMain Contributions:\nExtensive empirical evidence across various publicly available benchmarks along with private datasets demonstrates that pretrained DL rankers achieve superior results against traditional GBDT rankers even more so among outliers; improvements reaching up to 38%. This validates their approach's effectiveness toward leveraging vast quantities of unlabeled information efficiently alongside small sets manually curated ones thus closing gaps observed previously regarding robustness towards noisy inputs prevalent amongst real-world datasets encountered frequently nowadays"
    },
    {
        "title": "Calibration Attacks: A Comprehensive Study of Adversarial Attacks on Model Confidence",
        "abstract": "In this work, we highlight and perform a comprehensive study on calibration attacks, a form of adversarial attacks that aim to trap victim models to be heavily miscalibrated without altering their predicted labels, hence endangering the trustworthiness of the models and follow-up decision making based on their confidence. We propose four typical forms of calibration attacks: underconfidence, overconfidence, maximum miscalibration, and random confidence attacks, conducted in both the black-box and white-box setups. We demonstrate that the attacks are highly effective on both convolutional and attention-based models: with a small number of queries, they seriously skew confidence without changing the predictive performance. Given the potential danger, we further investigate the effectiveness of a wide range of adversarial defence and recalibration methods, including our proposed defences specifically designed for calibration attacks to mitigate the harm. From the ECE and KS scores, we observe that there are still significant limitations in handling calibration attacks. To the best of our knowledge, this is the first dedicated study that provides a comprehensive investigation on calibration-focused attacks. We hope this study helps attract more attention to these types of attacks and hence hamper their potential serious damages. To this end, this work also provides detailed analyses to understand the characteristics of the attacks.",
        "authors": "S. Obadinma, X. Zhu, H. Guo",
        "keywords": [
            "calibration attack",
            "adversarial defense",
            "model uncertainty"
        ],
        "venue": "TMLR 2024",
        "year": 2024,
        "url": "https://openreview.net/forum?id=TXzz9xwdpv",
        "pdf_src": "https://api2.openreview.net/pdf/0c7af0e8b8d3985245dc708e913e478047e3bab6.pdf",
        "Code_src": "",
        "Introduction": "Background:\nCalibration attacks pose threats by manipulating model predictions while maintaining label accuracy; however, existing studies have not focused solely on such attacks.\n\nResearch Problem:\nTo address the lack of research into calibration attacks against machine learning models' prediction uncertainty estimation capabilities.\n \nMethods:\nDeveloped new attack strategies categorized as \"underconfidence,\" \"overconfidence,\" \"maximum miscalibration,\" or \"random confidence\" within both black-box and white-box settings. Demonstrated efficacy across convolutional neural networks and transformer architectures using minimal query inputs before quantifying defense mechanisms like adversarial training techniques through empirical comparison metrics EC error rate (ECE) and Kullback-Leibler divergence (KS).\n\nMain Contributions:\n- Introduced novel calibration attack categories applicable broadly;\n- Provided evidence showing how adversaries can manipulate uncertainty estimates effectively despite preserving classification correctness;\n- Conducted an extensive comparative analysis evaluating defenses pertinent only toward mitigating calibration issues;\n- Offered insights regarding adversary tactics via thorough examination methodologies contributing towards future advancements safeguarding trustworthy ML systems from similar vulnerabilities."
    }
]